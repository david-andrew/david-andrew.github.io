<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="font" href="/_next/static/media/a34f9d1faa5f3315-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/91c4e337ea558be1.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/85fa6dafca566008.css" data-precedence="next"/><link rel="preload" href="/_next/static/chunks/webpack-04b31a640b9eef88.js" as="script" fetchPriority="low"/><script src="/_next/static/chunks/fd9d1056-bc91c3b3fa0b78fd.js" async=""></script><script src="/_next/static/chunks/596-fc24fa4abd14b1c2.js" async=""></script><script src="/_next/static/chunks/main-app-de654e813a62aea4.js" async=""></script><script src="/pyodideCommsService.js" async=""></script><title>David Samson</title><meta name="description" content="Generated by create next app"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_36bd41 overflow-hidden bg-slate-900"><div class="w-screen h-screen bg-black overflow-hidden"><div class="fixed w-screen bg-black z-50"><div><div class="flex flex-row md:justify-center justify-left px-4 py-2"><div class="md:hidden"><button><span class="text-white text-4xl align-middle">☰</span></button></div><div class="hidden md:flex"><a draggable="false" class="select-none" href="/"><div class="relative cursor-pointer text-center font-quadon
                m-2 text-white border-solid sm:border-white md:border-transparent hover:border-white 
                text-xl border-2
                md:text-2xl md:border-3
                lg:text-4xl lg:border-4
                "><div class="
                    py-3 px-4 
                    md:py-3 md:px-4 
                    lg:py-6 lg:px-9">Home</div></div></a><a draggable="false" class="select-none" href="/projects"><div class="relative cursor-pointer text-center font-quadon
                m-2 text-white border-solid sm:border-white md:border-transparent hover:border-white 
                text-xl border-2
                md:text-2xl md:border-3
                lg:text-4xl lg:border-4
                bg-accent"><div class="
                    py-3 px-4 
                    md:py-3 md:px-4 
                    lg:py-6 lg:px-9">Projects</div></div></a><a draggable="false" class="select-none" href="/about"><div class="relative cursor-pointer text-center font-quadon
                m-2 text-white border-solid sm:border-white md:border-transparent hover:border-white 
                text-xl border-2
                md:text-2xl md:border-3
                lg:text-4xl lg:border-4
                "><div class="
                    py-3 px-4 
                    md:py-3 md:px-4 
                    lg:py-6 lg:px-9">About</div></div></a><a draggable="false" class="select-none" href="/clovers"><div class="relative cursor-pointer text-center font-quadon
                m-2 text-white border-solid sm:border-white md:border-transparent hover:border-white 
                text-xl border-2
                md:text-2xl md:border-3
                lg:text-4xl lg:border-4
                "><div class="
                    py-3 px-4 
                    md:py-3 md:px-4 
                    lg:py-6 lg:px-9">Clovers</div></div></a><a draggable="false" class="select-none" href="/contact"><div class="relative cursor-pointer text-center font-quadon
                m-2 text-white border-solid sm:border-white md:border-transparent hover:border-white 
                text-xl border-2
                md:text-2xl md:border-3
                lg:text-4xl lg:border-4
                "><div class="
                    py-3 px-4 
                    md:py-3 md:px-4 
                    lg:py-6 lg:px-9">Contact</div></div></a></div></div></div></div><div class="pointer-events-none select-none" style="height:var(--navbar-height)"> </div><div style="height:calc(100vh - var(--navbar-height))" class="overflow-x-hidden"><!--$--><div class="mx-auto px-4 sm:px-6 lg:px-8 max-w-[1190px]"><h1 class="text-4xl my-6 font-quadon mb-0">Dewy Programming Language</h1><p class="mb-6 text-xl font-gentona text-justify">Unknown</p><!--$!--><template data-dgst="NEXT_DYNAMIC_NO_SSR_CODE"></template><!--/$--><h3 class="text-2xl mt-6 mb-2 font-quadon">About</h3><p class="mb-6 text-xl font-gentona text-justify">Dewy is a programming language I have been developing off and on since 2016. It is a general purpose language, designed with engineering applications in mind. Think the functionality and ease of use of matlab or python combined with the speed of a compiled language like C or Rust, but with its own unique flare.</p><p class="text-xl font-gentona text-justify mb-2">Some key planned features include:</p><ul class="list-disc mb-6 pl-10 text-xl font-gentona"><li><strong>Functional and Imperative</strong> - Dewy is an imperative language with strong support for functional programming. This allows for a very flexible programming style, where you can use the best tool for the job.</li><li><strong>Expression based syntax</strong> - Dewy uses an expression based syntax, meaning that everything is an expression. This allows for a very simple yet powerful syntax, where common language features often are just a free consequence of the syntax</li><li><strong>Garbage-collector-free memory management</strong> - Dewy uses a unique memory management system, allowing for fast and efficient memory management without the need for a garbage collector.</li><li><strong>Strong type system</strong> - Dewy has a powerful static type system with inference, reminiscent of those in Typescript and Julia.</li><li><strong>Built in unit system</strong> - Dewy has a built in unit system, allowing you to easily work with units and convert between them. This is especially useful for engineering applications.</li><li><strong>Strong math support</strong> - Dewy has strong support many math features, including complex numbers, quaternions, vectors, matrices, and more. This is especially useful for engineering applications.</li></ul><p class="mb-6 text-xl font-gentona text-justify">An example of the common FizzBuzz program implemented in Dewy might look like this:</p><div class="rounded-md overflow-hidden w-full bg-[#232323] text-xl md:text-lg my-6"><div class="flex flex-row overflow-y-hidden overflow-x-auto w-full"> <div><div><div></div></div></div></div></div><p class="mb-6 text-xl font-gentona text-justify">Or a more functional style implementation might look like this:</p><div class="rounded-md overflow-hidden w-full bg-[#232323] text-xl md:text-lg my-6"><div class="flex flex-row overflow-y-hidden overflow-x-auto w-full"> <div><div><div></div></div></div></div></div><p class="mb-6 text-xl font-gentona text-justify">For clarity, the variables at each step look like so:</p><div class="rounded-md overflow-hidden w-full bg-[#232323] text-xl md:text-lg my-6"><div class="flex flex-row overflow-y-hidden overflow-x-auto w-full"> <div><div><div></div></div></div></div></div><h3 class="text-2xl mt-6 mb-2 font-quadon">Current Status</h3><p class="mb-6 text-xl font-gentona text-justify">Currently I&#x27;m working through a simple interpreter for the language (powering the demo above). I&#x27;ve got a tokenizer, a basic interpreter backend, and handling of a few basic types of syntaxes via the parser. But much of the syntax is still unimplemented, hence the long list of &quot;Broken Examples&quot;. Thus the current focus is finishing parser support for the rest of the syntax features.</p><p class="mb-6 text-xl font-gentona text-justify">Previously I had been doing a lot of development on bleeding edge<!-- --> <a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="/projects/dewy_old">parser generators</a>, but that ended up being too big of a time sink for not much visible progress. Instead, for the time being, I ended up just hand rolling a parser in python, which has led to actually runnable code! I&#x27;ll definitely revisit parser generators in the future when the language is further along.</p><p class="mb-6 text-xl font-gentona text-justify">After the parser is complete, the next steps will be working on compiling to different backends. Initially I was planning to target<!-- --> <a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="https://en.wikipedia.org/wiki/LLVM#Intermediate_representation">LLVM</a> as the primary backend, however I recently discovered <a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="https://c9x.me/compile/">QBE</a>, which is a much lighter alternative that supposedly gets 70% of the performance of LLVM for only 10% of the code. Longer term I&#x27;m interested in supporting a wider range of backend targets, like C, a universal<!-- --> <a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="https://en.wikipedia.org/wiki/Polyglot_(computing)">polyglot</a> targeting many scripting languages simultaneously (sh, bash, windows cmd, powershell, javascript, python, etc.), and eventually LLVM too. At some point I&#x27;ll start building out the standard library, and bootstrapping the compiler to be able to compile itself—at which point we might be ready for a version 0 release!</p><h3 class="text-2xl mt-6 mb-2 font-quadon">About the Demo</h3><p class="mb-6 text-xl font-gentona text-justify">The demo above was actually pretty complex to put together. The current interpreter is written in python, and this website is statically hosted, which meant the demo required some way to statically run python code without a server. For this, I used <a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="https://pyodide.org/">Pyodide</a>, which is basically <a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="https://github.com/python/cpython">CPython</a> compiled to<!-- --> <a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="https://webassembly.org/">WebAssembly</a> via<!-- --> <a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="https://emscripten.org/">Emscripten.</a></p><p class="mb-6 text-xl font-gentona text-justify">Pyodide itself isn&#x27;t too difficult to use, except for the fact that it doesn&#x27;t have good support for asynchronous standard input—it really wants to halt the entire UI while you type input into a stock browser popup prompt. To get around this, I found a<!-- --> <a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="https://www.npmjs.com/package/sync-message">handy library</a> where you run pyodide in a web worker, and then any time it wants to read input, the worker makes a synchronous XHR request to a service worker, blocking the pyodide web worker until the service worker receives a response from the main thread with the input, which the service worker can then pass back to the pyodide worker. Suffice it to say, I don&#x27;t think I ever want to deal with service workers again.</p><p class="mb-6 text-xl font-gentona text-justify">Now that python is handled, the next aspect is getting the Dewy interpreter itself to run. For this, I fetch (at website build time) the source code directly from<!-- --> <a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="https://github.com/david-andrew/dewy-lang/tree/master/src/compiler">github</a>. I then abuse the python import lib to allow loading &quot;modules&quot; directly from strings, and then pass all of the dewy source in as string modules. Then I have a little wrapper function for the entry point which receives a dewy source code string, and runs the program. The entry point can then be called from the browser via a javascript wrapper function.</p><p class="mb-6 text-xl font-gentona text-justify">The final piece of the puzzle is the text entry, and terminal emulator. For text input, I&#x27;m using the <a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="https://codemirror.net/">Code Mirror Library</a> with a custom syntax highlighter. For the terminal, I use the <a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="https://xtermjs.org/">xterm.js</a> library. I then hooked up stdin and stdout from pyodide to interact with the terminal, and voila! A Dewy interpreter running in the browser.</p><p class="mb-6 text-xl font-gentona text-justify">There are definitely some rough edges, and the parser only supports a small handful of features, but it runs! It&#x27;s probably the easiest way to try out the language, and I&#x27;m looking forward to getting all of the broken example programs working!</p><h3 class="text-2xl mt-6 mb-2 font-quadon">Links</h3><div class="flex flex-col space-y-3"><div class="flex flex-row text-sm items-center"><img alt="github icon" draggable="false" loading="lazy" width="496" height="484" decoding="async" data-nimg="1" class="inline-block w-8 h-8 mr-2  pointer-events-none select-none" style="color:transparent" src="/_next/static/media/github.8d24eda3.svg"/><span class="align-middle"><a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="https://github.com/david-andrew/dewy-lang">Github Repo</a></span></div><div class="flex flex-row text-sm items-center"><img alt="docs icon" draggable="false" loading="lazy" width="384" height="512" decoding="async" data-nimg="1" class="inline-block w-8 h-8 mr-2  pointer-events-none select-none" style="color:transparent" src="/_next/static/media/docs.71257e0f.svg"/><span class="align-middle"><a class="text-blue-400 hover:text-blue-500 font-gentona text-xl" href="https://david-andrew.github.io/dewy-lang/">Language Docs</a></span></div></div><div class="pointer-events-none select-none" style="height:var(--navbar-height)"> </div></div><!--/$--></div><div class="fixed bottom-0 right-0 w-full flex flex-row-reverse pointer-events-none" style="height:var(--navbar-height)"><div class="flex flex-row justify-center" style="width:var(--navbar-height);height:var(--navbar-height)"><div class="flex flex-col justify-center"><div class="hidden md:block hover:cursor-pointer pointer-events-auto"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" class=" lg:h-16 lg:w-16 md:h-12 md:w-12 sm:h-8 sm:w-8 h-8 w-8 "><title>Select Accent Color. Color is saved in a cookie.</title><path stroke-linecap="round" stroke-linejoin="round" d="M4.098 19.902a3.75 3.75 0 005.304 0l6.401-6.402M6.75 21A3.75 3.75 0 013 17.25V4.125C3 3.504 3.504 3 4.125 3h5.25c.621 0 1.125.504 1.125 1.125v4.072M6.75 21a3.75 3.75 0 003.75-3.75V8.197M6.75 21h13.125c.621 0 1.125-.504 1.125-1.125v-5.25c0-.621-.504-1.125-1.125-1.125h-4.072M10.5 8.197l2.88-2.88c.438-.439 1.15-.439 1.59 0l3.712 3.713c.44.44.44 1.152 0 1.59l-2.879 2.88M6.75 17.25h.008v.008H6.75v-.008z"></path></svg></div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" class=" pointer-events-auto lg:h-16 lg:w-16 md:h-12 md:w-12 sm:h-8 sm:w-8 h-8 w-8 md:hidden "><title>Select Accent Color. Color is saved in a cookie.</title><path stroke-linecap="round" stroke-linejoin="round" d="M4.098 19.902a3.75 3.75 0 005.304 0l6.401-6.402M6.75 21A3.75 3.75 0 013 17.25V4.125C3 3.504 3.504 3 4.125 3h5.25c.621 0 1.125.504 1.125 1.125v4.072M6.75 21a3.75 3.75 0 003.75-3.75V8.197M6.75 21h13.125c.621 0 1.125-.504 1.125-1.125v-5.25c0-.621-.504-1.125-1.125-1.125h-4.072M10.5 8.197l2.88-2.88c.438-.439 1.15-.439 1.59 0l3.712 3.713c.44.44.44 1.152 0 1.59l-2.879 2.88M6.75 17.25h.008v.008H6.75v-.008z"></path></svg></div></div></div></div><script src="/_next/static/chunks/webpack-04b31a640b9eef88.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/91c4e337ea558be1.css\",{\"as\":\"style\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:HL[\"/_next/static/css/85fa6dafca566008.css\",{\"as\":\"style\"}]\n"])</script><script>self.__next_f.push([1,"5:I{\"id\":57948,\"chunks\":[\"2272:static/chunks/webpack-04b31a640b9eef88.js\",\"2971:static/chunks/fd9d1056-bc91c3b3fa0b78fd.js\",\"596:static/chunks/596-fc24fa4abd14b1c2.js\"],\"name\":\"default\",\"async\":false}\n7:I{\"id\":56628,\"chunks\":[\"2272:static/chunks/webpack-04b31a640b9eef88.js\",\"2971:static/chunks/fd9d1056-bc91c3b3fa0b78fd.js\",\"596:static/chunks/596-fc24fa4abd14b1c2.js\"],\"name\":\"\",\"async\":false}\n3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/91c4e337ea558be1.css\",\"precedence\":\"next\"}]],[\"$\",\""])</script><script>self.__next_f.push([1,"$L5\",null,{\"buildId\":\"YSUigxtM53n6I1f6L9Nnp\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/projects/dewy\",\"initialTree\":[\"\",{\"children\":[\"projects\",{\"children\":[\"dewy\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L6\"],\"globalErrorComponent\":\"$7\",\"children\":[null,\"$L8\",null]}]]\n"])</script><script>self.__next_f.push([1,"9:I{\"id\":27250,\"chunks\":[\"6685:static/chunks/6685-d55f1e0fd750c950.js\",\"4769:static/chunks/4769-afbb25c509513e18.js\",\"4809:static/chunks/4809-4864fcdd4c57b3e2.js\",\"5570:static/chunks/app/projects/layout-2fc06375f6014da3.js\"],\"name\":\"Navbar\",\"async\":false}\na:I{\"id\":19885,\"chunks\":[\"6685:static/chunks/6685-d55f1e0fd750c950.js\",\"4769:static/chunks/4769-afbb25c509513e18.js\",\"4809:static/chunks/4809-4864fcdd4c57b3e2.js\",\"5570:static/chunks/app/projects/layout-2fc06375f6014da3.js\"],\"name\":\"GithubTimestampsProvide"])</script><script>self.__next_f.push([1,"r\",\"async\":false}\nb:I{\"id\":19885,\"chunks\":[\"6685:static/chunks/6685-d55f1e0fd750c950.js\",\"4769:static/chunks/4769-afbb25c509513e18.js\",\"4809:static/chunks/4809-4864fcdd4c57b3e2.js\",\"5570:static/chunks/app/projects/layout-2fc06375f6014da3.js\"],\"name\":\"ProjectsContextProvider\",\"async\":false}\nc:I{\"id\":47767,\"chunks\":[\"2272:static/chunks/webpack-04b31a640b9eef88.js\",\"2971:static/chunks/fd9d1056-bc91c3b3fa0b78fd.js\",\"596:static/chunks/596-fc24fa4abd14b1c2.js\"],\"name\":\"default\",\"async\":false}\nd:I{\"id\":57920,\"chun"])</script><script>self.__next_f.push([1,"ks\":[\"2272:static/chunks/webpack-04b31a640b9eef88.js\",\"2971:static/chunks/fd9d1056-bc91c3b3fa0b78fd.js\",\"596:static/chunks/596-fc24fa4abd14b1c2.js\"],\"name\":\"default\",\"async\":false}\nf:I{\"id\":49488,\"chunks\":[\"6685:static/chunks/6685-d55f1e0fd750c950.js\",\"4769:static/chunks/4769-afbb25c509513e18.js\",\"8861:static/chunks/8861-ed8850a5f8092c40.js\",\"4809:static/chunks/4809-4864fcdd4c57b3e2.js\",\"3185:static/chunks/app/layout-98d8732c49a2dc77.js\"],\"name\":\"ColorPicker\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"script\",null,{\"src\":\"/pyodideCommsService.js\",\"async\":true}]}],[\"$\",\"body\",null,{\"className\":\"__className_36bd41 overflow-hidden bg-slate-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"w-screen h-screen bg-black overflow-hidden\",\"children\":[[\"$\",\"$L9\",null,{}],[\"$\",\"div\",null,{\"style\":{\"height\":\"calc(100vh - var(--navbar-height))\"},\"className\":\"overflow-x-hidden\",\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"$Lc\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":[\"$\",\"div\",null,{\"className\":\"fixed w-screen h-screen bg-black flex justify-center items-center\",\"children\":[\"$\",\"div\",null,{\"role\":\"status\",\"children\":[[\"$\",\"svg\",null,{\"aria-hidden\":\"true\",\"className\":\"w-32 h-32 mr-2 text-gray-200 animate-spin dark:text-gray-600 fill-accent\",\"viewBox\":\"0 0 100 101\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M100 50.5908C100 78.2051 77.6142 100.591 50 100.591C22.3858 100.591 0 78.2051 0 50.5908C0 22.9766 22.3858 0.59082 50 0.59082C77.6142 0.59082 100 22.9766 100 50.5908ZM9.08144 50.5908C9.08144 73.1895 27.4013 91.5094 50 91.5094C72.5987 91.5094 90.9186 73.1895 90.9186 50.5908C90.9186 27.9921 72.5987 9.67226 50 9.67226C27.4013 9.67226 9.08144 27.9921 9.08144 50.5908Z\",\"fill\":\"currentColor\"}],[\"$\",\"path\",null,{\"d\":\"M93.9676 39.0409C96.393 38.4038 97.8624 35.9116 97.0079 33.5539C95.2932 28.8227 92.871 24.3692 89.8167 20.348C85.8452 15.1192 80.8826 10.7238 75.2124 7.41289C69.5422 4.10194 63.2754 1.94025 56.7698 1.05124C51.7666 0.367541 46.6976 0.446843 41.7345 1.27873C39.2613 1.69328 37.813 4.19778 38.4501 6.62326C39.0873 9.04874 41.5694 10.4717 44.0505 10.1071C47.8511 9.54855 51.7191 9.52689 55.5402 10.0491C60.8642 10.7766 65.9928 12.5457 70.6331 15.2552C75.2735 17.9648 79.3347 21.5619 82.5849 25.841C84.9175 28.9121 86.7997 32.2913 88.1811 35.8758C89.083 38.2158 91.5421 39.6781 93.9676 39.0409Z\",\"fill\":\"currentFill\"}]]}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"Loading...\"}]]}]}],\"loadingStyles\":[],\"hasLoading\":true,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Ld\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[null,\"$Le\",null],\"segment\":\"projects\"},\"styles\":[]}]}]}]}],[\"$\",\"$Lf\",null,{}]]}]}]]}]\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"David Samson\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Generated by create next app\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"5\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"10:I{\"id\":19885,\"chunks\":[\"6685:static/chunks/6685-d55f1e0fd750c950.js\",\"4769:static/chunks/4769-afbb25c509513e18.js\",\"4809:static/chunks/4809-4864fcdd4c57b3e2.js\",\"5570:static/chunks/app/projects/layout-2fc06375f6014da3.js\"],\"name\":\"GithubTimestampsFetcher\",\"async\":false}\n11:I{\"id\":52160,\"chunks\":[\"6685:static/chunks/6685-d55f1e0fd750c950.js\",\"4769:static/chunks/4769-afbb25c509513e18.js\",\"4809:static/chunks/4809-4864fcdd4c57b3e2.js\",\"5570:static/chunks/app/projects/layout-2fc06375f6014da3.js\"],\"name\":\"Head"])</script><script>self.__next_f.push([1,"ing\",\"async\":false}\n14:I{\"id\":27250,\"chunks\":[\"6685:static/chunks/6685-d55f1e0fd750c950.js\",\"4769:static/chunks/4769-afbb25c509513e18.js\",\"4809:static/chunks/4809-4864fcdd4c57b3e2.js\",\"5570:static/chunks/app/projects/layout-2fc06375f6014da3.js\"],\"name\":\"NavbarDummy\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"$L10\",null,{\"projects\":[{\"title\":\"Blob Opera Performances\",\"imgSrc\":{\"src\":\"/_next/static/media/blob_opera_nox.e6f3aa2a.png\",\"height\":414,\"width\":512,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAJ1BMVEU4EgowEQlEIRV0Vzx9ZEo+JhlXOyBCT1RTMx0+LyVEUlxuTS5AS1HBZ0aTAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALklEQVR4nC3IuREAIAwDMMcOeYD95+XgKNQIhpahHYwUawuGoQuM6az0OwvPnwMUSwDO9qMceAAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":6},\"summary\":\"Virtual choir performances leveraging the blob opera as a front end for voice synthesis\",\"lastUpdated\":\"February 2021\",\"tags\":[\"Python\",\"Blob Opera\",\"choir\",\"music\",\"synthesis\"],\"route\":\"blob_opera\"},{\"title\":\"Boat Simulator\",\"imgSrc\":{\"src\":\"/_next/static/media/boat_simulator.7cc391fd.jpg\",\"height\":1280,\"width\":2048,\"blurDataURL\":\"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAb/xAAdEAABAgcAAAAAAAAAAAAAAAAAAQQCAxESFSFT/8QAFAEBAAAAAAAAAAAAAAAAAAAABf/EABYRAAMAAAAAAAAAAAAAAAAAAAABEf/aAAwDAQACEQMRAD8AkFks7K49tuLmABWIOrP/2Q==\",\"blurWidth\":8,\"blurHeight\":5},\"summary\":\"Spring 2017 HopHacks submission\",\"lastUpdated\":\"March 2017\",\"tags\":[\"Unity\",\"C#\",\"3D game\"],\"route\":\"boat_simulator\"},{\"title\":\"Bueller Board\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"Fall 2015 HopHacks submission: Midi keyboard that used user provided audio samples, a.k.a. the 'goat keyboard'\",\"lastUpdated\":\"September 2015\",\"tags\":[\"midi\",\"music\"],\"route\":\"bueller_board\"},{\"title\":\"Composer\",\"imgSrc\":{\"src\":\"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"React based composing software that acts as a front-end for LilyPond\",\"lastUpdated\":\"January 2021\",\"tags\":[\"React\",\"TypeScript\",\"SMuFL\",\"LilyPond\",\"music\",\"composition\"],\"route\":\"composer\"},{\"title\":\"Choir Compositions\",\"imgSrc\":{\"src\":\"/_next/static/media/music_staff.3145785a.png\",\"height\":1616,\"width\":2745,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAHlBMVEUGBgZISEhSUlIeHh5paWk+Pj4vLy9cXFx7e3ufn5/bdycMAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAJ0lEQVR4nCXBtwEAIAwDMLe0/x9mQIKFjwN3jZBwxGowpFZGm7cpPAkxAH3f0FE4AAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":5},\"summary\":\"\",\"lastUpdated\":\"May 2015\",\"tags\":[\"music\",\"choral\",\"composition\"],\"route\":\"compositions\"},{\"title\":\"Dewy Programming Language\",\"github\":\"dewy\",\"imgSrc\":{\"src\":\"/_next/static/media/dewy_dandelion.e2efa7ee.jpg\",\"height\":1600,\"width\":1600,\"blurDataURL\":\"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"An engineering focused programming language I am developing\",\"tags\":[\"Python\",\"compilers\",\"parsers\",\"LLVM\",\"Programming Languages\"],\"route\":\"dewy\"},{\"title\":\"Generalized Parsing\",\"lastUpdated\":\"2022-02-06\",\"imgSrc\":{\"src\":\"/_next/static/media/dewy_dandelion.e2efa7ee.jpg\",\"height\":1600,\"width\":1600,\"blurDataURL\":\"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"Previous work on the Dewy Programming Language, namely a custom SRNGLR parser written entirely in C\",\"tags\":[\"C\",\"compilers\",\"parsers\",\"SRNGLR\",\"LLVM\"],\"route\":\"dewy_old\"},{\"title\":\"UR5 Draw Robot\",\"imgSrc\":{\"src\":\"/_next/static/media/mona_lisa_contour.9bc5cbc9.jpg\",\"height\":1016,\"width\":1600,\"blurDataURL\":\"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAeEAACAgEFAQAAAAAAAAAAAAABAgAEAwUHEiFRsf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAExAv/aAAwDAQACEQMRAD8AtbT3HvV9WGZVPB8RHXqsT8iIgoVqn//Z\",\"blurWidth\":8,\"blurHeight\":5},\"summary\":\"UR5 robot arm project\",\"lastUpdated\":\"December 2017\",\"tags\":[\"Matlab\",\"UR5 robot\",\"ROS\"],\"route\":\"drawbot\"},{\"title\":\"EasyREPL\",\"imgSrc\":{\"src\":\"/_next/static/media/pypi_logo.f6536a80.svg\",\"height\":58,\"width\":66,\"blurWidth\":0,\"blurHeight\":0},\"summary\":\"Python package for easily creating Read-Eval-Print Loops (REPLs)\",\"github\":\"easyrepl\",\"tags\":[\"Python\",\"PyPI\",\"REPL\"],\"route\":\"easyrepl\"},{\"title\":\"Hacking Harmony or The Demon Chipmunk Choir\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"github\":\"Ensemble\",\"summary\":\"2019 Peabody Hackathon Submission. Choral music synthesis via autotuned google text-to-speech, AKA the demon chipmunk choir\",\"tags\":[\"Google text-to-speech API\",\"matlab\",\"python\"],\"route\":\"ensemble_peabody\"},{\"title\":\"Escort Mission 2020\",\"imgSrc\":{\"src\":\"/_next/static/media/escort_mission_lamb.4c525bc4.png\",\"height\":128,\"width\":128,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAHlBMVEUA/gAAugD/+//h9+EAdAAirCKG/YZf2l9ri2uA/4BNpwkIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAK0lEQVR4nE2KuREAMAyDbPnff+HcSU1ooMDsI1P2XWdURDEOOMYAo6dbj3gNJgBhtLya0gAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":8},\"github\":\"escort_mission_2020\",\"summary\":\"Submission for the 2020 GMTK Game Jam\",\"tags\":[\"Godot\",\"GDScript\",\"2D game\"],\"route\":\"escort_mission\"},{\"title\":\"Foxing Animatronic\",\"imgSrc\":{\"src\":\"/_next/static/media/foxing_animatronic.91d20002.png\",\"height\":1252,\"width\":1540,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAJ1BMVEUGFwsJIxEILhUiJQkdSRwWIQoUORYULA8RRSArMAYZEwYyVzM6OglI+AgfAAAACXBIWXMAAAsTAAALEwEAmpwYAAAANElEQVR4nBXKuREAIAwEsT2/GOi/XgbFoqs9I6DvdY8CW7O9DBSzvQ3OKFOCE5Vp+mdJBg8iegD5GB/ChgAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":7},\"summary\":\"Manually actuated animatronic robot featured in the Foxing music video 'Slapstick'\",\"lastUpdated\":\"June 2018\",\"tags\":[\"Solidworks\",\"mechanical design\",\"animatronic\",\"Foxing\",\"music\"],\"route\":\"foxing_animatronic\"},{\"title\":\"Mechatronics Robots\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"Robots from mechatronics\",\"lastUpdated\":\"May 2019\",\"tags\":[\"Arduino\",\"C++\",\"SolidWorks\",\"mechanical design\"],\"route\":\"mechatronics\"},{\"title\":\"Mehve (Working Title)\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"github\":\"mehve\",\"summary\":\"3D adventure game inspire by \\\"Nausicaa of the Valley of the Wind\\\"\",\"tags\":[\"Godot\",\"GDScript\",\"3D game\"],\"route\":\"mehve\"},{\"title\":\"Musical DL\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"github\":\"MusicalDL\",\"summary\":\"Using deep learning to generate choral music in the style of JS Bach\",\"tags\":[\"Python\",\"Pytorch\",\"AI/ML\",\"choral\",\"music\",\"generation\"],\"route\":\"musical_dl\"},{\"title\":\"NAND 2 Tetris\",\"imgSrc\":{\"src\":\"/_next/static/media/nand2tetris.660feb3d.png\",\"height\":346,\"width\":396,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAPFBMVEUAAABvKgIhHhw+KR0uJAFlRBU2GAYJBAENDw/TVQF/ZRBuWwrEoxytlRBiVBJJHQC7SwG5YCWSOwDqziPUksrRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAN0lEQVR4nCWKWw7AIAyA0Fbbqnvo7n/XZZMfEgKAscllu870J7tj2Tk6NSK151BcLilNwd3z974dVQEdrZJh0wAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":7},\"github\":\"nand2tetris\",\"summary\":\"A 16-bit computer built from scratch, starting with NAND gates\",\"tags\":[\"Computer Architecture\",\"HDL\",\"Assembly\",\"Hack\",\"Jack\",\"Compilers\",\"Operating Systems\",\"Virtual Machines\"],\"route\":\"nand2tetris\"},{\"title\":\"PDF Chatter\",\"imgSrc\":{\"src\":\"/_next/static/media/pypi_logo.f6536a80.svg\",\"height\":58,\"width\":66,\"blurWidth\":0,\"blurHeight\":0},\"summary\":\"LLM powered Q\u0026A over extracted PDF text\",\"github\":\"pdf-chatter\",\"tags\":[\"Python\",\"Optical Character Recognition (OCR)\",\"Large Language Models (LLMs)\",\"Nougat-OCR\",\"GPT-4\"],\"route\":\"pdf_chatter\"},{\"title\":\"pOngBot\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"Autonomous beer pong playing robot\",\"lastUpdated\":\"June 2020\",\"tags\":[\"Arduino\",\"C++\",\"computer vision\",\"Viola-Jones\",\"mechanical design\"],\"route\":\"pongbot\"},{\"title\":\"PRS19: Fret Press Robot\",\"imgSrc\":{\"src\":\"/_next/static/media/prs2019_preview.aed05a2a.png\",\"height\":2093,\"width\":3061,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAPFBMVEVJPTmhmIiCa1SBfXdBQD1IOzB3bGA6My5dUkyXlpNPRTyio6ChpadSTUm0q5e+taCun2xwYl2qrq5mXU/I9hK/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAL0lEQVR4nAXBhwEAIAjAsKoo4B7//2rCXFZLjoGwh4poIj5TJRfOsAok3G/v3toHGpkBOY6KaOIAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":5},\"github\":\"PRS_robot\",\"summary\":\"Automatic guitar fret press robot. Mechanical Engineering Master's Design captsone project\",\"tags\":[\"C++\",\"Arduino\",\"mechanical design\"],\"route\":\"prs19\"},{\"title\":\"Rewind\",\"imgSrc\":{\"src\":\"/_next/static/media/rewind_title.a1e43a09.png\",\"height\":540,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAElBMVEUqHjguITs4J0JmWW1PRFo/MUqO6aYhAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAH0lEQVR4nGNggAMmJkYmRhCDhZmFmZUBxGRkgIhAAQADKgAhbD4/MgAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":5},\"summary\":\"2018 Video Game Desgn (EN.601.355) capstone project\",\"lastUpdated\":\"May 2018\",\"tags\":[\"Unity\",\"C#\",\"2D game\"],\"route\":\"rewind\"},{\"title\":\"RoboJay\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"lastUpdated\":\"May 2018\",\"summary\":\"A balancing robot designed to give campus tours to incoming JHU freshmen\",\"tags\":[\"robotics\",\"feedback control\",\"navigation\",\"BeagleBone\",\"ROS\"],\"route\":\"robojay\"},{\"title\":\"High Power Rocketry\",\"imgSrc\":{\"src\":\"/_next/static/media/rebel_scum.34e7823c.jpg\",\"height\":1365,\"width\":2048,\"blurDataURL\":\"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAaEAACAgMAAAAAAAAAAAAAAAAAARESIjFB/8QAFQEBAQAAAAAAAAAAAAAAAAAAAgP/xAAXEQADAQAAAAAAAAAAAAAAAAAAAhNR/9oADAMBAAIRAxEAPwC6o4w5OwATo+immH//2Q==\",\"blurWidth\":8,\"blurHeight\":5},\"summary\":\"Level 1 \u0026 2 High Powered Rocket built with the Johns Hopkins Rocketry Club, and Spaceport America Cup 2018\",\"lastUpdated\":\"January 2018\",\"tags\":[\"High Power Rocketry\",\"Arduino\",\"C++\",\"mechanical design\",\"Tripoli\"],\"route\":\"rocketry\"},{\"title\":\"Silver Void\",\"imgSrc\":{\"src\":\"/_next/static/media/silver_void_cover_art.cc703812.png\",\"height\":1439,\"width\":1823,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAIVBMVEUCAgEWGhAMDQgiFiEnJyYwMC4aCRZHJygZDgQ9EDxLU0cSomUyAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALUlEQVR4nC3KyQkAMBDDQHntzdV/wSEQ/QaEYkeAutPhJ0NBFdv0kWDM57euCwrsAGo10HJLAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":6},\"github\":\"SilverVoid\",\"summary\":\"Submission for Acerola Jam 0\",\"tags\":[\"Godot\",\"GDScript\",\"3D Game\",\"Space Simulator\",\"Bullet Hell\"],\"route\":\"silver_void\"},{\"title\":\"so voice!\",\"imgSrc\":{\"src\":\"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":8},\"lastUpdated\":\"December 2022\",\"summary\":\"Choral music synthesis with deep learning (Continuation of Musical DL)\",\"tags\":[\"Python\",\"Pytorch\",\"AI/ML\",\"choral\",\"music\",\"synthesis\"],\"route\":\"so_voice\"},{\"title\":\"Terminal Ray Tracer\",\"imgSrc\":{\"src\":\"/_next/static/media/terminal_ray_tracer.00db7e4a.png\",\"height\":2043,\"width\":2881,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAbFBMVEWSamvNrLWWP0yUKVeYinmmnKTsEheTcnnmh4+Xfo6pV2GMMDasvsa5aXiPRVFVs2uOWF9WnGxXtVObh4ypREzbYmnsPkXIba/y/P10bHmfbcouPtDAMDdJbZOtmERQbkXfwcqTb3g+yNdOcdnEQLLCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAOklEQVR4nAXBBQKAIAAAsSMl7MZG//9HN/Igu6YtYDT+7qnhSK++tI+IcH5BViXZqn0Tk8M8Ti3rbH9KhwLIHy1s0QAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":6},\"github\":\"TerminalRayTracer\",\"summary\":\"A dependency-free ray tracer written in C that runs directly in a linux terminal\",\"tags\":[\"C\",\"ray tracing\",\"CLI\",\"linux\"],\"route\":\"terminal_ray_tracer\"},{\"title\":\"Cloud Timelapse\",\"imgSrc\":{\"src\":\"/_next/static/media/timelapse.b57dd258.jpg\",\"height\":3468,\"width\":4624,\"blurDataURL\":\"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAGAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAcEAACAgIDAAAAAAAAAAAAAAAAAQIFAxETYdH/xAAVAQEBAAAAAAAAAAAAAAAAAAADBP/EABYRAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAwDAQACEQMRAD8AlG4r56fBlT6ivQAUqwYX/9k=\",\"blurWidth\":8,\"blurHeight\":6},\"github\":\"timelapse\",\"summary\":\"A simple python project for taking timelapses of clouds from a webcam\",\"tags\":[\"Python\",\"OpenCV\",\"Raspberry Pi\",\"timelapse\",\"clouds\"],\"route\":\"timelapse\"},{\"title\":\"uSkipSpoilers\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"github\":\"uSkipSpoilers\",\"summary\":\"A small chrome extension for blocking spoilers in YouTube videos\",\"tags\":[\"React\",\"TypeScript\",\"Chrome\",\"Extension\"],\"route\":\"uskipspoilers\"},{\"title\":\"This Website\",\"github\":\"david-andrew.github.io\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"externalLink\":\"https://github.com/david-andrew/david-andrew.github.io\",\"summary\":\"This website, written in react/typescript\",\"tags\":[\"Next.js\",\"React\",\"TypeScript\",\"Tailwind CSS\",\"WebAssembly\"],\"route\":\"website\"},{\"title\":\"WSE18: Machine Shop Biometric Interlock\",\"imgSrc\":{\"src\":\"/_next/static/media/wse18.7405315e.png\",\"height\":2054,\"width\":2456,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAUVBMVEU8OTSVjYNlX1nIwbpLQjaqnpBOSUOJgHU8MCsyKSXo4dUfHh1iU0R8aWMUDAdoYlull4ialI2BZkzBuKyNdleyrKaQosO9iGCNY0PTj19hbYD9ZLzaAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAO0lEQVR4nAXBhQHAMAwDMBdCZRj/f+gkiLgdPDJcpJS6MbxKOJEBa1pH5whYpXl9DfEoc2A90MJC671/OekCOef5C1cAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":7},\"summary\":\"Biometric security interlock system. Mechanical Engineering Senior Design capstone project\",\"lastUpdated\":\"May 2018\",\"tags\":[\"Raspberry Pi\",\"Python\",\"C++\",\"Qt\",\"interlock\",\"fingerprint\",\"biometric\"],\"route\":\"wse18\"},{\"title\":\"Ziggy V (Working Title)\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"Concept for a Real-Time-Strategy crossed with First-Person-Shooter\",\"lastUpdated\":\"January 2021\",\"tags\":[\"Godot\",\"GDScript\",\"FPS x RTS\",\"3D game\"],\"route\":\"ziggy_v\"}]}],[\"$\",\"div\",null,{\"className\":\"mx-auto px-4 sm:px-6 lg:px-8 max-w-[1190px]\",\"children\":[[\"$\",\"$L11\",null,{\"projects\":[{\"title\":\"Blob Opera Performances\",\"imgSrc\":{\"src\":\"/_next/static/media/blob_opera_nox.e6f3aa2a.png\",\"height\":414,\"width\":512,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAJ1BMVEU4EgowEQlEIRV0Vzx9ZEo+JhlXOyBCT1RTMx0+LyVEUlxuTS5AS1HBZ0aTAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALklEQVR4nC3IuREAIAwDMMcOeYD95+XgKNQIhpahHYwUawuGoQuM6az0OwvPnwMUSwDO9qMceAAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":6},\"summary\":\"Virtual choir performances leveraging the blob opera as a front end for voice synthesis\",\"lastUpdated\":\"February 2021\",\"tags\":[\"Python\",\"Blob Opera\",\"choir\",\"music\",\"synthesis\"],\"route\":\"blob_opera\"},{\"title\":\"Boat Simulator\",\"imgSrc\":{\"src\":\"/_next/static/media/boat_simulator.7cc391fd.jpg\",\"height\":1280,\"width\":2048,\"blurDataURL\":\"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAb/xAAdEAABAgcAAAAAAAAAAAAAAAAAAQQCAxESFSFT/8QAFAEBAAAAAAAAAAAAAAAAAAAABf/EABYRAAMAAAAAAAAAAAAAAAAAAAABEf/aAAwDAQACEQMRAD8AkFks7K49tuLmABWIOrP/2Q==\",\"blurWidth\":8,\"blurHeight\":5},\"summary\":\"Spring 2017 HopHacks submission\",\"lastUpdated\":\"March 2017\",\"tags\":[\"Unity\",\"C#\",\"3D game\"],\"route\":\"boat_simulator\"},{\"title\":\"Bueller Board\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"Fall 2015 HopHacks submission: Midi keyboard that used user provided audio samples, a.k.a. the 'goat keyboard'\",\"lastUpdated\":\"September 2015\",\"tags\":[\"midi\",\"music\"],\"route\":\"bueller_board\"},{\"title\":\"Composer\",\"imgSrc\":{\"src\":\"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"React based composing software that acts as a front-end for LilyPond\",\"lastUpdated\":\"January 2021\",\"tags\":[\"React\",\"TypeScript\",\"SMuFL\",\"LilyPond\",\"music\",\"composition\"],\"route\":\"composer\"},{\"title\":\"Choir Compositions\",\"imgSrc\":{\"src\":\"/_next/static/media/music_staff.3145785a.png\",\"height\":1616,\"width\":2745,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAHlBMVEUGBgZISEhSUlIeHh5paWk+Pj4vLy9cXFx7e3ufn5/bdycMAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAJ0lEQVR4nCXBtwEAIAwDMLe0/x9mQIKFjwN3jZBwxGowpFZGm7cpPAkxAH3f0FE4AAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":5},\"summary\":\"\",\"lastUpdated\":\"May 2015\",\"tags\":[\"music\",\"choral\",\"composition\"],\"route\":\"compositions\"},{\"title\":\"Dewy Programming Language\",\"github\":\"dewy\",\"imgSrc\":{\"src\":\"/_next/static/media/dewy_dandelion.e2efa7ee.jpg\",\"height\":1600,\"width\":1600,\"blurDataURL\":\"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"An engineering focused programming language I am developing\",\"tags\":[\"Python\",\"compilers\",\"parsers\",\"LLVM\",\"Programming Languages\"],\"route\":\"dewy\"},{\"title\":\"Generalized Parsing\",\"lastUpdated\":\"2022-02-06\",\"imgSrc\":{\"src\":\"/_next/static/media/dewy_dandelion.e2efa7ee.jpg\",\"height\":1600,\"width\":1600,\"blurDataURL\":\"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"Previous work on the Dewy Programming Language, namely a custom SRNGLR parser written entirely in C\",\"tags\":[\"C\",\"compilers\",\"parsers\",\"SRNGLR\",\"LLVM\"],\"route\":\"dewy_old\"},{\"title\":\"UR5 Draw Robot\",\"imgSrc\":{\"src\":\"/_next/static/media/mona_lisa_contour.9bc5cbc9.jpg\",\"height\":1016,\"width\":1600,\"blurDataURL\":\"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAeEAACAgEFAQAAAAAAAAAAAAABAgAEAwUHEiFRsf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAExAv/aAAwDAQACEQMRAD8AtbT3HvV9WGZVPB8RHXqsT8iIgoVqn//Z\",\"blurWidth\":8,\"blurHeight\":5},\"summary\":\"UR5 robot arm project\",\"lastUpdated\":\"December 2017\",\"tags\":[\"Matlab\",\"UR5 robot\",\"ROS\"],\"route\":\"drawbot\"},{\"title\":\"EasyREPL\",\"imgSrc\":{\"src\":\"/_next/static/media/pypi_logo.f6536a80.svg\",\"height\":58,\"width\":66,\"blurWidth\":0,\"blurHeight\":0},\"summary\":\"Python package for easily creating Read-Eval-Print Loops (REPLs)\",\"github\":\"easyrepl\",\"tags\":[\"Python\",\"PyPI\",\"REPL\"],\"route\":\"easyrepl\"},{\"title\":\"Hacking Harmony or The Demon Chipmunk Choir\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"github\":\"Ensemble\",\"summary\":\"2019 Peabody Hackathon Submission. Choral music synthesis via autotuned google text-to-speech, AKA the demon chipmunk choir\",\"tags\":[\"Google text-to-speech API\",\"matlab\",\"python\"],\"route\":\"ensemble_peabody\"},{\"title\":\"Escort Mission 2020\",\"imgSrc\":{\"src\":\"/_next/static/media/escort_mission_lamb.4c525bc4.png\",\"height\":128,\"width\":128,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAHlBMVEUA/gAAugD/+//h9+EAdAAirCKG/YZf2l9ri2uA/4BNpwkIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAK0lEQVR4nE2KuREAMAyDbPnff+HcSU1ooMDsI1P2XWdURDEOOMYAo6dbj3gNJgBhtLya0gAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":8},\"github\":\"escort_mission_2020\",\"summary\":\"Submission for the 2020 GMTK Game Jam\",\"tags\":[\"Godot\",\"GDScript\",\"2D game\"],\"route\":\"escort_mission\"},{\"title\":\"Foxing Animatronic\",\"imgSrc\":{\"src\":\"/_next/static/media/foxing_animatronic.91d20002.png\",\"height\":1252,\"width\":1540,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAJ1BMVEUGFwsJIxEILhUiJQkdSRwWIQoUORYULA8RRSArMAYZEwYyVzM6OglI+AgfAAAACXBIWXMAAAsTAAALEwEAmpwYAAAANElEQVR4nBXKuREAIAwEsT2/GOi/XgbFoqs9I6DvdY8CW7O9DBSzvQ3OKFOCE5Vp+mdJBg8iegD5GB/ChgAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":7},\"summary\":\"Manually actuated animatronic robot featured in the Foxing music video 'Slapstick'\",\"lastUpdated\":\"June 2018\",\"tags\":[\"Solidworks\",\"mechanical design\",\"animatronic\",\"Foxing\",\"music\"],\"route\":\"foxing_animatronic\"},{\"title\":\"Mechatronics Robots\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"Robots from mechatronics\",\"lastUpdated\":\"May 2019\",\"tags\":[\"Arduino\",\"C++\",\"SolidWorks\",\"mechanical design\"],\"route\":\"mechatronics\"},{\"title\":\"Mehve (Working Title)\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"github\":\"mehve\",\"summary\":\"3D adventure game inspire by \\\"Nausicaa of the Valley of the Wind\\\"\",\"tags\":[\"Godot\",\"GDScript\",\"3D game\"],\"route\":\"mehve\"},{\"title\":\"Musical DL\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"github\":\"MusicalDL\",\"summary\":\"Using deep learning to generate choral music in the style of JS Bach\",\"tags\":[\"Python\",\"Pytorch\",\"AI/ML\",\"choral\",\"music\",\"generation\"],\"route\":\"musical_dl\"},{\"title\":\"NAND 2 Tetris\",\"imgSrc\":{\"src\":\"/_next/static/media/nand2tetris.660feb3d.png\",\"height\":346,\"width\":396,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAPFBMVEUAAABvKgIhHhw+KR0uJAFlRBU2GAYJBAENDw/TVQF/ZRBuWwrEoxytlRBiVBJJHQC7SwG5YCWSOwDqziPUksrRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAN0lEQVR4nCWKWw7AIAyA0Fbbqnvo7n/XZZMfEgKAscllu870J7tj2Tk6NSK151BcLilNwd3z974dVQEdrZJh0wAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":7},\"github\":\"nand2tetris\",\"summary\":\"A 16-bit computer built from scratch, starting with NAND gates\",\"tags\":[\"Computer Architecture\",\"HDL\",\"Assembly\",\"Hack\",\"Jack\",\"Compilers\",\"Operating Systems\",\"Virtual Machines\"],\"route\":\"nand2tetris\"},{\"title\":\"PDF Chatter\",\"imgSrc\":{\"src\":\"/_next/static/media/pypi_logo.f6536a80.svg\",\"height\":58,\"width\":66,\"blurWidth\":0,\"blurHeight\":0},\"summary\":\"LLM powered Q\u0026A over extracted PDF text\",\"github\":\"pdf-chatter\",\"tags\":[\"Python\",\"Optical Character Recognition (OCR)\",\"Large Language Models (LLMs)\",\"Nougat-OCR\",\"GPT-4\"],\"route\":\"pdf_chatter\"},{\"title\":\"pOngBot\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"Autonomous beer pong playing robot\",\"lastUpdated\":\"June 2020\",\"tags\":[\"Arduino\",\"C++\",\"computer vision\",\"Viola-Jones\",\"mechanical design\"],\"route\":\"pongbot\"},{\"title\":\"PRS19: Fret Press Robot\",\"imgSrc\":{\"src\":\"/_next/static/media/prs2019_preview.aed05a2a.png\",\"height\":2093,\"width\":3061,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAPFBMVEVJPTmhmIiCa1SBfXdBQD1IOzB3bGA6My5dUkyXlpNPRTyio6ChpadSTUm0q5e+taCun2xwYl2qrq5mXU/I9hK/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAL0lEQVR4nAXBhwEAIAjAsKoo4B7//2rCXFZLjoGwh4poIj5TJRfOsAok3G/v3toHGpkBOY6KaOIAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":5},\"github\":\"PRS_robot\",\"summary\":\"Automatic guitar fret press robot. Mechanical Engineering Master's Design captsone project\",\"tags\":[\"C++\",\"Arduino\",\"mechanical design\"],\"route\":\"prs19\"},{\"title\":\"Rewind\",\"imgSrc\":{\"src\":\"/_next/static/media/rewind_title.a1e43a09.png\",\"height\":540,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAElBMVEUqHjguITs4J0JmWW1PRFo/MUqO6aYhAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAH0lEQVR4nGNggAMmJkYmRhCDhZmFmZUBxGRkgIhAAQADKgAhbD4/MgAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":5},\"summary\":\"2018 Video Game Desgn (EN.601.355) capstone project\",\"lastUpdated\":\"May 2018\",\"tags\":[\"Unity\",\"C#\",\"2D game\"],\"route\":\"rewind\"},{\"title\":\"RoboJay\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"lastUpdated\":\"May 2018\",\"summary\":\"A balancing robot designed to give campus tours to incoming JHU freshmen\",\"tags\":[\"robotics\",\"feedback control\",\"navigation\",\"BeagleBone\",\"ROS\"],\"route\":\"robojay\"},{\"title\":\"High Power Rocketry\",\"imgSrc\":{\"src\":\"/_next/static/media/rebel_scum.34e7823c.jpg\",\"height\":1365,\"width\":2048,\"blurDataURL\":\"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAaEAACAgMAAAAAAAAAAAAAAAAAARESIjFB/8QAFQEBAQAAAAAAAAAAAAAAAAAAAgP/xAAXEQADAQAAAAAAAAAAAAAAAAAAAhNR/9oADAMBAAIRAxEAPwC6o4w5OwATo+immH//2Q==\",\"blurWidth\":8,\"blurHeight\":5},\"summary\":\"Level 1 \u0026 2 High Powered Rocket built with the Johns Hopkins Rocketry Club, and Spaceport America Cup 2018\",\"lastUpdated\":\"January 2018\",\"tags\":[\"High Power Rocketry\",\"Arduino\",\"C++\",\"mechanical design\",\"Tripoli\"],\"route\":\"rocketry\"},{\"title\":\"Silver Void\",\"imgSrc\":{\"src\":\"/_next/static/media/silver_void_cover_art.cc703812.png\",\"height\":1439,\"width\":1823,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAIVBMVEUCAgEWGhAMDQgiFiEnJyYwMC4aCRZHJygZDgQ9EDxLU0cSomUyAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALUlEQVR4nC3KyQkAMBDDQHntzdV/wSEQ/QaEYkeAutPhJ0NBFdv0kWDM57euCwrsAGo10HJLAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":6},\"github\":\"SilverVoid\",\"summary\":\"Submission for Acerola Jam 0\",\"tags\":[\"Godot\",\"GDScript\",\"3D Game\",\"Space Simulator\",\"Bullet Hell\"],\"route\":\"silver_void\"},{\"title\":\"so voice!\",\"imgSrc\":{\"src\":\"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":8},\"lastUpdated\":\"December 2022\",\"summary\":\"Choral music synthesis with deep learning (Continuation of Musical DL)\",\"tags\":[\"Python\",\"Pytorch\",\"AI/ML\",\"choral\",\"music\",\"synthesis\"],\"route\":\"so_voice\"},{\"title\":\"Terminal Ray Tracer\",\"imgSrc\":{\"src\":\"/_next/static/media/terminal_ray_tracer.00db7e4a.png\",\"height\":2043,\"width\":2881,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAbFBMVEWSamvNrLWWP0yUKVeYinmmnKTsEheTcnnmh4+Xfo6pV2GMMDasvsa5aXiPRVFVs2uOWF9WnGxXtVObh4ypREzbYmnsPkXIba/y/P10bHmfbcouPtDAMDdJbZOtmERQbkXfwcqTb3g+yNdOcdnEQLLCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAOklEQVR4nAXBBQKAIAAAsSMl7MZG//9HN/Igu6YtYDT+7qnhSK++tI+IcH5BViXZqn0Tk8M8Ti3rbH9KhwLIHy1s0QAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":6},\"github\":\"TerminalRayTracer\",\"summary\":\"A dependency-free ray tracer written in C that runs directly in a linux terminal\",\"tags\":[\"C\",\"ray tracing\",\"CLI\",\"linux\"],\"route\":\"terminal_ray_tracer\"},{\"title\":\"Cloud Timelapse\",\"imgSrc\":{\"src\":\"/_next/static/media/timelapse.b57dd258.jpg\",\"height\":3468,\"width\":4624,\"blurDataURL\":\"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAGAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAcEAACAgIDAAAAAAAAAAAAAAAAAQIFAxETYdH/xAAVAQEBAAAAAAAAAAAAAAAAAAADBP/EABYRAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAwDAQACEQMRAD8AlG4r56fBlT6ivQAUqwYX/9k=\",\"blurWidth\":8,\"blurHeight\":6},\"github\":\"timelapse\",\"summary\":\"A simple python project for taking timelapses of clouds from a webcam\",\"tags\":[\"Python\",\"OpenCV\",\"Raspberry Pi\",\"timelapse\",\"clouds\"],\"route\":\"timelapse\"},{\"title\":\"uSkipSpoilers\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"github\":\"uSkipSpoilers\",\"summary\":\"A small chrome extension for blocking spoilers in YouTube videos\",\"tags\":[\"React\",\"TypeScript\",\"Chrome\",\"Extension\"],\"route\":\"uskipspoilers\"},{\"title\":\"This Website\",\"github\":\"david-andrew.github.io\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"externalLink\":\"https://github.com/david-andrew/david-andrew.github.io\",\"summary\":\"This website, written in react/typescript\",\"tags\":[\"Next.js\",\"React\",\"TypeScript\",\"Tailwind CSS\",\"WebAssembly\"],\"route\":\"website\"},{\"title\":\"WSE18: Machine Shop Biometric Interlock\",\"imgSrc\":{\"src\":\"/_next/static/media/wse18.7405315e.png\",\"height\":2054,\"width\":2456,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAUVBMVEU8OTSVjYNlX1nIwbpLQjaqnpBOSUOJgHU8MCsyKSXo4dUfHh1iU0R8aWMUDAdoYlull4ialI2BZkzBuKyNdleyrKaQosO9iGCNY0PTj19hbYD9ZLzaAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAO0lEQVR4nAXBhQHAMAwDMBdCZRj/f+gkiLgdPDJcpJS6MbxKOJEBa1pH5whYpXl9DfEoc2A90MJC671/OekCOef5C1cAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":7},\"summary\":\"Biometric security interlock system. Mechanical Engineering Senior Design capstone project\",\"lastUpdated\":\"May 2018\",\"tags\":[\"Raspberry Pi\",\"Python\",\"C++\",\"Qt\",\"interlock\",\"fingerprint\",\"biometric\"],\"route\":\"wse18\"},{\"title\":\"Ziggy V (Working Title)\",\"imgSrc\":{\"src\":\"/_next/static/media/logo.43586adb.png\",\"height\":960,\"width\":960,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":8},\"summary\":\"Concept for a Real-Time-Strategy crossed with First-Person-Shooter\",\"lastUpdated\":\"January 2021\",\"tags\":[\"Godot\",\"GDScript\",\"FPS x RTS\",\"3D game\"],\"route\":\"ziggy_v\"}]}],[\"$\",\"$Lc\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"projects\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Ld\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$Lc\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"projects\",\"children\",\"dewy\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Ld\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$L12\",\"$L13\",null],\"segment\":\"__PAGE__\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/85fa6dafca566008.css\",\"precedence\":\"next\"}]]}],\"segment\":\"dewy\"},\"styles\":[]}],[\"$\",\"$L14\",null,{}]]}]]\n"])</script><script>self.__next_f.push([1,"12:null\n"])</script><script>self.__next_f.push([1,"15:\"$Sreact.suspense\"\n"])</script><script>self.__next_f.push([1,"16:I{\"id\":33699,\"chunks\":[\"6401:static/chunks/363642f4-e868532fae175454.js\",\"9838:static/chunks/30d07d85-388c23c5cce2d192.js\",\"6685:static/chunks/6685-d55f1e0fd750c950.js\",\"3222:static/chunks/3222-d51cb51b116d5c1d.js\",\"4769:static/chunks/4769-afbb25c509513e18.js\",\"9659:static/chunks/9659-468d9b7f9560cf55.js\",\"6413:static/chunks/6413-9fe1335479bb3ecd.js\",\"6896:static/chunks/6896-a56055cb12157616.js\",\"3215:static/chunks/3215-a5826776a257bdd1.js\",\"1134:static/chunks/app/projects/dewy/page-0bf6d62c6ea500af.js\"],\"name\":\"NoSSR\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"18:I{\"id\":53123,\"chunks\":[\"6401:static/chunks/363642f4-e868532fae175454.js\",\"9838:static/chunks/30d07d85-388c23c5cce2d192.js\",\"6685:static/chunks/6685-d55f1e0fd750c950.js\",\"3222:static/chunks/3222-d51cb51b116d5c1d.js\",\"4769:static/chunks/4769-afbb25c509513e18.js\",\"9659:static/chunks/9659-468d9b7f9560cf55.js\",\"6413:static/chunks/6413-9fe1335479bb3ecd.js\",\"6896:static/chunks/6896-a56055cb12157616.js\",\"3215:static/chunks/3215-a5826776a257bdd1.js\",\"1134:static/chunks/app/projects/dewy/page-0bf6d62c6ea500af.js\"],\"name\":\"DewyCodeBlock\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"19:I{\"id\":46685,\"chunks\":[\"6401:static/chunks/363642f4-e868532fae175454.js\",\"9838:static/chunks/30d07d85-388c23c5cce2d192.js\",\"6685:static/chunks/6685-d55f1e0fd750c950.js\",\"3222:static/chunks/3222-d51cb51b116d5c1d.js\",\"4769:static/chunks/4769-afbb25c509513e18.js\",\"9659:static/chunks/9659-468d9b7f9560cf55.js\",\"6413:static/chunks/6413-9fe1335479bb3ecd.js\",\"6896:static/chunks/6896-a56055cb12157616.js\",\"3215:static/chunks/3215-a5826776a257bdd1.js\",\"1134:static/chunks/app/projects/dewy/page-0bf6d62c6ea500af.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"1a:I{\"id\":63222,\"chunks\":[\"6401:static/chunks/363642f4-e868532fae175454.js\",\"9838:static/chunks/30d07d85-388c23c5cce2d192.js\",\"6685:static/chunks/6685-d55f1e0fd750c950.js\",\"3222:static/chunks/3222-d51cb51b116d5c1d.js\",\"4769:static/chunks/4769-afbb25c509513e18.js\",\"9659:static/chunks/9659-468d9b7f9560cf55.js\",\"6413:static/chunks/6413-9fe1335479bb3ecd.js\",\"6896:static/chunks/6896-a56055cb12157616.js\",\"3215:static/chunks/3215-a5826776a257bdd1.js\",\"1134:static/chunks/app/projects/dewy/page-0bf6d62c6ea500af.js\"],\"name\":\"Image\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"13:[[\"$\",\"$15\",null,{\"fallback\":null,\"children\":[\"$\",\"$L16\",null,{\"children\":\"$L17\"}]}],[\"$\",\"h3\",null,{\"className\":\"text-2xl mt-6 mb-2 font-quadon\",\"children\":\"About\"}],[\"$\",\"p\",null,{\"className\":\"mb-6 text-xl font-gentona text-justify\",\"children\":\"Dewy is a programming language I have been developing off and on since 2016. It is a general purpose language, designed with engineering applications in mind. Think the functionality and ease of use of matlab or python combined with the speed of a compiled language like C or Rust, but with its own unique flare.\"}],[\"$\",\"p\",null,{\"className\":\"text-xl font-gentona text-justify mb-2\",\"children\":\"Some key planned features include:\"}],[\"$\",\"ul\",null,{\"className\":\"list-disc mb-6 pl-10 text-xl font-gentona\",\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Functional and Imperative\"}],\" - Dewy is an imperative language with strong support for functional programming. This allows for a very flexible programming style, where you can use the best tool for the job.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Expression based syntax\"}],\" - Dewy uses an expression based syntax, meaning that everything is an expression. This allows for a very simple yet powerful syntax, where common language features often are just a free consequence of the syntax\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Garbage-collector-free memory management\"}],\" - Dewy uses a unique memory management system, allowing for fast and efficient memory management without the need for a garbage collector.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Strong type system\"}],\" - Dewy has a powerful static type system with inference, reminiscent of those in Typescript and Julia.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Built in unit system\"}],\" - Dewy has a built in unit system, allowing you to easily work with units and convert between them. This is especially useful for engineering applications.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Strong math support\"}],\" - Dewy has strong support many math features, including complex numbers, quaternions, vectors, matrices, and more. This is especially useful for engineering applications.\"]}]]}],[\"$\",\"p\",null,{\"className\":\"mb-6 text-xl font-gentona text-justify\",\"children\":\"An example of the common FizzBuzz program implemented in Dewy might look like this:\"}],[\"$\",\"$L18\",null,{\"src\":\"multiples = [3 -\u003e 'Fizz' 5 -\u003e 'Buzz' /{7 -\u003e 'Bazz' 11 -\u003e 'Bar'}/]\\nloop i in [0..100)\\n{\\n    printed_words = false\\n    loop [multiple word] in multiples \\n    {\\n        if i % multiple =? 0 \\n        { \\n            print(multiple)\\n            printed_words = true\\n        }\\n    }\\n    if not printed_words print(i)\\n    printl()\\n}\"}],[\"$\",\"p\",null,{\"className\":\"mb-6 text-xl font-gentona text-justify\",\"children\":\"Or a more functional style implementation might look like this:\"}],[\"$\",\"$L18\",null,{\"src\":\"multiples = [3 -\u003e 'Fizz' 5 -\u003e 'Buzz' /{7 -\u003e 'Bazz' 11 -\u003e 'Bar'}/]\\nrange = [0..100)\\n\\n//indexing at [, ..] and [..,] adds singleton dimensions\\nword_bools = range[, ..] .% multiples.keys[..,] .=? 0\\n\\n// ` means transpose, which behaves like python's zip()\\nword_grid = [multiples.values word_bools]`.map(\\n[word bools] =\u003e bools.map(b =\u003e if b word else '')\\n)\\n\\nraw_lines = word_grid`.map(line_words =\u003e line_words.join(''))\\n\\nlines = [raw_lines range]`.map(\\n    (raw_line i) =\u003e if raw_line.length =? 0 '{i}' else raw_line\\n)\\n\\nlines.join('\\\\n') |\u003e printl\"}],[\"$\",\"p\",null,{\"className\":\"mb-6 text-xl font-gentona text-justify\",\"children\":\"For clarity, the variables at each step look like so:\"}],[\"$\",\"$L18\",null,{\"src\":\"word_bools = [[true false false true false false true false ...]\\n             [true false false false false true false false ...]]\\n\\nword_grid = [['Fizz' '' '' 'Fizz' '' '' 'Fizz' '' '' 'Fizz' '' '' ...]\\n            ['Buzz' '' '' '' '' 'Buzz' '' '' '' '' 'Buzz' '' '' ...]]\\n\\nraw_lines = ['FizzBuzz' '' '' 'Fizz' '' 'Buzz' 'Fizz' '' '' 'Fizz' 'Buzz' '' ...]\\n\\nlines = ['FizzBuzz' '1' '2' 'Fizz' '4' 'Buzz' 'Fizz' '7' '8' 'Fizz' 'Buzz' '11' ...]\"}],[\"$\",\"h3\",null,{\"className\":\"text-2xl mt-6 mb-2 font-quadon\",\"children\":\"Current Status\"}],[\"$\",\"p\",null,{\"className\":\"mb-6 text-xl font-gentona text-justify\",\"children\":\"Currently I'm working through a simple interpreter for the language (powering the demo above). I've got a tokenizer, a basic interpreter backend, and handling of a few basic types of syntaxes via the parser. But much of the syntax is still unimplemented, hence the long list of \\\"Broken Examples\\\". Thus the current focus is finishing parser support for the rest of the syntax features.\"}],[\"$\",\"p\",null,{\"className\":\"mb-6 text-xl font-gentona text-justify\",\"children\":[\"Previously I had been doing a lot of development on bleeding edge\",\" \",[\"$\",\"$L19\",null,{\"href\":\"/projects/dewy_old\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"parser generators\"}],\", but that ended up being too big of a time sink for not much visible progress. Instead, for the time being, I ended up just hand rolling a parser in python, which has led to actually runnable code! I'll definitely revisit parser generators in the future when the language is further along.\"]}],[\"$\",\"p\",null,{\"className\":\"mb-6 text-xl font-gentona text-justify\",\"children\":[\"After the parser is complete, the next steps will be working on compiling to different backends. Initially I was planning to target\",\" \",[\"$\",\"$L19\",null,{\"href\":\"https://en.wikipedia.org/wiki/LLVM#Intermediate_representation\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"LLVM\"}],\" as the primary backend, however I recently discovered \",[\"$\",\"$L19\",null,{\"href\":\"https://c9x.me/compile/\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"QBE\"}],\", which is a much lighter alternative that supposedly gets 70% of the performance of LLVM for only 10% of the code. Longer term I'm interested in supporting a wider range of backend targets, like C, a universal\",\" \",[\"$\",\"$L19\",null,{\"href\":\"https://en.wikipedia.org/wiki/Polyglot_(computing)\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"polyglot\"}],\" targeting many scripting languages simultaneously (sh, bash, windows cmd, powershell, javascript, python, etc.), and eventually LLVM too. At some point I'll start building out the standard library, and bootstrapping the compiler to be able to compile itself—at which point we might be ready for a version 0 release!\"]}],[\"$\",\"h3\",null,{\"className\":\"text-2xl mt-6 mb-2 font-quadon\",\"children\":\"About the Demo\"}],[\"$\",\"p\",null,{\"className\":\"mb-6 text-xl font-gentona text-justify\",\"children\":[\"The demo above was actually pretty complex to put together. The current interpreter is written in python, and this website is statically hosted, which meant the demo required some way to statically run python code without a server. For this, I used \",[\"$\",\"$L19\",null,{\"href\":\"https://pyodide.org/\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"Pyodide\"}],\", which is basically \",[\"$\",\"$L19\",null,{\"href\":\"https://github.com/python/cpython\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"CPython\"}],\" compiled to\",\" \",[\"$\",\"$L19\",null,{\"href\":\"https://webassembly.org/\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"WebAssembly\"}],\" via\",\" \",[\"$\",\"$L19\",null,{\"href\":\"https://emscripten.org/\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"Emscripten.\"}]]}],[\"$\",\"p\",null,{\"className\":\"mb-6 text-xl font-gentona text-justify\",\"children\":[\"Pyodide itself isn't too difficult to use, except for the fact that it doesn't have good support for asynchronous standard input—it really wants to halt the entire UI while you type input into a stock browser popup prompt. To get around this, I found a\",\" \",[\"$\",\"$L19\",null,{\"href\":\"https://www.npmjs.com/package/sync-message\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"handy library\"}],\" where you run pyodide in a web worker, and then any time it wants to read input, the worker makes a synchronous XHR request to a service worker, blocking the pyodide web worker until the service worker receives a response from the main thread with the input, which the service worker can then pass back to the pyodide worker. Suffice it to say, I don't think I ever want to deal with service workers again.\"]}],[\"$\",\"p\",null,{\"className\":\"mb-6 text-xl font-gentona text-justify\",\"children\":[\"Now that python is handled, the next aspect is getting the Dewy interpreter itself to run. For this, I fetch (at website build time) the source code directly from\",\" \",[\"$\",\"$L19\",null,{\"href\":\"https://github.com/david-andrew/dewy-lang/tree/master/src/compiler\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"github\"}],\". I then abuse the python import lib to allow loading \\\"modules\\\" directly from strings, and then pass all of the dewy source in as string modules. Then I have a little wrapper function for the entry point which receives a dewy source code string, and runs the program. The entry point can then be called from the browser via a javascript wrapper function.\"]}],[\"$\",\"p\",null,{\"className\":\"mb-6 text-xl font-gentona text-justify\",\"children\":[\"The final piece of the puzzle is the text entry, and terminal emulator. For text input, I'm using the \",[\"$\",\"$L19\",null,{\"href\":\"https://codemirror.net/\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"Code Mirror Library\"}],\" with a custom syntax highlighter. For the terminal, I use the \",[\"$\",\"$L19\",null,{\"href\":\"https://xtermjs.org/\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"xterm.js\"}],\" library. I then hooked up stdin and stdout from pyodide to interact with the terminal, and voila! A Dewy interpreter running in the browser.\"]}],[\"$\",\"p\",null,{\"className\":\"mb-6 text-xl font-gentona text-justify\",\"children\":\"There are definitely some rough edges, and the parser only supports a small handful of features, but it runs! It's probably the easiest way to try out the language, and I'm looking forward to getting all of the broken example programs working!\"}],[\"$\",\"h3\",null,{\"className\":\"text-2xl mt-6 mb-2 font-quadon\",\"children\":\"Links\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-3\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-row text-sm items-center\",\"children\":[[\"$\",\"$L1a\",null,{\"src\":{\"src\":\"/_next/static/media/github.8d24eda3.svg\",\"height\":484,\"width\":496,\"blurWidth\":0,\"blurHeight\":0},\"alt\":\"github icon\",\"className\":\"inline-block w-8 h-8 mr-2  pointer-events-none select-none\",\"draggable\":false}],[\"$\",\"span\",null,{\"className\":\"align-middle\",\"children\":[\"$\",\"$L19\",null,{\"href\":\"https://github.com/david-andrew/dewy-lang\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"Github Repo\"}]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-row text-sm items-center\",\"children\":[[\"$\",\"$L1a\",null,{\"src\":{\"src\":\"/_next/static/media/docs.71257e0f.svg\",\"height\":512,\"width\":384,\"blurWidth\":0,\"blurHeight\":0},\"alt\":\"docs icon\",\"className\":\"inline-block w-8 h-8 mr-2  pointer-events-none select-none\",\"draggable\":false}],[\"$\",\"span\",null,{\"className\":\"align-middle\",\"children\":[\"$\",\"$L19\",null,{\"href\":\"https://david-andrew.github.io/dewy-lang/\",\"className\":\"text-blue-400 hover:text-blue-500 font-gentona text-xl\",\"children\":\"Language Docs\"}]}]]}]]}]]\n"])</script><script>self.__next_f.push([1,"1b:I{\"id\":43215,\"chunks\":[\"6401:static/chunks/363642f4-e868532fae175454.js\",\"9838:static/chunks/30d07d85-388c23c5cce2d192.js\",\"6685:static/chunks/6685-d55f1e0fd750c950.js\",\"3222:static/chunks/3222-d51cb51b116d5c1d.js\",\"4769:static/chunks/4769-afbb25c509513e18.js\",\"9659:static/chunks/9659-468d9b7f9560cf55.js\",\"6413:static/chunks/6413-9fe1335479bb3ecd.js\",\"6896:static/chunks/6896-a56055cb12157616.js\",\"3215:static/chunks/3215-a5826776a257bdd1.js\",\"1134:static/chunks/app/projects/dewy/page-0bf6d62c6ea500af.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"1c:T863,"])</script><script>self.__next_f.push([1,"from pathlib import Path\nfrom argparse import ArgumentParser, REMAINDER\nfrom .backend import backend_names, get_backend, python_interpreter, qbe_compiler, get_version\n\nimport pdb\n\n\n\ndef main():\n    arg_parser = ArgumentParser(description='Dewy Compiler')\n\n    # positional argument for the file to compile\n    arg_parser.add_argument('file', help='.dewy file to run')\n\n    # mutually exclusive flags for specifying the backend to use\n    group = arg_parser.add_mutually_exclusive_group()\n    group.add_argument('-i', action='store_true', help='(DEFAULT) Run in interpreter mode with the python backend')\n    group.add_argument('-c', action='store_true', help='Run in compiler mode with the llvm backend (not implemented yet)')\n    group.add_argument('--backend', type=str, help=f'Specify a backend compiler/interpreter by name to use. Backends will include: {backend_names} (however currently only python is available).')\n\n    arg_parser.add_argument('-v', '--version', action='version', version=f'Dewy {get_version()}', help='Print version information and exit')\n    arg_parser.add_argument('-p', '--disable-rich-print', action='store_true', help='Disable using rich for printing stack traces')\n    arg_parser.add_argument('args', nargs=REMAINDER, help='Arguments after the file are passed directly to program')\n\n    args = arg_parser.parse_args()\n\n    # use rich for pretty traceback printing\n    #TODO: maybe add a util or something for trying to import rich and replacing print in all files\n    if not args.disable_rich_print:\n        try:\n            from rich import traceback\n            traceback.install(show_locals=True)\n        except:\n            print('rich unavailable for import. using built-in printing')\n\n    # default interpreter is python. default compiler is qbe. default with no args is python.\n    if args.backend:\n        backend = get_backend(args.backend)\n    elif args.c:\n        backend = qbe_compiler\n    elif args.i:\n        backend = python_interpreter\n    else:\n        backend = python_interpreter\n\n    # run with the selected backend\n    backend(Path(args.file), args.args)\n\n\nif __name__ == '__main__':\n    main()\n"])</script><script>self.__next_f.push([1,"1d:T8b56,"])</script><script>self.__next_f.push([1,"from typing import Generator, Sequence, cast, overload, Literal, Type as PyType\nfrom enum import Enum, auto\nfrom dataclasses import dataclass, field\nfrom itertools import groupby, chain as iterchain\n\nfrom .syntax import (\n    AST,\n    Access,\n    Declare,\n    PointsTo, BidirPointsTo,\n    Type,\n    ListOfASTs, PrototypeTuple, Block, BareRange, Ellipsis, Spread, Array, Group, Range, Object, Dict, BidirDict, TypeParam,\n    Void, Undefined, void, undefined, untyped,\n    String, IString,\n    Flowable, Flow, If, Loop, Default,\n    PrototypeFunctionLiteral, PrototypePyAction, Call,\n    Index,\n    PrototypeIdentifier, Identifier, TypedIdentifier, ReturnTyped, UnpackTarget, Assign,\n    Int, Bool,\n    Range, IterIn,\n    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,\n    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,\n    Add, Sub, Mul, Div, IDiv, Mod, Pow,\n    And, Or, Xor, Nand, Nor, Xnor,\n    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv, AtHandle,\n    DeclarationType,\n    DeclareGeneric, Parameterize,\n)\nfrom .tokenizer import (\n    Token,\n    Block_t,\n    Operator_t,\n    ShiftOperator_t,\n    Juxtapose_t,\n    Comma_t,\n    String_t,\n    Escape_t,\n    TypeParam_t,\n    Undefined_t,\n    Identifier_t,\n    Integer_t,\n    Boolean_t,\n    BasedNumber_t,\n    RawString_t,\n    DotDot_t, DotDotDot_t,\n    Keyword_t,\n)\nfrom .postok import (\n    RangeJuxtapose_t,\n    EllipsisJuxtapose_t,\n    TypeParamJuxtapose_t,\n    get_next_chain,\n    Chain,\n    is_op,\n    Flow_t,\n    Declare_t,\n)\nfrom .utils import (\n    bool_to_bool,\n    based_number_to_int,\n)\n\nimport pdb\n\n\n\n# Scope class only used during parsing to keep track of callables\n@dataclass\nclass Scope:\n    @dataclass\n    class _var():\n        # name:str #name is stored in the dict key\n        decltype: DeclarationType\n        type: Type\n        value: AST\n\n    parent: 'Scope | None' = None\n    # callables: dict[str, AST | None] = field(default_factory=dict) #TODO: maybe replace str-\u003eAST with str-\u003esignature (where signature might be constructed based on the func structure)\n    vars: 'dict[str, Scope._var]' = field(default_factory=dict)\n\n    @overload\n    def get(self, name:str, throw:Literal[True]=True) -\u003e 'Scope._var': ...\n    @overload\n    def get(self, name:str, throw:Literal[False]) -\u003e 'Scope._var|None': ...\n    def get(self, name:str, throw:bool=True) -\u003e 'Scope._var|None':\n        for s in self:\n            if name in s.vars:\n                return s.vars[name]\n\n        if throw:\n            raise KeyError(f'variable {name} not found in scope')\n        return None\n\n    def assign(self, name:str, value:AST):\n        assert len(DeclarationType.__members__) == 2, f'expected only 2 declaration types: let, const. found {DeclarationType.__members__}'\n\n        # var is already declared in current scope\n        if name in self.vars:\n            var = self.vars[name]\n            assert var.decltype != DeclarationType.CONST, f\"Attempted to assign to constant variable: {name=}{var=}. {value=}\"\n            var.value = value\n            return\n\n        var = self.get(name, throw=False)\n\n        # var is not declared in any scope\n        if var is None:\n            self.let(name, value, untyped)\n            return\n\n        # var was declared in a parent scope\n        if var.decltype == DeclarationType.LET:\n            var.value = value\n            return\n\n        raise ValueError(f'Attempted to assign to constant variable: {name=}{var=}. {value=}')\n\n    def declare(self, name:str, value:AST, type:Type, decltype:DeclarationType):\n        if name in self.vars:\n            var = self.vars[name]\n            assert var.decltype != DeclarationType.CONST, f\"Attempted to {decltype.name.lower()} declare a value that is const in this current scope. {name=}{var=}. {value=}\"\n\n        self.vars[name] = Scope._var(decltype, type, value)\n\n    def let(self, name:str, value:AST, type:Type):\n        self.declare(name, value, type, DeclarationType.LET)\n\n    def const(self, name:str, value:AST, type:Type):\n        self.declare(name, value, type, DeclarationType.CONST)\n\n    def __iter__(self) -\u003e Generator['Scope', None, None]:\n        \"\"\"return an iterator that walks up each successive parent scope. Starts with self\"\"\"\n        s = self\n        while s is not None:\n            yield s\n            s = s.parent\n\n    #TODO: these should actually be defined in python.py. There should maybe only be stubs here..\n    @classmethod\n    def default(cls: PyType['Scope']) -\u003e 'Scope':\n        return cls(vars={\n            'printl': Scope._var(\n                DeclarationType.CONST,\n                Type('callable'),\n                PrototypePyAction(\n                    Group([Assign(TypedIdentifier(Identifier('s'), Type('string')), String(''))]),\n                    Type('void')\n                )\n            ),\n            'print': Scope._var(\n                DeclarationType.CONST,\n                Type('callable'),\n                PrototypePyAction(\n                    Group([Assign(TypedIdentifier(Identifier('s'), Type('string')), String(''))]),\n                    Type('void')\n                )\n            ),\n            'readl': Scope._var(\n                DeclarationType.CONST,\n                Type('callable'),\n                PrototypePyAction(\n                    Group([]),\n                    Type('string')\n                )\n            )\n        })\n\n\n\n\ndef top_level_parse(tokens: list[Token]) -\u003e AST:\n    \"\"\"Main entrypoint to kick off parsing a sequence of tokens\"\"\"\n\n    scope = Scope.default()\n    ast = parse(tokens, scope)\n    if isinstance(ast, ListOfASTs):\n        ast = Group(ast.asts)\n\n    # post processing on the parsed AST\n    # express_identifiers(ast)\n    # tuples_to_arrays(ast)\n    # ensure_no_prototypes(ast) #ensure all settled...\n    # ensure_no_unwrapped_ranges(ast)\n    # set_ast_scopes(ast, scope)\n\n    return ast\n\ndef parse_generator(tokens: list[Token], scope: Scope) -\u003e Generator[AST, None, None]:\n    \"\"\"\n    Parse all tokens into a sequence of ASTs\n    \"\"\"\n\n    while len(tokens) \u003e 0:\n        chain, tokens = get_next_chain(tokens)\n        yield parse_chain(chain, scope)\n\n\ndef parse(tokens: list[Token], scope: Scope) -\u003e AST:\n    items = [*parse_generator(tokens, scope)]\n\n    # depending on how many expressions were parsed, return an AST or container\n    if len(items) == 0:\n        ast = void # literally nothing was parsed\n    elif len(items) == 1:\n        ast = items[0]\n    else:\n        ast = ListOfASTs(items)\n\n    return ast\n\n@dataclass\nclass qint:\n    \"\"\"\n    quantum int for dealing with precedences that are multiple values at the same time\n    qint's can only be strictly greater or strictly less than other values. Otherwise it's ambiguous\n    In the case of ambiguous precedences, the symbol table is needed for helping resolve the ambiguity\n    \"\"\"\n    values: set[int]\n\n    def __post_init__(self):\n        assert len(self.values) \u003e 1, f'qint must have more than one value. Got {self.values}'\n\n    def __gt__(self, other: 'int|qint') -\u003e bool:\n        if isinstance(other, int):\n            return all(v \u003e other for v in self.values)\n        return all(v \u003e other for v in self.values)\n\n    def __lt__(self, other: 'int|qint') -\u003e bool:\n        if isinstance(other, int):\n            return all(v \u003c other for v in self.values)\n        return all(v \u003c other for v in self.values)\n\n    def __ge__(self, other: 'int|qint') -\u003e bool: return self.__gt__(other)\n    def __le__(self, other: 'int|qint') -\u003e bool: return self.__lt__(other)\n    def __eq__(self, other: object) -\u003e bool: return False\n\n\n######### Operator Precedence Table #########\n# TODO: class for compund operators, e.g. += -= .+= .-= not=? not\u003e? etc.\n# TODO: how to handle unary operators in the table? perhaps make PrefixOperator_t/PostfixOperator_t classes?\n# TODO: add specification of associativity for each row\nclass Associativity(Enum):\n    left = auto()  # left-to-right\n    right = auto()  # right-to-left\n    prefix = auto()\n    postfix = auto()\n    none = auto()\n    fail = auto()\n\n\n\"\"\"\n[HIGHEST PRECEDENCE]\n    (prefix) @\n    . \u003cjux call\u003e \u003cjux index access\u003e\n    \u003cjux ellipsis\u003e                      //e.g. [...args]\n    :                                   //e.g. let x:int\n    :\u003e                                  //e.g. let x:():\u003eint =\u003e 42\n    (prefix) not\n    (postfix) ? `\n    ^                                   //right-associative\n    \u003cjux mul\u003e\n    / * %\n    + -\n    \u003c\u003c \u003e\u003e \u003c\u003c\u003c \u003e\u003e\u003e \u003c\u003c! !\u003e\u003e\n    ,                                   //tuple maker\n    \u003cjux range\u003e                         //e.g. [first,second..last]\n    in\n    =? \u003e? \u003c? \u003e=? \u003c=? not=? \u003c=\u003e is? isnt? @?\n    and nand \u0026\n    xor xnor                            //following C's precedence: and \u003e xor \u003e or\n    or nor |\n    as transmute\n    =\u003e\n    |\u003e                                  //function pipe operators\n    \u003c|\n    -\u003e \u003c-\u003e                              //dict pointers\n    = .= \u003cop\u003e= .\u003cop\u003e=  (e.g. += .+=)    //right-associative (but technically causes a type error since assignments can't be chained)\n    else\n    (postfix) ;\n    \u003cseq\u003e (i.e. space)\n[LOWEST PRECEDENCE]\n\n[Notes]\n.. for ranges is not an operator, it is an expression. it uses juxtapose to bind to left/right arguments (or empty), and type-checks left and right\nif-else-loop chain expr is more like a single unit, so it doesn't really have a precedence. but they act like they have the lowest precedence since the expressions they capture will be full chains only broken by space/seq\nthe unary versions of + - * / % have the same precedence as their binary versions\n\"\"\"\noperator_groups: list[tuple[Associativity, Sequence[Operator_t]]] = list(reversed([\n    (Associativity.prefix, [Operator_t('@')]),\n    (Associativity.left, [Operator_t('.'), Juxtapose_t(None)]),  # jux-call, jux-index\n    (Associativity.right, [EllipsisJuxtapose_t(None)]),  # jux-ellipsis\n    (Associativity.none, [TypeParamJuxtapose_t(None)]),\n    (Associativity.none, [Operator_t(':')]),\n    (Associativity.right, [Operator_t(':\u003e')]),\n    (Associativity.prefix, [Operator_t('not')]),\n    (Associativity.right,  [Operator_t('^')]),\n    (Associativity.left, [Juxtapose_t(None)]),  # jux-multiply\n    (Associativity.left, [Operator_t('*'), Operator_t('/'), Operator_t('%')]),\n    (Associativity.left, [Operator_t('+'), Operator_t('-')]),\n    (Associativity.left, [*map(ShiftOperator_t, ['\u003c\u003c', '\u003e\u003e', '\u003c\u003c\u003c', '\u003e\u003e\u003e', '\u003c\u003c!', '!\u003e\u003e'])]),\n    (Associativity.none,  [Comma_t(',')]),\n    (Associativity.left, [RangeJuxtapose_t(None)]),  # jux-range\n    (Associativity.none, [Operator_t('in')]),\n    (Associativity.left, [Operator_t('=?'), Operator_t('\u003e?'), Operator_t('\u003c?'), Operator_t('\u003e=?'), Operator_t('\u003c=?')]),\n    (Associativity.left, [Operator_t('and'), Operator_t('nand'), Operator_t('\u0026')]),\n    (Associativity.left, [Operator_t('xor'), Operator_t('xnor')]),\n    (Associativity.left, [Operator_t('or'), Operator_t('nor'), Operator_t('|')]),\n    (Associativity.none,  [Operator_t('as'), Operator_t('transmute')]),\n    (Associativity.right,  [Operator_t('=\u003e')]),  # () =\u003e () =\u003e () =\u003e 42\n    (Associativity.right, [Operator_t('|\u003e')]),\n    (Associativity.left, [Operator_t('\u003c|')]),\n    (Associativity.fail,  [Operator_t('-\u003e'), Operator_t('\u003c-\u003e')]),\n    (Associativity.fail,  [Operator_t('=')]),\n    (Associativity.none,  [Operator_t('else')]),\n]))\nprecedence_table: dict[Operator_t, int | qint] = {}\nassociativity_table: dict[int, Associativity] = {}\nfor i, (assoc, group) in enumerate(operator_groups):\n\n    # mark precedence level i as the specified associativity\n    associativity_table[i] = assoc\n\n    # insert all ops in the row into the precedence table at precedence level i\n    for op in group:\n        if op not in precedence_table:\n            precedence_table[op] = i\n            continue\n\n        val = precedence_table[op]\n        if isinstance(val, int):\n            precedence_table[op] = qint({val, i})\n        else:\n            precedence_table[op] = qint(val.values | {i})\n\n\ndef operator_precedence(op: Operator_t) -\u003e int | qint:\n\n    # TODO: handling compound operators like .+, +=, .+=, etc.\n    # if isinstance(op, CompoundOperator_t):\n    #     op = op.base\n\n    try:\n        return precedence_table[op]\n    except:\n        raise ValueError(f\"ERROR: expected operator, got {op=}\") from None\n\n\ndef operator_associativity(op: Operator_t | int) -\u003e Associativity:\n    if not isinstance(op, int):\n        i = operator_precedence(op)\n        assert isinstance(i, int), f'Cannot determine associativity of operator ({op}) with multiple precedence levels ({i})'\n    else:\n        i = op\n    try:\n        return associativity_table[i]\n    except:\n        raise ValueError(f\"Error: failed to determine associativity for operator {op}\") from None\n\n\ndef is_callable(ast:AST, scope: Scope):\n    match ast:\n        # ASTs the have to be evaluated to determine the type\n        case PrototypeIdentifier(name):\n            #TODO: use full type checker here to determine the type\n            # DEBUG: for now, hardcode to call\n            return True\n        #TODO: any other types that need to be evaluated to determine if callable\n\n        # known callable ASTs\n        case PrototypePyAction() | PrototypeFunctionLiteral():\n            return True\n\n        #TODO: may change this in the future, but for now, assume at handle can only be used on callables\n        case AtHandle():\n            return True\n\n        # known non-callables\n        case Int() | String() | Bool(): #TODO: rest of them..\n            return False\n\n        case _:\n            raise ValueError(f\"ERROR: unhandled case to check if is_callable: {ast=}\")\n\n    pdb.set_trace()\n\ndef is_indexable(ast:AST, scope: Scope):\n    pdb.set_trace()\n    raise NotImplementedError\n\n\ndef parse_chain(chain: Chain[Token], scope: Scope) -\u003e AST:\n    assert isinstance(chain, Chain), f\"ERROR: parse chain must be called on Chain[Token], got {type(chain)}\"\n\n    if len(chain) == 0:\n        return void\n    if len(chain) == 1:\n        return parse_single(chain[0], scope)\n\n    left, op, right = split_by_lowest_precedence(chain, scope)\n    left, right = parse_chain(left, scope), parse_chain(right, scope)\n\n    assert not (left is void and right is void), f\"Internal Error: both left and right returned void during parse chain, implying both left and right side of operator were empty, i.e. chain was invalid: {chain}\"\n\n    # 3 cases are prefix expr, postfix expr, or binary expr\n    if left is void:\n        return build_unary_prefix_expr(op, right, scope)\n    if right is void:\n        return build_unary_postfix_expr(left, op, scope)\n    return build_bin_expr(left, op, right, scope)\n\n\n\n\ndef split_by_lowest_precedence(tokens: Chain[Token], scope: Scope) -\u003e tuple[Chain[Token], Token, Chain[Token]]:\n    \"\"\"\n    return the integer index/indices of the lowest precedence operator(s) in the given list of tokens\n    \"\"\"\n    assert isinstance(\n        tokens, Chain), f\"ERROR: `split_by_lowset_precedence()` may only be called on explicitly known Chain[Token], got {type(tokens)}\"\n\n    # collect all operators and their indices in the list of tokens\n    idxs, ops = zip(*[(i, token) for i, token in enumerate(tokens) if is_op(token)])\n\n    if len(ops) == 0:\n        pdb.set_trace()\n        # TODO: how to handle this case?\n        # return Chain(), None, Chain()\n        raise ValueError()\n    if len(ops) == 1:\n        i, = idxs\n        op, = ops\n        return Chain(tokens[:i]), op, Chain(tokens[i+1:])\n\n    # when more than one op present, find the lowest precedence one\n    ranks = [operator_precedence(op) for op in ops]\n    min_rank = min(ranks)\n    min_idx = ranks.index(min_rank)\n\n    # verify that the min is strictly less than or equal to all other ranks\n    if not all(min_rank \u003c= r for r in ranks[:min_idx] + ranks[min_idx+1:]):\n        # TODO: probably enumerate out all permutations of the ambiguous operators and return all of them as a list of lists of indices\n        # make use of scope/chain typeof to disambiguate if need be\n        pdb.set_trace()\n        raise NotImplementedError(f\"TODO: ambiguous precedence for {ops=} with {ranks=}, in token stream {tokens=}\")\n\n    # find operators with precedence equal to the current minimum\n    op_idxs = [i for i, r in zip(idxs, ranks) if r == min_rank or r is min_rank]\n\n    if len(op_idxs) == 1:\n        i, = op_idxs\n        return Chain(tokens[:i]), tokens[i], Chain(tokens[i+1:])\n\n    # handling when multiple ops have the same precedence, select based on associativity rules\n    if isinstance(min_rank, qint):\n        assocs = {operator_associativity(i) for i in min_rank.values}\n        if len(assocs) \u003e 1:\n            raise NotImplementedError(\n                f'TODO: need to type check to deal with multiple/ambiguous operator associativities: {assocs}')\n        assoc, = assocs\n    else:\n        assoc = operator_associativity(min_rank)\n\n    match assoc:\n        case Associativity.left: i = op_idxs[-1]\n        case Associativity.right: i = op_idxs[0]\n        case Associativity.prefix: i = op_idxs[0]\n        case Associativity.postfix: i = op_idxs[-1]\n        case Associativity.none: i = op_idxs[-1]  # default to left. handled later in parsing\n        case Associativity.fail: raise ValueError(f'Cannot handle multiple given operators in chain {tokens}, as lowest precedence operator is marked as un-associable.')\n\n    return Chain(tokens[:i]), tokens[i], Chain(tokens[i+1:])\n\n\n\n\n\ndef parse_single(token: Token, scope: Scope) -\u003e AST:\n    \"\"\"Parse a single token into an AST\"\"\"\n    match token:\n        case Undefined_t(): return undefined\n        case Identifier_t(): return PrototypeIdentifier(token.src)\n        case Integer_t(): return Int(int(token.src))\n        case Boolean_t(): return Bool(bool_to_bool(token.src))\n        case BasedNumber_t(): return Int(based_number_to_int(token.src))\n        case RawString_t(): return String(token.to_str())\n        case DotDot_t(): return BareRange(void, void)\n        case DotDotDot_t(): return Ellipsis()\n        case String_t(): return parse_string(token, scope)\n        case Block_t(): return parse_block(token, scope)\n        case TypeParam_t(): return parse_type_param(token, scope)\n        case Flow_t(): return parse_flow(token, scope)\n        case Declare_t(): return parse_declare(token, scope)\n\n        case _:\n            # TODO handle other types...\n            pdb.set_trace()\n            ...\n\n    pdb.set_trace()\n    raise NotImplementedError()\n    ...\n\n\ndef build_bin_expr(left: AST, op: Token, right: AST, scope: Scope) -\u003e AST:\n    \"\"\"create a unary prefix expression AST from the op and right AST\"\"\"\n\n    match op:\n        case Juxtapose_t():\n            if is_callable(left, scope):\n                return Call(left, right)\n            elif isinstance(right, (Range, Array)) and is_indexable(left, scope):\n                return Index(left, right)\n            else:\n                return Mul(left, right)\n\n        case Operator_t(op='='): return Assign(left, right)\n        case Operator_t(op='=\u003e'): return PrototypeFunctionLiteral(left, right)\n        case Operator_t(op='-\u003e'): return PointsTo(left, right)\n        # case Operator_t(op='\u003c-'): return PointsTo(right, left) #TBD if we just remove this one...\n        case Operator_t(op='\u003c-\u003e'): return BidirPointsTo(left, right)\n        case Operator_t(op='.'): return Access(left, right)\n\n        # a bunch of simple cases:\n        case ShiftOperator_t(op='\u003c\u003c'):  return LeftShift(left, right)\n        case ShiftOperator_t(op='\u003e\u003e'):  return RightShift(left, right)\n        case ShiftOperator_t(op='\u003c\u003c\u003c'): return LeftRotate(left, right)\n        case ShiftOperator_t(op='\u003e\u003e\u003e'): return RightRotate(left, right)\n        case ShiftOperator_t(op='\u003c\u003c!'): return LeftRotateCarry(left, right)\n        case ShiftOperator_t(op='!\u003e\u003e'): return RightRotateCarry(left, right)\n        case Operator_t(op='+'): return Add(left, right)\n        case Operator_t(op='-'): return Sub(left, right)\n        case Operator_t(op='*'): return Mul(left, right)\n        case Operator_t(op='/'): return Div(left, right)\n        case Operator_t(op='÷'): return IDiv(left, right)\n        case Operator_t(op='%'): return Mod(left, right)\n        case Operator_t(op='^'): return Pow(left, right)\n\n        # comparison operators\n        case Operator_t(op='=?'): return Equal(left, right)\n        case Operator_t(op='\u003e?'): return Greater(left, right)\n        case Operator_t(op='\u003c?'): return Less(left, right)\n        case Operator_t(op='\u003e=?'): return GreaterEqual(left, right)\n        case Operator_t(op='\u003c=?'): return LessEqual(left, right)\n        case Operator_t(op='in?'): return MemberIn(left, right)\n        # case Operator_t(op='is?'): return Is(left, right)\n        # case Operator_t(op='isnt?'): return Isnt(left, right)\n        # case Operator_t(op='\u003c=\u003e'): return ThreewayCompare(left, right)\n\n        # Logical Operators. TODO: outtype=Bool is not flexible enough...\n        case Operator_t(op='and'): return And(left, right)\n        case Operator_t(op='or'): return Or(left, right)\n        case Operator_t(op='nand'): return Nand(left, right)\n        case Operator_t(op='nor'): return Nor(left, right)\n        case Operator_t(op='xor'): return Xor(left, right)\n        case Operator_t(op='xnor'): return Xnor(left, right)\n\n        # Misc Operators\n        case Operator_t(op=':'):\n            if isinstance(left, PrototypeIdentifier): return TypedIdentifier(Identifier(left.name), right)\n            #TBD if there are other things that can have type annotations beyond identifiers\n            raise ValueError(f'ERROR: can only apply a type to an identifier. Got {left=}, {right=}')\n        case Operator_t(op=':\u003e'): return ReturnTyped(left, right)\n\n        case TypeParamJuxtapose_t():\n            if isinstance(left, TypeParam):\n                return DeclareGeneric(left, right)\n            if isinstance(right, TypeParam):\n                return Parameterize(left, right)\n            raise ValueError(f\"INTERNAL ERROR: TypeParamJuxtapose must be attached to a type param. {left=}, {right=}\")\n\n        case EllipsisJuxtapose_t():\n            assert isinstance(left, Ellipsis), f'INTERNAL ERROR: EllipsisJuxtapose was attached to a non ellipsis token. {left=}, {right=}'\n            return Spread(right)\n\n        case RangeJuxtapose_t():\n            if isinstance(right, BareRange):\n                assert right.left is void, f\"ERROR: can't attach expression to range, range already has values. Got {left=}, {right=}\"\n                right.left = left\n                return right\n\n            if isinstance(left, BareRange):\n                assert left.right is void, f\"ERROR: can't attach expression to range, range already has values. Got {left=}, {right=}\"\n                left.right = right\n                return left\n\n            raise ValueError(f'INTERNAL ERROR: Range Juxtapose must be next to a range. Got {left=}, {right=}')\n\n        case Comma_t():\n            # TODO: combine left or right tuples into a single tuple\n            if isinstance(left, PrototypeTuple) and isinstance(right, PrototypeTuple):\n                return PrototypeTuple([*left.items, *right.items])\n            elif isinstance(left, PrototypeTuple):\n                return PrototypeTuple([*left.items, right])\n            elif isinstance(right, PrototypeTuple):\n                return PrototypeTuple([left, *right.items])\n            else:\n                return PrototypeTuple([left, right])\n\n        case Operator_t(op='else'):\n            if isinstance(left, Flow) and isinstance(right, Flow):\n                # merge left+right as single flow\n                return Flow([*left.branches, *right.branches])\n            elif isinstance(left, Flow):\n                # append right to left\n                assert not isinstance(left.branches[-1], Default), f\"ERROR: can't merge default branch into middle of flow. Got: {left=}, {right=}\"\n                if isinstance(right, Flowable):\n                    return Flow([*left.branches, right])\n                return Flow([*left.branches, Default(right)])\n\n            elif isinstance(right, Flow):\n                # prepend left to right\n                assert isinstance(left, Flowable), f\"ERROR: can only prepend Flowables to left of a Flow. Got: {left=}, {right=}\"\n                return Flow([left, *right.branches])\n            else:\n                # create a new flow out of the left and right\n                assert isinstance(left, Flowable), f\"ERROR: can only create a Flow from Flowables. Got: {left=}, {right=}\"\n                if isinstance(right, Flowable):\n                    return Flow([left, right])\n                return Flow([left, Default(right)])\n\n        case Operator_t(op='in'):\n            return IterIn(left, right)\n\n        case _:\n            pdb.set_trace()\n            raise NotImplementedError(f'Parsing of operator {op} has not been implemented yet')\n\n\ndef build_unary_prefix_expr(op: Token, right: AST, scope: Scope) -\u003e AST:\n    \"\"\"create a unary prefix expression AST from the op and right AST\"\"\"\n    match op:\n        # normal prefix operators\n        case Operator_t(op='+'): return UnaryPos(right)\n        case Operator_t(op='-'): return UnaryNeg(right)\n        case Operator_t(op='*'): return UnaryMul(right)\n        case Operator_t(op='/'): return UnaryDiv(right)\n        case Operator_t(op='not'): return Not(right)  # TODO: don't want to hardcode Bool here!\n        case Operator_t(op='@'): return AtHandle(right)\n\n        # binary operators that appear to be unary because the left can be void\n        # =\u003e called as unary prefix op means left was ()/void\n        case Operator_t(op='=\u003e'): return PrototypeFunctionLiteral(void, right)\n\n        case _:\n            raise ValueError(f\"INTERNAL ERROR: {op=} is not a known unary prefix operator\")\n\n\ndef build_unary_postfix_expr(left: AST, op: Token, scope: Scope) -\u003e AST:\n    \"\"\"create a unary postfix expression AST from the left AST and op token\"\"\"\n    match op:\n        # normal postfix operators\n        case Operator_t(op='!'): raise NotImplementedError(f\"TODO: postfix op: {op=}\")  # return Fact(left)\n\n        # binary operators that appear to be unary because the right can be void\n        # anything juxtaposed with void is treated as a zero-arg call()\n        case Juxtapose_t():\n            return Call(left)\n\n        case _:\n            raise NotImplementedError(f\"TODO: {op=}\")\n\ndef parse_string(token: String_t, scope: Scope) -\u003e String | IString:\n    \"\"\"Convert a string token to an AST\"\"\"\n\n    if len(token.body) == 1 and isinstance(token.body[0], str):\n        return String(token.body[0])\n\n    # else handle interpolation strings\n    parts = []\n    for chunk in token.body:\n        if isinstance(chunk, str):\n            parts.append(chunk)\n        elif isinstance(chunk, Escape_t):\n            parts.append(chunk.to_str())\n        else:\n            # put any interpolation expressions in a new scope\n            ast = parse(chunk.body, scope)\n            if isinstance(ast, Block):\n                parts.append(ast)\n            elif isinstance(ast, ListOfASTs):\n                pdb.set_trace()\n                # not sure if this should ever come up, might be a parse bug\n                # or might just need to convert to a block...\n            else:\n                parts.append(Block([ast]))\n\n    # combine any adjacent Strings into a single string (e.g. if there were escapes)\n    parts = iterchain(*((''.join(g),) if issubclass(t, str) else (*g,) for t, g in groupby(parts, type)))\n    # convert any free strings to ASTs\n    parts = [p if not isinstance(p, str) else String(p) for p in parts]\n\n    # cast because pyright complains\n    parts = cast(list[AST], parts)\n    return IString(parts)\n\n\ndef as_dict_inners(items:list[AST]) -\u003e list[PointsTo] | None:\n    \"\"\"Determine if the inner items indicate the container is a Dict (i.e. all items are points-to)\"\"\"\n    if all(isinstance(i, PointsTo) for i in items):\n        return cast(list[PointsTo], items)\n    return None\n\ndef as_bidir_dict_inners(items:list[AST]) -\u003e list[BidirPointsTo] | None:\n    \"\"\"Determine if the inner items indicate the container is a BidirDict (i.e. all items are bidir-points-to)\"\"\"\n    if all(isinstance(i, BidirPointsTo) for i in items):\n        return cast(list[BidirPointsTo], items)\n    return None\n\ndef as_array_inners(items:list[AST]) -\u003e list[AST] | None:\n    \"\"\"Determine if the inner items indicate the container is an Array (i.e. no points-to, assigns, or declarations)\"\"\"\n    invalid_types = (Declare, Assign, PointsTo, BidirPointsTo)\n    if any(isinstance(i, invalid_types) for i in items):\n        return None\n    return items\n\ndef as_object_inners(items:list[AST]) -\u003e list[AST] | None:\n    \"\"\"determine if the inner items indicate the container is an Object (i.e. no points-to, and should contain at least one assign or declaration)\"\"\"\n    invalid_types = (PointsTo, BidirPointsTo)\n    expected_types = (Assign, Declare)\n    if any(isinstance(i, invalid_types) for i in items):\n        return None\n    if not any(isinstance(i, expected_types) for i in items):\n        return None\n    return items\n\ndef parse_block(block: Block_t, scope: Scope) -\u003e AST:\n    \"\"\"Convert a block token to an AST\"\"\"\n\n    # if new scope block, nest the current scope\n    newscope = block.left == '{' and block.right == '}'\n    if newscope:\n        scope = Scope(scope)\n\n    # parse the inside of the block\n    inner = parse(block.body, scope)\n\n    delims = block.left + block.right\n    match delims, inner:\n        case '()' | '{}' | '[]', Void():\n            return inner\n        case '()', ListOfASTs():\n            return Group(inner.asts)\n        case '{}', ListOfASTs():\n            return Block(inner.asts)\n        case '[]', ListOfASTs():\n            if (asts:=as_dict_inners(inner.asts)) is not None:\n                return Dict(asts)\n            elif (asts:=as_bidir_dict_inners(inner.asts)) is not None:\n                return BidirDict(asts)\n            elif (asts:=as_array_inners(inner.asts)) is not None:\n                return Array(asts)\n            elif (asts:=as_object_inners(inner.asts)) is not None:\n                return Object(inner.asts)\n            #error cases\n            if any(isinstance(i, PointsTo) for i in inner.asts) and not all(isinstance(i, PointsTo) for i in inner.asts):\n                raise ValueError(f\"ERROR: cannot mix PointsTo with other types in a dict: {inner=}\")\n            #TBD other known cases\n            #otherwise there is an issue with the parser\n            raise ValueError(f\"INTERNAL ERROR: could not determine container type for {inner=}. Should have been suitably disambiguated by parser...\")\n        case '()' | '[]' | '(]' | '[)', BareRange():\n            return Range(inner.left, inner.right, delims)\n\n        # catch all cases for any type of AST inside a block or range\n        case '()', _:\n            return Group([inner])\n        case '{}', _:\n            return Block([inner])\n        case '[]', PointsTo():\n            return Dict([inner])\n        case '[]', BidirPointsTo():\n            return BidirDict([inner])\n        case '[]', Assign() | Declare():\n            return Object([inner])\n        case '[]', _:\n            # TODO: handle if this should be an object or dictionary instead of an array\n            return Array([inner])\n        case _:\n            pdb.set_trace()\n            raise NotImplementedError(f'block parse not implemented for {block.left+block.right}, {type(inner)}')\n\n\n\ndef parse_type_param(param: TypeParam_t, scope: Scope) -\u003e TypeParam:\n    items = parse(param.body, scope)\n    if isinstance(items, ListOfASTs):\n        return TypeParam(items.asts)\n    return TypeParam([items])\n\n\ndef parse_flow(flow: Flow_t, scope: Scope) -\u003e Flowable:\n\n    # special case for closing else clause in a flow chain. Treat as `\u003cif\u003e \u003ctrue\u003e \u003cclause\u003e`\n    if flow.keyword is None:\n        return Default(parse_chain(flow.clause, scope))\n\n    assert flow.condition is not None, f\"ERROR: flow condition must be present for {flow=}\"\n    cond = parse_chain(flow.condition, scope)\n    clause = parse_chain(flow.clause, scope)\n\n    match flow.keyword:\n        case Keyword_t(src='if'): return If(cond, clause)\n        case Keyword_t(src='loop'): return Loop(cond, clause)\n        case _:\n            pdb.set_trace()\n            ...\n            raise NotImplementedError('TODO: other flow keywords, namely lazy')\n    pdb.set_trace()\n    ...\n\n\ndef parse_declare(declare: Declare_t, scope: Scope) -\u003e Declare:\n    expr = parse_chain(declare.expr, scope)\n    assert isinstance(expr, (PrototypeIdentifier, Identifier, TypedIdentifier, ReturnTyped, UnpackTarget, Assign)), f'ERROR: expected identifier, typed-identifier, or unpack target for declare expression, got {expr=}'\n    match declare:\n        case Declare_t(keyword=Keyword_t(src='let')): return Declare(DeclarationType.LET, expr)\n        case Declare_t(keyword=Keyword_t(src='const')): return Declare(DeclarationType.CONST, expr)\n        # case Declare_t(keyword=Keyword_t(src='local_const')): return Declare(DeclarationType.LOCAL_CONST, expr)\n        # case Declare_t(keyword=Keyword_t(src='fixed_type')): return Declare(DeclarationType.FIXED_TYPE, expr)\n        case _:\n            raise ValueError(f\"ERROR: unknown declare keyword {declare.keyword=}. Expected one of {DeclarationType.__members__}. {declare=}\")\n    pdb.set_trace()\n    raise NotImplementedError\n\n\n\n\n\n\n################################ Docs Markdown Helpers ################################\nopname_map = {\n    '@': 'reference',\n    '.': 'access',\n    '^': 'power',\n    '*': 'multiply',\n    '/': 'divide',\n    '%': 'modulus',\n    '+': 'add',\n    '-': 'subtract',\n    '\u003c\u003c': 'left shift',\n    '\u003e\u003e': 'right shift',\n    '\u003e\u003e\u003e': 'rotate left no carry',\n    '\u003c\u003c\u003c': 'rotate right no carry',\n    '\u003c\u003c!': 'rotate left with carry',\n    '!\u003e\u003e': 'rotate right with carry',\n    '\u003e?': 'greater than',\n    '\u003c?': 'less than',\n    '\u003e=?': 'greater than or equal',\n    '\u003c=?': 'less than or equal',\n    '=?': 'equal',\n    'and': 'and',\n    'nand': 'nand',\n    '\u0026': 'and',\n    'xor': 'xor',\n    'xnor': 'xnor',\n    'or': 'or',\n    'nor': 'nor',\n    '|': 'or',\n    '=\u003e': 'function arrow',\n    '=': 'bind',\n    'else': 'flow alternate',\n    ';': 'semicolon',\n    'in': 'in',\n    'as': 'as',\n    'transmute': 'transmute',\n    '|\u003e': 'pipe',\n    '\u003c|': 'reverse pipe',\n    '-\u003e': 'right pointer',\n    '\u003c-\u003e': 'bidir pointer',\n    '\u003c-': 'left pointer',\n    ':': 'type annotation',\n\n    Comma_t(None): 'comma',\n    Juxtapose_t(None): 'unknown juxtapose',\n    EllipsisJuxtapose_t(None): 'ellipsis juxtapose',\n    RangeJuxtapose_t(None): 'range juxtapose',\n    TypeParamJuxtapose_t(None): 'type param juxtapose',\n}\n\n\ndef get_precedence_table_markdown() -\u003e str:\n    \"\"\"return a string that is the markdown table for the docs containing all the operators\"\"\"\n    header = '| Precedence | Operator | Name | Associativity |\\n| --- | --- | --- | --- |'\n\n    def get_ops_str(ops: list[Operator_t | ShiftOperator_t | Juxtapose_t | Comma_t]) -\u003e str:\n        return '\u003cbr\u003e'.join(f'`{op.op if isinstance(op, (Operator_t, ShiftOperator_t)) else op.__class__.__name__[:-2].lower()}`' for op in ops)\n\n    def get_opnames_str(ops: list[Operator_t | ShiftOperator_t | Juxtapose_t | Comma_t]) -\u003e str:\n        return '\u003cbr\u003e'.join(f'{opname_map.get(op.op, None) if isinstance(op, (Operator_t, ShiftOperator_t)) else op.__class__.__name__[:-2].lower()}' for op in ops)\n\n    def get_row_str(row: tuple[Associativity, list[Operator_t | ShiftOperator_t | Juxtapose_t | Comma_t]]) -\u003e str:\n        assoc, group = row\n        return f'{get_ops_str(group)} | {get_opnames_str(group)} | {assoc.name}'\n\n    rows = [\n        f'| {i} | {get_row_str(row)} |'\n        for i, row in reversed([*enumerate(operator_groups)])\n    ]\n\n    return header + '\\n' + '\\n'.join(rows)\n"])</script><script>self.__next_f.push([1,"1e:T53e7,"])</script><script>self.__next_f.push([1,"from .tokenizer import (\n    tokenize, tprint, full_traverse_tokens,\n    unary_prefix_operators,\n    unary_postfix_operators,\n    binary_operators,\n    opchain_starters,\n    Token,\n    Keyword_t, Undefined_t, Void_t, End_t,\n    WhiteSpace_t, Escape_t,\n    Identifier_t, Hashtag_t,\n    Block_t, TypeParam_t,\n    RawString_t, String_t,\n    Integer_t, BasedNumber_t, Boolean_t,\n    DotDot_t, DotDotDot_t,\n    Juxtapose_t, Operator_t, ShiftOperator_t, Comma_t,\n)\n\nfrom typing import Generator, overload, cast\nfrom abc import ABC\n\n\nimport pdb\n\n\n# A chain is just a list of tokens that is known to be directly parsable as an expression without any other syntax\n# i.e. it is the result of calls to `get_next_chain()`\n# all other syntax is wrapped up into compound tokens\n# it should literally just be a sequence of atoms and operators\nfrom typing import TypeVar\nT = TypeVar('T', bound=Token)\nclass Chain(list[T]):\n    \"\"\"class for explicitly annotating that a token list is a single chain\"\"\"\n\n# class Chain[T](list[T]):\n#     \"\"\"class for explicitly annotating that a token list is a single chain\"\"\"\n\n\n############### NEW TOKENS CREATED BY POST-TOKENIZATION PROCESS ###############\n\nclass Flow_t(Token):\n    @overload\n    def __init__(self, keyword: None, condition: None, clause: Chain[Token]): ...  # closing else\n    @overload\n    def __init__(self, keyword: Keyword_t, condition: Chain[Token], clause: Chain[Token]): ...  # if, loop, lazy\n\n    def __init__(self, keyword: Keyword_t | None, condition: Chain[Token] | None, clause: Chain[Token]):\n        if keyword is None and condition is not None:\n            raise ValueError(\"closing else should have no condition. `keyword` and `condition` should both be None\")\n        self.keyword = keyword\n        self.condition = condition\n        self.clause = clause\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cFlow_t: {self.keyword}: {self.condition} {self.clause}\u003e\"\n\n    def __iter__(self) -\u003e Generator[list[Token], None, None]:\n        if self.condition is not None:\n            yield self.condition\n        yield self.clause\n\n\n# class Do_t(Token):...\n# class Return_t(Token):...\n# class Express_t(Token):...\n\n\nclass Declare_t(Token):\n    def __init__(self, keyword: Keyword_t, expr: Chain[Token]):\n        self.keyword = keyword\n        self.expr = expr\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cDeclare_t: {self.keyword} {self.expr}\u003e\"\n\n    def __iter__(self) -\u003e Generator[list[Token], None, None]:\n        yield [self.keyword] #appraently flow doesn't yield the keyword. tbd if it matters...\n        yield self.expr\n\n\nclass RangeJuxtapose_t(Operator_t):\n    def __init__(self, _):\n        super().__init__('')\n\n    def __repr__(self) -\u003e str:\n        return \"\u003cRangeJuxtapose_t\u003e\"\n\n    def __hash__(self) -\u003e int:\n        return hash(RangeJuxtapose_t)\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, RangeJuxtapose_t)\n\n\nclass EllipsisJuxtapose_t(Operator_t):\n    def __init__(self, _):\n        super().__init__('')\n\n    def __repr__(self) -\u003e str:\n        return \"\u003cEllipsisJuxtapose_t\u003e\"\n\n    def __hash__(self) -\u003e int:\n        return hash(EllipsisJuxtapose_t)\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, EllipsisJuxtapose_t)\n\n\nclass TypeParamJuxtapose_t(Operator_t):\n    def __init__(self, _):\n        super().__init__('')\n\n    def __repr__(self) -\u003e str:\n        return \"\u003cTypeParamJuxtapose_t\u003e\"\n\n    def __hash__(self) -\u003e int:\n        return hash(TypeParamJuxtapose_t)\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, TypeParamJuxtapose_t)\n\nclass OpChain_t(Token):\n    def __init__(self, ops:list[Operator_t]):\n        assert len(ops) \u003e 1, f\"OpChain_t must have at least 2 operators. Got {len(ops)} operators\"\n        self.ops = ops\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cOpChain_t: {''.join(op.op for op in self.ops)}\u003e\"\n\n    def __hash__(self) -\u003e int:\n        return hash((OpChain_t, tuple(self.ops)))\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, OpChain_t) and self.ops == other.ops\n\n    def __iter__(self) -\u003e Generator[list[Token], None, None]:\n        yield cast(list[Token], self.ops)\n\nclass VectorizedOp_t(Token):\n    def __init__(self, dot:Operator_t, op:Operator_t|OpChain_t):\n        assert isinstance(dot, Operator_t) and dot.op == '.', f\"VectorizedOp_t must have a '.' operator. Got {dot}\"\n        self.dot = dot\n        self.op = op\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cVectorizedOp_t: {self.dot}, {self.op}\u003e\"\n\n    def __hash__(self) -\u003e int:\n        return hash((VectorizedOp_t, self.dot, self.op))\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, VectorizedOp_t) and self.dot == other.dot and self.op == other.op\n\n    def __iter__(self) -\u003e Generator[list[Token], None, None]:\n        yield [self.dot]\n        yield [self.op]\n\n\nclass CombinedAssignmentOp_t(Token):\n    def __init__(self, op:Operator_t, assign:Operator_t):\n        assert isinstance(assign, Operator_t) and assign.op == '=', f\"CombinedAssignmentOp_t must have an '=' operator. Got {assign}\"\n        self.op = op\n        self.assign = assign\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cCombinedAssignmentOp_t: {self.op}, {self.assign}\u003e\"\n\n    def __hash__(self) -\u003e int:\n        return hash((CombinedAssignmentOp_t, self.op, self.assign))\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, CombinedAssignmentOp_t) and self.op == other.op and self.assign == other.assign\n\n    def __iter__(self) -\u003e Generator[list[Token], None, None]:\n        yield [self.op]\n        yield [self.assign]\n\n\natom_tokens = (\n    Identifier_t,\n    Integer_t,\n    Boolean_t,\n    BasedNumber_t,\n    RawString_t,\n    String_t,\n    Block_t,\n    TypeParam_t,\n    Hashtag_t,\n    DotDot_t,\n    DotDotDot_t,\n    Flow_t,\n    Undefined_t,\n)\n\n\nclass ShouldBreakTracker(ABC):\n    def op_breaks_chain(self, token: Token) -\u003e bool:\n        raise NotImplementedError(\"op_breaks_chain must be implemented by subclass\")\n\n    def view(self, tokens: list[Token]) -\u003e None:\n        raise NotImplementedError(\"view must be implemented by subclass\")\n\n\nclass ShouldBreakFlowTracker(ShouldBreakTracker):\n    def __init__(self):\n        self.flows_seen = 0\n\n    def op_breaks_chain(self, token: Token) -\u003e bool:\n        # should only be operators\n        if isinstance(token, Operator_t) and token.op == 'else':\n            if self.flows_seen == 0:\n                return True\n            self.flows_seen -= 1\n\n        return False\n\n    def view(self, tokens: list[Token]) -\u003e None:\n        # view each token without any ability to do anything\n        # keep track of how many flows we've seen\n        for token in tokens:\n            if isinstance(token, Flow_t) and token.keyword is not None:\n                self.flows_seen += 1\n            if isinstance(token, Operator_t) and token.op == 'else':\n                raise ValueError(\"should not be seeing else here\")\n            if isinstance(token, Keyword_t) and token.src in ('if', 'loop', 'lazy'):\n                raise ValueError(\"should not be seeing if/loop/lazy here. Everything should be bundled up into a flow\")\n\n\ndef invert_whitespace(tokens: list[Token]) -\u003e None:\n    \"\"\"\n    removes all whitespace tokens, and insert juxtapose tokens between adjacent pairs (i.e. not separated by whitespace)\n\n    Args:\n        tokens (list[Token]): list of tokens to modify. This is modified in place.\n    \"\"\"\n\n    # juxtapose singleton token so we aren't wasting memory\n    jux = Juxtapose_t(None)\n\n    i = 0\n    while i \u003c len(tokens):\n        # delete whitespace if it comes up\n        if isinstance(tokens[i], WhiteSpace_t):\n            tokens.pop(i)\n            continue\n\n        # recursively handle inverting whitespace for blocks\n        if isinstance(tokens[i], (Block_t, TypeParam_t)):\n            invert_whitespace(tokens[i].body)\n        elif isinstance(tokens[i], String_t):\n            for child in tokens[i].body:\n                if isinstance(child, Block_t):\n                    invert_whitespace(child.body)\n\n        # insert juxtapose if no whitespace between tokens\n        if i + 1 \u003c len(tokens) and not isinstance(tokens[i + 1], WhiteSpace_t):\n            tokens.insert(i + 1, jux)\n            i += 1\n        i += 1\n\n    # finally, remove juxtapose tokens next to operators that are not whitespace sensitive\n    i = 1\n    while i \u003c len(tokens) - 1:\n        left, middle, right = tokens[i-1:i+2]\n        if isinstance(middle, Juxtapose_t) and (isinstance(left, (Operator_t, ShiftOperator_t, Comma_t)) or isinstance(right, (Operator_t, ShiftOperator_t, Comma_t))):\n            tokens.pop(i)\n            continue\n        i += 1\n\n\ndef _get_next_prefixes(tokens: list[Token]) -\u003e tuple[list[Token], list[Token]]:\n    prefixes = []\n    # isinstance(tokens[0], Operator_t) and tokens[0].op in unary_prefix_operators:\n    while len(tokens) \u003e 0 and is_unary_prefix_op(tokens[0]):\n        prefixes.append(tokens.pop(0))\n    return prefixes, tokens\n\n\ndef _get_next_postfixes(tokens: list[Token]) -\u003e tuple[list[Token], list[Token]]:\n    postfixes = []\n    # isinstance(tokens[0], Operator_t) and tokens[0].op in unary_postfix_operators - {';'}:\n    while len(tokens) \u003e 0 and is_unary_postfix_op(tokens[0], exclude_semicolon=True):\n        postfixes.append(tokens.pop(0))\n    return postfixes, tokens\n\n\ndef _get_next_atom(tokens: list[Token]) -\u003e tuple[Token, list[Token]]:\n    if len(tokens) == 0:\n        raise ValueError(f\"ERROR: expected atom, got {tokens=}\")\n\n    # TODO: this is going to be unnecessary as expressions will have been bundled up into single tokens\n    if isinstance(tokens[0], Keyword_t):\n        return _get_next_keyword_expr(tokens)\n\n    # (Integer_t, BasedNumber_t, String_t, RawString_t, Identifier_t, Hashtag_t, Block_t, TypeParam_t, DotDot_t)):\n    if isinstance(tokens[0], atom_tokens):\n        return tokens[0], tokens[1:]\n\n    raise ValueError(f\"ERROR: expected atom, got {tokens[0]=}\")\n\n\ndef _get_next_chunk(tokens: list[Token]) -\u003e tuple[list[Token], list[Token]]:\n    chunk = []\n    t, tokens = _get_next_prefixes(tokens)\n    chunk.extend(t)\n\n    t, tokens = _get_next_atom(tokens)\n    if t is None:\n        raise ValueError(f\"ERROR: expected atom, got {tokens[0]=}\")\n    chunk.append(t)\n\n    t, tokens = _get_next_postfixes(tokens)\n    chunk.extend(t)\n\n    return chunk, tokens\n\n\ndef is_unary_prefix_op(token: Token) -\u003e bool:\n    \"\"\"\n    Determines if a token could be a unary prefix operator.\n    Note that this is not mutually exclusive with being a postfix operator or a binary operator.\n    \"\"\"\n    return isinstance(token, Operator_t) and token.op in unary_prefix_operators\n\n\ndef is_unary_postfix_op(token: Token, exclude_semicolon: bool = False) -\u003e bool:\n    \"\"\"\n    Determines if a token could be a unary postfix operator.\n    Optionally can exclude semicolon from the set of operators.\n    Note that this is not mutually exclusive with being a prefix operator or a binary operator.\n    \"\"\"\n    if exclude_semicolon:\n        return isinstance(token, Operator_t) and token.op in unary_postfix_operators - {';'}\n    return isinstance(token, Operator_t) and token.op in unary_postfix_operators\n\n\ndef is_binop(token: Token) -\u003e bool:\n    \"\"\"\n    Determines if a token could be a binary operator.\n    Note that this is not mutually exclusive with being a prefix operator or a postfix operator.\n    \"\"\"\n    return isinstance(token, Operator_t) and token.op in binary_operators or isinstance(token, (ShiftOperator_t, Comma_t, Juxtapose_t, RangeJuxtapose_t, EllipsisJuxtapose_t, TypeParamJuxtapose_t))\n\n\ndef is_op(token: Token) -\u003e bool:\n    return is_binop(token) or is_unary_prefix_op(token) or is_unary_postfix_op(token)\n\n\ndef is_opchain_starter(token: Token) -\u003e bool:\n    return isinstance(token, Operator_t) and token.op in opchain_starters\n\n\ndef _get_next_keyword_expr(tokens: list[Token]) -\u003e tuple[Token, list[Token]]:\n    \"\"\"package up the next keyword expression into a single token\"\"\"\n    if len(tokens) == 0:\n        raise ValueError(f\"ERROR: expected keyword expression, got {tokens=}\")\n    t, tokens = tokens[0], tokens[1:]\n\n    if not isinstance(t, Keyword_t):\n        raise ValueError(f\"ERROR: expected keyword expression, got {t=}\")\n\n    match t:\n        case Keyword_t(src='if' | 'loop' | 'lazy'):\n            cond, tokens = get_next_chain(tokens)\n            clause, tokens = get_next_chain(tokens, tracker=ShouldBreakFlowTracker())\n            return Flow_t(t, cond, clause), tokens\n        case Keyword_t(src='closing_else'):\n            clause, tokens = get_next_chain(tokens, tracker=ShouldBreakFlowTracker())\n            return Flow_t(None, None, clause), tokens\n        case Keyword_t(src='do'):\n            clause, tokens = get_next_chain(tokens)\n            # assert next token is a do_keyward\n            # depending on the keyward, get a condition, or condition+clause\n            pdb.set_trace()\n            ...\n        case Keyword_t(src='return'):\n            # TBD how to do this one...\n            pdb.set_trace()\n            ...\n        case Keyword_t(src='express'):\n            pdb.set_trace()\n            ...\n        case Keyword_t(src='let' | 'const' | 'local_const' | 'fixed_type'):\n            expr, tokens = get_next_chain(tokens)\n            return Declare_t(t, expr), tokens\n\n\n    raise NotImplementedError(\"TODO: handle keyword based expressions\")\n    # return #chain?\n    # yield #chain\n    # (break | continue) #hashtag? //note the hashtag should be an entire chain if present\n    # (let | const) #chain\n\n\ndef get_next_chain(tokens: list[Token], *, tracker: ShouldBreakTracker = None, op_blacklist: set[Token] = None) -\u003e tuple[Chain[Token], list[Token]]:\n    \"\"\"\n    grab the next single expression chain of tokens from the given list of tokens\n\n    Also wraps up keyword-based expressions (if loop etc.) into a single token\n\n    A chain is represented by the following grammar:\n        #chunk = #prefix_op* #atom_expr (#postfix_op - ';')*\n        #chain = #chunk (#binary_op #chunk)* ';'?\n\n    Args:\n        tokens (list[Token]): list of tokens to grab the next chain from\n        tracker (ShouldBreakTracker, optional): tracker for complex analysis to determine if an operator should break the chain. Defaults to None.\n        op_blacklist (set[Token], optional): simpler handler for operators that should break the chain. Defaults to None.\n\n    Returns:\n        next, rest (list[Token], list[Token]): the next chain of tokens, and the remaining tokens\n    \"\"\"\n\n    if op_blacklist is None:\n        op_blacklist = set()\n\n    chain = []\n\n    # grab the first chunk and let the tracker view it\n    chunk, tokens = _get_next_chunk(tokens)\n    chain.extend(chunk)\n    if tracker is not None:\n        tracker.view(chunk)\n\n    while len(tokens) \u003e 0 and is_binop(tokens[0]) and (tracker is None or not tracker.op_breaks_chain(tokens[0])) and tokens[0] not in op_blacklist:\n        # get the operator, and continuing chunk, then let the tracker view it\n        chain.append(tokens.pop(0))\n        chunk, tokens = _get_next_chunk(tokens)\n        chain.extend(chunk)\n        if tracker is not None:\n            tracker.view(chunk)\n\n    # if there's a semicolon, it ends the chain\n    if len(tokens) \u003e 0 and isinstance(tokens[0], Operator_t) and tokens[0].op == ';':\n        chain.append(tokens.pop(0))\n\n    return Chain(chain), tokens\n\n\ndef narrow_juxtapose(tokens: list[Token]) -\u003e None:\n    \"\"\"\n    range juxtapose:\n    convert [\u003ctoken\u003e, \u003cjux\u003e, \u003c..\u003e] into [\u003ctoken\u003e, \u003crange_jux\u003e, \u003c..\u003e]\n    convert [\u003c..\u003e, \u003cjux\u003e, \u003ctoken\u003e] into [\u003c..\u003e, \u003crange_jux\u003e, \u003ctoken\u003e]\n    if .. doesn't connect to anything on the left or right, connect it to undefined\n\n    ellipsis juxtapose:\n    convert [\u003c...\u003e, \u003cjux\u003e, \u003ctoken\u003e] into [\u003c...\u003e, \u003cellipsis_jux\u003e, \u003ctoken\u003e]\n\n    type param juxtapose:\n    convert [\u003ctoken\u003e, \u003cjux\u003e, \u003ctype_param\u003e] into [\u003ctoken\u003e, \u003ctype_param_jux\u003e, \u003ctype_param\u003e]\n    convert [\u003ctype_param\u003e, \u003cjux\u003e, \u003ctoken\u003e] into [\u003ctype_param\u003e, \u003ctype_param_jux\u003e, \u003ctoken\u003e]\n    \"\"\"\n    range_jux = RangeJuxtapose_t(None)\n    ellipsis_jux = EllipsisJuxtapose_t(None)\n    type_param_jux = TypeParamJuxtapose_t(None)\n    undefined = Undefined_t(None)\n    for i, token, stream in (gen := full_traverse_tokens(tokens)):\n        # handle range jux\n        if isinstance(token, DotDot_t):\n            if i + 1 \u003c len(stream):\n                if isinstance(stream[i+1], Juxtapose_t):\n                    stream[i+1] = range_jux\n                else:\n                    stream[i+1:i+1] = [range_jux, undefined]\n            if i \u003e 0:\n                if isinstance(stream[i-1], Juxtapose_t):\n                    stream[i-1] = range_jux\n                else:\n                    stream[i:i] = [undefined, range_jux]\n                    gen.send(i+3)\n\n        # handle ellipsis jux\n        elif isinstance(token, DotDotDot_t):\n            if i + 1 \u003c len(stream) and isinstance(stream[i+1], Juxtapose_t):\n                stream[i+1] = ellipsis_jux\n\n        # handle type param jux\n        elif isinstance(token, TypeParam_t):\n            if i \u003e 0 and isinstance(stream[i-1], Juxtapose_t):\n                stream[i-1] = type_param_jux\n            elif i + 1 \u003c len(stream) and isinstance(stream[i+1], Juxtapose_t):\n                stream[i+1] = type_param_jux\n\n\ndef convert_bare_else(tokens: list[Token]) -\u003e None:\n    \"\"\"\n    convert any instances of `else` without a flow keyword after, and convert to `else` `if` `true`\n    \"\"\"\n    for i, token, stream in (gen := full_traverse_tokens(tokens)):\n        if isinstance(token, Operator_t) and token.op == 'else':\n            if i+1 \u003c len(stream) and isinstance(stream[i+1], Keyword_t) and stream[i+1].src in ('if', 'loop', 'lazy'):\n                continue\n            # stream[i+1:i+1] = [Keyword_t('if'), Boolean_t('true')]\n            stream.insert(i+1, Keyword_t('closing_else'))\n\n\ndef bundle_conditionals(tokens: list[Token]) -\u003e None:\n    \"\"\"\n    Convert sequences of tokens that represent conditionals (if, loop, etc.) into a single expression token\n    \"\"\"\n    for i, token, stream in (gen := full_traverse_tokens(tokens)):\n        if isinstance(token, Keyword_t) and token.src in ('if', 'loop', 'lazy'):\n            flow_chain, tokens = get_next_chain(stream[i:])\n            stream[i] = flow_chain[0]\n            stream[i+1:] = [*flow_chain[1:], *tokens]\n\n\ndef chain_operators(tokens: list[Token]) -\u003e None:\n    \"\"\"Convert consecutive operator tokens into a single opchain token\"\"\"\n    \"\"\"\n    A chain is represented by the following grammar:\n        #chunk = #prefix_op* #atom_expr (#postfix_op - ';')*\n        #chain = #chunk (#binary_op #chunk)* ';'?\n\n        #prefix_op = '+' | '-' | '*' | '/' | 'not' | '@' | '...'\n        #postfix_op = '?' | '`' | ';'\n        #binary_op = '+' | '-' | '*' | '/' | '%' | '^'\n          | '=?' | '\u003e?' | '\u003c?' | '\u003e=?' | '\u003c=?' | 'in?' | 'is?' | 'isnt?' | '\u003c=\u003e'\n          | '|' | '\u0026'\n          | 'and' | 'or' | 'nand' | 'nor' | 'xor' | 'xnor' | '??'\n          | '=' | ':=' | 'as' | 'in' | 'transmute'\n          | '@?'\n          | '|\u003e' | '\u003c|' | '=\u003e'\n          | '-\u003e' | '\u003c-\u003e' | '\u003c-'\n          | '.' | ':'\n    \"\"\"\n\n    # TODO: skip for now. not needed by hello world\n    # also may not be necessary if we use a pratt parser. was necessary for split by lowest precedence parser\n\n    for i, token, stream in (gen := full_traverse_tokens(tokens)):\n\n        # TODO: this is not a correct way to detect these. need to verify that the operators are in between two #chunks\n        #   this will be conservative, but for now it will let us do a hello world happy path\n        if is_opchain_starter(token):\n            j = 1\n            while i+j \u003c len(stream) and is_unary_prefix_op(stream[i+j]):\n                j += 1\n            if j \u003e 1:\n                pdb.set_trace()\n                raise NotImplementedError('opchaining has not been implemented yet')\n\n\ndef post_process(tokens: list[Token]) -\u003e None:\n    \"\"\"post process the tokens to make them ready for parsing\"\"\"\n\n    # remove whitespace, and insert juxtapose tokens\n    invert_whitespace(tokens)\n\n    if len(tokens) == 0:\n        return\n\n    # find any instances of \u003celse\u003e without a flow keyword after, and convert to \u003celse\u003e \u003cif\u003e \u003ctrue\u003e\n    convert_bare_else(tokens)\n\n    # bundle up conditionals into single token expressions\n    bundle_conditionals(tokens)\n\n    # combine operator chains into a single operator token\n    chain_operators(tokens)\n\n    # convert juxtapose tokens to more specific types if possible\n    narrow_juxtapose(tokens)\n\n\n    # make the actual list of chains\n\n    # based on types, replace jux with jux_mul or jux_call\n    # TODO: actually this probably would need to be done during parsing, since we can't get a type for a complex/compound expression...\n\n\ndef test():\n    with open('../../../examples/hello.dewy') as f:\n        src = f.read()\n\n    tokens = tokenize(src)\n\n    # chainer process\n    post_process(tokens)\n\n    pdb.set_trace()\n    ...\n\n\ndef test2():\n    \"\"\"gauntlet of multiple tests from example file\"\"\"\n    with open('../../../examples/syntax3.dewyl') as f:\n        lines = f.readlines()\n\n    # filter out empty lines\n    lines = [l for line in lines if (l := line.strip())]\n\n    for line in lines:\n        tokens = tokenize(line)\n\n        # chainer process\n        post_process(tokens)\n\n        # other stuff? pass to the parser? etc.\n\n    pdb.set_trace()\n    ...\n\n\ndef test_hello():\n    line = \"printl'Hello, World!'\"\n\n    tokens = tokenize(line)\n    post_process(tokens)\n\n    pdb.set_trace()\n    ...\n\n\nif __name__ == '__main__':\n    # test()\n    # test2()\n    test_hello()\n"])</script><script>self.__next_f.push([1,"1f:T29d2,"])</script><script>self.__next_f.push([1,"\"\"\"after the main parsing, post parse to handle any remaining prototype asts within the main ast\"\"\"\n\nfrom .syntax import (\n    AST,\n    Access,\n    Declare,\n    PointsTo, BidirPointsTo,\n    Type,\n    ListOfASTs, PrototypeTuple, Block, BareRange, Ellipsis, Spread, Array, Group, Range, Object, Dict, BidirDict, TypeParam,\n    Void, Undefined, void, undefined, untyped,\n    String, IString,\n    Flowable, Flow, If, Loop, Default,\n    PrototypeFunctionLiteral, PrototypePyAction, Call,\n    Index,\n    PrototypeIdentifier, Express, Identifier, TypedIdentifier, ReturnTyped, UnpackTarget, Assign,\n    Int, Bool,\n    Range, IterIn,\n    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,\n    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,\n    Add, Sub, Mul, Div, IDiv, Mod, Pow,\n    And, Or, Xor, Nand, Nor, Xnor,\n    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv, AtHandle,\n    DeclarationType,\n    DeclareGeneric, Parameterize,\n)\n\nfrom typing import Callable as TypingCallable\nfrom dataclasses import field\nimport pdb\n\n\nclass Signature(AST):\n    pkwargs: list[AST] = field(default_factory=list)\n    pargs:   list[AST] = field(default_factory=list)\n    kwargs:  list[AST] = field(default_factory=list)\n    #TODO: probably keep track of spread args i.e. \"spargs\"\n\n    def _is_delimited(self) -\u003e bool:\n        if any(isinstance(i, Assign) for i in self.pkwargs + self.pargs + self.kwargs):\n            return False\n        return not bool(self.pargs or self.kwargs)\n\n    def __str__(self):\n        pkwargs = ' '.join(str(i) for i in self.pkwargs)\n        pargs   = ' '.join(str(i) for i in self.pargs)\n        kwargs  = ' '.join(str(i) for i in self.kwargs)\n\n        if pargs:\n            pargs = f' #pos_only {pargs}'\n        if kwargs:\n            kwargs = f' #kw_only {kwargs}'\n\n        s = f'{pkwargs}{pargs}{kwargs}'.strip()\n\n        if self._is_delimited():\n            return s\n        return f'({s})'\n\n\n# basically just convert all the different types of args to a normalized format (i.e. group)\nclass FunctionLiteral(AST):\n    args: Signature\n    body: AST\n\n    def __str__(self):\n        return f'{self.args} =\u003e {self.body}'\n\n\n\ndef post_parse(ast: AST) -\u003e AST:\n\n    # any conversions should probably run simplest to most complex\n    ast = convert_prototype_tuples(ast)\n    ast = convert_bare_ranges(ast)\n    ast = convert_prototype_function_literals(ast)\n    ast = convert_prototype_identifiers(ast)\n\n    # at the end of the post parse process\n    if not ast.is_settled():\n        raise ValueError(f'INTERNAL ERROR: Parse was incomplete. AST still has prototypes\\n{ast!r}')\n\n    return ast\n\n\ndef convert_prototype_identifiers(ast: AST) -\u003e AST:\n    \"\"\"Convert all PrototypeIdentifiers to either Identifier or Express, depending on the context\"\"\"\n    ast = Group([ast])\n    for i in (gen := ast.__full_traversal_iter__()):\n\n        # skip ASTs that don't have any Prototypes\n        if i.is_settled():\n            continue\n\n        # if we ever get to a bare identifier, treat it like an express\n        if isinstance(i, PrototypeIdentifier):\n            gen.send(Express(Identifier(i.name)))\n            continue\n\n        match i:\n            case Call(f=PrototypeIdentifier(name=name), args=args):\n                gen.send(Call(Identifier(name), args))\n            case Call(args=None) | Call(f=AtHandle()): ...\n            # case Call(args=args): ... #TODO: handling when args is not none... generally will be a list of identifiers that need to be converted directly to Identifier\n            case Call():\n                pdb.set_trace()\n                ...\n            case AtHandle(operand=PrototypeIdentifier(name=name)):\n                gen.send(AtHandle(Identifier(name)))\n            case AtHandle():\n                pdb.set_trace()\n                ...          \n            case Assign(left=PrototypeIdentifier(name=name), right=right):\n                gen.send(Assign(Identifier(name), right))\n            case Assign(left=Array() as arr, right=right):\n                target = convert_prototype_to_unpack_target(arr)\n                gen.send(Assign(target, right))\n            case Assign():\n                pdb.set_trace()\n                ...\n            case IterIn(left=PrototypeIdentifier(name=name), right=right):\n                gen.send(IterIn(Identifier(name), right))\n            case IterIn(left=Array() as arr, right=right):\n                target = convert_prototype_to_unpack_target(arr)\n                gen.send(IterIn(target, right))\n            case IterIn():\n                pdb.set_trace()\n                ...\n            case UnpackTarget(): #TODO: may not need this one\n                pdb.set_trace()\n                ...\n            case Declare(decltype=decltype, target=PrototypeIdentifier(name=name)):\n                gen.send(Declare(decltype, Identifier(name)))\n            case Declare(decltype=decltype, target=Array() as arr):\n                pdb.set_trace()\n                ...\n            case Declare(decltype=decltype, target=Group() as group):\n                pdb.set_trace()\n                ...\n            case Declare(): ... # all other declare cases are handled as normal\n            case Index():\n                pdb.set_trace()\n                ...\n            case Access():\n                pdb.set_trace()\n                ...\n\n            # cases that themselves don't get adjusted but may contain nested children that need to be converted\n            case IString() | Group() | Block() | PrototypeTuple() | Array() | Object() | Dict() | BidirDict() | FunctionLiteral() | Signature() | Range() | Loop() | If() | Flow() | Default() \\\n                | PointsTo() | BidirPointsTo() | Equal() | Less() | LessEqual() | Greater() | GreaterEqual() | LeftShift() | RightShift() | LeftRotate() | RightRotate() | LeftRotateCarry() | RightRotateCarry() | Add() | Sub() | Mul() | Div() | IDiv() | Mod() | Pow() | And() | Or() | Xor() | Nand() | Nor() | Xnor() | MemberIn() \\\n                | Not() | UnaryPos() | UnaryNeg() | UnaryMul() | UnaryDiv() \\\n                | TypedIdentifier():\n                ...\n            #TBD cases: Type() | ListOfASTs() | BareRange() | Ellipsis() | Spread() | TypeParam() | Flowable() | Flow() | PrototypePyAction() | PyAction() | Express() | ReturnTyped() | SequenceUnpackTarget() | ObjectUnpackTarget() | DeclarationType() | DeclareGeneric() | Parameterize():\n            case _:  # all others are traversed as normal\n                raise ValueError(f'Unhandled case {type(i)}')\n            #     pdb.set_trace()\n            #     ...\n\n    return ast.items[0]\n\n#TODO: maybe have one of these for Array, Object, Dict, BidirDict depending on what is to be unpacked\n#      hard though because also requires the type of whatever is being unpacked\n#      because array unpack and object unpack can look the same syntactically\ndef convert_prototype_to_unpack_target(ast: Array) -\u003e UnpackTarget:\n    \"\"\"Convert an Array of PrototypeIdentifiers or other ASTs to an UnpackTarget\"\"\"\n    for i in (gen := ast.__full_traversal_iter__()):\n        if i.is_settled():\n            continue\n\n        match i:\n            case PrototypeIdentifier(name=name):\n                gen.send(Identifier(name))\n            case Assign(left=PrototypeIdentifier(name=name), right=right):\n                gen.send(Assign(Identifier(name), right))\n            case Array() as arr:\n                gen.send(convert_prototype_to_unpack_target(arr))\n            case Spread(): ...\n            case TypedIdentifier(): ...\n            case _:\n                raise NotImplementedError(f'Unhandled case {type(i)} in convert_prototype_to_unpack_target')\n\n    return UnpackTarget(ast.items)\n\n\ndef convert_prototype_tuples(ast: AST) -\u003e AST:\n    \"\"\"For now, literally just turn all tuples into arrays\"\"\"\n    ast = Group([ast])\n    for i in (gen := ast.__full_traversal_iter__()):\n        if isinstance(i, PrototypeTuple):\n            gen.send(Array(i.items))\n    return ast.items[0]\n\ndef convert_bare_ranges(ast: AST) -\u003e AST:\n    \"\"\"Convert all BareRanges to Ranges with inclusive bounds\"\"\"\n    ast = Group([ast])\n    for i in (gen := ast.__full_traversal_iter__()):\n        if isinstance(i, BareRange):\n            gen.send(Range(i.left, i.right, '[]'))\n    return ast.items[0]\n\n\ndef convert_prototype_function_literals(ast: AST) -\u003e AST:\n    ast = Group([ast])\n    for i in (gen := ast.__full_traversal_iter__()):\n        if isinstance(i, PrototypeFunctionLiteral):\n            args = normalize_function_args(i.args)\n            gen.send(FunctionLiteral(args, i.body))\n    return ast.items[0]\n\n\ndef normalize_function_arg(arg: AST) -\u003e tuple[list[AST], list[AST], list[AST]]:\n    pkwarg, parg, kwarg = [], [], []\n    match arg:\n        case Void(): ...\n        case PrototypeIdentifier(name=name):\n            pkwarg.append(Identifier(name))\n        case Identifier() | TypedIdentifier() | Assign():\n            pkwarg.append(arg)\n        case Array() as arr:\n            pdb.set_trace()\n            parg.append(convert_prototype_to_unpack_target(arr))\n        # case Spread() | UnpackTarget():\n        #     pdb.set_trace()\n        #     ...\n        # case Dict() | BidirDict():\n        #     pdb.set_trace()\n        #     ...\n        #     #copilot suggested these could be kwargs, though I suspect it won't work (i.e. how is default vs no default handled? name -\u003e void)\n        #     #think about though. could use identifiers directly instead of strings\n\n        case _:\n            raise NotImplementedError(f'normalize_signature not implemented yet for {arg=}')\n\n    return pkwarg, parg, kwarg\n\n\n# def array_items_to_unpack_target(items: list[AST]) -\u003e UnpackTarget:\n#     \"\"\"Convert an Array of ASTs to an UnpackTarget\"\"\"\n#     unpack_items = []\n#     for i in items:\n#         match i:\n#             case Identifier() | PrototypeIdentifier() | TypedIdentifier() | Assign() | Spread() | UnpackTarget():\n#                 unpack_items.append(i)\n#             case Array(items):\n#                 unpack_items.append(array_items_to_unpack_target(items))\n#             case _:\n#                 raise NotImplementedError(f'array_items_to_unpack_target not implemented yet for {i=}')\n#     return UnpackTarget(unpack_items)\n\n\ndef normalize_function_args(signature: AST) -\u003e Signature:\n    \"\"\"Convert all the different function arg syntax options to a normalized format (group)\"\"\"\n    if not isinstance(signature, Group):\n        return Signature(*normalize_function_arg(signature))\n\n    pkwargs, pargs, kwargs = [], [], []\n    for i in signature.items:\n        pkw, p, kw = normalize_function_arg(i)\n        pkwargs.extend(pkw)\n        pargs.extend(p)\n        kwargs.extend(kw)\n    return Signature(pkwargs, pargs, kwargs)\n\n\n"])</script><script>self.__next_f.push([1,"20:T4e7f,"])</script><script>self.__next_f.push([1,"from abc import ABC, abstractmethod, ABCMeta\nfrom typing import get_args, get_origin, Generator, Any, Literal, Union, Type as TypingType, dataclass_transform, Callable as TypingCallable\nfrom types import UnionType\nfrom dataclasses import dataclass, field, fields\nfrom enum import Enum, auto\n# from fractions import Fraction\n\nfrom .tokenizer import Operator_t, escape_whitespace  # TODO: move into utils\n\nimport pdb\n\n\n@dataclass_transform()\nclass AST(ABC):\n    def __init_subclass__(cls: TypingType['AST'], **kwargs):\n        \"\"\"\n        - automatically applies the dataclass decorator with repr=False to AST subclasses\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Apply the dataclass decorator with repr=False to the subclass\n        dataclass(repr=False)(cls)\n\n    # TODO: add property to all ASTs for function complete/locked/etc. meaning it and all children are settled\n    @abstractmethod\n    def __str__(self) -\u003e str:\n        \"\"\"Return a string representation of the AST in a canonical dewy code format\"\"\"\n\n    def __repr__(self) -\u003e str:\n        \"\"\"\n        Returns a string representation of the AST tree with correct indentation for each sub-component\n\n        e.g.\n        SomeAST(prop0=..., prop1=...)\n        ├── child0=SomeSubAST(...)\n        ├── child1=SomeOtherAST(...)\n        │   ├── a=ThisAST(...)\n        │   └── b=ThatAST(...)\n        └── child2=AST2(...)\n            └── something=ThisLastAST(...)\n\n        Where all non-ast attributes of a node are printed on the same line as the node itself\n        and all children are recursively indented a level and printed on their own line\n        \"\"\"\n        return '\\n'.join(self._gentree())\n\n    def _gentree(self, prefix: str = '') -\u003e Generator[str, None, None]:\n        \"\"\"\n        a recursive generator helper function for __repr__\n\n        Args:\n            prefix: str - the string to prepend to each child line (root line already has prefix)\n            name: str - the name of the current node in the tree\n            # draw_branches: bool - whether each item should be drawn with branches or only use whitespace\n\n        Returns:\n            str: the string representation of the AST tree\n        \"\"\"\n        # prefix components:\n        space = '    '\n        branch = '│   '\n        # pointers:\n        tee = '├── '\n        last = '└── '\n\n        attrs_str = ', '.join(f'{k}={v}' for k, v in self.__iter_members__() if not isinstance(v, AST))\n        yield f'{self.__class__.__name__}({attrs_str})'\n        children = tuple((k, v) for k, v in self.__iter_members__() if isinstance(v, AST))\n        pointers = [tee] * (len(children) - 1) + [last]\n        for (k, v), pointer in zip(children, pointers):\n            extension = branch if pointer == tee else space\n            gen = v._gentree(f'{prefix}{extension}')\n            name = f'{k}=' if k else ''\n            yield f'{prefix}{pointer}{name}{next(gen)}'     # first line gets name and pointer\n            yield from gen                                  # rest of lines already have a prefix\n\n    def __iter_members__(self) -\u003e Generator[tuple[str, Any], 'AST', None]:\n        \"\"\"\n        A method for getting all properties on the AST instance (including child ASTs, and non-AST properties).\n        Returns a generator of tuples of the form (property_name, property_value)\n        Allows replacing the current AST with a new one during iteration via .send()\n        NOTE: Does not recurse into child ASTs\n        \"\"\"\n        for key, value in self.__dict__.items():\n            # any direct children are ASTs\n            if isinstance(value, AST):\n                replacement = yield key, value\n                if replacement is not None:\n                    setattr(self, key, replacement)\n                    yield\n\n            # any direct children are containers of ASTs\n            elif is_ast_container(self.__class__.__annotations__.get(key)):\n                if value is None:\n                    continue\n                if isinstance(value, list):\n                    for i, item in enumerate(value):\n                        replacement = yield '', item\n                        if replacement is not None:\n                            value[i] = replacement\n                            yield\n                # elif isinstance(value, some_other_container_type): ...\n                else:\n                    raise NotImplementedError(f'__iter_members__ over {type(value)} (from member \"{key}\") of {self} is not yet implemented')\n\n            # properties that are not ASTs\n            else:\n                _ = yield key, value\n                assert _ is None, f'ILLEGAL: attempted to replace non-AST value \"{key}\" during __iter_members__ for ast {self}'\n\n    def __iter__(self) -\u003e Generator['AST', None, None]:\n        \"\"\"DEPRECATED: Use __iter_asts__ instead\"\"\"\n        raise DeprecationWarning(f'__iter__ is deprecated. Use __iter_asts__ instead')\n\n    def __iter_asts__(self) -\u003e Generator['AST', None, None]:\n        \"\"\"Return a generator of the direct children ASTs of the AST\"\"\"\n        for _, child in self.__iter_members__():\n            if isinstance(child, AST):\n                yield child\n\n\n    def __full_traversal_iter__(self) -\u003e Generator['AST', 'AST', None]:\n        \"\"\"\n        Recursive in-order traversal of all child ASTs of the current AST instance\n        Has ability to replace the current AST with a new one during iteration via .send()\n        \"\"\"\n        for _, child in (gen := self.__iter_members__()):\n            if isinstance(child, AST):\n                replacement = yield child\n                if replacement is not None:\n                    gen.send(replacement)\n                    yield\n                    child = replacement # allow traversal over all children of the replacement\n                yield from child.__full_traversal_iter__()\n\n\n    def is_settled(self) -\u003e bool:\n        \"\"\"Return True if the neither the AST, nor any of its descendants, are prototypes\"\"\"\n        for child in self.__iter_asts__():\n            if not child.is_settled():\n                return False\n        return True\n\n\ndef is_ast_container(type_hint: TypingType | None) -\u003e bool:\n    \"\"\"\n    Determine if the type hint is a container of ASTs.\n    e.g. list[AST], set[SomeSubclassOfAST|OtherSubclassOfAST], etc.\n\n    Args:\n        type_hint: TypingType | None - the type hint to check. If None, returns False\n\n    Returns:\n        bool: True if any of the contained types are subclasses of AST, False otherwise\n    \"\"\"\n    if type_hint is None:\n        return False\n\n    # python callables are not containers regardless of if they take in or return ASTs\n    if get_origin(type_hint) == get_origin(TypingCallable):\n        return False\n\n\n    # Iterate over all contained types\n    args = get_args(type_hint)\n    for arg in args:\n        # Handle Union types (e.g., Union[B, C] or B | C)\n        if get_origin(arg) is Union:\n            if any(issubclass(sub_arg, AST) for sub_arg in get_args(arg) if isinstance(sub_arg, type)):\n                return True\n        elif isinstance(arg, UnionType):\n            if any(issubclass(sub_arg, AST) for sub_arg in arg.__args__ if isinstance(sub_arg, type)):\n                return True\n        # Check if the argument itself is a subclass of the base class\n        elif isinstance(arg, type) and issubclass(arg, AST):\n            return True\n\n    # no AST subclasses found\n    return False\n\n\nclass PrototypeAST(AST, ABC):\n    \"\"\"Used to represent AST nodes that are not complete, and must be removed before the whole AST is evaluated\"\"\"\n\n    def is_settled(self) -\u003e bool:\n        \"\"\"By definition, prototypes are not settled\"\"\"\n        return False\n\n\nclass Delimited(ABC):\n    \"\"\"used to track which ASTs are printed with their own delimiter so they can be juxtaposed without extra parentheses\"\"\"\n\nclass Type(AST):\n    name: str\n    parameters: list = field(default_factory=list)\n\n    def __str__(self) -\u003e str:\n        if self.parameters:\n            return f'{self.name}\u003c{\", \".join(map(str, self.parameters))}\u003e'\n        return self.name\n\n\n# TODO: turn into a singleton...\n# untyped type for when a declaration doesn't specify a type\nuntyped = Type('untyped')\n\n\nclass Undefined(AST):\n    \"\"\"undefined singleton\"\"\"\n    def __new__(cls):\n        if not hasattr(cls, 'instance'):\n            cls.instance = super(Undefined, cls).__new__(cls)\n        return cls.instance\n\n    def __str__(self) -\u003e str:\n        return 'undefined'\n\n\n# undefined shorthand, for convenience\nundefined = Undefined()\n\n\nclass Void(AST):\n    \"\"\"void singleton\"\"\"\n    def __new__(cls):\n        if not hasattr(cls, 'instance'):\n            cls.instance = super(Void, cls).__new__(cls)\n        return cls.instance\n\n    def __str__(self) -\u003e str:\n        return 'void'\n\n\n# void shorthand, for convenience\nvoid = Void()\n\n\n# assign is just a binop?\n# perhaps bring this one back since it's syntax that distinguishes it, not type checking\n# class Assign(AST):\n#     # TODO: allow bind to take in an unpack structure\n#     target: Declare | Identifier | UnpackTarget\n#     value: AST\n\n#     def __str__(self):\n#         return f'{self.target} = {self.value}'\n\n\nclass ListOfASTs(PrototypeAST):\n    \"\"\"Intermediate step for holding a list of ASTs that are probably captured by a container\"\"\"\n    asts: list[AST]\n\n    def __str__(self):\n        return f'{\", \".join(map(str, self.asts))}'\n\n\nclass PrototypeTuple(PrototypeAST):\n    \"\"\"\n    A comma separated list of expressions (not wrapped in parentheses) e.g. 1, 2, 3\n    There is no special in-memory representation of a tuple, it is literally just a const list\n    \"\"\"\n    items: list[AST]\n\n    def __str__(self):\n        return f'{\", \".join(map(str, self.items))}'\n\n\nclass Group(AST, Delimited):\n    items: list[AST]\n\n    def __str__(self):\n        return f'({\" \".join(map(str, self.items))})'\n\n\nclass Block(AST, Delimited):\n    items: list[AST]\n\n    def __str__(self):\n        return f'{{{\" \".join(map(str, self.items))}}}'\n\n\n# class Number(AST):\n#     val: int | float | Fraction\n\nclass Bool(AST):\n    val: bool\n\n    def __str__(self) -\u003e str:\n        return str(self.val).lower()\n\n\nclass Int(AST):\n    val: int\n\n    def __str__(self) -\u003e str:\n        return str(self.val)\n\n\nclass String(AST, Delimited):\n    val: str\n\n    def __str__(self) -\u003e str:\n        return f'\"{escape_whitespace(self.val)}\"'\n\n\nclass IString(AST, Delimited):\n    parts: list[AST]\n\n    def __str__(self):\n        s = ''\n        for part in self.parts:\n            if isinstance(part, String):\n                s += part.val\n            else:\n                s += f'{part}'\n        return f'\"{s}\"'\n\n\nclass Flowable(AST, ABC):\n    ...\n    # def was_entered(self) -\u003e bool:\n    #     \"\"\"Determine if the flowable branch was entered. Should reset before performing calls to flow and checking this.\"\"\"\n    #     raise NotImplementedError(f'flowables must implement `was_entered()`. No implementation found for {self.__class__}')\n\n    # def reset_was_entered(self) -\u003e None:\n    #     \"\"\"reset the state of was_entered, in preparation for executing branches in a flow\"\"\"\n    #     raise NotImplementedError(f'flowables must implement `reset_was_entered()`. No implementation found for {self.__class__}')\n\n\nclass Flow(AST):\n    branches: list[Flowable]\n\n    def __str__(self):\n        return ' else '.join(map(str, self.branches))\n\n\nclass If(Flowable):\n    condition: AST\n    body: AST\n\n    def __str__(self):\n        return f'if {self.condition} {self.body}'\n\n\nclass Loop(Flowable):\n    condition: AST\n    body: AST\n\n    def __str__(self):\n        return f'loop {self.condition} {self.body}'\n\n\nclass Default(Flowable):\n    body: AST\n\n    def __str__(self):\n        return f'{self.body}'\n\n\nclass PrototypeFunctionLiteral(PrototypeAST):\n    args: AST\n    body: AST\n\n    def __str__(self):\n        if isinstance(self.args, Delimited):\n            return f'{self.args} =\u003e {self.body}'\n        return f'({self.args}) =\u003e {self.body}'\n\n\nclass PrototypePyAction(PrototypeAST):\n    args: Group\n    return_type: AST\n\n    def __str__(self):\n        return f'({self.args}): {self.return_type} =\u003e ...'\n\n\nclass Call(AST):\n    f: AST\n    args: None | AST = None\n\n    def __str__(self):\n        if self.args is None:\n            return f'{self.f}()'\n        if isinstance(self.args, Delimited):\n            return f'{self.f}{self.args}'\n        return f'{self.f}({self.args})'\n\n\nclass BinOp(AST, ABC):\n    left: AST\n    right: AST\n\nclass Assign(BinOp):\n    def __str__(self): return f'{self.left} = {self.right}'\n\nclass PointsTo(BinOp):\n    def __str__(self): return f'{self.left} -\u003e {self.right}'\n\nclass BidirPointsTo(BinOp):\n    def __str__(self): return f'{self.left} \u003c-\u003e {self.right}'\n\nclass Access(BinOp):\n    def __str__(self): return f'{self.left}.{self.right}'\n\nclass Equal(BinOp):\n    def __str__(self): return f'{self.left} =? {self.right}'\n\n# covered by OpChain([Not, Equal])\n# class NotEqual(BinOp):\n#     def __str__(self): return f'{self.left} not=? {self.right}'\n\nclass Less(BinOp):\n    def __str__(self): return f'{self.left} \u003c? {self.right}'\n\nclass LessEqual(BinOp):\n    def __str__(self): return f'{self.left} \u003c=? {self.right}'\n\nclass Greater(BinOp):\n    def __str__(self): return f'{self.left} \u003e? {self.right}'\n\nclass GreaterEqual(BinOp):\n    def __str__(self): return f'{self.left} \u003e=? {self.right}'\n\nclass  LeftShift(BinOp):\n    def __str__(self): return f'{self.left} \u003c\u003c {self.right}'\n\nclass  RightShift(BinOp):\n    def __str__(self): return f'{self.left} \u003e\u003e {self.right}'\n\nclass LeftRotate(BinOp):\n    def __str__(self): return f'{self.left} \u003c\u003c\u003c {self.right}'\n\nclass RightRotate(BinOp):\n    def __str__(self): return f'{self.left} \u003e\u003e\u003e {self.right}'\n\nclass LeftRotateCarry(BinOp):\n    def __str__(self): return f'{self.left} \u003c\u003c! {self.right}'\n\nclass RightRotateCarry(BinOp):\n    def __str__(self): return f'{self.left} !\u003e\u003e {self.right}'\n\nclass Add(BinOp):\n    def __str__(self): return f'{self.left} + {self.right}'\n\nclass Sub(BinOp):\n    def __str__(self): return f'{self.left} - {self.right}'\n\nclass Mul(BinOp):\n    def __str__(self): return f'{self.left} * {self.right}'\n\nclass Div(BinOp):\n    def __str__(self): return f'{self.left} / {self.right}'\n\nclass IDiv(BinOp):\n    def __str__(self): return f'{self.left} ÷ {self.right}'\n\nclass Mod(BinOp):\n    def __str__(self): return f'{self.left} % {self.right}'\n\nclass Pow(BinOp):\n    def __str__(self): return f'{self.left} ^ {self.right}'\n\nclass And(BinOp):\n    def __str__(self): return f'{self.left} and {self.right}'\n\nclass Or(BinOp):\n    def __str__(self): return f'{self.left} or {self.right}'\n\nclass Xor(BinOp):\n    def __str__(self): return f'{self.left} xor {self.right}'\n\nclass Nand(BinOp):\n    def __str__(self): return f'{self.left} nand {self.right}'\n\nclass Nor(BinOp):\n    def __str__(self): return f'{self.left} nor {self.right}'\n\nclass Xnor(BinOp):\n    def __str__(self): return f'{self.left} xnor {self.right}'\n\nclass IterIn(BinOp):\n    def __str__(self): return f'{self.left} in {self.right}'\n\nclass MemberIn(BinOp):\n    def __str__(self): return f'{self.left} in? {self.right}'\n\nclass UnaryPrefixOp(AST, ABC):\n    operand: AST\n\nclass Not(UnaryPrefixOp):\n    def __str__(self): return f'not {self.operand}'\n\nclass UnaryNeg(UnaryPrefixOp):\n    def __str__(self): return f'-{self.operand}'\n\nclass UnaryPos(UnaryPrefixOp):\n    def __str__(self): return f'+{self.operand}'\n\nclass UnaryMul(UnaryPrefixOp):\n    def __str__(self): return f'*{self.operand}'\n\nclass UnaryDiv(UnaryPrefixOp):\n    def __str__(self): return f'/{self.operand}'\n\n\nclass AtHandle(UnaryPrefixOp):\n    def __str__(self):\n        if isinstance(self.operand, Delimited):\n            return f'@{self.operand}'\n        return f'@({self.operand})'\n\n\nclass UnaryPostfixOp(AST, ABC):\n    operand: AST\n\n\nclass BareRange(PrototypeAST):\n    left: AST\n    right: AST\n\n    def __str__(self) -\u003e str:\n        return f'{self.left}..{self.right}'\n\n\nclass Ellipsis(AST):\n    def __str__(self) -\u003e str:\n        return '...'\n\n\nclass Spread(AST):\n    right: AST\n\n    def __str__(self) -\u003e str:\n        return f'...{self.right}'\n\n\nclass Range(AST):\n    left: AST\n    right: AST\n    brackets: Literal['[]', '[)', '(]', '()']\n\n    def __str__(self) -\u003e str:\n        return f'{self.brackets[0]}{self.left}..{self.right}{self.brackets[1]}'\n\n\nclass Array(AST, Delimited):\n    items: list[AST] # list[T] where T is not Declare or Assign or PointsTo or BidirPointsTo\n\n    def __str__(self):\n        return f'[{\" \".join(map(str, self.items))}]'\n\n\nclass Dict(AST, Delimited):\n    items: list[PointsTo]\n\n    def __str__(self):\n        return f'[{\" \".join(map(str, self.items))}]'\n\n\nclass BidirDict(AST, Delimited):\n    items: list[BidirPointsTo]\n\n    def __str__(self):\n        return f'[{\" \".join(map(str, self.items))}]'\n\n\nclass Object(AST, Delimited):\n    items: list[AST] # list[Declare|Assign|AST] has to have at least 1 declare or assignment\n\n    def __str__(self):\n        return f'[{\" \".join(map(str, self.items))}]'\n\n\nclass TypeParam(AST, Delimited):\n    items: list[AST]\n\n    def __str__(self):\n        return f'\u003c{\" \".join(map(str, self.items))}\u003e'\n\n\nclass DeclareGeneric(AST):\n    left: TypeParam\n    right: AST\n\n    def __str__(self):\n        return f'{self.left}{self.right}'\n\n\nclass Parameterize(AST):\n    left: AST\n    right: TypeParam\n\n    def __str__(self):\n        return f'{self.left}{self.right}'\n\n\n#TODO: maybe this should just be a binop, i.e. does right need to be restricted to Range|Array?\n# perhaps keep since to parse an index, the right must be a Range|Array\nclass Index(AST):\n    left: AST\n    right: Range | Array\n\n    def __str__(self):\n        return f'{self.left}{self.right}'\n\n\nclass PrototypeIdentifier(PrototypeAST):\n    name: str\n    def __str__(self) -\u003e str:\n        return f'{self.name}'\n\nclass Identifier(AST):\n    name: str\n    def __str__(self) -\u003e str:\n        return f'{self.name}'\n\n\nclass Express(AST):\n    id: Identifier\n\n    def __str__(self) -\u003e str:\n        return f'{self.id}'\n\nclass TypedIdentifier(AST):\n    id: Identifier\n    type: AST\n\n    def __str__(self) -\u003e str:\n        return f'{self.id}:{self.type}'\n\n\nclass ReturnTyped(BinOp):\n    item: AST\n    type: AST\n\n    def __str__(self) -\u003e str:\n        return f'{self.item}:\u003e{self.type}'\n\nclass UnpackTarget(AST):\n    target: 'list[Identifier | TypedIdentifier | UnpackTarget | Assign | Spread]'\n    def __str__(self) -\u003e str:\n        return f'[{\" \".join(map(str, self.target))}]'\n\nclass DeclarationType(Enum):\n    LET = auto()\n    CONST = auto()\n    # LOCAL_CONST = auto()\n    # FIXED_TYPE = auto()\n\n    # default for binding without declaring\n    # DEFAULT = LET\n\n\nclass Declare(AST):\n    decltype: DeclarationType\n    target: Identifier | TypedIdentifier | ReturnTyped | UnpackTarget | Assign\n\n    def __str__(self):\n        return f'{self.decltype.name.lower()} {self.target}'\n\n\n\nif __name__ == '__main__':\n    # DEBUG testing tree string printing\n    class _Add(AST):\n        l: AST\n        r: AST\n\n        def __str__(self) -\u003e str:\n            return f'{self.l} + {self.r}'\n\n    class _Mul(AST):\n        l: AST\n        r: AST\n\n        def __str__(self) -\u003e str:\n            return f'{self.l} * {self.r}'\n\n    class _List(AST):\n        items: list[AST]\n\n        def __str__(self) -\u003e str:\n            return f'[{\", \".join(map(str, self.items))}]'\n\n    class _Int(AST):\n        value: int\n\n        def __str__(self) -\u003e str:\n            return str(self.value)\n\n    # big long test ast\n    test = _Add(\n        _Add(\n            _Int(1),\n            _List([_Int(2), _Int(3), _Int(4), _Int(5)])\n        ),\n        _Mul(\n            _Int(2),\n            _Add(\n                _Mul(\n                    _Int(3),\n                    _Int(4)\n                ),\n                _Mul(\n                    _Int(5),\n                    _Int(6)\n                )\n            )\n        )\n    )\n\n    print(repr(test))\n    print(str(test))\n    # class Broken(AST):\n    #     num: int\n    #     def __str__(self) -\u003e str:\n    #         return f'{self.num}'\n    #     def __iter__(self) -\u003e Generator['AST', None, None]:\n    #         yield Int(self.num)\n"])</script><script>self.__next_f.push([1,"21:T990e,"])</script><script>self.__next_f.push([1,"from abc import ABC\nimport inspect\nfrom typing import Callable, Type, Generator\nfrom types import UnionType\nfrom functools import lru_cache\nfrom .utils import CoordString\n\nimport pdb\n\n\n\"\"\"\n[tasks]\n- clean up eat_block\n    - general cleanup\n    - break out eat_ matching into smaller functions?\n    - figure out tie breaking process:\n        1. prefer @full_eat over @peek_eat ---\u003e TODO: implement\n        2. prefer longest matches\n        3. prefer higher precedence\n        4. error\n- make all tokens keep the source they come from (for error reporting/keeping track of row/col of the token)\n\"\"\"\n\n\nclass Token(ABC):\n    def __repr__(self) -\u003e str:\n        \"\"\"default repr for tokens is just the class name\"\"\"\n        return f\"\u003c{self.__class__.__name__}\u003e\"\n\n    def __hash__(self) -\u003e int:\n        raise NotImplementedError(f'hash is not implemented for token type {type(self)}')\n\n    def __eq__(self, __value: object) -\u003e bool:\n        raise NotImplementedError(f'equals is not implemented for token type {type(self)}')\n\n    def __iter__(self) -\u003e Generator['list[Token]', None, None]:\n        \"\"\"\n        Iter is used by full_traverse_tokens for iterating over any contained tokens.\n        e.g. Block_t.body TypeParam_t.body, String_t.body (interpolation blocks only), etc.\n        \"\"\"\n        raise NotImplementedError(f'iter is not implemented for token type {type(self)}')\n\n\nclass WhiteSpace_t(Token):\n    def __init__(self, _): ...\n\n\nclass Operator_t(Token):\n    def __init__(self, op: str):\n        self.op = op\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cOperator_t: `{self.op}`\u003e\"\n\n    def __hash__(self) -\u003e int:\n        return hash((Operator_t, self.op))\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, Operator_t) and self.op == other.op\n\n\nclass Juxtapose_t(Operator_t):\n    def __init__(self, _):\n        super().__init__('')\n\n    def __hash__(self) -\u003e int:\n        return hash(Juxtapose_t)\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, Juxtapose_t)\n\n\nclass Keyword_t(Token):\n    def __init__(self, src: str):\n        self.src = src.lower()\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cKeyword_t: {self.src}\u003e\"\n\n    def __hash__(self) -\u003e int:\n        return hash((Keyword_t, self.src))\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, Keyword_t) and self.src == other.src\n\n\nclass Identifier_t(Token):\n    def __init__(self, src: str):\n        self.src = src\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cIdentifier_t: {self.src}\u003e\"\n\n\nclass Hashtag_t(Token):\n    def __init__(self, src: str):\n        self.src = src\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cHashtag_t: {self.src}\u003e\"\n\n\nclass Block_t(Token):\n    def __init__(self, body: list[Token], left: str, right: str):\n        self.body = body\n        self.left = left\n        self.right = right\n\n    def __repr__(self) -\u003e str:\n        body_str = ', '.join(repr(token) for token in self.body)\n        return f\"\u003cBlock_t: {self.left}{body_str}{self.right}\u003e\"\n\n    def __iter__(self) -\u003e Generator[list[Token], None, None]:\n        yield self.body\n\n\nclass TypeParam_t(Token):\n    def __init__(self, body: list[Token]):\n        self.body = body\n\n    def __repr__(self) -\u003e str:\n        body_str = ', '.join(repr(token) for token in self.body)\n        return f\"\u003cTypeParam_t: `\u003c{body_str}\u003e`\u003e\"\n\n    def __iter__(self) -\u003e Generator[list[Token], None, None]:\n        yield self.body\n\n\nclass Escape_t(Token):\n    escape_map = {\n        '\\\\n': '\\n', '\\\\r': '\\r', '\\\\t': '\\t', '\\\\b': '\\b', '\\\\f': '\\f', '\\\\v': '\\v', '\\\\a': '\\a', '\\\\0': '\\0', '\\\\\\\\': '\\\\'\n    }\n\n    def __init__(self, src: str):\n        self.src = src\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cEscape_t: {self.src}\u003e\"\n\n    def to_str(self) -\u003e str:\n        \"\"\"Convert the escape sequence to the character it represents\"\"\"\n\n        # unicode escape (may be several characters long)\n        if self.src.startswith('\\\\U') or self.src.startswith('\\\\u'):\n            return chr(int(self.src[2:], 16))\n        assert len(self.src) == 2 and self.src[0] == '\\\\', \"internal error. Ill-posed escape sequence\"\n\n        # known escape sequence\n        if self.src in self.escape_map:\n            esc = self.escape_map[self.src]\n            # construct a CoordString at the position of the original escape\n            return CoordString.from_existing(esc, self.src[:len(esc)].row_col_map)\n\n        # unknown escape sequence (i.e. just replicate the character)\n        return self.src[1]\n\n\nclass RawString_t(Token):\n    def __init__(self, body: str):\n        self.body = body\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cRawString_t: {self.body}\u003e\"\n\n    def to_str(self) -\u003e str:\n        body = self.body\n        if body.startswith('r\"\"\"') or body.startswith(\"r'''\"):\n            body = body[4:-3]\n        elif body.startswith('r\"') or body.startswith(\"r'\"):\n            body = body[2:-1]\n        else:\n            raise ValueError(f\"Internal Error: unrecognized delimiters on raw string: {repr(self)}\")\n        return body\n\n\nclass String_t(Token):\n    def __init__(self, body: list[str | Escape_t | Block_t]):\n        self.body = body\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cString_t: {self.body}\u003e\"\n\n    def __iter__(self) -\u003e Generator[list[Token], None, None]:\n        for token in self.body:\n            if isinstance(token, Block_t):\n                yield token.body\n\n# class Number_t(Token, ABC):...\n\n\nclass Integer_t(Token):\n    def __init__(self, src: str):\n        self.src = src\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cInteger_t: {self.src}\u003e\"\n\n\nclass BasedNumber_t(Token):\n    def __init__(self, src: str):\n        self.src = src\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cBasedNumber_t: {self.src}\u003e\"\n\n\nclass Undefined_t(Token):\n    def __init__(self, _): ...\n\n    def __hash__(self) -\u003e int:\n        return hash(Undefined_t)\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, Undefined_t)\n\n    def __repr__(self) -\u003e str:\n        return \"\u003cUndefined_t\u003e\"\n\n\nclass Void_t(Token):\n    def __init__(self, _): ...\n\n    def __hash__(self) -\u003e int:\n        return hash(Void_t)\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, Void_t)\n\n    def __repr__(self) -\u003e str:\n        return \"\u003cVoid_t\u003e\"\n\n\nclass End_t(Token):\n    def __init__(self, _): ...\n\n    def __hash__(self) -\u003e int:\n        return hash(End_t)\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, End_t)\n\n    def __repr__(self) -\u003e str:\n        return \"\u003cEnd_t\u003e\"\n\n\nclass Boolean_t(Token):\n    def __init__(self, src: str):\n        self.src = src\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cBoolean_t: {self.src}\u003e\"\n\n\nclass ShiftOperator_t(Operator_t):\n    def __init__(self, op: str):\n        self.op = op\n\n    def __repr__(self) -\u003e str:\n        return f\"\u003cShiftOperator_t: `{self.op}`\u003e\"\n\n    def __hash__(self) -\u003e int:\n        return hash((ShiftOperator_t, self.op))\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, ShiftOperator_t) and self.op == other.op\n\n\nclass Comma_t(Operator_t):\n    def __init__(self, op: str):\n        self.op = op\n\n    def __hash__(self) -\u003e int:\n        return hash(Comma_t)\n\n    def __eq__(self, other) -\u003e bool:\n        return isinstance(other, Comma_t)\n\n\nclass DotDot_t(Token):\n    def __init__(self, src: str):\n        self.src = src\n\n\nclass DotDotDot_t(Token):\n    def __init__(self, src: str):\n        self.src = src\n\n# #TODO: these should probably each be their own class/token, or a single class..\n# these should all be case insensitive\n# reserved_values = ['true', 'false', 'void', 'undefined', 'end']\n\n\n# identify token classes that should take precedence over others when tokenizing\n# each row is a list of token types that are confusable in their precedence order. e.g. [Keyword, Unit, Identifier] means Keyword \u003e Unit \u003e Identifier\n# only confusable token classes need to be included in the table\nprecedence_table = [\n    [Keyword_t, Undefined_t, Void_t, End_t, Boolean_t, Operator_t, DotDot_t, Identifier_t],\n]\nprecedence = {cls: len(row)-i for row in precedence_table for i, cls in enumerate(row)}\n\n# mark which tokens cannot be repeated in a list of tokens. E.g. whitespace should always be merged into a single token\nidempotent_tokens = {\n    WhiteSpace_t\n}\n\n# paired delimiters for blocks, ranges, groups, etc.\npair_opening_delims = '{(['\npair_closing_delims = '})]'\n\n# which closing delimiters are allowed for each opening delimiter\nvalid_delim_closers = {\n    '{': '}',\n    '(': ')]',\n    '[': '])',\n    # '\u003c': '\u003e'\n}\n\n# list of all operators sorted from longest to shortest\n# TODO: make @ and ... into expressions (perhaps with lower precedence calling than regular calls?)\nunary_prefix_operators = {'+', '-', '*', '/', 'not', '@'}#, '...'}\nunary_postfix_operators = {'?', '`', ';'}\nbinary_operators = {\n    '+', '-', '*', '/', '%', '^',\n    '=?', '\u003e?', '\u003c?', '\u003e=?', '\u003c=?', 'in?', 'is?', 'isnt?', '\u003c=\u003e',\n    '|', '\u0026',\n    'and', 'or', 'nand', 'nor', 'xor', 'xnor', '??',\n    'else',\n    '=', ':=', 'as', 'in', 'transmute',\n    '@?',\n    '|\u003e', '\u003c|', '=\u003e',\n    '-\u003e', '\u003c-\u003e', #'\u003c-', #reverse arrow is dumb\n    '.', ':', ':\u003e'\n}\nopchain_starters = {'+', '-', '*', '/', '%', '^'}\noperators = sorted(\n    [*(unary_prefix_operators | unary_postfix_operators | binary_operators)],\n    key=len,\n    reverse=True\n)\n# TODO: may need to separate |\u003e from regular operators since it may confuse type param\nshift_operators = sorted(['\u003c\u003c', '\u003e\u003e', '\u003c\u003c\u003c', '\u003e\u003e\u003e', '\u003c\u003c!', '!\u003e\u003e'], key=len, reverse=True)\nkeywords = ['loop', 'lazy', 'do', 'if', 'match', 'return', 'yield', 'break', 'continue',\n            'async', 'await', 'import', 'from', 'let', 'const', 'local_const', 'fixed_type']\n# TODO: what about language values, e.g. void, undefined, end, units, etc.? probably define at compile time, rather than in the compiler\n\n# note that the prefix is case insensitive, so call .lower() when matching the prefix\n# numbers may have _ as a separator (if _ is not in the set of digits)\nnumber_bases = {\n    '0b': {*'01'},  # binary\n    '0t': {*'012'},  # ternary\n    '0q': {*'0123'},  # quaternary\n    '0s': {*'012345'},  # seximal\n    '0o': {*'01234567'},  # octal\n    '0d': {*'0123456789'},  # decimal\n    '0z': {*'0123456789xeXE'},  # dozenal\n    '0x': {*'0123456789abcdefABCDEF'},  # hexadecimal\n    '0u': {*'0123456789abcdefghijklmnopqrstuvABCDEFGHIJKLMNOPQRSTUV'},  # base 32 (duotrigesimal)\n    '0r': {*'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'},  # base 36 (hexatrigesimal)\n    '0y': {*'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!$'},  # base 64 (tetrasexagesimal)\n}\n\n\n# units = #actually units should probably not be specific tokens, but recognized identifiers since the user can make their own units\n\n\ndef peek_eat(cls: Type[Token], whitelist: list[Type[Token]] | None = None, blacklist: list[Type[Token]] | None = None):\n    \"\"\"\n    Decorator for functions that eat tokens, but only return how many characters would make up the token.\n    Makes function return include constructor for token class that it tries to eat, in tupled with return.\n\n    whitelist and blacklist can be used to specify parent token contexts that may or may not consume this type as a child\n    \"\"\"\n    assert issubclass(cls, Token), f\"cls must be a subclass of Token, but got {cls}\"\n    if whitelist is not None and blacklist is not None:\n        raise ValueError(\"cannot specify both whitelist and blacklist\")\n\n    def decorator(eat_func: Callable[[str], int | None]):\n        def wrapper(src: str) -\u003e tuple[int | None, Type[Token]]:\n            return eat_func(src), cls\n        wrapper._is_peek_eat_decorator = True  # make it easy to check if a function has this decorator\n        wrapper._eat_func = eat_func\n        wrapper._token_cls = cls\n        wrapper._whitelist = whitelist\n        wrapper._blacklist = blacklist\n        return wrapper\n    return decorator\n\n# TODO: full eat probably won't need to take the class as an argument, since the function will know how to construct the token itself\n\n\ndef full_eat(whitelist: list[Type[Token]] | None = None, blacklist: list[Type[Token]] | None = None):\n    def decorator(eat_func: Callable[[str], tuple[int, Token] | None]):\n        \"\"\"\n        Decorator for functions that eat tokens, and return the token itself if successful.\n        TBD what this actually does...for now, largely keep unmodified, but attach the metadata to the wrapped function\n        \"\"\"\n        # pull cls it from the return type of eat_func (which should be a Union[tuple[int, Token], None])\n        cls = inspect.signature(eat_func).return_annotation.__args__[0].__args__[1]\n        assert issubclass(cls, Token), f\"cls must be a subclass of Token, but got {cls}\"\n        if whitelist is not None and blacklist is not None:\n            raise ValueError(\"cannot specify both whitelist and blacklist\")\n\n        def wrapper(*args, **kwargs):\n            return eat_func(*args, **kwargs), cls\n        wrapper._is_full_eat_decorator = True  # make it easy to check if a function has this decorator\n        wrapper._eat_func = eat_func\n        wrapper._token_cls = cls\n        wrapper._whitelist = whitelist\n        wrapper._blacklist = blacklist\n\n        return wrapper\n    return decorator\n\n\ndef get_peek_eat_funcs_with_name() -\u003e tuple[tuple[str, Callable], ...]:\n    return tuple((name, func) for name, func in globals().items() if callable(func) and getattr(func, '_is_peek_eat_decorator', False))\n\n\ndef get_full_eat_funcs_with_name() -\u003e tuple[tuple[str, Callable], ...]:\n    return tuple((name, func) for name, func in globals().items() if callable(func) and getattr(func, '_is_full_eat_decorator', False))\n\n\ndef get_eat_funcs() -\u003e tuple[Callable, ...]:\n    return tuple(func for name, func in get_peek_eat_funcs_with_name() + get_full_eat_funcs_with_name())\n\n\n@lru_cache()\ndef get_contextual_eat_funcs(context: Type[Token]) -\u003e tuple[Callable, ...]:\n    \"\"\"Get all the eat functions that are valid in the given context\"\"\"\n    return tuple(func for func in get_eat_funcs() if (func._whitelist is None or context in func._whitelist) and (func._blacklist is None or context not in func._blacklist))\n\n\n@lru_cache()\ndef get_func_precedences(funcs: tuple[Callable]) -\u003e tuple[int, ...]:\n    assert isinstance(funcs, tuple)\n    return tuple(precedence.get(func._token_cls, 0) for func in funcs)\n\n\n@peek_eat(WhiteSpace_t)\ndef eat_line_comment(src: str) -\u003e int | None:\n    \"\"\"eat a line comment, return the number of characters eaten\"\"\"\n    if src.startswith('//'):\n        try:\n            return src.index('\\n') + 1\n        except ValueError:\n            return len(src)\n    return None\n\n\n@peek_eat(WhiteSpace_t)\ndef eat_block_comment(src: str) -\u003e int | None:\n    \"\"\"\n    Eat a block comment, return the number of characters eaten\n    Block comments are of the form /{ ... }/ and can be nested.\n    \"\"\"\n    if not src.startswith(\"/{\"):\n        return None\n\n    nesting_level = 0\n    i = 0\n\n    while i \u003c len(src):\n        if src[i:].startswith('/{'):\n            nesting_level += 1\n            i += 2\n        elif src[i:].startswith('}/'):\n            nesting_level -= 1\n            i += 2\n\n            if nesting_level == 0:\n                return i\n        else:\n            i += 1\n\n    raise ValueError(\"unterminated block comment\")\n    # return None\n\n\n@peek_eat(WhiteSpace_t)\ndef eat_whitespace(src: str) -\u003e int | None:\n    \"\"\"Eat whitespace, return the number of characters eaten\"\"\"\n    i = 0\n    while i \u003c len(src) and src[i].isspace():\n        i += 1\n    return i if i \u003e 0 else None\n\n\n@peek_eat(Keyword_t)\ndef eat_keyword(src: str) -\u003e int | None:\n    \"\"\"\n    Eat a reserved keyword, return the number of characters eaten\n\n    #keyword = {in} | {as} | {loop} | {lazy} | {if} | {and} | {or} | {xor} | {nand} | {nor} | {xnor} | {not};# | {true} | {false};\n\n    noting that keywords are case insensitive\n    \"\"\"\n\n    max_len = max(len(keyword) for keyword in keywords)\n\n    lower_src = src[:max_len].lower()\n    for keyword in keywords:\n        if lower_src.startswith(keyword):\n            # TBD if we need to check that the next character is not an identifier character\n            return len(keyword)\n\n    return None\n\n\n# TODO: expand the list of valid identifier characters\ndigits = set('0123456789')\nalpha = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz')\ngreek = set('ΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩαβγδεζηθικλμνξοπρςστυφχψω')\nmisc = set('_?!$\u0026°')\n\nstart_characters = (alpha | greek | misc) - {'?'}\ncontinue_characters = (alpha | digits | greek | misc)\n\n\n@peek_eat(Identifier_t)\ndef eat_identifier(src: str) -\u003e int | None:\n    \"\"\"\n    Eat an identifier, return the number of characters eaten\n\n    Identifiers:\n    - may not start with a number or a question mark\n    - may not end with a question mark\n    - may use (TODO enumerate the full chars list somewhere. for now copying from python)\n\n    \"\"\"\n    if not src[0] in start_characters:\n        return None\n\n    i = 1\n    while i \u003c len(src) and src[i] in continue_characters:\n        i += 1\n\n    # while last character is ?, remove it\n    while i \u003e 1 and src[i-1] == '?':\n        i -= 1\n\n    return i\n\n\n@peek_eat(Hashtag_t)\ndef eat_hashtag(src: str) -\u003e int | None:\n    \"\"\"\n    Eat a hashtag, return the number of characters eaten\n\n    hashtags are special identifiers that start with #\n    \"\"\"\n\n    if src.startswith('#'):\n        i, _ = eat_identifier(src[1:])\n        if i is not None:\n            return i + 1\n\n    return None\n\n\n@peek_eat(Escape_t, whitelist=[String_t])\ndef eat_escape(src: str) -\u003e int | None:\n    r\"\"\"\n    Eat an escape sequence, return the number of characters eaten\n    Escape sequences must be either a known escape sequence:\n    - \\n newline\n    - \\r carriage return\n    - \\t tab\n    - \\b backspace\n    - \\f form feed\n    - \\v vertical tab\n    - \\a alert\n    - \\0 null\n    - \\u##..# or \\U##..# for an arbitrary unicode character. May have any number of hex digits\n\n    or a \\ followed by an unknown character. In this case, the escape converts to just the unknown character\n    This is how to insert characters that are otherwise illegal inside a string, e.g.\n    - \\' converts to just a single quote '\n    - \\{ converts to just a single open brace {\n    - \\\\ converts to just a single backslash \\\n    - \\m converts to just a single character m\n    - etc.\n    \"\"\"\n    if not src.startswith('\\\\'):\n        return None\n\n    if len(src) == 1:\n        raise ValueError(\"unterminated escape sequence\")\n\n    if src[1] in 'uU':\n        i = 2\n        while i \u003c len(src) and src[i].isxdigit():\n            i += 1\n        if i == 2:\n            raise ValueError(\"invalid unicode escape sequence\")\n        return i\n\n    # if src[1] in 'nrtbfva0':\n    #     return 2\n\n    # all other escape sequences (known or unknown) are just a single character\n    return 2\n\n\n@full_eat()\ndef eat_string(src: str) -\u003e tuple[int, String_t] | None:\n    r\"\"\"\n    strings are delimited with either single (') or double quotes (\")\n    the character portion of a string may contain any character except the delimiter, \\, or {.\n    strings may be multiline\n    strings may contain escape sequences of the form \\s where s is either a known escape sequence or a single character\n    strings may interpolation blocks which open with { and close with }\n\n    Tokenizing of escape sequences and interpolation blocks is handled as sub-tokenization task via eat_block and eat_escape\n\n    returns the number of characters eaten and an instance of the String token, containing the list of tokens/string chunks/escape sequences\n    \"\"\"\n\n    # determine the starting delimiter, or exit if there is none\n    if src.startswith('\"\"\"') or src.startswith(\"'''\"):\n        delim = src[:3]\n        i = 3\n    elif src.startswith('\"') or src.startswith(\"'\"):\n        delim = src[0]\n        i = 1\n    else:\n        return None\n\n    # keep track of chunks, and the start index of the current chunk\n    chunk_start = i\n    body = []\n\n    # add character sequences, escapes, and block sections until the end of the string\n    while i \u003c len(src) and not src[i:].startswith(delim):\n\n        # regular characters\n        if src[i] not in '\\\\{':\n            i += 1\n            continue\n\n        # add the previous chunk before handling the escape/interpolation block\n        if i \u003e chunk_start:\n            body.append(src[chunk_start:i])\n\n        if src[i] == '\\\\':\n            res, _ = eat_escape(src[i:])\n            if res is None:\n                raise ValueError(\"invalid escape sequence\")\n            body.append(Escape_t(src[i:i+res]))\n            i += res\n\n        else:  # src[i] == '{':\n            assert src[i] == '{', \"internal error\"\n            res, _ = eat_block(src[i:])\n            if res is None:\n                raise ValueError(\"invalid block\")\n            n_eaten, block = res\n            body.append(block)\n            i += n_eaten\n\n        # update the chunk start\n        chunk_start = i\n\n    if i == len(src):\n        raise ValueError(\"unterminated string\")\n\n    # add the final chunk\n    if i \u003e chunk_start:\n        body.append(src[chunk_start:i])\n\n    return i + len(delim), String_t(body)\n\n\n@peek_eat(RawString_t)\ndef eat_raw_string(src: str) -\u003e int | None:\n    \"\"\"\n    raw strings start with `r`, followed by a delimiter, one of ' \" ''' or \\\"\"\"\n    raw strings may contain any character except the delimiter.\n    Escapes and interpolations are ignored.\n    The string ends at the first instance of the delimiter\n    \"\"\"\n    if not src.startswith('r'):\n        return None\n    i = 1\n\n    if src[i:].startswith('\"\"\"') or src[i:].startswith(\"'''\"):\n        delim = src[i:i+3]\n        i += 3\n    elif src[i:].startswith('\"') or src[i:].startswith(\"'\"):\n        delim = src[i]\n        i += 1\n    else:\n        return None\n\n    while i \u003c len(src) and not src[i:].startswith(delim):\n        i += 1\n\n    if i == len(src):\n        raise ValueError(\"unterminated raw string\")\n\n    return i + len(delim)\n\n\n@peek_eat(Integer_t)\ndef eat_integer(src: str) -\u003e int | None:\n    \"\"\"\n    eat an integer, return the number of characters eaten\n    integers are of the form [0-9]+\n    \"\"\"\n    if not src[0].isdigit():\n        return None\n\n    i = 1\n    while i \u003c len(src) and src[i].isdigit() or src[i] == '_':\n        i += 1\n    return i\n\n\n@peek_eat(BasedNumber_t)\ndef eat_based_number(src: str) -\u003e int | None:\n    \"\"\"\n    eat a based number, return the number of characters eaten\n\n    based numbers have a (case-insensitive) prefix (0p) identifying the base, and (case-sensitive) allowed digits\n    \"\"\"\n    try:\n        digits = number_bases[src[:2].lower()]\n    except KeyError:\n        return None\n\n    i = 2\n    while i \u003c len(src) and src[i] in digits or src[i] == '_':\n        i += 1\n\n    return i if i \u003e 2 else None\n\n\n@peek_eat(Undefined_t)\ndef eat_undefined(src: str) -\u003e int | None:\n    \"\"\"\n    eat the undefined token, return the number of characters eaten\n    \"\"\"\n    sample = src[:9].lower()\n    if sample.startswith('undefined'):\n        return 9\n    return None\n\n\n@peek_eat(Void_t)\ndef eat_void(src: str) -\u003e int | None:\n    \"\"\"\n    eat the void token, return the number of characters eaten\n    \"\"\"\n    sample = src[:4].lower()\n    if sample.startswith('void'):\n        return 4\n    return None\n\n\n@peek_eat(End_t)\ndef eat_end(src: str) -\u003e int | None:\n    \"\"\"\n    eat the end token, return the number of characters eaten\n    \"\"\"\n    sample = src[:3].lower()\n    if sample.startswith('end'):\n        return 3\n    return None\n\n\n@peek_eat(Boolean_t)\ndef eat_boolean(src: str) -\u003e int | None:\n    \"\"\"\n    eat a boolean, return the number of characters eaten\n\n    booleans are either true or false (case-insensitive)\n    \"\"\"\n    sample = src[:5].lower()\n    if sample.startswith('true'):\n        return 4\n    elif sample.startswith('false'):\n        return 5\n\n    return None\n\n\n@peek_eat(Operator_t)\ndef eat_operator(src: str) -\u003e int | None:\n    \"\"\"\n    eat a unary or binary operator, return the number of characters eaten\n\n    picks the longest matching operator\n\n    see `operators` for full list of operators\n    \"\"\"\n    for op in operators:\n        if src.startswith(op):\n            return len(op)\n    return None\n\n\n@peek_eat(ShiftOperator_t, blacklist=[TypeParam_t])\ndef eat_shift_operator(src: str) -\u003e int | None:\n    \"\"\"\n    eat a shift operator, return the number of characters eaten\n\n    picks the longest matching operator.\n    Shift operators are not allowed in type parameters, e.g. `\u003e\u003e` is not recognized in `Foo\u003cBar\u003cBaz\u003cT\u003e\u003e, U\u003e`\n\n    see `shift_operators` for full list of operators\n    \"\"\"\n    for op in shift_operators:\n        if src.startswith(op):\n            return len(op)\n    return None\n\n\n@peek_eat(Comma_t)\ndef eat_comma(src: str) -\u003e int | None:\n    \"\"\"\n    eat a comma, return the number of characters eaten\n    \"\"\"\n    return 1 if src.startswith(',') else None\n\n\n@peek_eat(DotDot_t)\ndef eat_dotdot(src: str) -\u003e int | None:\n    \"\"\"\n    eat a dotdot, return the number of characters eaten\n    \"\"\"\n    return 2 if src.startswith('..') else None\n\n\n@peek_eat(DotDotDot_t)\ndef eat_dotdotdot(src: str) -\u003e int | None:\n    \"\"\"\n    eat a dotdotdot, return the number of characters eaten\n    \"\"\"\n    return 3 if src.startswith('...') else None\n\n\nclass EatTracker:\n    i: int\n    tokens: list[Token]\n\n\n@full_eat()\ndef eat_type_param(src: str) -\u003e tuple[int, TypeParam_t] | None:\n    \"\"\"\n    eat a type parameter, return the number of characters eaten and an instance of the TypeParam token\n\n    type parameters are of the form \u003c...\u003e where ... is a sequence of tokens.\n    Type parameters may not start with `\u003c\u003c` or contain any shift operators (`\u003c\u003c`, `\u003c\u003c\u003c`, `\u003e\u003e`, `\u003e\u003e\u003e`)\n    Internally encountered shift operators are considered to be delimiters for the type parameter\n    \"\"\"\n    if not src.startswith('\u003c') or src.startswith('\u003c\u003c'):\n        return None\n\n    i = 1\n    body: list[Token] = []\n\n    while i \u003c len(src) and src[i] != '\u003e':\n\n        funcs = get_contextual_eat_funcs(TypeParam_t)\n        precedences = get_func_precedences(funcs)\n        res = get_best_match(src[i:], funcs, precedences)\n\n        if res is None:\n            return None\n        n_eaten, token = res\n\n        if isinstance(token, Token):\n            # add the already-eaten token to the list of tokens\n            body.append(token)\n        else:\n            # add a new instance of the token to the list of tokens (handling idempotent token cases)\n            token_cls = token\n            if not body or token_cls not in idempotent_tokens or not isinstance(body[-1], token_cls):\n                body.append(token_cls(src[i:i+n_eaten]))\n\n        # increment the index\n        i += n_eaten\n\n    if i == len(src):\n        return None\n\n    return i + 1, TypeParam_t(body)\n\n\n@full_eat()\ndef eat_block(src: str, tracker: EatTracker | None = None) -\u003e tuple[int, Block_t] | None:\n    \"\"\"\n    Eat a block, return the number of characters eaten and an instance of the Block token\n\n    blocks are { ... } or ( ... ) and may contain sequences of any other tokens including other blocks\n\n    if return_partial is True, then returns (i, body) in the case where the eat process fails, instead of None\n    \"\"\"\n\n    if not src or src[0] not in pair_opening_delims:\n        return None\n\n    # save the opening delimiter\n    left = src[0]\n\n    i = 1\n    body: list[Token] = []\n\n    if tracker:\n        tracker.i = i\n        tracker.tokens = body\n\n    while i \u003c len(src) and src[i] not in pair_closing_delims:\n        # run all root eat functions\n        # if multiple, resolve for best match (TBD... current is longest match + precedence)\n        # if no match, return None\n\n        # TODO: probably break this inner part into a function that eats the next token, given a list of eat functions\n        # could also think about ways to specify other multi-match resolutions, other than longest match + precedence...\n        # run all the eat functions on the current src\n        funcs = get_contextual_eat_funcs(Block_t)\n        precedences = get_func_precedences(funcs)\n        res = get_best_match(src[i:], funcs, precedences)\n\n        # if we didn't match anything, return None\n        if res is None:\n            return None\n\n        n_eaten, token = res\n\n        if isinstance(token, Token):\n            # add the already-eaten token to the list of tokens\n            body.append(token)\n        else:\n            # add a new instance of the token to the list of tokens (handling idempotent token cases)\n            token_cls = token\n            if not body or token_cls not in idempotent_tokens or not isinstance(body[-1], token_cls):\n                body.append(token_cls(src[i:i+n_eaten]))\n\n        # increment the index\n        i += n_eaten\n        if tracker:\n            tracker.i = i\n\n    if i == len(src):\n        if tracker:  # only return an exception for the top level block. nested blocks can return None\n            raise ValueError(\"unterminated block\")\n        return None\n\n    # closing delim (doesn't need to match opening delim)\n    right = src[i]\n    assert left in pair_opening_delims and right in pair_closing_delims, f\"invalid block delimiters: {left} {right}\"\n\n    # include closing delimiter in character count\n    i += 1\n    if tracker:\n        tracker.i = i\n\n    return i, Block_t(body, left=left, right=right)\n\n\ndef get_best_match(src: str, eat_funcs: list, precedences: list[int]) -\u003e tuple[int, Type[Token] | Token] | None:\n    # TODO: handle selecting between full_eat and peek_eat functions that were successful...\n    #      general, just need to clarify the selection order precedence\n\n    # may return none if no match\n    # may return (i, token_cls) if peek match\n    # may return (i, token) if full match\n\n    matches = [eat_func(src) for eat_func in eat_funcs]\n\n    # find the longest token that matched. if multiple tied for longest, use the one with the highest precedence.\n    # raise an error if multiple tokens tied, and they have the same precedence\n    def key(x):\n        (res, _cls), precedence = x\n        if res is None:\n            return 0, precedence\n        if isinstance(res, tuple):\n            res, _token = res  # full_eat functions return a tuple of (num_chars_eaten, token)\n        return res, precedence\n\n    matches = [*zip(matches, precedences)]\n    best = max(matches, key=key)\n    ties = [match for match in matches if key(match) == key(best)]\n    if len(ties) \u003e 1:\n        raise ValueError(f\"multiple tokens matches tied {[match[0][1].__name__ for match in ties]}: {repr(src)}\\nPlease disambiguate by providing precedence levels for these tokens.\")\n\n    (res, token_cls), _ = best\n\n    # force the type annotations\n    res: tuple[int, Token] | int | None\n    token_cls: type[Token]\n\n    if res is None:\n        return None\n\n    if isinstance(res, int):\n        return res, token_cls\n\n    if isinstance(res, tuple):\n        return res\n\n    raise ValueError(f\"Internal Error: invalid return type from eat function: {res}\")\n\n\ndef tokenize(src: str) -\u003e list[Token]:\n\n    # insert src into a block\n    src = f'{{\\n{src}\\n}}'\n\n    # convert string to a coordinate string (for keeping track of row/col numbers)\n    src = CoordString(src, anchor=(-1, 0))\n\n    # eat tokens for a block\n    tracker = EatTracker()\n    try:\n        res, _cls = eat_block(src, tracker=tracker)\n    except Exception as e:\n        raise ValueError(f\"failed to tokenize: ```{escape_whitespace(src[tracker.i:])}```.\\nCurrent tokens: {tracker.tokens}\") from e\n\n    # check if the process failed\n    if res is None:\n        raise ValueError(f\"failed to tokenize: ```{escape_whitespace(src[tracker.i:])}```.\\nCurrent tokens: {tracker.tokens}\")\n\n    (i, block) = res\n    tokens = block.body\n\n    # ensure that all blocks have valid open/close pairs\n    validate_block_braces(tokens)\n\n    return tokens\n\n\ndef full_traverse_tokens(tokens: list[Token]) -\u003e Generator[tuple[int, Token, list[Token]], int, None]:\n    \"\"\"\n    Walk all tokens recursively, allowing for modification of the tokens list as it is traversed.\n\n    So long as modifications do not occur before the current token, this will safely iterate over all tokens.\n    This will not yield string or escape chunks in strings, but will yield interpolated blocks.\n\n    While traversing, the user can overwrite the current index by calling .send(new_index).\n\n    e.g.\n    ```python\n    gen = full_traverse_tokens(tokens)\n    for i, token, stream in gen:\n        #do something with current token\n        #...\n\n        #maybe overwrite the current index\n        if should_overwrite:\n            gen.send(new_index)\n    ```\n\n    Do not call .send() twice in a row without calling next() in between. This will cause unexpected behavior.\n\n    Args:\n        tokens: the list of tokens to traverse\n\n    Yields:\n        i: the index of the current token in the current token list\n        token: the current token\n        stream: the current token list\n    \"\"\"\n\n    i = 0\n\n    while i \u003c len(tokens):\n        \"\"\"\n        1. get next token\n        2. send current to user\n        3. increment index (or overwrite it)\n        4. recurse into blocks\n        \"\"\"\n\n        # get the current token\n        token = tokens[i]\n\n        # send the current index to the user. possibly receive a new index to continue from\n        overwrite_i = yield i, token, tokens\n\n        # only calls to next() will continue execution. calls to .send do nothing wait\n        if overwrite_i is not None:\n            assert (yield) is None, \".send() may only be called once per iteration.\"\n            i = overwrite_i\n        else:\n            i += 1\n\n        # for tokens that have defined __iter__ methods, yield their contents\n        try:\n            for children in token:\n                yield from full_traverse_tokens(children)\n        except NotImplementedError:\n            pass\n\n\ndef traverse_tokens(tokens: list[Token]) -\u003e Generator[Token, None, None]:\n    \"\"\"\n    Convenience function over full_traverse_tokens. Walk all tokens recursively\n\n    Does not allow for modification of the tokens list as it is traversed.\n    To modify during traversal, use `full_traverse_tokens` instead.\n\n    Args:\n        tokens: the list of tokens to traverse\n\n    Yields:\n        token: the current token\n    \"\"\"\n    for _, token, _ in full_traverse_tokens(tokens):\n        yield token\n\n\ndef validate_block_braces(tokens: list[Token]) -\u003e None:\n    \"\"\"\n    Checks that all blocks have valid open/close pairs.\n\n    For example, ranges may have differing open/close pairs, e.g. [0..10), (0..10], etc.\n    But regular blocks must have matching open/close pairs, e.g. { ... }, ( ... ), [ ... ]\n    Performs some validation, without knowing if the block is a range or a block.\n    So more validation is needed when the actual block type is known.\n\n    Raises:\n        AssertionError: if a block is found with an invalid open/close pair\n    \"\"\"\n    for token in traverse_tokens(tokens):\n        if isinstance(token, Block_t):\n            assert token.left in valid_delim_closers, f'INTERNAL ERROR: left block opening token is not a valid token. Expected one of {[*valid_delim_closers.keys()]}. Got \\'{token.left}\\''\n            assert token.right in valid_delim_closers[token.left], f'ERROR: mismatched opening and closing braces. For opening brace \\'{token.left}\\', expected one of \\'{valid_delim_closers[token.left]}\\''\n\n\ndef validate_functions():\n\n    # Validate the @peek_eat function signatures\n    peek_eat_functions = get_peek_eat_funcs_with_name()\n    for name, wrapper_func in peek_eat_functions:\n        func = wrapper_func._eat_func\n        signature = inspect.signature(func)\n        param_types = [param.annotation for param in signature.parameters.values()\n                       if param.default is inspect.Parameter.empty]\n        return_type = signature.return_annotation\n\n        # Check if the function has the correct signature\n        if len(param_types) != 1 or param_types[0] != str or return_type != int | None:\n            pdb.set_trace()\n            raise ValueError(f\"{func.__name__} has an invalid signature: `{signature}`. Expected `(src: str) -\u003e int | None`\")\n\n    # Validate the @full_eat function signatures\n    full_eat_functions = get_full_eat_funcs_with_name()\n    for name, wrapper_func in full_eat_functions:\n        func = wrapper_func._eat_func\n        signature = inspect.signature(func)\n        param_types = [param.annotation for param in signature.parameters.values()\n                       if param.default is inspect.Parameter.empty]\n        return_type = signature.return_annotation\n\n        # Check if the function has the correct signature\n        error_message = f\"{func.__name__} has an invalid signature: `{signature}`. Expected `(src: str) -\u003e tuple[int, Token] | None`\"\n        if not (isinstance(return_type, UnionType) and len(return_type.__args__) == 2 and type(None) in return_type.__args__):\n            raise ValueError(error_message)\n        A, B = return_type.__args__\n        if B is not type(None):\n            B, A = A, B\n        if not (isinstance(A, type(tuple)) and len(A.__args__) == 2 and A.__args__[0] is int and issubclass(A.__args__[1], Token)):\n            raise ValueError(error_message)\n        if len(param_types) != 1 or param_types[0] != str:\n            pdb.set_trace()\n            raise ValueError(error_message)\n\n    # check for any functions that start with eat_ but are not decorated with @eat\n    peek_eat_func_names = {name for name, _ in peek_eat_functions}\n    full_eat_func_names = {name for name, _ in full_eat_functions}\n    for name, func in globals().items():\n        if name.startswith(\"eat_\") and callable(func) and name not in peek_eat_func_names and name not in full_eat_func_names:\n            raise ValueError(f\"`{name}()` function is not decorated with @peek_eat or @full_eat\")\n\n\ndef escape_whitespace(s: str):\n    \"\"\"convert a string to one where all non-space whitespace is escaped\"\"\"\n    escape_map = {\n        '\\t': '\\\\t',\n        '\\r': '\\\\r',\n        '\\f': '\\\\f',\n        '\\v': '\\\\v',\n        '\\n': '\\\\n',\n    }\n    return ''.join(escape_map.get(c, c) for c in s)\n\n\ndef tprint(token: Token, level=0):\n    \"\"\"\n    print a token with a certain indentation level.\n\n    If tokens contain nested tokens, they will be printed recursively with an increased indentation level\n    \"\"\"\n    print(f'{\"    \"*level}', end='')\n    if isinstance(token, Block_t):\n        print(f'\u003cBlock {token.left}{token.right}\u003e')\n        for t in token.body:\n            tprint(t, level=level+1)\n    elif isinstance(token, String_t):\n        print(f'\u003cString\u003e')\n        for t in token.body:\n            tprint(t, level=level+1)\n    elif isinstance(token, TypeParam_t):\n        print(f'\u003cTypeParam\u003e')\n        for t in token.body:\n            tprint(t, level=level+1)\n    else:\n        print(token)\n\n\ndef test():\n    import sys\n    \"\"\"simple test dewy program\"\"\"\n\n    try:\n        path = sys.argv[1]\n    except IndexError:\n        raise ValueError(\"Usage: `python tokenizer.py path/to/file.dewy\u003e`\")\n\n    with open(path) as f:\n        src = f.read()\n\n    tokens = tokenize(src)\n    print(f'matched tokens:')\n    tprint(Block_t(left='{', right='}', body=tokens))\n    # for t in tokens:\n    #     tprint(t, level=1)\n\n\nif __name__ == \"__main__\":\n    validate_functions()\n    test()\n"])</script><script>self.__next_f.push([1,"22:T226c,"])</script><script>self.__next_f.push([1,"from typing import TypeVar, Generic, Callable\nimport pdb\n\n\ndef wrap_coords(method: Callable):\n    def wrapped_method(self, *args, **kwargs):\n        result = method(self, *args, **kwargs)\n        if isinstance(result, str) and len(result) == len(self):\n            custom_str = CoordString(result)\n            custom_str.row_col_map = self.row_col_map\n            return custom_str\n        else:\n            raise ValueError(\"coord_string_method must return a string of the same length as the original string\")\n        return result\n\n    return wrapped_method\n\n\ndef fail_coords(method: Callable):\n    def wrapped_method(self, *args, **kwargs):\n        raise ValueError(f\"coord_string_method {method} cannot be called on a CoordString, as it will not return a CoordString\")\n    return wrapped_method\n\n\nclass CoordString(str):\n    \"\"\"\n    Drop-in replacement for str that keeps track of the coordinates of each character in the string\n\n    Identical to normal strings, but attaches the `row_col(i:int) -\u003e tuple[int, int]` method\n    which returns the (row, column) of the character at index i\n\n    Args:\n        anchor (tuple[int,int], optional): The row and column of the top left of the string. Defaults to (0, 0).\n    \"\"\"\n    def __new__(cls, *args, anchor: tuple[int, int] = (0, 0), **kwargs):\n        self = super().__new__(cls, *args, **kwargs)\n        row, col = anchor\n        self.row_col_map = self._generate_row_col_map(row, col)\n\n        return self\n\n    # TODO: make init so that class recognized .row_col_map as property on instances\n    # def __init__(self, s:str, row_col_map:list[tuple[int,int]]):\n\n    def _generate_row_col_map(self, row=0, col=0) -\u003e list[tuple[int, int]]:\n        row_col_map = []\n        for c in self:\n            if c == '\\n':\n                row_col_map.append((row, col))\n                row += 1\n                col = 0\n            else:\n                row_col_map.append((row, col))\n                col += 1\n        return row_col_map\n\n    def __getitem__(self, key):\n        if isinstance(key, slice):\n            sliced_str = super().__getitem__(key)\n            sliced_row_col_map = self.row_col_map[key]\n            custom_str = CoordString(sliced_str)\n            custom_str.row_col_map = sliced_row_col_map\n            return custom_str\n        return super().__getitem__(key)\n\n    def loc(self, index):\n        return self.row_col_map[index]\n\n    @staticmethod\n    def from_existing(new_str: str, old_coords: list[tuple[int, int]]) -\u003e 'CoordString':\n        new_coord_str = CoordString(new_str)\n        new_coord_str.row_col_map = old_coords\n        return new_coord_str\n\n    # wrappers for string methods that should return CoordStrings\n    def lstrip(self, *args, **kwargs):\n        result = super().lstrip(*args, **kwargs)\n        custom_str = CoordString(result)\n        custom_str.row_col_map = self.row_col_map[len(self)-len(result):]\n        return custom_str\n\n    def rstrip(self, *args, **kwargs):\n        result = super().rstrip(*args, **kwargs)\n        custom_str = CoordString(result)\n        custom_str.row_col_map = self.row_col_map[:len(result)]\n        return custom_str\n\n    def strip(self, *args, **kwargs):\n        return self.lstrip(*args, **kwargs).rstrip(*args, **kwargs)\n\n    @wrap_coords\n    def capitalize(self): return super().capitalize()\n\n    @wrap_coords\n    def casefold(self): return super().casefold()\n\n    @wrap_coords\n    def lower(self): return super().lower()\n\n    @wrap_coords\n    def upper(self): return super().upper()\n\n    @wrap_coords\n    def swapcase(self): return super().swapcase()\n\n    @wrap_coords\n    def title(self): return super().title()\n\n    @wrap_coords\n    def translate(self, table): return super().translate(table)\n\n    @wrap_coords\n    def replace(self, old, new, count=-1): return super().replace(old, new, count)\n\n    @fail_coords\n    def center(self, *args, **kwargs): ...\n\n    @fail_coords\n    def expandtabs(self, *args, **kwargs): ...\n\n    @fail_coords\n    def ljust(self, *args, **kwargs): ...\n\n    @fail_coords\n    def zfill(self, *args, **kwargs): ...\n\n\n# TODO: maybe make adding this string with other regular str illegal\n\n\nint_parsable_base_prefixes = {\n    '0b': 2,  '0B': 2,\n    '0t': 3,  '0T': 3,\n    '0q': 4,  '0Q': 4,\n    '0s': 6,  '0S': 6,\n    '0o': 8,  '0O': 8,\n    '0d': 10, '0D': 10,\n    # '0z':12, #uses different digits Z/z and X/x (instead of A/a and B/b expected by int())\n    '0x': 16, '0X': 16,\n    '0u': 32, '0U': 32,\n    '0r': 36, '0R': 36,\n    # '0y':64, #more than int's max parsable base (36)\n}\n\n\ndef based_number_to_int(src: str) -\u003e int:\n    \"\"\"\n    convert a number in a given base to an int\n    \"\"\"\n    prefix, digits = src[:2], src[2:]\n    if prefix in int_parsable_base_prefixes:\n        return int(digits, int_parsable_base_prefixes[prefix])\n    elif prefix == '0z':\n        raise NotImplementedError(f\"base {prefix} is not supported\")\n    elif prefix == '0y':\n        raise NotImplementedError(f\"base {prefix} is not supported\")\n    else:\n        raise ValueError(f\"INTERNAL ERROR: base {prefix} is not a valid base\")\n\n\ndef bool_to_bool(src: str) -\u003e bool:\n    \"\"\"\n    convert a (case-insensitive) bool literal to a bool\n    \"\"\"\n    try:\n        return bool(['false', 'true'].index(src.lower()))\n    except ValueError:\n        raise ValueError(f\"INTERNAL ERROR: bool {src} is not a valid bool\") from None\n\n\nclass CaselessStr(str):\n    def __init__(self, s: str):\n        super().__init__()\n\n    def __eq__(self, other: str):\n        return self.casefold() == other.casefold()\n\n    def __hash__(self):\n        return hash(self.casefold())\n\n    def __repr__(self):\n        return f\"(caseless)'{self.casefold()}'\"\n\n    def __str__(self):\n        return self.casefold()\n\n\nT = TypeVar('T')\nU = TypeVar('U')\n\n\nclass CaseSelectiveDict(Generic[T]):\n    \"\"\"A dictionary where keys may be either case sensitive or case insensitive\"\"\"\n\n    def __init__(self, **kwargs):\n        self.caseless: dict[CaselessStr, any] = {}\n        self.caseful: dict[str, any] = {}\n        for k, v in kwargs.items():\n            self.__setitem__(k, v)  # Use setitem to handle initial assignments\n\n    def __getitem__(self, key: str | CaselessStr) -\u003e T:\n        try:\n            return self.caseless[CaselessStr(key)]\n        except KeyError:\n            pass\n        try:\n            return self.caseful[key]\n        except KeyError:\n            pass\n\n        raise KeyError(f\"Key {key} not found\") from None\n\n    def __setitem__(self, key: str | CaselessStr, value: T):\n        if not isinstance(key, (str, CaselessStr)):\n            raise TypeError(f\"Key must be of type str or CaselessStr, not '{type(key)}'\")\n\n        if CaselessStr(key) in self.caseless:\n            if not isinstance(key, CaselessStr):\n                key = CaselessStr(key)\n            self.caseless[key] = value\n            return\n\n        if isinstance(key, CaselessStr):\n            # need to check if the new key was in the caseful keys. This is an ERROR\n            caseful_keys = [*self.caseful.keys()]\n            folded_caseful_keys = [k.casefold() for k in caseful_keys]\n            if key in folded_caseful_keys:\n                existing_key = caseful_keys[folded_caseful_keys.index(key)]\n                raise KeyError(f\"Cannot insert key {repr(key)}. Caseful version {repr(existing_key)} already exists\")\n            self.caseless[key] = value\n            return\n\n        self.caseful[key] = value\n\n    def __delitem__(self, key: str | CaselessStr):\n        key = str(key)\n        try:\n            del self.caseless[CaselessStr(key)]\n            return\n        except KeyError:\n            pass\n        try:\n            del self.caseful[key]\n            return\n        except KeyError:\n            pass\n\n        raise KeyError(f\"Key {key} not found\") from None\n\n    def __contains__(self, key: str | CaselessStr):\n        try:\n            return CaselessStr(key) in self.caseless or key in self.caseful\n        except TypeError:\n            return False\n\n    def has_key(self, key: str | CaselessStr):\n        return key in self\n\n    def get(self, key: str | CaselessStr, default: U = None) -\u003e T | U:\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __len__(self):\n        return len(self.caseless) + len(self.caseful)\n\n    def __iter__(self):\n        return iter(self.keys())\n\n    def items(self):\n        return {**self.caseless, **self.caseful}.items()\n\n    def keys(self) -\u003e list[str | CaselessStr]:\n        return [*self.caseless.keys(), *self.caseful.keys()]\n\n    def values(self) -\u003e list[T]:\n        return [*self.caseless.values(), *self.caseful.values()]\n\n    def __repr__(self):\n        return f\"CaseSelectiveDict({self.caseless}, {self.caseful})\"\n\n    def __str__(self):\n        items = {**self.caseless, **self.caseful}\n        return f'{items}'\n"])</script><script>self.__next_f.push([1,"23:T4d7,"])</script><script>self.__next_f.push([1,"\"\"\"\nCollection of all the Dewy Language backends\n\"\"\"\n\nfrom .python import python_interpreter\nfrom .qbe import qbe_compiler\nfrom .llvm import llvm_compiler\nfrom .c import c_compiler\nfrom .x86_64 import x86_64_compiler\nfrom .arm import arm_compiler\nfrom .riscv import riscv_compiler\nfrom .shell import shell_compiler\nfrom typing import Protocol\nfrom pathlib import Path\n\nclass Backend(Protocol):\n    def __call__(self, path: Path, args: list[str]) -\u003e None:\n        ...\n\n\nbackend_map: dict[str, Backend] = {\n    'python': python_interpreter,\n    'qbe': qbe_compiler,\n    'llvm': llvm_compiler,\n    'c': c_compiler,\n    'x86_64': x86_64_compiler,\n    'arm': arm_compiler,\n    'riscv': riscv_compiler,\n    'sh': shell_compiler,\n    'zsh': shell_compiler,\n    'bash': shell_compiler,\n    'fish': shell_compiler,\n    'posix': shell_compiler,\n    'powershell': shell_compiler,\n}\nbackend_names = [*backend_map.keys()]\n\n\ndef get_backend(name: str) -\u003e Backend:\n    try:\n        return backend_map[name.lower()]\n    except:\n        raise ValueError(f'Unknown backend \"{name}\"') from None\n\n\ndef get_version() -\u003e str:\n    \"\"\"Return the semantic version of the language\"\"\"\n    return (Path(__file__).parent.parent.parent / 'VERSION').read_text().strip()\n"])</script><script>self.__next_f.push([1,"24:T97a3,"])</script><script>self.__next_f.push([1,"from ..postparse import post_parse\nfrom ..tokenizer import tokenize\nfrom ..postok import post_process\nfrom ..parser import top_level_parse, Scope as ParserScope\nfrom ..syntax import (\n    AST,\n    Type,\n    PointsTo, BidirPointsTo,\n    ListOfASTs, PrototypeTuple, Block, Array, Group, Range, Object, Dict, BidirDict, UnpackTarget,\n    TypedIdentifier,\n    Void, void, Undefined, undefined, untyped,\n    String, IString,\n    Flowable, Flow, If, Loop, Default,\n    Identifier, Express, Declare,\n    PrototypePyAction, Call,\n    Assign,\n    Int, Bool,\n    Range, IterIn,\n    BinOp,\n    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,\n    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,\n    Add, Sub, Mul, Div, IDiv, Mod, Pow,\n    And, Or, Xor, Nand, Nor, Xnor,\n    UnaryPrefixOp,\n    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv, AtHandle,\n    Spread,\n)\n\nfrom ..postparse import FunctionLiteral, Signature, normalize_function_args\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Protocol, cast, Callable as TypingCallable, Any\nfrom functools import cache\nfrom collections import defaultdict\nfrom types import SimpleNamespace\nfrom enum import Enum, auto\n\nfrom argparse import ArgumentParser\n\nimport pdb\n\n\n\ndef python_interpreter(path: Path, args: list[str]):\n    arg_parser = ArgumentParser(description='Dewy Compiler: Python Interpreter Backend')\n    arg_parser.add_argument('--verbose', action='store_true', help='Print verbose output')\n    args = arg_parser.parse_args(args)\n\n    # get the source code and tokenize\n    src = path.read_text()\n    tokens = tokenize(src)\n    post_process(tokens)\n\n    # parse tokens into AST\n    ast = top_level_parse(tokens)\n    ast = post_parse(ast)\n\n    # debug printing\n    if args.verbose:\n        print_ast(ast)\n        print(repr(ast))\n\n    # run the program\n    res = top_level_evaluate(ast)\n    if res is not void:\n        print(res)\n\ndef print_ast(ast: AST):\n    \"\"\"little helper function to print out the equivalent source code of an AST\"\"\"\n    print('```dewy')\n    if isinstance(ast, (Block, Group)):\n        for i in ast.__iter_asts__(): print(i)\n    else:\n        print(ast)\n    print('```')\n\n\ndef top_level_evaluate(ast:AST) -\u003e AST:\n    scope = Scope.default()\n    insert_pyactions(scope)\n    return evaluate(ast, scope)\n\n\n############################ Runtime helper classes ############################\n\nclass MetaNamespace(SimpleNamespace):\n    \"\"\"A simple namespace for storing AST meta attributes for use at runtime\"\"\"\n    def __getattribute__(self, key: str) -\u003e Any | None:\n        \"\"\"Get the attribute associated with the key, or None if it doesn't exist\"\"\"\n        try:\n            return super().__getattribute__(key)\n        except AttributeError:\n            return None\n\n    def __setattr__(self, key: str, value: Any) -\u003e None:\n        \"\"\"Set the attribute associated with the key\"\"\"\n        super().__setattr__(key, value)\n\n\nclass MetaNamespaceDict(defaultdict):\n    \"\"\"A defaultdict that preprocesses AST keys to use the classname + memory address as the key\"\"\"\n    def __init__(self):\n        super().__init__(MetaNamespace)\n\n    # add preprocessing to both __getitem__ and __setitem__ to handle AST keys\n    # apparently __setitem__ always calls __getitem__ so we only need to override __getitem__\n    def __getitem__(self, item: AST) -\u003e Any | None:\n        key = f'::{item.__class__.__name__}@{hex(id(item))}'\n        return super().__getitem__(key)\n\n@dataclass\nclass Scope(ParserScope):\n    \"\"\"An extension of the Scope used during parsing to support runtime\"\"\"\n    meta: dict[AST, MetaNamespace] = field(default_factory=MetaNamespaceDict)\n\n    def __repr__(self):\n        return f'\u003cScope@{hex(id(self))}\u003e'\n\n\nclass Iter(AST):\n    item: AST\n    i: int\n\n    def __str__(self):\n        return f'Iter({self.item}, i={self.i})'\n\nclass PyActionArgsPreprocessor(Protocol):\n    def __call__(self, args: list[AST], kwargs: dict[str, AST], scope: Scope) -\u003e tuple[list[Any], dict[str, Any]]: ...\n\nclass PyAction(AST):\n    signature: Signature\n    preprocessor: PyActionArgsPreprocessor\n    action: TypingCallable[..., AST]\n    return_type: AST\n\n    def __str__(self):\n        return f'{self.signature}: {self.return_type} =\u003e {self.action}'\n\n    def from_prototype(proto: PrototypePyAction, preprocessor: PyActionArgsPreprocessor, action: TypingCallable[..., AST]) -\u003e 'PyAction':\n        return PyAction(\n            signature=normalize_function_args(proto.args),\n            preprocessor=preprocessor,\n            action=action,\n            return_type=proto.return_type,\n        )\n\nclass Closure(AST):\n    fn: FunctionLiteral\n    scope: Scope\n\n    def __str__(self):\n        return f'{self.fn} with \u003cScope@{hex(id(self.scope))}\u003e'\n        # scope_lines = []\n        # for name, var in self.scope.vars.items():\n        #     line = f'{var.decltype.name.lower()} {name}'\n        #     if var.type is not untyped:\n        #         line += f': {var.type}'\n        #     if var.value is self:\n        #         line += ' = \u003cself\u003e'\n        #     elif var.value is not void:\n        #         line += f' = {var.value}'\n        #     scope_lines.append(line)\n        # scope_contents = ', '.join(scope_lines)\n        # return f'{self.fn} with scope=[{scope_contents}]'\n\n\n\nclass Suspense(AST):\n    getter: TypingCallable[[], AST]\n\n    def __str__(self):\n        return f'Suspense(\u003cgetter@{hex(id(self.getter))}\u003e)'\n    \n    # def from_value(value: AST, scope: Scope) -\u003e 'Suspense':\n    #     return Suspense(lambda: evaluate(value, scope))\n\n############################ Evaluation functions ############################\n\n#DEBUG supporting py3.11\nfrom typing import TypeVar\nT = TypeVar('T', bound=AST)\nclass EvalFunc(Protocol):\n    def __call__(self, ast: T, scope: Scope) -\u003e AST: ...\n\ndef no_op(ast: T, scope: Scope) -\u003e T:\n    \"\"\"For ASTs that just return themselves when evaluated\"\"\"\n    return ast\n\n# class EvalFunc[T](Protocol):\n#     def __call__(self, ast: T, scope: Scope) -\u003e AST: ...\n\n\n# def no_op[T](ast: T, scope: Scope) -\u003e T:\n#     \"\"\"For ASTs that just return themselves when evaluated\"\"\"\n#     return ast\n\ndef cannot_evaluate(ast: AST, scope: Scope) -\u003e AST:\n    raise ValueError(f'INTERNAL ERROR: evaluation of type {type(ast)} is not possible')\n\n\n@cache\ndef get_eval_fn_map() -\u003e dict[type[AST], EvalFunc]:\n    return {\n        Declare: evaluate_declare,\n        Call: evaluate_call,\n        Block: evaluate_block,\n        Group: evaluate_group,\n        Array: evaluate_array,\n        Dict: evaluate_dict,\n        PointsTo: evaluate_points_to,\n        BidirDict: evaluate_bidir_dict,\n        BidirPointsTo: evaluate_bidir_points_to,\n        Assign: evaluate_assign,\n        IterIn: evaluate_iter_in,\n        FunctionLiteral: evaluate_function_literal,\n        Closure: evaluate_closure,\n        PyAction: evaluate_pyaction,\n        Suspense: evaluate_suspense,\n        String: no_op,\n        IString: evaluate_istring,\n        Identifier: cannot_evaluate,\n        Express: evaluate_express,\n        Int: no_op,\n        Bool: no_op,\n        Range: no_op,\n        Flow: evaluate_flow,\n        Default: evaluate_default,\n        If: evaluate_if,\n        Loop: evaluate_loop,\n        Less: evaluate_less,\n        Equal: evaluate_equal,\n        And: evaluate_and,\n        Or: evaluate_or,\n        Not: evaluate_not,\n        UnaryNeg: evaluate_unary_neg,\n        Add: evaluate_add,\n        Mul: evaluate_mul,\n        Div: evaluate_div,\n        Mod: evaluate_mod,\n        AtHandle: evaluate_at_handle,\n        Undefined: no_op,\n        Void: no_op,\n        #TODO: other AST types here\n    }\n\ndef evaluate(ast:AST, scope:Scope) -\u003e AST:\n    eval_fn_map = get_eval_fn_map()\n\n    ast_type = type(ast)\n    if ast_type in eval_fn_map:\n        return eval_fn_map[ast_type](ast, scope)\n\n    raise NotImplementedError(f'evaluation not implemented for {ast_type}')\n\n\ndef suspend(ast:AST, scope:Scope) -\u003e Suspense:\n    return Suspense(lambda: evaluate(ast, scope))\n\n\ndef evaluate_declare(ast: Declare, scope: Scope):\n    match ast.target:\n        case Identifier(name):\n            value = void\n            type = untyped\n        case TypedIdentifier(id=Identifier(name), type=type):\n            value = void\n        case Assign(left=Identifier(name), right=right):\n            value = evaluate(right, scope)\n            type = untyped\n        case Assign(left=TypedIdentifier(id=Identifier(name), type=type), right=right):\n            value = evaluate(right, scope)\n        case _:\n            raise NotImplementedError(f'Declare not implemented yet for {ast.target=}')\n\n    scope.declare(name, value, type, ast.decltype)\n    return void\n\n\ndef evaluate_call(call: Call, scope: Scope) -\u003e AST:\n    f = call.f\n\n    # get the expression of the group\n    if isinstance(f, Group):\n        f = evaluate(f, scope)\n\n    # get the value pointed to by the identifier\n    if isinstance(f, Identifier):\n        f = scope.get(f.name).value\n\n    # if this is a handle, do a partial evaluation rather than a call\n    if isinstance(f, AtHandle):\n        return apply_partial_eval(f.operand, call.args, scope)\n\n    # AST being called must be TypingCallable\n    assert isinstance(f, (PyAction, Closure)), f'expected Function or PyAction, got {f}'\n\n    # save the args of the call as metadata for the function AST\n    call_args, call_kwargs = collect_calling_args(call.args, scope)\n    scope.meta[f].call_args = call_args, call_kwargs\n\n    # run the function and return the result\n    if isinstance(f, PyAction):\n        return evaluate_pyaction(f, scope)\n    if isinstance(f, Closure):\n        return evaluate_closure(f, scope)\n\n    pdb.set_trace()\n    raise NotImplementedError(f'Function evaluation not implemented yet')\n\n#TODO: longer term this might also return a list/dict of spread args passed into the function\ndef collect_calling_args(args: AST | None, scope: Scope) -\u003e tuple[list[AST], dict[str, AST]]:\n    \"\"\"\n    Collect the arguments that a function is being called with\n    e.g. `let fn = (a b c) =\u003e a + b + c; fn(1 c=2 3)`\n    then the calling args are [1, 3] and {c: 2}\n\n    Args:\n        args: the arguments being passed to the function. If None, then treat as a no-arg call\n        scope: the scope in which the function is being called\n\n    Returns:\n        a tuple of the positional arguments and keyword arguments\n    \"\"\"\n    match args:\n        case None | Void(): return [], {}\n        case Identifier(name): return [scope.get(name).value], {}\n        case Assign(left=Identifier(name)|TypedIdentifier(id=Identifier(name)), right=right): return [], {name: right}\n        # case Assign(left=UnpackTarget() as target, right=right): raise NotImplementedError('UnpackTarget not implemented yet')\n        case Assign(): raise NotImplementedError('Assign not implemented yet') #called recursively if a calling arg was an keyword arg rather than positional\n        case Spread(right=right):\n            pdb.set_trace()\n            ... #right should be iterable, so extend with the values it expresses\n                #whether to add to args or kwargs depends on each type from right\n            val = evaluate(right, scope)\n            match val:\n                case Array(items): ... #return [collect_calling_args(i, scope) for i in items]\n        case Int() | String() | IString() | Call() | Express() | UnaryPrefixOp():\n            return [args], {}\n        # case Call(): return [args], {}\n        case Group(items):\n            call_args, call_kwargs = [], {}\n            for i in items:\n                a, kw = collect_calling_args(i, scope)\n                call_args.extend(a)\n                call_kwargs.update(kw)\n            return call_args, call_kwargs\n        case _:\n            pdb.set_trace()\n            raise NotImplementedError(f'collect_args not implemented yet for {args}')\n\n\n    raise NotImplementedError(f'collect_args not implemented yet for {args}')\n\n\ndef get_arg_name(arg: AST) -\u003e str:\n    \"\"\"little helper function to get the name of an argument\"\"\"\n    match arg:\n        case Identifier(name): return name\n        case TypedIdentifier(id=Identifier(name)): return name\n        case Assign(left=Identifier(name)): return name\n        case Assign(left=TypedIdentifier(id=Identifier(name))): return name\n        case _:\n            raise NotImplementedError(f'get_arg_name not implemented for {arg=}')\n\n# TODO: this should maybe take in 2 scopes, one for the closure scope, and one for the callings scope... or closure scope args should already be evaluated...\n# TODO: resolve args should handle the full gamut of possible types of arguments in a function\n#       position or keyword arguments (with or without defaults)\n#       position only arguments (with or without defaults)\n#       keyword only arguments (with or without defaults)\n# Note: the function signature will stay the same since calling a function or pyaction just amounts to setting args and kwargs\n# TODO: probably expand to include a container for unpack targets\ndef resolve_calling_args(signature: Signature, args: list[AST], kwargs: dict[str, AST], caller_scope: Scope, closure_scope: Scope = Scope()) -\u003e tuple[dict[str, AST], dict[str, AST]]:\n    \"\"\"\n    Resolve the final list of arguments the function actually receives\n    Properly handles when the signature includes defaults, keyword args, positional args, partial evaluation, etc.\n\n    e.g. if we have:\n\n    ```dewy\n    let fn = (a, b, c) =\u003e a + b + c\n    fn = @fn(c=3)\n    fn(1, a=2)\n    ```\n\n    then we would resolve to args={b: 1} kwargs={a: 2, c: 3}\n\n    This is mainly for interfacing with python functions which want *args, **kwargs\n    \"\"\"\n    # for now, just assume all args are position or keyword args\n    # partial eval converts that particular arg to keyword only\n    sig_pkwargs, sig_pargs, sig_kwargs = signature.pkwargs, signature.pargs, signature.kwargs\n    dewy_args, dewy_kwargs = {}, {}\n\n    # evaluate all the args and kwargs\n    args = [evaluate(arg, caller_scope) for arg in args]\n    kwargs = {name: evaluate(arg, caller_scope) for name, arg in kwargs.items()}\n\n\n    # first pull out the calling keyword arguments\n    dewy_kwargs.update(kwargs)\n    sig_pkwargs = [*filter(lambda item: get_arg_name(item) not in kwargs, sig_pkwargs)]\n    sig_kwargs = [*filter(lambda item: get_arg_name(item) not in kwargs, sig_kwargs)]\n\n    #remaining kwargs are added to the dewy_kwargs\n    for arg in sig_kwargs:\n        assert isinstance(arg, Assign), f'INTERNAL ERROR: {arg=} is not an Assign'\n        name = get_arg_name(arg)\n        dewy_kwargs[name] = evaluate(arg.right, closure_scope)\n\n    if len(sig_pargs) + len(sig_pkwargs) \u003c len(args):\n        raise ValueError(f'Too many positional arguments for function. {signature=}, {args=}, {kwargs=}')\n\n    # next, pair up the positional arguments\n    for spec, arg in zip(sig_pargs + sig_pkwargs, args):\n        name = get_arg_name(spec)\n        dewy_args[name] = arg\n\n    # all remaining arguments must have a value provided by the signature\n    for arg in (sig_pargs + sig_pkwargs)[len(args):]:\n        #TODO: handle unpacking/spread\n        name = get_arg_name(arg)\n        if not isinstance(arg, Assign):\n            pdb.set_trace()\n            raise ValueError(f'Non-Assign arguments remaining unpaired in signature. {signature=}, {args=}, {kwargs=}')\n        dewy_kwargs[name] = evaluate(arg.right, closure_scope)\n\n    return dewy_args, dewy_kwargs\n\n\n# class ArgBinding(Enum):\n#     KEYWORD = auto()\n#     POSITIONAL = auto()\n\n# @dataclass\n# class ResolvedArg:\n#     name: str\n#     value: AST\n#     binding: ArgBinding\n\n\ndef update_signature(signature: Signature, args: list[AST], scope: Scope) -\u003e Signature:\n    \"\"\"Given values to partially apply to a function, update the call signature to reflect the new values\"\"\"\n    call_args, call_kwargs = collect_calling_args(args, scope)\n    sig_pkwargs, sig_pargs, sig_kwargs = signature.pkwargs.copy(), signature.pargs.copy(), signature.kwargs.copy()\n    for item in sig_kwargs:\n        name = get_arg_name(item)\n        if name in call_kwargs:\n            sig_kwargs = [*filter(lambda i: get_arg_name(i) != name, sig_kwargs)]\n            right = suspend(call_kwargs[name], scope) #((lambda ast, scope: lambda: evaluate(ast, scope))(call_kwargs[name], scope))\n            sig_kwargs.append(Assign(left=Identifier(name), right=right))\n    for item in sig_pkwargs:\n        name = get_arg_name(item)\n        if name in call_kwargs:\n            sig_pkwargs = [*filter(lambda i: get_arg_name(i) != name, sig_pkwargs)]\n            right = suspend(call_kwargs[name], scope) #Suspense((lambda ast, scope: lambda: evaluate(ast, scope))(call_kwargs[name], scope))\n            # any pkwargs become kwargs when a value is given by keyword or position\n            sig_kwargs.append(Assign(left=Identifier(name), right=right))\n\n    # update the positional arguments\n    for item in call_args:\n        if len(sig_pargs) \u003e 0:\n            parg, sig_pargs = sig_pargs[0], sig_pargs[1:]\n            name = get_arg_name(parg)\n        elif len(sig_pkwargs) \u003e 0:\n            parg, sig_pkwargs = sig_pkwargs[0], sig_pkwargs[1:]\n            name = get_arg_name(parg)\n        else:\n            raise ValueError(f'Too many positional arguments for function. {signature=}, {args=}')\n        right = suspend(item, scope) #Suspense((lambda ast, scope: lambda: evaluate(ast, scope))(item, scope))\n        sig_kwargs.append(Assign(left=Identifier(name), right=right))\n\n    return Signature(pkwargs=sig_pkwargs, pargs=sig_pargs, kwargs=sig_kwargs)\n\ndef apply_partial_eval(f: AST, args: list[AST], scope: Scope) -\u003e AST:\n    match f:\n        # # this case shouldn't really be possible since you have to wrap a function literal in parenthesis to @ it, turning it into a Closure\n        # case FunctionLiteral(args=signature, body=body):\n        #     new_signature = update_signature(signature, args, scope)\n        #     return FunctionLiteral(args=new_signature, body=body)\n\n        case Closure(fn=FunctionLiteral(args=signature, body=body), scope=closure_scope):\n            new_signature = update_signature(signature, args, scope)\n            return Closure(fn=FunctionLiteral(args=new_signature, body=body), scope=closure_scope)\n\n        case PyAction(signature=signature, preprocessor=preprocessor, action=action, return_type=return_type):\n            new_signature = update_signature(signature, args, scope)\n            return PyAction(signature=new_signature, preprocessor=preprocessor, action=action, return_type=return_type)\n\n        case Identifier(name):\n            f = scope.get(name).value\n            return apply_partial_eval(f, args, scope)\n        case _:\n            raise NotImplementedError(f'Partial evaluation not implemented yet for {f=}')\n\n\n# def find_arg_index(name: str, signature: Group) -\u003e int:\n#     for i, arg in enumerate(signature.items):\n#         match arg:\n#             case Identifier(arg_name) | TypedIdentifier(id=Identifier(arg_name)) | Assign(left=Identifier(arg_name)):\n#                 if arg_name == name: return i\n#             case _:\n#                 raise NotImplementedError(f'find_arg_index not implemented yet for {arg=}')\n\n#     raise ValueError(f'Argument {name} not found in signature {signature}')\n\ndef evaluate_group(ast: Group, scope: Scope):\n\n    expressed: list[AST] = []\n    for expr in ast.items:\n        res = evaluate(expr, scope)\n        if res is not void:\n            expressed.append(res)\n    if len(expressed) == 0:\n        return void\n    if len(expressed) == 1:\n        return expressed[0]\n    raise NotImplementedError(f'Block with multiple expressions not yet supported. {ast=}, {expressed=}')\n\n\ndef evaluate_block(ast: Block, scope: Scope):\n    scope = Scope(scope)\n    return evaluate_group(Group(ast.items), scope)\n\ndef evaluate_array(ast: Array, scope: Scope) -\u003e Array:\n    return Array([evaluate(i, scope) for i in ast.items])\n\ndef evaluate_dict(ast: Dict, scope: Scope) -\u003e Dict:\n    return Dict([evaluate(kv, scope) for kv in ast.items])\n\ndef evaluate_points_to(ast: PointsTo, scope: Scope) -\u003e PointsTo:\n    return PointsTo(evaluate(ast.left, scope), evaluate(ast.right, scope))\n\ndef evaluate_bidir_dict(ast: BidirDict, scope: Scope) -\u003e BidirDict:\n    return BidirDict([evaluate(kv, scope) for kv in ast.items])\n\ndef evaluate_bidir_points_to(ast: BidirPointsTo, scope: Scope) -\u003e BidirPointsTo:\n    return BidirPointsTo(evaluate(ast.left, scope), evaluate(ast.right, scope))\n\ndef evaluate_assign(ast: Assign, scope: Scope):\n    match ast:\n        case Assign(left=Identifier(name), right=right):\n            right = evaluate(right, scope)\n            scope.assign(name, right)\n            return void\n        case Assign(left=UnpackTarget() as target, right=right):\n            right = evaluate(right, scope)\n            unpack_assign(target, right, scope)\n            return void\n    pdb.set_trace()\n    raise NotImplementedError('Assign not implemented yet')\n\n\ndef evaluate_iter_in(ast: IterIn, scope: Scope):\n\n    # helper function for progressing the iterator\n    def step_iter_in(iter_props: tuple[TypingCallable, Iter], scope: Scope) -\u003e AST:\n        binder, iterable = iter_props\n        cond, val = iter_next(iterable).items\n        binder(val)\n        return cond\n\n    # if the iterator properties are already in the scope, use them\n    if (res := scope.meta[ast].props) is not None:\n        return step_iter_in(cast(tuple[TypingCallable, Iter], res), scope)\n\n    # otherwise initialize since this is the first time we're hitting this IterIn\n    match ast:\n        case IterIn(left=Identifier(name), right=right):\n            right = evaluate(right, scope)\n            props = lambda x: scope.assign(name, x), Iter(item=right, i=0)\n            scope.meta[ast].props = props\n            return step_iter_in(props, scope)\n        case IterIn(left=UnpackTarget() as target, right=right):\n            right = evaluate(right, scope)\n            props = lambda x: unpack_assign(target, x, scope), Iter(item=right, i=0)\n            scope.meta[ast].props = props\n            return step_iter_in(props, scope)\n\n    pdb.set_trace()\n    raise NotImplementedError('IterIn not implemented yet')\n\n# def determine_how_many_to_take\ndef take_n(gen, n:int):\n    return [next(gen) for _ in range(n)]\n\n#TODO: this is really only for array unpacking. need to handle object unpacking as well...\n#      need to check what type value\ndef unpack_assign(target: UnpackTarget, value: AST, scope: Scope):\n\n    # current inefficient hack to unpack strings\n    if isinstance(value, String):\n        value = Array([String(c) for c in value.val])\n\n    # current types supporting unpacking\n    if not isinstance(value, (Array, Dict, PointsTo, BidirDict, BidirPointsTo, Undefined)):\n        raise NotImplementedError(f'unpack_assign() is not yet implemented for {value=}')\n\n    # determine how many targets will be assigned, and if spread is present\n    num_targets = len(target.target)\n    num_spread = sum(isinstance(t, Spread) for t in target.target)\n    if num_spread \u003e 1: raise RuntimeError(f'Only one spread is allowed in unpacking. {target=}, {value=}')\n\n    # undefined unpacks as many undefineds as there are non-spread targets\n    if isinstance(value, Undefined):\n        value = Array([undefined for _ in range(num_targets - num_spread)])\n\n    # verify if enough values to unpack, and set up generator (using built in iteration over ASTs children)\n    num_values = len([*value.__iter_asts__()])\n    spread_size = num_values - num_targets + 1  # if a spread is present, how many elements it will take\n    if num_targets - num_spread \u003e num_values: raise RuntimeError(f'Not enough values to unpack. {num_targets=}, {target=}, {value=}')\n    gen = value.__iter_asts__()\n\n    for left in target.target:\n        match left:\n            case Identifier(name):\n                scope.assign(name, next(gen))\n            # #TODO: object member renamed unpack. need to get the member of the object and assign it to the new name\n            # case Assign(left=Identifier(name), right=right):\n            #     scope.assign(name, right)\n            case UnpackTarget():\n                unpack_assign(left, next(gen), scope)\n            case Spread(right=Identifier(name)):\n                scope.assign(name, Array([next(gen) for _ in range(spread_size)]))\n            case Spread(right=UnpackTarget() as left):\n                unpack_assign(left, Array([next(gen) for _ in range(spread_size)]), scope)\n            case _:\n                pdb.set_trace()\n                raise NotImplementedError(f'unpack_assign not implemented for {left=} and right={next(gen)}')\n\n    # if there are any remaining values, raise an error\n    if (remaining := [*gen]):\n        raise RuntimeError(f'Too many values to unpack. {num_targets=}, {target=}, {value=}, {remaining=}')\n\n# TODO: probably break this up into one function per type of iterable\ndef iter_next(iter: Iter):\n    match iter.item:\n        case Array(items):\n            if iter.i \u003e= len(items):\n                cond, val = Bool(False), undefined\n            else:\n                cond, val = Bool(True), items[iter.i]\n            iter.i += 1\n            return Array([cond, val])\n        case Dict(items):\n            if iter.i \u003e= len(items):\n                cond, val = Bool(False), undefined\n            else:\n                cond, val = Bool(True), items[iter.i]\n            iter.i += 1\n            return Array([cond, val])\n        case Range(left=Int(val=l), right=Void()|Undefined(), brackets=brackets):\n            offset = int(brackets[0] == '(') # handle if first value is exclusive\n            cond, val = Bool(True), Int(l + iter.i + offset)\n            iter.i += 1\n            return Array([cond, val])\n        case Range(left=Int(val=l), right=Int(val=r), brackets=brackets):\n            offset = int(brackets[0] == '(') # handle if first value is exclusive\n            end_offset = int(brackets[1] == ']')\n            i = l + iter.i + offset\n            if i \u003e r + end_offset - 1:\n                cond, val = Bool(False), undefined\n            else:\n                cond, val = Bool(True), Int(i)\n            iter.i += 1\n            return Array([cond, val])\n        case Range(left=Array(items=[Int(val=r0), Int(val=r1)]), right=Void()|Undefined(), brackets=brackets):\n            offset = int(brackets[0] == '(') # handle if first value is exclusive\n            step = r1 - r0\n            cond, val = Bool(True), Int(r0 + (iter.i + offset) * step)\n            iter.i += 1\n            return Array([cond, val])\n        case Range(left=Array(items=[Int(val=r0), Int(val=r1)]), right=Int(val=r2), brackets=brackets):\n            offset = int(brackets[0] == '(') # handle if first value is exclusive\n            end_offset = int(brackets[1] == ']')\n            step = r1 - r0\n            i = r0 + (iter.i + offset) * step\n            if i \u003e r2 + end_offset - 1:\n                cond, val = Bool(False), undefined\n            else:\n                cond, val = Bool(True), Int(i)\n            iter.i += 1\n            return Array([cond, val])\n        #TODO: other range cases...\n        case _:\n            pdb.set_trace()\n            raise NotImplementedError(f'iter_next not implemented yet for {iter.item=}')\n\n\n\ndef evaluate_function_literal(ast: FunctionLiteral, scope: Scope):\n    return Closure(fn=ast, scope=scope)\n\ndef evaluate_closure(ast: Closure, scope: Scope):\n    closure_scope = Scope(ast.scope)\n    caller_scope = Scope(scope)\n    args, kwargs = scope.meta[ast].call_args or ([], {})\n    signature = ast.fn.args\n\n    # attach all the args to scope so body can access them\n    dewy_args, dewy_kwargs = resolve_calling_args(signature, args, kwargs, caller_scope, closure_scope)\n    for name, value in (dewy_args | dewy_kwargs).items():\n        closure_scope.assign(name, value)\n\n    return evaluate(ast.fn.body, closure_scope)\n\n\ndef evaluate_pyaction(ast: PyAction, scope: Scope):\n    caller_scope = Scope(scope)\n    call_args, call_kwargs = scope.meta[ast].call_args or ([], {})\n    dewy_args, dewy_kwargs = resolve_calling_args(ast.signature, call_args, call_kwargs, caller_scope)\n    py_args, py_kwargs = ast.preprocessor([*dewy_args.values()], dewy_kwargs, caller_scope)\n    return ast.action(*py_args, **py_kwargs)\n\n\ndef evaluate_suspense(ast: Suspense, scope: Scope) -\u003e AST:\n    # suspense is independent of the calling scope\n    return ast.getter()\n\n\ndef evaluate_istring(ast: IString, scope: Scope) -\u003e String:\n    parts = (py_stringify(i, scope) for i in ast.parts)\n    return String(''.join(parts))\n\n\ndef evaluate_express(ast: Express, scope: Scope):\n    val = scope.get(ast.id.name).value\n    return evaluate(val, scope)\n\ndef evaluate_flow(ast: Flow, scope: Scope):\n    for branch in ast.branches:\n        #TODO: slightly hacky way to get the child scope created by the branch (so we can check if it was entered)\n        child_scope = None\n        def save_child_scope(scope: Scope):\n            nonlocal child_scope\n            child_scope = scope\n\n        match branch:\n            case Default(): res = evaluate_default(branch, scope, save_child_scope)\n            case If(): res = evaluate_if(branch, scope, save_child_scope)\n            case Loop(): res = evaluate_loop(branch, scope, save_child_scope)\n            case _:\n                pdb.set_trace()\n                raise NotImplementedError(f'evaluate_flow not implemented for flow type {branch=}')\n\n        # if the branch was entered, return its result\n        assert child_scope.meta[branch].was_entered is not None, f'INTERNAL ERROR: {branch=} .was_entered was not set'\n        if child_scope.meta[branch].was_entered:\n            return res\n\n    # if no branches were entered, return void\n    return void\n\ndef evaluate_default(ast: Default, scope: Scope, save_child_scope:TypingCallable[[Scope], None]=lambda _: None):\n    scope = Scope(scope)\n    save_child_scope(scope)\n    scope.meta[ast].was_entered = True\n    return evaluate(ast.body, scope)\n\n#TODO: this needs improvements!\n#Issue URL: https://github.com/david-andrew/dewy-lang/issues/2\ndef evaluate_if(ast: If, scope: Scope, save_child_scope:TypingCallable[[Scope], None]=lambda _: None):\n    scope = Scope(scope)\n    save_child_scope(scope)\n    scope.meta[ast].was_entered = False\n    if cast(Bool, evaluate(ast.condition, scope)).val:\n        scope.meta[ast].was_entered = True\n        return evaluate(ast.body, scope)\n\n    # is this correct if the If isn't entered?\n    return void\n\n#TODO: this needs improvements!\ndef evaluate_loop(ast: Loop, scope: Scope, save_child_scope:TypingCallable[[Scope], None]=lambda _: None):\n    scope = Scope(scope)\n    save_child_scope(scope)\n    scope.meta[ast].was_entered = False\n    while cast(Bool, evaluate(ast.condition, scope)).val:\n        scope.meta[ast].was_entered = True\n        evaluate(ast.body, scope)\n\n    # for now loops can't return anything\n    return void\n    # ast.body\n    # ast.condition\n    # pdb.set_trace()\n\nclass Comparable(Protocol):\n    def __lt__(self, other: \"Comparable\") -\u003e bool: ...\n    def __le__(self, other: \"Comparable\") -\u003e bool: ...\n    def __gt__(self, other: \"Comparable\") -\u003e bool: ...\n    def __ge__(self, other: \"Comparable\") -\u003e bool: ...\n    def __eq__(self, other: \"Comparable\") -\u003e bool: ...\n    def __ne__(self, other: \"Comparable\") -\u003e bool: ...\n\ndef evaluate_comparison_op(op: TypingCallable[[Comparable, Comparable], bool], ast: AST, scope: Scope):\n    left = evaluate(ast.left, scope)\n    right = evaluate(ast.right, scope)\n    match left, right:\n        case Int(val=l), Int(val=r): return Bool(op(l, r))\n        case String(val=l), String(val=r): return Bool(op(l, r))\n        case _:\n            raise NotImplementedError(f'{op.__name__} not implemented for {left=} and {right=}')\n\ndef evaluate_less(ast: Less, scope: Scope):\n    return evaluate_comparison_op(lambda l, r: l \u003c r, ast, scope)\n\ndef evaluate_equal(ast: Equal, scope: Scope):\n    return evaluate_comparison_op(lambda l, r: l == r, ast, scope)\n\n\n\n# TODO: op depends on what type of operands. bools use built-in and/or/etc, but ints need to use the bitwise operators\ndef evaluate_logical_op(logical_op: TypingCallable[[bool, bool], bool], bitwise_op: TypingCallable[[int, int], int], ast: AST, scope: Scope):\n    left = evaluate(ast.left, scope)\n    right = evaluate(ast.right, scope)\n    match left, right:\n        case Bool(val=l), Bool(val=r): return Bool(logical_op(l, r))\n        case Int(val=l), Int(val=r): return Int(bitwise_op(l, r))\n        case _:\n            raise NotImplementedError(f'evaluate logical op not implemented for {left=} and {right=}')\n\ndef evaluate_and(ast: And, scope: Scope):\n    return evaluate_logical_op(lambda l, r: l and r, lambda l, r: l \u0026 r, ast, scope)\n\ndef evaluate_or(ast: Or, scope: Scope):\n    return evaluate_logical_op(lambda l, r: l or r, lambda l, r: l | r, ast, scope)\n\ndef evaluate_not(ast: Not, scope: Scope):\n    val = evaluate(ast.operand, scope)\n    match val:\n        case Bool(val=v): return Bool(not v)\n        case Int(val=v): return Int(~v) #TODO: bitwise not depends on the size of the int...\n        case _:\n            raise NotImplementedError(f'Not not implemented for {val=}')\n\ndef evaluate_unary_neg(ast: UnaryNeg, scope: Scope):\n    val = evaluate(ast.operand, scope)\n    match val:\n        case Int(val=v): return Int(-v)\n        case _:\n            raise NotImplementedError(f'Negation not implemented for {val=}')\n\n#TODO: long term, probably convert this into a matrix for all the input types and ops, where pairs can register to it\n# def evaluate_arithmetic_op[T](op: TypingCallable[[T, T], T], ast: AST, scope: Scope):\n#     left = evaluate(ast.left, scope)\n#     right = evaluate(ast.right, scope)\n#     match left, right:\n#         case Int(val=l), Int(val=r): return Int(op(l, r))\n#         case Array(), Array(): return Array(op(left.items, right.items)) #TODO: restrict this to add only...\n#         case _:\n#             raise NotImplementedError(f'{op.__name__} not implemented for {left=} and {right=}')\n\n#TODO: unified arithmetic evaluation function\ndef evaluate_add(ast: Add, scope: Scope):\n    left = evaluate(ast.left, scope)\n    right = evaluate(ast.right, scope)\n    match left, right:\n        case Int(val=l), Int(val=r): return Int(l + r)\n        case Array(items=l), Array(items=r): return Array(l + r)\n        case _:\n            raise NotImplementedError(f'Add not implemented for {left=} and {right=}')\n\ndef evaluate_mul(ast: Mul, scope: Scope):\n    left = evaluate(ast.left, scope)\n    right = evaluate(ast.right, scope)\n    match left, right:\n        case Int(val=l), Int(val=r): return Int(l * r)\n        case _:\n            raise NotImplementedError(f'Mul not implemented for {left=} and {right=}')\n\ndef evaluate_div(ast: Div, scope: Scope):\n    left = evaluate(ast.left, scope)\n    right = evaluate(ast.right, scope)\n    match left, right:\n        case Int(val=l), Int(val=r): return Int(l / r)\n        case _:\n            raise NotImplementedError(f'Div not implemented for {left=} and {right=}')\n\ndef evaluate_mod(ast: Mod, scope: Scope):\n    left = evaluate(ast.left, scope)\n    right = evaluate(ast.right, scope)\n    match left, right:\n        case Int(val=l), Int(val=r): return Int(l % r)\n        case _:\n            raise NotImplementedError(f'Mod not implemented for {left=} and {right=}')\n\n\ndef evaluate_at_handle(ast: AtHandle, scope: Scope):\n    match ast.operand:\n        case Identifier(name):\n            return scope.get(name).value\n        case _:\n            pdb.set_trace()\n            raise NotImplementedError(f'AtHandle not implemented for {ast.operand=}')\n\n############################ Builtin functions and helpers ############################\n\n# all references to python functions go through this interface to allow for easy swapping\nfrom functools import partial\nclass BuiltinFuncs:\n    printl=print\n    print=partial(print, end='')\n    readl=input\n\n#TODO: consider adding a flag repr vs str, where initially str is used, but children get repr. \n# as is, stringifying should put quotes around strings that are children of other objects \n# but top level printed strings should not show their quotes\ndef py_stringify(ast: AST, scope: Scope, top_level:bool=False) -\u003e str:    \n    # don't evaluate. already evaluated by resolve_calling_args\n    ast = evaluate(ast, scope) if not isinstance(ast, (PyAction, Closure)) else ast\n    match ast:\n        # types that require special handling (i.e. because they have children that need to be stringified)\n        case String(val): return val# if top_level else f'\"{val}\"'\n        case Array(items): return f\"[{' '.join(py_stringify(i, scope) for i in items)}]\"\n        case Dict(items): return f\"[{' '.join(py_stringify(kv, scope) for kv in items)}]\"\n        case PointsTo(left, right): return f'{py_stringify(left, scope)}-\u003e{py_stringify(right, scope)}'\n        case BidirDict(items): return f\"[{' '.join(py_stringify(kv, scope) for kv in items)}]\"\n        case BidirPointsTo(left, right): return f'{py_stringify(left, scope)}\u003c-\u003e{py_stringify(right, scope)}'\n        case Closure(fn): return f'{fn}'\n        case FunctionLiteral() as fn: return f'{fn}'\n        case PyAction() as fn: return f'{fn}'\n        # case AtHandle() as at: return py_stringify(evaluate(at, scope), scope)\n\n        # can use the built-in __str__ method for these types\n        case Int() | Bool() | Undefined(): return str(ast)\n\n        # TBD what other types need to be handled\n        case _:\n            pdb.set_trace()\n            raise NotImplementedError(f'stringify not implemented for {type(ast)}')\n    pdb.set_trace()\n\n\n    raise NotImplementedError('stringify not implemented yet')\n\ndef preprocess_py_print_args(args: list[AST], kwargs: dict[str, AST], scope: Scope) -\u003e tuple[list[Any], dict[str, Any]]:\n    py_args = [py_stringify(i, scope, top_level=True) for i in args]\n    py_kwargs = {k: py_stringify(v, scope) for k, v in kwargs.items()}\n    return py_args, py_kwargs\n\ndef py_printl(s:str) -\u003e Void:\n    BuiltinFuncs.printl(s)\n    return void\n\ndef py_print(s:str) -\u003e Void:\n    BuiltinFuncs.print(s)\n    return void\n\ndef py_readl() -\u003e String:\n    return String(BuiltinFuncs.readl())\n\ndef insert_pyactions(scope: Scope):\n    \"\"\"replace pyaction stubs with actual implementations\"\"\"\n    if 'printl' in scope.vars:\n        assert isinstance((proto:=scope.vars['printl'].value), PrototypePyAction)\n        scope.vars['printl'].value = PyAction.from_prototype(proto, preprocess_py_print_args, py_printl)\n    if 'print' in scope.vars:\n        assert isinstance((proto:=scope.vars['print'].value), PrototypePyAction)\n        scope.vars['print'].value = PyAction.from_prototype(proto, preprocess_py_print_args, py_print)\n    if 'readl' in scope.vars:\n        assert isinstance((proto:=scope.vars['readl'].value), PrototypePyAction)\n        scope.vars['readl'].value = PyAction.from_prototype(proto, lambda *a: ([],{}), py_readl)\n"])</script><script>self.__next_f.push([1,"25:T8e5,"])</script><script>self.__next_f.push([1,"//TODO: uncomment when types are supported\n/{\n// simple xorshift+ generator\nstate:uint64 = 123456789\nrand = ():uint64 =\u003e {\n    state xor= state \u003e\u003e 21\n    state xor= state \u003c\u003c 35\n    state xor= state \u003e\u003e 4\n    \n    return state * 2_685821_657736_338717 //TODO: divide by uint64.MAX (18_446744_073709_551615)\n}\nhalf = 9_223372_036854_775807\na = rand \u003c? half\nb = rand \u003c? half\nc = rand \u003c? half\nd = rand \u003c? half\ne = rand \u003c? half\nf = rand \u003c? half\ng = rand \u003c? half\nh = rand \u003c? half\ni = rand \u003c? half\nj = rand \u003c? half\nk = rand \u003c? half\nl = rand \u003c? half\nm = rand \u003c? half\nn = rand \u003c? half\no = rand \u003c? half\np = rand \u003c? half\nq = rand \u003c? half\nr = rand \u003c? half\ns = rand \u003c? half\nt = rand \u003c? half\nu = rand \u003c? half\nv = rand \u003c? half\nw = rand \u003c? half\nx = rand \u003c? half\ny = rand \u003c? half\nz = rand \u003c? half\n}/\n\n//manually specify bools\na = false\nb = true\nc = true\nd = false\ne = true\nf = false\ng = true\nh = false\ni = true\nj = false\nk = true\nl = false\nm = true\nn = false\no = true\np = false\nq = true\nr = false\ns = true\nt = false\nu = true\nv = false\nw = true\nx = false\ny = true\nz = false\n\n\n\nif a\n    printl'a'\nelse if b\n    if c\n        if d\n            if e\n                printl'bcde'\n            else if f\n                if g\n                    if h\n                        printl'bcdfgh'\n                    else if i\n                        printl'bcdfgi'\n                    else if j\n                        printl'bcdfgj'\n                    else\n                        printl'bcdfg[]'\n                else if k\n                    printl'bcdfk'\n                else if l\n                    if m\n                        printl'bcdflm'\n                    else\n                        printl'bcdfl[]'\n                else\n                    printl'bcdf[]'\n            else if n\n                printl'bcdn'\n            else\n                printl'bcd[]'\n        else if o\n            printl'bco'\n        else if p\n            if q\n                printl'bcpq'\n            else\n                printl'bcp[]'\n        else\n            printl'bc[]'\n    else\n        printl'b[]'\nelse if r\n    printl'r'\nelse if s\n    if t\n        printl'st'\n    else if u\n        printl'su'\nif v\n    if w\n        printl'vw'\n    else if x\n        printl'x'\nif y\n    printl'y'\nelse if z\n    printl'z'\nelse\n    printl'[]'\n"])</script><script>self.__next_f.push([1,"26:T626,"])</script><script>self.__next_f.push([1,"// simplest case\nx = 10\nplus_5 = () =\u003e x + 5\nprintl(plus_5())\n\n// taking in args\nx = 12\nmy_closure = y =\u003e x + y\nprintl(my_closure(-5))\n\n\n// returning a closure from a child scope\na = 13\nmy_closure = {\n    b = 10\n    fn = c =\u003e a + b + c // uses `b` from this scope and `a` from parent scope\n    @fn\n}\nprintl(my_closure(5))\n\n\n\n// print the handle to the closure itself\nprintl(@my_closure)\nprintl(@printl)\n\n\n// some pathological cases\nmy_print = {\n    x = 5\n    s = 'string with internal reference to x={x}'\n    @printl(s)\n}\nmy_print\n\n\nmy_print = {\n    my_str = {\n        x = 5\n        s = 'string with internal reference to x={x}'\n        s\n    }\n    fn = @printl(my_str)\n    @fn\n}\nmy_print\n\n\n\n// Very deeply nested closures combined with partial evaluation\nX = 'xpple'\nY = 'yanana'\nfn = {\n    Z = 'zeach'\n    fn = {\n        A = () =\u003e '@Apricot'\n        fn = {\n            B = () =\u003e '@Blueberry'\n            fn = {\n                fn = (x y z a b c d) =\u003e {\n                    printl'x=\"{x}\"\\ny=\"{y}\"\\nz=\"{z}\"\\na=\"{a}\"\\nb=\"{b}\"\\nc=\"{c}\"\\nd=\"{d}\"'\n                }\n                @fn\n            }\n            printl'fn={@fn}'\n            @fn(b=B) // note just B should evaluate the string on calling fn\n        }\n        printl'fn={@fn}'\n        @fn(a=@A)  // note that @A should evaluate the string inside of the IString\n    }\n    printl'fn={@fn}'\n    @fn(z=Z d='manually assigning D' c='unused C')\n}\nprintl'fn={@fn}'\nfn = @fn(y=Y x=X)\nprintl'fn={@fn}'\nfn = @fn(c='C')\nprintl'fn={@fn}'\nprintl(@fn)\nfn\nprintl()\nprintl(@fn(c='a different C'))\nfn(c='an even more different C')"])</script><script>self.__next_f.push([1,"27:T57f,"])</script><script>self.__next_f.push([1,"//examples of each of the possible function signatures\n\n// all args start out as positional or keyword (regardless of if they have defaults)\n// partial application with positional args turns them into keyword only args\n// partial application with keyword args turns them into positional args\n// TBD unpack types probably need to be positional?\n// TBD spread types probably need to be positional? \n//     or can we spread each of the possible types? I feel like spread just collects up everything not already specified in the signature\n\n//////////////// Positional Arguments ////////////////\n// no arguments\nf0 = () =\u003e 0 // can call with `f0` or `f0()`\n\n// 1 positional argument\nf1 = x =\u003e x + 1     // can call with `f1(5)`\nf1b = (x) =\u003e x + 1  // can call with `f1b(5)`\n\n// 2 positional arguments\nf2 = (x y) =\u003e x + y // can call with `f2(5 6)`\n\n// 3 positional arguments\nf3 = (x y z) =\u003e x + y + z // can call with `f3(5 6 7)`\n\nprintl'Positional Arguments'\nprintl(f0)\nprintl(f1(5))\nprintl(f2(5 6))\nprintl(f3(5 6 7))\n\n\n//////////////// Optional Arguments ////////////////\n// 1 positional and 1 optional keyword-only argument\nf2b = (x y=2) =\u003e x + y // can call with `f2b(5)` or `f2b(5 y=6)`\n\n// 1 optional keyword-only argument\nf1c = (x=2) =\u003e x + 1 // can call with 'f1c' or `f1c()` or `f1c(x=5)`\n\nprintl'Optional Arguments'\nprintl(f2b(5))\nprintl(f2b(5 y=6))\nprintl(f1c)\nprintl(f1c(x=3))\n\n\n//TODO: more examples"])</script><script>self.__next_f.push([1,"28:Tbe6,"])</script><script>self.__next_f.push([1,"/{ \n    Dewy Docs mdbook preprocessor:\n    find code blocks labelled 'dewy' or 'dewy, editable' and convert them to iframes\n}/\n\n#main = () =\u003e {\n    // mdbook calls preprocessor twice, first with args [\"supports\", \u003crenderer\u003e], \n    // and then if the first call exited with 0, the actual preprocessor run occurs\n    // ignore the second argument (meaning support for all renderers)\n    if sys.argv.length \u003e? 1 and sys.argv[1] =? \"supports\"\n        sys.exit(0)\n\n    // get and parse the json input from stdin\n    // TODO: need to make a json parser\n    context, book = parse_json(read())\n\n    loop section in book['sections']\n    {\n        section['Chapter']['content'] |\u003e= process_markdown\n\n        loop subitem in section['Chapter']['sub_items']\n            subitem['Chapter']['content'] |\u003e= process_markdown\n\n        print(dump_json(book))\n    }\n}\n\n\ncounter = iter[0..]\nprocess_markdown = (input_markdown) =\u003e {\n    lines = input_markdown.split('\\n')\n\n    // lines starting with ```dewy\n    starts = [\n        loop i in [0..] and line in lines \n            if line[..6] =? '```dewy' \n                i\n    ]\n\n    // early return if no dewy code blocks\n    if starts.length =? 0\n        return input_markdown\n\n    // ```dewy lines that are followed by ', editable' (whitespace invariant)\n    editables = [\n        loop i in starts\n        {\n            remainder = lines[i][7..].strip\n            remainder[0] = ',' and remainder[1..].strip =? 'editable'\n        }\n    ]\n\n    // match closing ``` lines\n    stops = [\n        loop i in starts\n            loop line in lines[i..]\n                if line =? '```'\n                {\n                    j\n                    break\n                }\n    ]\n\n    if starts.length not=? stops.length\n        printl'Error: mismatched dewy code block starts and ends'\n        sys.exit(1)\n\n    return [\n        loop \n            start in starts \n            and stop in stops \n            and editable in editables\n            and prev_stop in [0 ...(stops.+1)]\n        {\n            //push the previous non-code block content\n            if start \u003e? prev_stop\n                lines[prev_stop..start).join('\\n')\n\n            i = next(counter) // to give each iframe a unique id\n            page = if editable 'demo_only' else 'src_only'\n            code = lines(start..stop).join('\\n')\n            encoded_code = url_quote(code)\n            \n            // push the iframe replacement\n            f'\n                \u003ciframe\n                    src=\"https://david-andrew.github.io/iframes/dewy/{page}?src={encoded_code}\u0026id=DewyIframe{i}\"\n                    style=\"width: 100%; border-radius: 0.5rem;\"\n                    id=\"DewyIframe{i}\"\n                    frameBorder=\"0\"\n                \u003e\u003c/iframe\u003e\n            '\n        }\n\n        // push the last non-code block content\n        if stops[-1]+1 \u003c? lines.length\n            lines[stops[-1]+1..].join('\\n')\n \n    ].join('\\n')\n}\n\n\n//TODO: implement these functions\nlet parse_json = () =\u003e {}\nlet dump_json = () =\u003e {}\nlet url_quote = () =\u003e {}\nlet f = () =\u003e {}"])</script><script>self.__next_f.push([1,"29:T1093,"])</script><script>self.__next_f.push([1,"//simple, fast, high quality, dependency-free RNG generation\n//uniform distribution via XORSHIFT*\n//normal distribution via PPND16\n\n\nRNG = (s:uint64) =\u003e [\n    next_u64 = () =\u003e {\n        s ^= s \u003e\u003e 21\n        s ^= s \u003c\u003c 35\n        s ^= s \u003e\u003e 4\n        s * 2685821657736338717\n    }\n    next_uniform = () =\u003e fast_to_uniform(next_u64())\n    next_normal = () =\u003e ppnd16(fast_to_uniform(next_u64()))\n    \n    /{\n        Use bit hacks to quickly convert a 64-bit number to a double in the range [0, 1)\n\n        @param x the number to convert. Only the lowest 23 bits are used.\n        @return the number in the range [0, 1)\n    }/\n    fast_to_uniform = (x:uint64):float64 =\u003e {\n        const mask1 = 0x3FF0_0000_0000_0000\n        const mask2 = 0x3FFF_FFFF_FFFF_FFFF\n        out: uint64 = (x or mask1) and mask2\n        (out as float64) - 1\n    }\n\n    full_to_uniform = (x:uint64):float64 =\u003e truediv(x uint64.max float64)\n\n    /{\n        Convert a uniformly distributed double in the range (0, 1) to a normally distributed double\n        Uses the PPND16 algorithm from Algorithm AS241: The Percentage Points of the Normal Distribution\n\n        @param x the uniformly distributed double in the range (0, 1)\n        @return the normally distributed double\n    }/\n    ppnd16 = (x:float64) =\u003e {\n\n        // zero area at x=0/x=1\n        if x \u003c=? 0 or x \u003e=? 1\n            return 0\n\n        const split1:float64 = 0.425\n        const split2:float64 = 5.0\n        const C1:float64 = 0.180625\n        const C2:float64 = 1.6\n\n        // Cofficients for x close to 0.5\n        const A:float64[] =\n        [\n            3.3871328727963665\n            133.14166789178438\n            1971.5909503065514\n            13731.693765509461\n            45921.95393154987\n            67265.7709270087\n            33430.575583588128\n            2509.0809287301227            \n        ]\n        const B:float64[] =\n        [\n            1.0\n            42.313330701600911\n            687.18700749205789\n            5394.1960214247511\n            21213.794301586597\n            39307.895800092709\n            28729.085735721943\n            5226.4952788528544\n        ]\n        \n        // Coefficients for x not close to 0, 0.5 or 1\n        const C:float64[] =\n        [\n            1.4234371107496835\n            4.6303378461565456\n            5.769497221460691\n            3.6478483247632045\n            1.2704582524523684\n            0.24178072517745061\n            0.022723844989269184\n            0.00077454501427834139\n        ]\n        const D:float64[] =\n        [\n            1.0\n            2.053191626637759\n            1.6763848301838038\n            0.6897673349851\n            0.14810397642748008\n            0.015198666563616457\n            0.00054759380849953455\n            0.0000000010507500716444169\n        ]\n\n        // Coefficients for x near 0 or 1\n        const E:float64[] =\n        [\n            6.6579046435011033\n            5.4637849111641144\n            1.7848265399172913\n            0.29656057182850487\n            0.026532189526576124\n            0.0012426609473880784\n            0.000027115555687434876\n            0.00000020103343992922882\n        ]\n        const F:float64[] =\n        [\n            1.0\n            0.599832206555888\n            0.13692988092273581\n            0.014875361290850615\n            0.00078686913114561329\n            0.000018463183175100548\n            0.0000001421511758316446\n            0.0000000000000020442631033899397\n        ]\n\n        let r:float64\n\n        // shift x to the range (-0.5, 0.5)\n        const q = x - 0.5\n        if abs(q) \u003c=? split1\n        {\n            r = C1 - q^2\n            powers = r.^[0..7]\n            return q * (A .* powers).sum / (B .* powers).sum\n        }\n        \n        // shift x back to (0,1) and invert if it was positive\n        r = if q \u003c? 0 x else 1 - x\n        r = sqrt(-log(r))\n        \n        if r \u003c=? split2\n        {\n            r -= C2\n            powers = r.^[0..7]\n            return sign(q) * (C .* powers).sum / (D .* powers).sum\n        }\n        else\n        {\n            r -= split2\n            powers = r.^[0..7]\n            return sign(q) * (E .* powers).sum / (F .* powers).sum\n        }\n    }\n]\n\n\n\nr = RNG(42)\nloop i in 0..100 printl(r.next_normal())"])</script><script>self.__next_f.push([1,"2a:Tc05,"])</script><script>self.__next_f.push([1,"// examples of syntax used in dewy\n// line comments\n/{ block/multiline comments }/\n\n// typed declaration\napple: uint64\nbanana: map\u003cint string\u003e\npeach: array\u003cint length=N\u003e  //array of ints with length N...\nlet pear: set\u003crange\u003e  // let indicates that this is definitely a new declaration, even if the identifier already exists\n\n\n\n// unpack assignment examples\nA = 1..10\nB = [loop a in A -a]\nloop [a b] in [A B] {}\n\n// object with nested objects to unpack\nmy_obj = [\n    apple = [1 2 3 4 [\n        ultimate_answer = 42\n    ]]\n    banana = 10\n    peach = [\n        purple = 23\n        blue = 12\n        orange = 'orange'\n    ]\n]\n\n// nested unpack assignment. tbd if the top level is `[unpack, params, etc] = obj`, or `obj as [unpack, params, etc]`\n[\n    [\n        a1\n        a2 \n        a3 \n        a4 \n        [answer = ultimate_answer] = a5\n    ] = apple\n    renamed_banana = banana\n    [purple blue orange] = peach\n] = my_obj\n// unpacked variables are:\n//   a1 = 1, a2 = 2, a3 = 3, a4 = 4\n//   answer = 42\n//   renamed_banana = 10\n//   purple = 23, blue = 12, orange = 'orange'\n\n// unpacking dictionaries probably treats them as just the list of key -\u003e value pairs\n// unpacking sets, probably just treats the elements like a normal array\n// unpacking ranges treats them as a normal array\n\n// `...` can be used in unpack to coalesce extra elements for list-like containers\n// there may only be 1 `...` in an unpack (otherwise it would be ambiguous which elements to collect)\n// the variable receiving the `...` will be of the same type as the original object being unpacked\nmy_arr = [1 2 3 4 5 6 7 8 9]\n[a1 a2 a3 ...my_arr a8 a9] = my_arr  //a1 = 1, a2 = 2, a3 = 3, my_arr = [4, 5, 6, 7], a8 = 8, a9 = 9\n\nmy_dict = ['apple' -\u003e 1 'banana' -\u003e 2 'peach' -\u003e 3 'pie' -\u003e 4]\n[d1 ...dict_left] = my_dict //d1 = ('apple' -\u003e 1), dict_left = ['banana' -\u003e 2 'peach' -\u003e 3 'pie' -\u003e 4]\n\n// random range note: for step sizes other than +1, use range_iter constructor e.g. range_iter(start to stop, step=5)\nmy_range = 1..inf\nloop my_range.length \u003e? 0 ( [i ...my_range] = my_range )\n// first iteration: i = 1, my_range = 2..inf\n// second iteration: i = 2, my_range = 3..inf\n// third iteration: i = 3, my_range = 4..inf\n// ...\n// for forever\n\n//[...my_range i] = my_range //will probably set my_range = 1 to inf, i = inf\n\n// unpacking too many values, or named values that don't exist just sets them to undefined\n\n\n\n// assignment expressions (i.e. python's walrus operator from https://www.python.org/dev/peps/pep-0572/)\n// Handle a matched regex\nif (match = pattern.search(data); match) not =? undefined\n{\n    // Do something with match\n}\n\n// A loop that can't be trivially rewritten using 2-arg iter()\nloop (chunk = file.read(8192) chunk.length \u003e? 0)\n{\n   process(chunk)\n}\n\n// Reuse a value that's expensive to compute\n[y=f(x) y y**2 y**3]\n\n// Share a subexpression between a comprehension filter clause and its output\nfiltered_data = [for x in data if (y=f(x) y) not =? undefined y]\n//though you could also just write this like so\nfiltered_data = [for x in data {y=f(x) if y not =? undefined y}]\n"])</script><script>self.__next_f.push([1,"2b:T14e1,"])</script><script>self.__next_f.push([1,"///////////////////// STRING INTERPOLATION /////////////////////\n\n/{Todo: probably break each section for different syntaxes into different files?}/\n\n//silly example with keyword vs identifier\nloop i i\n\nr'this is a raw string \\'  expr  'a separate string later'\n\n// simple blocks\n{   }\n( /{comment inside}/ )\n{ 2+2 }\n( 2+2 )\n\n\n//string interpolation\nmy_string = '2 + 2 = {2+2}'\n\n//complex interpolation\ns = \"first 10 primes are: {\n    primes = [2]\n    loop i in [3, 5..) and primes.length \u003c? 10\n        if i .% primes |\u003e product not =? 0 \n            primes.push(i)\n    primes\n}\"\n\n\n//alternative prime generator + getting first 10 primes\n#ctx\nprimes = [\n    2\n    lazy i in [3, 5..)\n        if i .% #ctx.primes .=? 0 |\u003e @reduce(, (prev, v) =\u003e prev and v)\n            i\n][..10)\n\n//TBD if there is a parallel way to do this where the i .% primes dispatches each operation, and fails immediately on any returning false\n#label\nprimes = [\n    2\n    lazy i in [3, 5..)\n        if not parallel_or(p =\u003e i % p =? 0, #label.primes)\n            i\n][..10)\n//parallel or is like goroutines with cancel once any is true...should have it be more flexible, e.g. able to use any of the boolean keywords that can short circuit\n//actually probably don't want to need to specify that it's parallel. Instead if there's an operation over a vector, it gets parallelized if possible.\n\n//nested interpolation\ns2 = 'this is an outer string, and {'this is an interior string with \"{my_string}\" in it'}'\n\n\n\n\n\nconst add = (a:int b:int): int =\u003e { /{return sum of a and b}/ }\nlet div = (a:real b:real): real? =\u003e { /{return a / b}/ }\n\n// function type with named default argument\nmy_func = (s:string kwarg:bool=false): void =\u003e {}\n\n//you probably can do the verbose version as well (probably useful for when you're just defining the interface without the implementation)\nmy_func: (s:string kwarg:bool=false): void\n\n\n\n// example annotations for function types\n() =\u003e ()\n() =\u003e void\n() =\u003e bool\nint =\u003e bool\na: int =\u003e bool\n(int int) =\u003e int\n\u003cT\u003e(T T) =\u003e T\n\u003cT\u003e(a:T b:T) =\u003e T\n\n// object type\n[a:int? b:string]\n\n//? (optional) is sugar for |void\n[a:int|void b:string]\n\n// operators juxtaposed to identifiers\naorb\na or b\na+b\n\n\n//based number literals\n0b1010_0011_0101_0110_1001_1010_1100_1111\n0B0101_1111_1010_1110_0011_0101_1001_1100\n\n0t012_221_012_221_012_221_012_221\n0t211_001_211_001_211_001_211_001\n\n0q331_231_223_131_331_231_223_131\n0Q123_321_123_321_123_321_123_321\n\n0s123_450_123_450_123_450_123_450\n0S543_210_543_210_543_210_543_210\n\n0o123_456_701_234_567_012_345_670\n0O012_345_670_123_456_701_234_567\n\n0d123_456_789_012_345_678_901_234\n0D987_654_321_098_765_432_109_876\n\n0z123_456_789_xe0_123_456_789_xe0\n0ZEX9_876_543_210_987_654_321_09E\n\n0x1234_5678_9abc_def0_1234_5678_9abc_def0\n0XFEDC_BA98_7654_3210_fedc_ba98_7654_3210\n\n0u0123456789abcdefghijklmnopqrstuv0123456\n0UVUTSRQPONMLKJIHGFEDCBA9876543210vutsrq\n\n0r0123456789abcdefghijklmnopqrstuvwxyz012345\n0RZYXWVUTSRQPONMLKJIHGFEDCBA9876543210zyxwv\n\n0y0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!$\n0Y$!ZYXWVUTSRQPONMLKJIHGFEDCBA9876543210zyxwvutsrqponmlkjihgfedcba\n\n//Units TODO\n\n\n\n\n[a b c] = [1 2 3]                                       //a=1, b=2, c=3\n[a [b c]] = [1 [2 3]]                                   //a=1, b=2, c=3\n[a [b c] d] = [1 [2 3] 4]                               //a=1, b=2, c=3, d=4\n[a ...b] = [1 2 3 4]                                    //a=1, b=[2 3 4]\n[a ...b c] = [1 2 3 4 5]                                //a=1, b=[2 3 4], c=5\n[a ...b [c [...d e f]]] = [1 2 3 4 [5 [6 7 8 9 10]]]    //a=1, b=[2 3 4], c=5, d=[6 7 8], e=9, f=10\n\n\n\n// silly things that are technically valid\nx = loop i in [0..10] i //x=10\n\ny = [\n    if something \n        x\n    else loop i in something_else\n        y\n    else if z\n        z\n    else loop i in last_thing\n        w\n    else\n        ()\n]\n\n\napple \u0026 banana\napple\u0026banana\napple | banana\napple|banana\n\n\n\n\n\n///////////// String prefixes ////////////////\np = s:string =\u003e [\n    //process s based on / and \\ separators\n    //store result in this object\n    route:array\u003cstring\u003e = ...\n    filename:string? = ...\n    extension:string? = ...\n]\n\np\"this/is/a/file/path.ext\"\n\n//other prefixes\nre\"[^i*\u00262@]\"                            // regex literal\nt'my_token'                             //token literal. probably my version of enums\nr'this is a raw string'                 //raw string. technically handled during tokenizing, there is no r function\n(dewy)r'''printl(\"Hello, World!\")'''    //dewy source code literal. uses raw string so that we don't have to worry about {}.\n\nipa\"ɛt vɔkavit dɛus aɾidam tɛɾam kɔngɾɛgatsiɔnɛskwɛ akwaɾum apɛlavit maɾia ɛt vidit dɛus kwɔd ɛsɛt bɔnum\" //international phonetic alphabet literal\n(apl)r\"life ← {⊃1 ⍵ ∨.∧ 3 4 = +/ +⌿ ¯1 0 1 ∘.⊖ ¯1 0 1 ⌽¨ ⊂⍵}\"  //apl expression literal\napl\u003c|r\"life ← {⊃1 ⍵ ∨.∧ 3 4 = +/ +⌿ ¯1 0 1 ∘.⊖ ¯1 0 1 ⌽¨ ⊂⍵}\"  //same as above\n\n'''this is a regular string with triple quotes'''\n\"\"\"this is a regular string with triple quotes\"\"\"\n\n///////////// object prefixes ////////////////\n//doubly linked list\ndll[1 2 3 4 6 5 3 6 3 2]\n\n//set literal syntax\nset[4 3 6 4 6 4 2 2 4 5]\n\n\n\n// silly example for generating a list of ones\nones = n =\u003e {l = [...[1..n]] l.=1 l}\nones(10) // [1 1 1 1 1 1 1 1 1 1]\n//alternate\nones = n =\u003e [loop i in 1..n 1]"])</script><script>self.__next_f.push([1,"17:[\"$\",\"$L1b\",null,{\"dewy_interpreter_source\":[{\"name\":\"src/__init__.py\",\"code\":\"\"},{\"name\":\"src/frontend.py\",\"code\":\"$1c\"},{\"name\":\"src/parser.py\",\"code\":\"$1d\"},{\"name\":\"src/postok.py\",\"code\":\"$1e\"},{\"name\":\"src/postparse.py\",\"code\":\"$1f\"},{\"name\":\"src/syntax.py\",\"code\":\"$20\"},{\"name\":\"src/tokenizer.py\",\"code\":\"$21\"},{\"name\":\"src/utils.py\",\"code\":\"$22\"},{\"name\":\"src/backend/__init__.py\",\"code\":\"$23\"},{\"name\":\"src/backend/arm.py\",\"code\":\"from pathlib import Path\\n\\ndef arm_compiler(path: Path, args: list[str]):\\n    raise NotImplementedError('ARM backend is not yet supported')\\n\"},{\"name\":\"src/backend/c.py\",\"code\":\"from pathlib import Path\\n\\ndef c_compiler(path: Path, args: list[str]):\\n    raise NotImplementedError('C backend is not yet supported')\\n\"},{\"name\":\"src/backend/llvm.py\",\"code\":\"from pathlib import Path\\n\\ndef llvm_compiler(path: Path, args: list[str]):\\n    raise NotImplementedError('LLVM backend is not yet supported')\\n\"},{\"name\":\"src/backend/python.py\",\"code\":\"$24\"},{\"name\":\"src/backend/qbe.py\",\"code\":\"from pathlib import Path\\n\\ndef qbe_compiler(path: Path, args: list[str]):\\n    raise NotImplementedError('QBE backend is not yet supported')\\n\"},{\"name\":\"src/backend/riscv.py\",\"code\":\"from pathlib import Path\\n\\ndef riscv_compiler(path: Path, args: list[str]):\\n    raise NotImplementedError('RISC-V backend is not yet supported')\\n\"},{\"name\":\"src/backend/shell.py\",\"code\":\"from pathlib import Path\\n\\ndef shell_compiler(path: Path, args: list[str]):\\n    \\\"\\\"\\\"this would target sh/powershell/etc. all simultaneously\\\"\\\"\\\"\\n    # TODO: find the explanation of how this works\\n    # https://en.wikipedia.org/wiki/Polyglot_(computing)\\n    raise NotImplementedError('Shell backend is not yet supported')\\n\"},{\"name\":\"src/backend/x86_64.py\",\"code\":\"from pathlib import Path\\n\\ndef x86_64_compiler(path: Path, args: list[str]):\\n    raise NotImplementedError('x86_64 backend is not yet supported')\\n\"}],\"dewy_examples\":{\"good_examples\":[{\"name\":\"hello.dewy\",\"code\":\"printl'Hello, World!'\"},{\"name\":\"hello_func.dewy\",\"code\":\"main = () =\u003e printl'Hello, World!'\\nmain\"},{\"name\":\"hello_name.dewy\",\"code\":\"print\\\"What's your name? \\\"\\nname = readl\\nprintl'Hello {name}!'\"},{\"name\":\"hello_loop.dewy\",\"code\":\"print\\\"What's your name? \\\"\\nname = readl\\ni = 0\\nloop i \u003c? 10 {\\n    printl'Hello {name}!'\\n    i = i + 1\\n}\"},{\"name\":\"anonymous_func.dewy\",\"code\":\"(() =\u003e printl'Hello from an anonymous function!')()\"},{\"name\":\"if_else.dewy\",\"code\":\"print\\\"What's your name? \\\"\\nname = readl\\nif name =? 'Alice' printl'Hello Alice!'\\nelse printl'Hello stranger!'\"},{\"name\":\"if_else_if.dewy\",\"code\":\"print\\\"What's your name? \\\"\\nname = readl\\nif name =? 'Alice' printl'Hello Alice!'\\nelse if name =? 'Bob' printl'Hello Bob!'\\nelse printl'Hello stranger!'\"},{\"name\":\"dangling_else.dewy\",\"code\":\"// if a then if b then s else s2\\n// See: https://en.wikipedia.org/wiki/Dangling_else\\n\\na = false\\nb = true\\n\\nif a\\n    if b\\n        printl's'\\n    else\\n        printl's2'\\nelse\\n    printl's3'\"},{\"name\":\"if_tree.dewy\",\"code\":\"$25\"},{\"name\":\"loop_in_iter.dewy\",\"code\":\"loop i in 0,2..20\\n    printl(i)\"},{\"name\":\"loop_and_iters.dewy\",\"code\":\"loop i in 0.. and j in (2..20)\\n    printl'{i} and {j}'\"},{\"name\":\"enumerate_list.dewy\",\"code\":\"// example enumerating a list\\nfruits = ['apple' 'banana' 'peach' 'pear' 'pineapple']\\nloop i in 0.. and fruit in fruits\\n    printl'{i}: {fruit}'\"},{\"name\":\"loop_or_iters.dewy\",\"code\":\"loop i in (0..20] or j in [0,2..10) \\n    printl'{i} or {j}'\"},{\"name\":\"nested_loop.dewy\",\"code\":\"loop i in 0,2..10\\n    loop j in 0,2..10\\n        printl'{i},{j}'\"},{\"name\":\"block_printing.dewy\",\"code\":\"loop i in 0,2..5 {\\n    loop j in 0,2..5 {\\n        loop k in 0,2..5 {\\n            loop l in 0,2..5 {\\n                loop m in 0,2..5 {\\n                    printl'{i},{j},{k},{l},{m}'\\n                }\\n            }\\n        }\\n    }\\n}\"},{\"name\":\"row_vs_col.dewy\",\"code\":\"unit = [1 2 3]\\nrow = [1,2,3]\\ncol = [[1] [2] [3]] // tbd if better way to make this, but generally not necessary since 1d arrays are treated as column vectors\\nmat = [1,2,3 4,5,6 7,8,9]\\ntensor = [(1,2,3),(4,5,6),(7,8,9) (10,11,12),(13,14,15),(16,17,18) (19,20,21),(22,23,24),(25,26,27)]\\n\\n// currently not supported\\nmat2 = [\\n    1 2 3\\n    4 5 6\\n    7 8 9\\n]\\n\\n\\nprintl(unit)\\nprintl(row)\\nprintl(col)\\nprintl(mat)\\nprintl(tensor)\\nprintl(mat2)\"},{\"name\":\"unpack_array.dewy\",\"code\":\"s = ['Hello' ['World' '!'] 5 10]\\nprintl's={s}'\\n\\na, b, c, d = s\\nprintl'a={a} b={b} c={c} d={d}'\\n\\na, ...b = s\\nprintl'a={a} b={b}'\\n\\n...a, b = s\\nprintl'a={a} b={b}'\\n\\na, [b c], ...d = s\\nprintl'a={a} b={b} c={c} d={d}'\\n\\na, ...b, c, d, e = s\\nprintl'a={a} b={b} c={c} d={d} e={e}'\\n\\n//error tests\\n//a, b, c, d, e = s         //error: not enough values to unpack\\n//a, b = s                  //error: too many values to unpack\\n//a, ...b, c, d, e, f = s   //error: too many values to unpack\\n\"},{\"name\":\"unpack_dict.dewy\",\"code\":\"d1 = ['a' -\u003e 1 'b' -\u003e 2 'c' -\u003e 3]\\n\\na, b, c = d1\\nprintl'a={a} b={b} c={c}'\\n\\na, ...b = d1\\nprintl'a={a} b={b}'\\n\\n...a, b = d1\\nprintl'a={a} b={b}'\\n\\na, [b c], ...d = d1\\nprintl'a={a} b={b} c={c} d={d}'\\n\\n\\n\\nd2 = ['a' \u003c-\u003e 1 'b' \u003c-\u003e 2 'c' \u003c-\u003e 3 'd' \u003c-\u003e ['e' -\u003e 4 'f' -\u003e 5]]\\n\\na, b, c, d = d2\\nprintl'a={a} b={b} c={c} d={d}'\\n\\na, ...b, c, d, e = d2\\nprintl'a={a} b={b} c={c} d={d} e={e}'\\n\\n[ka va], [kb vb], [kc vc], [kd [ke vf]] = d2\\nprintl'ka={ka} va={va} kb={kb} vb={vb} kc={kc} vc={vc} kd={kd} ke={ke} vf={vf}'\"},{\"name\":\"functions.dewy\",\"code\":\"let fn = (x y) =\u003e x + 5 + y\\nprintl(fn(8 1))\"},{\"name\":\"partial_functions.dewy\",\"code\":\"let add = (a b) =\u003e a + b\\nlet add5 = @add(5)\\nlet thirteen = @add5(8)\\nlet add7 = @add(a=7)\\nlet add10 = @add(b=10)\\n\\nprintl(add(3 5))\\nprintl(add5(2))\\nprintl(thirteen)\\nprintl(add7(3))\\nprintl(add10(3))\\n\\nlet fortytwo = @add10(32)\\nlet fortythree = @add10(0 b=43)\\nlet fortyfour = @add10(a=34)\\nlet fortyfive = @add5(40)\\nlet fortysix = @add5(a=6 40)\\n\\nprintl(fortytwo)\\nprintl(fortythree)\\nprintl(fortyfour)\\nprintl(fortyfive)\\nprintl(fortysix)\"},{\"name\":\"closure.dewy\",\"code\":\"$26\"},{\"name\":\"functions.dewy\",\"code\":\"let fn = (x y) =\u003e x + 5 + y\\nprintl(fn(8 1))\"},{\"name\":\"function_signatures.dewy\",\"code\":\"$27\"},{\"name\":\"fizzbuzz-1.dewy\",\"code\":\"// fizbuzz that works with the current version of dewy\\nmultiples = [3 5]\\nwords = ['Fizz' 'Buzz']\\nloop i in [0..100)\\n{\\n    printed_words = false\\n    loop multiple in multiples and word in words\\n    {\\n        if i % multiple =? 0 \\n        { \\n            print(word)\\n            printed_words = true\\n        }\\n    }\\n    if not printed_words print(i)\\n    printl()\\n}\"},{\"name\":\"fizzbuzz0.dewy\",\"code\":\"taps = [3 -\u003e 'Fizz' 5 -\u003e 'Buzz' /{7 -\u003e 'Bazz' 11 -\u003e 'Bar'}/]\\nloop i in [0..100)\\n{\\n    printed_words = false\\n    loop [tap string] in taps \\n    {\\n        if i % tap =? 0 \\n        { \\n            print(string)\\n            printed_words = true\\n        }\\n    }\\n    if not printed_words print(i)\\n    printl()\\n}\"},{\"name\":\"primes.dewy\",\"code\":\"/{simple program for generating prime numbers}/\\nprintl(2)\\nprimes = [2]\\nloop candidate in 3,5..100 {\\n    no_factors = true\\n    loop p in primes and p*p \u003c? candidate+1 {\\n        if candidate % p =? 0 {\\n            no_factors = false\\n            // break //not supported yet..\\n        }\\n    }\\n    if no_factors {\\n        printl(candidate)\\n        primes = primes + [candidate]\\n        //primes = [...primes candidate]\\n        //primes.push(i) // not supported yet.. ambiguous parse since jux has qint precedence, while . equals the higher precedence\\n    }\\n}\"}],\"bad_examples\":[{\"name\":\"tensors.dewy\",\"code\":\"\\ntensor1 = [(1,2,3),(4,5,6),(7,8,9) (10,11,12),(13,14,15),(16,17,18) (19,20,21),(22,23,24),(25,26,27)]\\n\\n\\ntensor2 = [\\n     1  2  3\\n     4  5  6\\n     7  8  9\\n\\n    10 11 12\\n    13 14 15\\n    16 17 18\\n\\n    19 20 21\\n    22 23 24\\n    25 26 27\\n]\\n\\ntensor3 = [\\n     1  2  3\\n     4  5  6\\n     7  8  9\\n\\n    10 11 12\\n    13 14 15\\n    16 17 18\\n\\n    19 20 21\\n    22 23 24\\n    25 26 27\\n\\n\\n    28 29 30\\n    31 32 33\\n    34 35 36\\n\\n    37 38 39\\n    40 41 42\\n    43 44 45\\n\\n    46 47 48\\n    49 50 51\\n    52 53 54\\n\\n\\n    55 56 57\\n    58 59 60\\n    61 62 63\\n\\n    64 65 66\\n    67 68 69\\n    70 71 72\\n\\n    73 74 75\\n    76 77 78\\n    79 80 81\\n]\\n\\n// multiline expression means non-whitespace sensitive for outer array, but inner array is whitespace sensitive\\ntensor4 = [\\n    loop i in 0..3 9i .+\\n    [\\n        1 2 3\\n        4 5 6\\n        7 8 9\\n    ]\\n]\\n\\n\\nprintl(tensor1)\\nprintl(tensor2)\\nprintl(tensor3)\\nprintl(tensor4)\"},{\"name\":\"unpack_object.dewy\",\"code\":\"o1 = [a='Hello' b=['World' '!'] c=5 d=10]\\n\\na, b, c, d = o1\\nprintl'a={a} b={b} c={c} d={d}'\\n\\nb, c = o1\\nprintl'b={b} c={c}'\\n\\na, ...rest = o1\\nprintl'a={a} rest={rest}'\\n\\nd, c, ...rest, b = o1\\nprintl'd={d} c={c} rest={rest} b={b}'\\n\\n[a [b1 b2]=b] = o1\\nprintl'a={a} b1={b1} b2={b2}'\"},{\"name\":\"declare.dewy\",\"code\":\"//some examples of declarations\\nlet x\\nlet y = 10\\nlet z: int = 100 + 1000\\nlet w: SomeType\u003ca=10 b=20\u003e = 15\\nlet v = (a:int b:int): int =\u003e a + b + 10\\n\\nconst a\\nconst b = '{1000/10}'\\nconst c: int = { 10000 }\\n\\nlocal_const α\\nlocal_const β = 100(100)\\nlocal_const γ: int = 1_000_000\\n\\nfixed_type A\\nfixed_type B = 0x1000\\nfixed_type C: int = 0b1000\\n\"},{\"name\":\"loop_iter_manual.dewy\",\"code\":\"it = [0,2..10].iter\\n[cond i] = it.next\\nloop cond {\\n    printl(i)\\n    [cond i] = it.next\\n}\"},{\"name\":\"range_iter_test.dewy\",\"code\":\"r = 0,2..20\\nit = r.iter\\nprintl(it.next)\\nprintl(it.next)\\nprintl(it.next)\\nprintl(it.next)\\nprintl(it.next)\\nprintl(it.next)\\nprintl(it.next)\\nprintl(it.next)\\nprintl(it.next)\\nprintl(it.next)\\nprintl(it.next) //last iteration. should return [true, 20]\\nprintl(it.next) //should return [false, undefined]\\nprintl(it.next)\\nprintl(it.next)\"},{\"name\":\"shebang.dewy\",\"code\":\"#!//home/david/dev/dewy-lang/dewy\\n\\n//shows how shebangs can be syntactically valid as a hashtag `#!` followed by a comment\\nprintl'this code was invoked as an executable script with a shebang line!'\"},{\"name\":\"fizzbuzz1.dewy\",\"code\":\"taps = [3 -\u003e 'Fizz' 5 -\u003e 'Buzz' /{7 -\u003e 'Bazz' 11 -\u003e 'Bar'}/]\\nrange = [0..100)\\n\\n//indexing at [new ..] and [.. new] adds singleton dimensions wherever there is new\\nword_bools = range[new ..] .% taps.keys[.. new] .=? 0\\n\\n// ` means transpose, which behaves like python's zip()\\nwords_grid = [taps.values word_bools]`.map(\\n    [word bools] =\u003e bools.map(b =\u003e if b word else '')\\n)\\n\\nraw_lines = word_grid`.map(line_words =\u003e line_words.join(''))\\n\\nlines = [raw_lines range]`.map(\\n    (raw_line i) =\u003e if raw_line.length =? 0 '{i}' else raw_line\\n)\\n\\nlines.join'\\\\n' |\u003e printl\\n\"},{\"name\":\"primes2.dewy\",\"code\":\"// generating primes with more advanced features\\n#ctx\\nprimes = [\\n    2\\n    loop i in [3, 5..)\\n        if i .% #ctx.primes .=? 0 |\u003e @any |\u003e @not\\n            i\\n][..10)\"},{\"name\":\"mdbook_preprocessor.dewy\",\"code\":\"$28\"},{\"name\":\"random.dewy\",\"code\":\"$29\"},{\"name\":\"fast_inverse_sqrt.dewy\",\"code\":\"\\n\\n// fast inverse square-root. see: https://en.wikipedia.org/wiki/Fast_inverse_square_root#Overview_of_the_code\\nfast_isqrt = (x:f32) =\u003e {\\n    let y:f32, i:u32\\n    \\n    i = 0.5x transmute u32      // evil floating point bit level hacking\\n    i = 0x5f3759df - (i \u003e\u003e 1)   // what the fuck?\\n    y = i transmute f32\\n    y *= 1.5 - (0.5x)y^2        // 1st iteration of newton's method\\n    //y *= 1.5 - (0.5x)y^2      // 2nd iteration (optional)\\n\\n    return y\\n}\\n\\n\\n//TODO: use autodiff to calculate the derivative automatically?\\n//isqrt = (x:number) =\u003e 1/x^0.5\\n//diff(isqrt)\"},{\"name\":\"rule110.dewy\",\"code\":\"// proof that dewy is turing complete\\n// rule 110 would grow the vector from the front, so instead we reverse everything for efficiency\\n// for now use parenthesis where precedence filter needed. eventually should be able to remove with precedence filter\\n\\nprogress = world:vector\u003cbit\u003e =\u003e {\\n    update:bit = 0\\n    loop i in 0..world.length\\n    {\\n        if i \u003e? 0 world[i-1] = update //TODO: #notfirst handled by compiler unrolling the loop into prelude, interludes, and postlude\\n        update = 0b01110110 \u003c\u003c (world[i-1..i+1] .?? 0 .\u003c\u003c [2 1 0])\\n    }\\n    world.push(update)\\n}\\n\\nworld: vector\u003cbit\u003e = [1]\\nloop true\\n{\\n    printl(world)\\n    progress(world)\\n}\"},{\"name\":\"dewy_syntax_examples.dewy\",\"code\":\"$2a\"},{\"name\":\"syntax.dewy\",\"code\":\"$2b\"},{\"name\":\"tokenizer.dewy\",\"code\":\"//demo of manual dewy tokenizer written in dewy\\n\\n// (template) instance of the token class\\nTokenBase = [\\n    name = 'Token'\\n    __repr__ = () =\u003e '\u003c{name}\u003e'\\n]\\nToken = type(TokenBase)\\n\\n\\n// class constructor for Keyword token type\\nKeyword = src:string =\u003e [\\n    ...TokenBase\\n    name = 'Keyword'\\n    __repr__ = () =\u003e '\u003c{name}: {src}\u003e'\\n]\\n\\n\\neat_fn_type = src:string :\u003e int?\\n\\n\\n/{\\n    Eat a reserved keyword, return the number of characters eaten\\n\\n    #keyword = {in} | {as} | {loop} | {lazy} | {if} | {and} | {or} | {xor} | {nand} | {nor} | {xnor} | {not}; \\n\\n    noting that keywords are case insensitive\\n}/\\neat_keyword = src:string :\u003e int? =\u003e {\\n    keywords = ['in' 'as' 'loop' 'lazy' 'if' 'and' 'or' 'xor' 'nand' 'nor' 'xnor' 'not']\\n    max_len = [loop k in keywords k.length].max\\n    lower_src = src[..max_len].lowercase\\n    loop k in keywords\\n        if lower_src.startswith(k)\\n            return k.length\\n    return undefined\\n}\\n\"}]}}]\n"])</script></body></html>