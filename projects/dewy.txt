1:HL["/_next/static/media/c9a5bc6a7c948fb0-s.p.woff2",{"as":"font","type":"font/woff2"}]
2:HL["/_next/static/css/88bedb8f8248c8e6.css",{"as":"style"}]
0:["BQujDWDdIF4VNbVkXY9G6",[[["",{"children":["projects",{"children":["dewy",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],"$L3",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/88bedb8f8248c8e6.css","precedence":"next"}]],"$L4"]]]]
5:HL["/_next/static/css/85fa6dafca566008.css",{"as":"style"}]
3:[null,"$L6",null]
4:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"David Samson"}],["$","meta","2",{"name":"description","content":"Generated by create next app"}],["$","meta","3",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","link","4",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","meta","5",{"name":"next-size-adjust"}]]
7:I{"id":27250,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","8213:static/chunks/8213-ef98b168d3fb6523.js","8825:static/chunks/8825-d6234b2a4056589a.js","5570:static/chunks/app/projects/layout-9b841f3f90b5ed09.js"],"name":"Navbar","async":false}
8:I{"id":15274,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","8213:static/chunks/8213-ef98b168d3fb6523.js","3905:static/chunks/3905-01dc2d4b158832d0.js","8825:static/chunks/8825-d6234b2a4056589a.js","3185:static/chunks/app/layout-8e14ce7aa7691afe.js"],"name":"GithubTimestampsProvider","async":false}
9:I{"id":15274,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","8213:static/chunks/8213-ef98b168d3fb6523.js","3905:static/chunks/3905-01dc2d4b158832d0.js","8825:static/chunks/8825-d6234b2a4056589a.js","3185:static/chunks/app/layout-8e14ce7aa7691afe.js"],"name":"GithubTimestampsFetcher","async":false}
a:I{"id":15274,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","8213:static/chunks/8213-ef98b168d3fb6523.js","3905:static/chunks/3905-01dc2d4b158832d0.js","8825:static/chunks/8825-d6234b2a4056589a.js","3185:static/chunks/app/layout-8e14ce7aa7691afe.js"],"name":"ProjectsContextProvider","async":false}
b:I{"id":47767,"chunks":["2272:static/chunks/webpack-5951a70e0bcd85c9.js","2971:static/chunks/fd9d1056-bc91c3b3fa0b78fd.js","596:static/chunks/596-fc24fa4abd14b1c2.js"],"name":"default","async":false}
c:I{"id":57920,"chunks":["2272:static/chunks/webpack-5951a70e0bcd85c9.js","2971:static/chunks/fd9d1056-bc91c3b3fa0b78fd.js","596:static/chunks/596-fc24fa4abd14b1c2.js"],"name":"default","async":false}
e:I{"id":49488,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","8213:static/chunks/8213-ef98b168d3fb6523.js","3905:static/chunks/3905-01dc2d4b158832d0.js","8825:static/chunks/8825-d6234b2a4056589a.js","3185:static/chunks/app/layout-8e14ce7aa7691afe.js"],"name":"ColorPicker","async":false}
6:["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","script",null,{"src":"/pyodideCommsService.js","async":true}]}],["$","body",null,{"className":"__className_e66fe9 overflow-hidden bg-slate-900","children":["$","div",null,{"className":"w-screen h-screen bg-black overflow-hidden","children":[["$","$L7",null,{}],["$","div",null,{"style":{"height":"calc(100vh - var(--navbar-height))"},"className":"overflow-x-hidden","children":["$","$L8",null,{"children":[["$","$L9",null,{"projects":[{"title":"Blob Opera Performances","imgSrc":{"src":"/_next/static/media/blob_opera_nox.e6f3aa2a.png","height":414,"width":512,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAJ1BMVEU4EgowEQlEIRV0Vzx9ZEo+JhlXOyBCT1RTMx0+LyVEUlxuTS5AS1HBZ0aTAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALklEQVR4nC3IuREAIAwDMMcOeYD95+XgKNQIhpahHYwUawuGoQuM6az0OwvPnwMUSwDO9qMceAAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":6},"summary":"Virtual choir performances leveraging the blob opera as a front end for voice synthesis","lastUpdated":"February 2021","tags":["Python","Blob Opera","choir","music","synthesis"],"route":"blob_opera"},{"title":"Boat Simulator","imgSrc":{"src":"/_next/static/media/boat_simulator.7cc391fd.jpg","height":1280,"width":2048,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAb/xAAdEAABAgcAAAAAAAAAAAAAAAAAAQQCAxESFSFT/8QAFAEBAAAAAAAAAAAAAAAAAAAABf/EABYRAAMAAAAAAAAAAAAAAAAAAAABEf/aAAwDAQACEQMRAD8AkFks7K49tuLmABWIOrP/2Q==","blurWidth":8,"blurHeight":5},"summary":"Spring 2017 HopHacks submission","lastUpdated":"March 2017","tags":["Unity","C#","3D game"],"route":"boat_simulator"},{"title":"Bueller Board","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Fall 2015 HopHacks submission: Midi keyboard that used user provided audio samples, a.k.a. the 'goat keyboard'","lastUpdated":"September 2015","tags":["midi","music"],"route":"bueller_board"},{"title":"Composer","imgSrc":{"src":"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":8},"summary":"React based composing software that acts as a front-end for LilyPond","lastUpdated":"January 2021","tags":["React","TypeScript","SMuFL","LilyPond","music","composition"],"route":"composer"},{"title":"Choir Compositions","imgSrc":{"src":"/_next/static/media/music_staff.3145785a.png","height":1616,"width":2745,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAHlBMVEUGBgZISEhSUlIeHh5paWk+Pj4vLy9cXFx7e3ufn5/bdycMAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAJ0lEQVR4nCXBtwEAIAwDMLe0/x9mQIKFjwN3jZBwxGowpFZGm7cpPAkxAH3f0FE4AAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":5},"summary":"","lastUpdated":"May 2015","tags":["music","choral","composition"],"route":"compositions"},{"title":"Dewy Programming Language","github":"dewy","imgSrc":{"src":"/_next/static/media/dewy_dandelion.e2efa7ee.jpg","height":1600,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z","blurWidth":8,"blurHeight":8},"summary":"An engineering focused programming language I am developing","tags":["Python","compilers","parsers","LLVM","Programming Languages"],"route":"dewy"},{"title":"Generalized Parsing","lastUpdated":"2022-02-06","imgSrc":{"src":"/_next/static/media/dewy_dandelion.e2efa7ee.jpg","height":1600,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z","blurWidth":8,"blurHeight":8},"summary":"Previous work on the Dewy Programming Language, namely a custom SRNGLR parser written entirely in C","tags":["C","compilers","parsers","SRNGLR","LLVM"],"route":"dewy_old"},{"title":"UR5 Draw Robot","imgSrc":{"src":"/_next/static/media/mona_lisa_contour.9bc5cbc9.jpg","height":1016,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAeEAACAgEFAQAAAAAAAAAAAAABAgAEAwUHEiFRsf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAExAv/aAAwDAQACEQMRAD8AtbT3HvV9WGZVPB8RHXqsT8iIgoVqn//Z","blurWidth":8,"blurHeight":5},"summary":"UR5 robot arm project","lastUpdated":"December 2017","tags":["Matlab","UR5 robot","ROS"],"route":"drawbot"},{"title":"Hacking Harmony or The Demon Chipmunk Choir","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"Ensemble","summary":"2019 Peabody Hackathon Submission. Choral music synthesis via autotuned google text-to-speech, AKA the demon chipmunk choir","tags":["Google text-to-speech API","matlab","python"],"route":"ensemble_peabody"},{"title":"Escort Mission 2020","imgSrc":{"src":"/_next/static/media/escort_mission_lamb.4c525bc4.png","height":128,"width":128,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAHlBMVEUA/gAAugD/+//h9+EAdAAirCKG/YZf2l9ri2uA/4BNpwkIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAK0lEQVR4nE2KuREAMAyDbPnff+HcSU1ooMDsI1P2XWdURDEOOMYAo6dbj3gNJgBhtLya0gAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":8},"github":"escort_mission_2020","summary":"Submission for the 2020 GMTK Game Jam","tags":["Godot","GDScript","2D game"],"route":"escort_mission"},{"title":"Foxing Animatronic","imgSrc":{"src":"/_next/static/media/foxing_animatronic.91d20002.png","height":1252,"width":1540,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAJ1BMVEUGFwsJIxEILhUiJQkdSRwWIQoUORYULA8RRSArMAYZEwYyVzM6OglI+AgfAAAACXBIWXMAAAsTAAALEwEAmpwYAAAANElEQVR4nBXKuREAIAwEsT2/GOi/XgbFoqs9I6DvdY8CW7O9DBSzvQ3OKFOCE5Vp+mdJBg8iegD5GB/ChgAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":7},"summary":"Manually actuated animatronic robot featured in the Foxing music video 'Slapstick'","lastUpdated":"June 2018","tags":["Solidworks","mechanical design","animatronic","Foxing","music"],"route":"foxing_animatronic"},{"title":"Mechatronics Robots","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Robots from mechatronics","lastUpdated":"May 2019","tags":["Arduino","C++","SolidWorks","mechanical design"],"route":"mechatronics"},{"title":"Mehve (Working Title)","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"mehve","summary":"3D adventure game inspire by \"Nausicaa of the Valley of the Wind\"","tags":["Godot","GDScript","3D game"],"route":"mehve"},{"title":"Musical DL","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"MusicalDL","summary":"Using deep learning to generate choral music in the style of JS Bach","tags":["Python","Pytorch","AI/ML","choral","music","generation"],"route":"musical_dl"},{"title":"NAND 2 Tetris","imgSrc":{"src":"/_next/static/media/nand2tetris.660feb3d.png","height":346,"width":396,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAPFBMVEUAAABvKgIhHhw+KR0uJAFlRBU2GAYJBAENDw/TVQF/ZRBuWwrEoxytlRBiVBJJHQC7SwG5YCWSOwDqziPUksrRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAN0lEQVR4nCWKWw7AIAyA0Fbbqnvo7n/XZZMfEgKAscllu870J7tj2Tk6NSK151BcLilNwd3z974dVQEdrZJh0wAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":7},"github":"nand2tetris","summary":"A 16-bit computer built from scratch, starting with NAND gates","tags":["Computer Architecture","HDL","Assembly","Hack","Jack","Compilers","Operating Systems","Virtual Machines"],"route":"nand2tetris"},{"title":"pOngBot","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Autonomous beer pong playing robot","lastUpdated":"June 2020","tags":["Arduino","C++","computer vision","Viola-Jones","mechanical design"],"route":"pongbot"},{"title":"PRS19: Fret Press Robot","imgSrc":{"src":"/_next/static/media/prs2019_preview.aed05a2a.png","height":2093,"width":3061,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAPFBMVEVJPTmhmIiCa1SBfXdBQD1IOzB3bGA6My5dUkyXlpNPRTyio6ChpadSTUm0q5e+taCun2xwYl2qrq5mXU/I9hK/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAL0lEQVR4nAXBhwEAIAjAsKoo4B7//2rCXFZLjoGwh4poIj5TJRfOsAok3G/v3toHGpkBOY6KaOIAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":5},"github":"PRS_robot","summary":"Automatic guitar fret press robot. Mechanical Engineering Master's Design captsone project","tags":["C++","Arduino","mechanical design"],"route":"prs19"},{"title":"Rewind","imgSrc":{"src":"/_next/static/media/rewind_title.a1e43a09.png","height":540,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAElBMVEUqHjguITs4J0JmWW1PRFo/MUqO6aYhAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAH0lEQVR4nGNggAMmJkYmRhCDhZmFmZUBxGRkgIhAAQADKgAhbD4/MgAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":5},"summary":"2018 Video Game Desgn (EN.601.355) capstone project","lastUpdated":"May 2018","tags":["Unity","C#","2D game"],"route":"rewind"},{"title":"RoboJay","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"lastUpdated":"May 2018","summary":"A balancing robot designed to give campus tours to incoming JHU freshmen","tags":["robotics","feedback control","navigation","BeagleBone","ROS"],"route":"robojay"},{"title":"High Power Rocketry","imgSrc":{"src":"/_next/static/media/rebel_scum.34e7823c.jpg","height":1365,"width":2048,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAaEAACAgMAAAAAAAAAAAAAAAAAARESIjFB/8QAFQEBAQAAAAAAAAAAAAAAAAAAAgP/xAAXEQADAQAAAAAAAAAAAAAAAAAAAhNR/9oADAMBAAIRAxEAPwC6o4w5OwATo+immH//2Q==","blurWidth":8,"blurHeight":5},"summary":"Level 1 & 2 High Powered Rocket built with the Johns Hopkins Rocketry Club, and Spaceport America Cup 2018","lastUpdated":"January 2018","tags":["High Power Rocketry","Arduino","C++","mechanical design","Tripoli"],"route":"rocketry"},{"title":"so voice!","imgSrc":{"src":"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":8},"lastUpdated":"December 2022","summary":"Choral music synthesis with deep learning (Continuation of Musical DL)","tags":["Python","Pytorch","AI/ML","choral","music","synthesis"],"route":"so_voice"},{"title":"Terminal Ray Tracer","imgSrc":{"src":"/_next/static/media/terminal_ray_tracer.00db7e4a.png","height":2043,"width":2881,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAbFBMVEWSamvNrLWWP0yUKVeYinmmnKTsEheTcnnmh4+Xfo6pV2GMMDasvsa5aXiPRVFVs2uOWF9WnGxXtVObh4ypREzbYmnsPkXIba/y/P10bHmfbcouPtDAMDdJbZOtmERQbkXfwcqTb3g+yNdOcdnEQLLCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAOklEQVR4nAXBBQKAIAAAsSMl7MZG//9HN/Igu6YtYDT+7qnhSK++tI+IcH5BViXZqn0Tk8M8Ti3rbH9KhwLIHy1s0QAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":6},"github":"TerminalRayTracer","summary":"A dependency-free ray tracer written in C that runs directly in a linux terminal","route":"terminal_ray_tracer"},{"title":"Cloud Timelapse","imgSrc":{"src":"/_next/static/media/timelapse.b57dd258.jpg","height":3468,"width":4624,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAGAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAcEAACAgIDAAAAAAAAAAAAAAAAAQIFAxETYdH/xAAVAQEBAAAAAAAAAAAAAAAAAAADBP/EABYRAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAwDAQACEQMRAD8AlG4r56fBlT6ivQAUqwYX/9k=","blurWidth":8,"blurHeight":6},"github":"timelapse","summary":"A simple python project for taking timelapses of clouds from a webcam","tags":["Python","OpenCV","Raspberry Pi","timelapse","clouds"],"route":"timelapse"},{"title":"uSkipSpoilers","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"uSkipSpoilers","summary":"A small chrome extension for blocking spoilers in YouTube videos","tags":["React","TypeScript","Chrome","Extension"],"route":"uskipspoilers"},{"title":"This Website","github":"david-andrew.github.io","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"externalLink":"https://github.com/david-andrew/david-andrew.github.io","summary":"This website, written in react/typescript","tags":["Next.js","React","TypeScript","Tailwind CSS","WebAssembly"],"route":"website"},{"title":"WSE18: Machine Shop Biometric Interlock","imgSrc":{"src":"/_next/static/media/wse18.7405315e.png","height":2054,"width":2456,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAUVBMVEU8OTSVjYNlX1nIwbpLQjaqnpBOSUOJgHU8MCsyKSXo4dUfHh1iU0R8aWMUDAdoYlull4ialI2BZkzBuKyNdleyrKaQosO9iGCNY0PTj19hbYD9ZLzaAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAO0lEQVR4nAXBhQHAMAwDMBdCZRj/f+gkiLgdPDJcpJS6MbxKOJEBa1pH5whYpXl9DfEoc2A90MJC671/OekCOef5C1cAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":7},"summary":"Biometric security interlock system. Mechanical Engineering Senior Design capstone project","lastUpdated":"May 2018","tags":["Raspberry Pi","Python","C++","Qt","interlock","fingerprint","biometric"],"route":"wse18"},{"title":"Ziggy V (Working Title)","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Concept for a Real-Time-Strategy crossed with First-Person-Shooter","lastUpdated":"January 2021","tags":["Godot","GDScript","FPS x RTS","3D game"],"route":"ziggy_v"}]}],["$","$La",null,{"children":["$","$Lb",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":["$","div",null,{"className":"fixed w-screen h-screen bg-black flex justify-center items-center","children":["$","div",null,{"role":"status","children":[["$","svg",null,{"aria-hidden":"true","className":"w-32 h-32 mr-2 text-gray-200 animate-spin dark:text-gray-600 fill-accent","viewBox":"0 0 100 101","fill":"none","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M100 50.5908C100 78.2051 77.6142 100.591 50 100.591C22.3858 100.591 0 78.2051 0 50.5908C0 22.9766 22.3858 0.59082 50 0.59082C77.6142 0.59082 100 22.9766 100 50.5908ZM9.08144 50.5908C9.08144 73.1895 27.4013 91.5094 50 91.5094C72.5987 91.5094 90.9186 73.1895 90.9186 50.5908C90.9186 27.9921 72.5987 9.67226 50 9.67226C27.4013 9.67226 9.08144 27.9921 9.08144 50.5908Z","fill":"currentColor"}],["$","path",null,{"d":"M93.9676 39.0409C96.393 38.4038 97.8624 35.9116 97.0079 33.5539C95.2932 28.8227 92.871 24.3692 89.8167 20.348C85.8452 15.1192 80.8826 10.7238 75.2124 7.41289C69.5422 4.10194 63.2754 1.94025 56.7698 1.05124C51.7666 0.367541 46.6976 0.446843 41.7345 1.27873C39.2613 1.69328 37.813 4.19778 38.4501 6.62326C39.0873 9.04874 41.5694 10.4717 44.0505 10.1071C47.8511 9.54855 51.7191 9.52689 55.5402 10.0491C60.8642 10.7766 65.9928 12.5457 70.6331 15.2552C75.2735 17.9648 79.3347 21.5619 82.5849 25.841C84.9175 28.9121 86.7997 32.2913 88.1811 35.8758C89.083 38.2158 91.5421 39.6781 93.9676 39.0409Z","fill":"currentFill"}]]}],["$","span",null,{"className":"sr-only","children":"Loading..."}]]}]}],"loadingStyles":[],"hasLoading":true,"error":"$undefined","errorStyles":"$undefined","template":["$","$Lc",null,{}],"templateStyles":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"childProp":{"current":[null,"$Ld",null],"segment":"projects"},"styles":[]}]}]]}]}],["$","$Le",null,{}]]}]}]]}]
f:I{"id":52160,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","8213:static/chunks/8213-ef98b168d3fb6523.js","8825:static/chunks/8825-d6234b2a4056589a.js","5570:static/chunks/app/projects/layout-9b841f3f90b5ed09.js"],"name":"Heading","async":false}
12:I{"id":27250,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","8213:static/chunks/8213-ef98b168d3fb6523.js","8825:static/chunks/8825-d6234b2a4056589a.js","5570:static/chunks/app/projects/layout-9b841f3f90b5ed09.js"],"name":"NavbarDummy","async":false}
d:["$","div",null,{"className":"mx-auto px-4 sm:px-6 lg:px-8 max-w-[1190px]","children":[["$","$Lf",null,{"projects":[{"title":"Blob Opera Performances","imgSrc":{"src":"/_next/static/media/blob_opera_nox.e6f3aa2a.png","height":414,"width":512,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAJ1BMVEU4EgowEQlEIRV0Vzx9ZEo+JhlXOyBCT1RTMx0+LyVEUlxuTS5AS1HBZ0aTAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALklEQVR4nC3IuREAIAwDMMcOeYD95+XgKNQIhpahHYwUawuGoQuM6az0OwvPnwMUSwDO9qMceAAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":6},"summary":"Virtual choir performances leveraging the blob opera as a front end for voice synthesis","lastUpdated":"February 2021","tags":["Python","Blob Opera","choir","music","synthesis"],"route":"blob_opera"},{"title":"Boat Simulator","imgSrc":{"src":"/_next/static/media/boat_simulator.7cc391fd.jpg","height":1280,"width":2048,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAb/xAAdEAABAgcAAAAAAAAAAAAAAAAAAQQCAxESFSFT/8QAFAEBAAAAAAAAAAAAAAAAAAAABf/EABYRAAMAAAAAAAAAAAAAAAAAAAABEf/aAAwDAQACEQMRAD8AkFks7K49tuLmABWIOrP/2Q==","blurWidth":8,"blurHeight":5},"summary":"Spring 2017 HopHacks submission","lastUpdated":"March 2017","tags":["Unity","C#","3D game"],"route":"boat_simulator"},{"title":"Bueller Board","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Fall 2015 HopHacks submission: Midi keyboard that used user provided audio samples, a.k.a. the 'goat keyboard'","lastUpdated":"September 2015","tags":["midi","music"],"route":"bueller_board"},{"title":"Composer","imgSrc":{"src":"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":8},"summary":"React based composing software that acts as a front-end for LilyPond","lastUpdated":"January 2021","tags":["React","TypeScript","SMuFL","LilyPond","music","composition"],"route":"composer"},{"title":"Choir Compositions","imgSrc":{"src":"/_next/static/media/music_staff.3145785a.png","height":1616,"width":2745,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAHlBMVEUGBgZISEhSUlIeHh5paWk+Pj4vLy9cXFx7e3ufn5/bdycMAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAJ0lEQVR4nCXBtwEAIAwDMLe0/x9mQIKFjwN3jZBwxGowpFZGm7cpPAkxAH3f0FE4AAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":5},"summary":"","lastUpdated":"May 2015","tags":["music","choral","composition"],"route":"compositions"},{"title":"Dewy Programming Language","github":"dewy","imgSrc":{"src":"/_next/static/media/dewy_dandelion.e2efa7ee.jpg","height":1600,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z","blurWidth":8,"blurHeight":8},"summary":"An engineering focused programming language I am developing","tags":["Python","compilers","parsers","LLVM","Programming Languages"],"route":"dewy"},{"title":"Generalized Parsing","lastUpdated":"2022-02-06","imgSrc":{"src":"/_next/static/media/dewy_dandelion.e2efa7ee.jpg","height":1600,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z","blurWidth":8,"blurHeight":8},"summary":"Previous work on the Dewy Programming Language, namely a custom SRNGLR parser written entirely in C","tags":["C","compilers","parsers","SRNGLR","LLVM"],"route":"dewy_old"},{"title":"UR5 Draw Robot","imgSrc":{"src":"/_next/static/media/mona_lisa_contour.9bc5cbc9.jpg","height":1016,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAeEAACAgEFAQAAAAAAAAAAAAABAgAEAwUHEiFRsf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAExAv/aAAwDAQACEQMRAD8AtbT3HvV9WGZVPB8RHXqsT8iIgoVqn//Z","blurWidth":8,"blurHeight":5},"summary":"UR5 robot arm project","lastUpdated":"December 2017","tags":["Matlab","UR5 robot","ROS"],"route":"drawbot"},{"title":"Hacking Harmony or The Demon Chipmunk Choir","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"Ensemble","summary":"2019 Peabody Hackathon Submission. Choral music synthesis via autotuned google text-to-speech, AKA the demon chipmunk choir","tags":["Google text-to-speech API","matlab","python"],"route":"ensemble_peabody"},{"title":"Escort Mission 2020","imgSrc":{"src":"/_next/static/media/escort_mission_lamb.4c525bc4.png","height":128,"width":128,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAHlBMVEUA/gAAugD/+//h9+EAdAAirCKG/YZf2l9ri2uA/4BNpwkIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAK0lEQVR4nE2KuREAMAyDbPnff+HcSU1ooMDsI1P2XWdURDEOOMYAo6dbj3gNJgBhtLya0gAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":8},"github":"escort_mission_2020","summary":"Submission for the 2020 GMTK Game Jam","tags":["Godot","GDScript","2D game"],"route":"escort_mission"},{"title":"Foxing Animatronic","imgSrc":{"src":"/_next/static/media/foxing_animatronic.91d20002.png","height":1252,"width":1540,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAJ1BMVEUGFwsJIxEILhUiJQkdSRwWIQoUORYULA8RRSArMAYZEwYyVzM6OglI+AgfAAAACXBIWXMAAAsTAAALEwEAmpwYAAAANElEQVR4nBXKuREAIAwEsT2/GOi/XgbFoqs9I6DvdY8CW7O9DBSzvQ3OKFOCE5Vp+mdJBg8iegD5GB/ChgAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":7},"summary":"Manually actuated animatronic robot featured in the Foxing music video 'Slapstick'","lastUpdated":"June 2018","tags":["Solidworks","mechanical design","animatronic","Foxing","music"],"route":"foxing_animatronic"},{"title":"Mechatronics Robots","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Robots from mechatronics","lastUpdated":"May 2019","tags":["Arduino","C++","SolidWorks","mechanical design"],"route":"mechatronics"},{"title":"Mehve (Working Title)","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"mehve","summary":"3D adventure game inspire by \"Nausicaa of the Valley of the Wind\"","tags":["Godot","GDScript","3D game"],"route":"mehve"},{"title":"Musical DL","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"MusicalDL","summary":"Using deep learning to generate choral music in the style of JS Bach","tags":["Python","Pytorch","AI/ML","choral","music","generation"],"route":"musical_dl"},{"title":"NAND 2 Tetris","imgSrc":{"src":"/_next/static/media/nand2tetris.660feb3d.png","height":346,"width":396,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAPFBMVEUAAABvKgIhHhw+KR0uJAFlRBU2GAYJBAENDw/TVQF/ZRBuWwrEoxytlRBiVBJJHQC7SwG5YCWSOwDqziPUksrRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAN0lEQVR4nCWKWw7AIAyA0Fbbqnvo7n/XZZMfEgKAscllu870J7tj2Tk6NSK151BcLilNwd3z974dVQEdrZJh0wAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":7},"github":"nand2tetris","summary":"A 16-bit computer built from scratch, starting with NAND gates","tags":["Computer Architecture","HDL","Assembly","Hack","Jack","Compilers","Operating Systems","Virtual Machines"],"route":"nand2tetris"},{"title":"pOngBot","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Autonomous beer pong playing robot","lastUpdated":"June 2020","tags":["Arduino","C++","computer vision","Viola-Jones","mechanical design"],"route":"pongbot"},{"title":"PRS19: Fret Press Robot","imgSrc":{"src":"/_next/static/media/prs2019_preview.aed05a2a.png","height":2093,"width":3061,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAPFBMVEVJPTmhmIiCa1SBfXdBQD1IOzB3bGA6My5dUkyXlpNPRTyio6ChpadSTUm0q5e+taCun2xwYl2qrq5mXU/I9hK/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAL0lEQVR4nAXBhwEAIAjAsKoo4B7//2rCXFZLjoGwh4poIj5TJRfOsAok3G/v3toHGpkBOY6KaOIAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":5},"github":"PRS_robot","summary":"Automatic guitar fret press robot. Mechanical Engineering Master's Design captsone project","tags":["C++","Arduino","mechanical design"],"route":"prs19"},{"title":"Rewind","imgSrc":{"src":"/_next/static/media/rewind_title.a1e43a09.png","height":540,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAElBMVEUqHjguITs4J0JmWW1PRFo/MUqO6aYhAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAH0lEQVR4nGNggAMmJkYmRhCDhZmFmZUBxGRkgIhAAQADKgAhbD4/MgAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":5},"summary":"2018 Video Game Desgn (EN.601.355) capstone project","lastUpdated":"May 2018","tags":["Unity","C#","2D game"],"route":"rewind"},{"title":"RoboJay","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"lastUpdated":"May 2018","summary":"A balancing robot designed to give campus tours to incoming JHU freshmen","tags":["robotics","feedback control","navigation","BeagleBone","ROS"],"route":"robojay"},{"title":"High Power Rocketry","imgSrc":{"src":"/_next/static/media/rebel_scum.34e7823c.jpg","height":1365,"width":2048,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAaEAACAgMAAAAAAAAAAAAAAAAAARESIjFB/8QAFQEBAQAAAAAAAAAAAAAAAAAAAgP/xAAXEQADAQAAAAAAAAAAAAAAAAAAAhNR/9oADAMBAAIRAxEAPwC6o4w5OwATo+immH//2Q==","blurWidth":8,"blurHeight":5},"summary":"Level 1 & 2 High Powered Rocket built with the Johns Hopkins Rocketry Club, and Spaceport America Cup 2018","lastUpdated":"January 2018","tags":["High Power Rocketry","Arduino","C++","mechanical design","Tripoli"],"route":"rocketry"},{"title":"so voice!","imgSrc":{"src":"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":8},"lastUpdated":"December 2022","summary":"Choral music synthesis with deep learning (Continuation of Musical DL)","tags":["Python","Pytorch","AI/ML","choral","music","synthesis"],"route":"so_voice"},{"title":"Terminal Ray Tracer","imgSrc":{"src":"/_next/static/media/terminal_ray_tracer.00db7e4a.png","height":2043,"width":2881,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAbFBMVEWSamvNrLWWP0yUKVeYinmmnKTsEheTcnnmh4+Xfo6pV2GMMDasvsa5aXiPRVFVs2uOWF9WnGxXtVObh4ypREzbYmnsPkXIba/y/P10bHmfbcouPtDAMDdJbZOtmERQbkXfwcqTb3g+yNdOcdnEQLLCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAOklEQVR4nAXBBQKAIAAAsSMl7MZG//9HN/Igu6YtYDT+7qnhSK++tI+IcH5BViXZqn0Tk8M8Ti3rbH9KhwLIHy1s0QAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":6},"github":"TerminalRayTracer","summary":"A dependency-free ray tracer written in C that runs directly in a linux terminal","route":"terminal_ray_tracer"},{"title":"Cloud Timelapse","imgSrc":{"src":"/_next/static/media/timelapse.b57dd258.jpg","height":3468,"width":4624,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAGAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAcEAACAgIDAAAAAAAAAAAAAAAAAQIFAxETYdH/xAAVAQEBAAAAAAAAAAAAAAAAAAADBP/EABYRAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAwDAQACEQMRAD8AlG4r56fBlT6ivQAUqwYX/9k=","blurWidth":8,"blurHeight":6},"github":"timelapse","summary":"A simple python project for taking timelapses of clouds from a webcam","tags":["Python","OpenCV","Raspberry Pi","timelapse","clouds"],"route":"timelapse"},{"title":"uSkipSpoilers","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"uSkipSpoilers","summary":"A small chrome extension for blocking spoilers in YouTube videos","tags":["React","TypeScript","Chrome","Extension"],"route":"uskipspoilers"},{"title":"This Website","github":"david-andrew.github.io","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"externalLink":"https://github.com/david-andrew/david-andrew.github.io","summary":"This website, written in react/typescript","tags":["Next.js","React","TypeScript","Tailwind CSS","WebAssembly"],"route":"website"},{"title":"WSE18: Machine Shop Biometric Interlock","imgSrc":{"src":"/_next/static/media/wse18.7405315e.png","height":2054,"width":2456,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAUVBMVEU8OTSVjYNlX1nIwbpLQjaqnpBOSUOJgHU8MCsyKSXo4dUfHh1iU0R8aWMUDAdoYlull4ialI2BZkzBuKyNdleyrKaQosO9iGCNY0PTj19hbYD9ZLzaAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAO0lEQVR4nAXBhQHAMAwDMBdCZRj/f+gkiLgdPDJcpJS6MbxKOJEBa1pH5whYpXl9DfEoc2A90MJC671/OekCOef5C1cAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":7},"summary":"Biometric security interlock system. Mechanical Engineering Senior Design capstone project","lastUpdated":"May 2018","tags":["Raspberry Pi","Python","C++","Qt","interlock","fingerprint","biometric"],"route":"wse18"},{"title":"Ziggy V (Working Title)","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Concept for a Real-Time-Strategy crossed with First-Person-Shooter","lastUpdated":"January 2021","tags":["Godot","GDScript","FPS x RTS","3D game"],"route":"ziggy_v"}]}],["$","$Lb",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$Lc",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$","$Lb",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children","dewy","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$Lc",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$L10","$L11",null],"segment":"__PAGE__"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/85fa6dafca566008.css","precedence":"next"}]]}],"segment":"dewy"},"styles":[]}],["$","$L12",null,{}]]}]
10:null
13:"$Sreact.suspense"
14:I{"id":33699,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","8213:static/chunks/8213-ef98b168d3fb6523.js","9659:static/chunks/9659-468d9b7f9560cf55.js","8875:static/chunks/8875-80c7a7c0a48099bf.js","1134:static/chunks/app/projects/dewy/page-3f91522a2e3c9d25.js"],"name":"NoSSR","async":false}
15:I{"id":38469,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","8213:static/chunks/8213-ef98b168d3fb6523.js","9659:static/chunks/9659-468d9b7f9560cf55.js","8875:static/chunks/8875-80c7a7c0a48099bf.js","1134:static/chunks/app/projects/dewy/page-3f91522a2e3c9d25.js"],"name":"","async":false}
21:I{"id":53123,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","8213:static/chunks/8213-ef98b168d3fb6523.js","9659:static/chunks/9659-468d9b7f9560cf55.js","8875:static/chunks/8875-80c7a7c0a48099bf.js","1134:static/chunks/app/projects/dewy/page-3f91522a2e3c9d25.js"],"name":"DewyCodeBlock","async":false}
22:I{"id":46685,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","8213:static/chunks/8213-ef98b168d3fb6523.js","9659:static/chunks/9659-468d9b7f9560cf55.js","8875:static/chunks/8875-80c7a7c0a48099bf.js","1134:static/chunks/app/projects/dewy/page-3f91522a2e3c9d25.js"],"name":"","async":false}
23:I{"id":63222,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","8213:static/chunks/8213-ef98b168d3fb6523.js","9659:static/chunks/9659-468d9b7f9560cf55.js","8875:static/chunks/8875-80c7a7c0a48099bf.js","1134:static/chunks/app/projects/dewy/page-3f91522a2e3c9d25.js"],"name":"Image","async":false}
16:T6a7,from typing import Callable


def python_interpreter(path:str, args:list[str]):
    from tokenizer import tokenize
    from postok import post_process
    from parser import top_level_parse # type: ignore[reportShadowedImports]
    from dewy import Scope, void

    with open(path) as f:
        src = f.read()
    
    tokens = tokenize(src)
    post_process(tokens)

    root = Scope.default()
    ast = top_level_parse(tokens, root)
    res = ast.eval(root)
    if res and res is not void: print(res)


def llvm_compiler(path:str, args:list[str]):
    raise NotImplementedError('LLVM backend is not yet supported')

def c_compiler(path:str, args:list[str]):
    raise NotImplementedError('C backend is not yet supported')

def x86_64_compiler(path:str, args:list[str]):
    raise NotImplementedError('x86_64 backend is not yet supported')

def shell(path:str, args:list[str]):
    """this would target sh/powershell/etc. all simultaneously"""
    raise NotImplementedError('Shell backend is not yet supported')

backend_map = {
    'python': python_interpreter,
    'llvm': llvm_compiler,
    'c': c_compiler,
    'x86_64': x86_64_compiler,
    # 'arm': arm,
    # 'riscv': riscv,
    'sh': shell,
    # 'posix': posix_shell,
    # 'powershell': powershell_shell,
}
backends = [*backend_map.keys()]

def get_backend(name:str) -> Callable[[str, list[str]], None]:
    try:
        return backend_map[name.lower()]
    except:
        raise ValueError(f'Unknown backend "{name}"') from None


def get_version() -> str:
    """Return the semantic version of the language"""
    from pathlib import Path
    with open(Path(__file__).parent.parent.parent / 'VERSION') as f:
        return f.read().strip()17:Tfd66,from abc import ABC
from dataclasses import dataclass
from types import EllipsisType
from typing import Any, Callable as PyCallable, Type as PyType, Union, Optional
from functools import partial
import operator

import pdb

#Written in python3.10

#dumb look at interpreting/compiling dewy
#for now, construct the AST directly, skipping the parsing step


# [Tasks]
# - instead of If with a block of multiple conditional checks: ConditionalChain, which can have any conditonals, e.g. if, loop, etc, and the first one that gets entered stops the chain
# - write function for crawling AST, and replacing 
# loop <var> in <expr> 
#   <body> 
# with 
# <_itr> = <expr>.iter()
# <var> = <itr>.next()
# loop <var>
#   <body>
#   <var> = <itr>.next()
#
# - make all type checking happen at compile time, and be based on calls to expr.type
#   -> need to be able to handle type graph with child types matching where parent types are expected, etc. e.g. int is a number, etc.
# also make the current functionality not worker (where it checks if the variable exists). should throw an error about how it's syntax sugar

#convenient for inside lambdas
def notimplemented():
    raise NotImplementedError()

tab = '    ' #for printing ASTs
newline = '\n' # or ' ' to make it all one line
# tabin = '|>>>>|'
# tabout = '|<<<<|'


def insert_tabs_inner(s):
    """given the output of __str__ from an AST, insert tabs at \n based on how many {} were encountered so far"""
    #TODO: this runs into problems b/c {} is also used in string interpolation syntax...
    level = 0
    out = []
    for c in s:
        if c == '{':
            level += 1
        elif c == '}':
            level -= 1
            if out[-1].isspace():
                out[-1] = out[-1][:-4] #remove 1 tab
        out.append(c)

        if c == '\n':
            out.append(tab*level)
            continue

    return ''.join(out)

inserting_tabs = False #so that only the top level inserts the tabs
def insert_tabs(func):
    def wrapper(*args, **kwargs):
        global inserting_tabs
        if inserting_tabs:
            out = func(*args, **kwargs)
        else:
            inserting_tabs = True
            out = insert_tabs_inner(func(*args, **kwargs))
            inserting_tabs = False
        return out

    return wrapper


class AST(ABC):
    
    #TODO: make accessing this raise better error if not overwritten by child class
    #      for now, just rely on exception for missing property
    # type:'Type' = None

    def eval(self, scope:'Scope'=None) -> 'AST':
        """Evaluate the AST in the given scope, and return the result (as a dewy obj) if any"""
        raise NotImplementedError(f'{self.__class__.__name__}.eval')
    def topy(self, scope:'Scope'=None) -> Any:
        """Convert the AST to a python equivalent object (usually unboxing the dewy object)"""
        raise NotImplementedError(f'{self.__class__.__name__}.topy')
    def comp(self, scope:'Scope'=None) -> str:
        """TODO: future handle compiling an AST to LLVM IR"""
        raise NotImplementedError(f'{self.__class__.__name__}.comp')
    # @abstractclassmethod
    # def type(cls, scope:'Scope'=None) -> 'Type':
    #     """Return the type of the object that would be returned by eval"""
    #     raise NotImplementedError(f'{cls.__name__}.type')
    #TODO: other methods, e.g. semantic analysis
    def treestr(self, indent=0) -> str:
        """Return a string representation of the AST tree"""
        raise NotImplementedError(f'{self.__class__.__name__}.treestr')
    def __str__(self) -> str:
        """Return a string representation of the AST as dewy code"""
        raise NotImplementedError(f'{self.__class__.__name__}.__str__')
    def __repr__(self) -> str:
        """Return a string representation of the python objects making up the AST"""
        raise NotImplementedError(f'{self.__class__.__name__}.__repr__')

class PrototypeAST(AST):
    def eval(self, scope:'Scope'=None) -> AST:
        raise ValueError(f'Prototype ASTs may not define eval. Attempted to call eval on prototype {self}, of type ({type(self)})')

class Undefined(AST):
    """undefined singleton"""

    #type value is set in __new__, since class Type isn't declared yet
    type:'Type'

    def __new__(cls):
        if not hasattr(cls, 'instance'):
            cls.instance = super(Undefined, cls).__new__(cls)
        return cls.instance
    def eval(self, scope:'Scope'=None):
        return self
    def topy(self, scope:'Scope'=None):
        return None
    def typeof(self, scope:'Scope'=None):
        return Type('undefined')
    def treesr(self, indent=0):
        return tab * indent + 'Undefined'
    def __str__(self):
        return 'undefined'
    def __repr__(self):
        return 'Undefined()'

#undefined shorthand, for convenience
undefined = Undefined()




class Scope():
    
    @dataclass
    class _var():
        # name:str #name is stored in the dict key
        type:AST
        value:AST
        const:bool
    
    def __init__(self, parent:Optional['Scope']=None):
        self.parent = parent
        self.vars:dict[str, Scope._var] = {}
        
        #used for function calls
        self.args:Array|None = None

    @property
    def root(self) -> 'Scope':
        """Return the root scope"""
        return [*self][-1]

    def let(self, name:str, type:Union['Type',Undefined]=undefined, value:AST=undefined, const=False):
        #overwrite anything that might have previously been there
        self.vars[name] = Scope._var(type, value, const)

    def get(self, name:str, default:AST=None) -> AST:
        #get a variable from this scope or any of its parents
        for s in self:
            if name in s.vars:
                return s.vars[name].value
        if default is not None:
            return default
        raise NameError(f'{name} not found in scope {self}')

    def bind(self, name:str, value:AST):

        #update an existing variable in this scope or  any of the parent scopes
        for s in self:
            if name in s.vars:
                var = s.vars[name]
                assert not var.const, f'cannot assign to const {name}'
                assert Type.is_instance(value.typeof(), var.type), f'cannot assign {value}:{value.typeof()} to {name}:{var.type}'
                var.value = value
                return

        #otherwise just create a new instance of the variable
        self.vars[name] = Scope._var(undefined, value, False)

    def __iter__(self):
        """return an iterator that walks up each successive parent scope. Starts with self"""
        s = self
        while s is not None:
            yield s
            s = s.parent

    def __repr__(self):
        if self.parent is not None:
            return f'Scope({self.vars}, {repr(self.parent)})'
        return f'Scope({self.vars})'

    def copy(self):
        s = Scope(self.parent)
        s.vars = self.vars.copy()
        return s

    def attach_args(self, args:Union['Array', None]):
        self.args = args

    @staticmethod
    def default():
        """return a scope with the standard library (of builtins) included"""
        root = Scope()
        root.bind('print', Builtin('print', [Arg('text')], None, Type('callable', [Array([String.type]), Undefined.type])))
        root.bind('printl', Builtin('printl', [Arg('text')], None, Type('callable', [Array([String.type]), Undefined.type])))
        root.bind('readl', Builtin('readl', [], String, Type('callable', [Array([]), String.type])))
        #TODO: eventually add more builtins

        return root


#probably won't use this, except possibly for when calling functions and providing enums from the function's scope
# def merge_scopes(*scopes:list[Scope], onto:Scope=None):
#     #TODO... this probably could actually be a scope union class that inherits from Scope
#     #            that way we don't have to copy the scopes
#     pdb.set_trace()


class Type(AST):

    type:'Type'

    #map by name from simple types to their parent type in the type graph
    graph: dict[str, str|None] = {
        'callable': None,
        'function': 'callable',
        'builtin': 'callable'
    }

    def __init__(self, name:str|AST, params:list[AST]=None, parent:str=None):
        self.name = name
        self.params = params or []
        
        # register type in the type graph
        if isinstance(name, str) and name not in Type.graph:
            Type.graph[name] = parent
        
    def eval(self, scope:Scope=None):
        return self

    def typeof(self, scope:Scope=None):
        return Type('type')
    

    def treestr(self, indent=0):
        s = tab * indent + f'Type: {self.name}\n'
        for p in self.params:
            s += p.treestr(indent + 1) + '\n'
        return s

    @insert_tabs
    def __str__(self):
        if self.params is not None and len(self.params) > 0:
            return f'{self.name}<{", ".join(map(str, self.params))}>'
        return self.name

    def __repr__(self):
        return f'Type({self.name}, {self.params})'

    def __eq__(self, other):
        if isinstance(other, Type):
            return self.name == other.name and self.params == other.params
        return False
    
    def __and__(self, other:AST):
        return And(self, other, Type)
    
    def __rand__(self, other:AST):
        return And(other, self, Type)
    
    def __or__(self, other:AST):
        return Or(self, other, Type)
    
    def __ror__(self, other:AST):
        return Or(other, self, Type)

    def __eq__(self, other):
        raise NotImplementedError('Type comparisons should be performed with `Type.is_instance()`')
    
    #TODO: come up with better names for the method arguments
    @staticmethod
    def is_instance(obj_t:Union['Type',Undefined], target_t:Union['Type',Undefined]) -> bool:
        """
        Check if the object is an instance (or descendent) of the specified type

        if target_t is undefined, short circuit results to True.
        Parameters are only checked based on those present in target_t, 
            e.g. if obj_t=array<int> and target_t=array, then no params would be checked (and is_instance would be true)

        Args:
            obj_t (Type|Undefined): The type of the object to be checked (i.e. for some dewy obj, obj_t = obj.typeof())
            target_t: (Type|Undefined): The target type for determining if obj_t is an instance or not

        Returns:
            (bool): whether or not obj_t is an instance (or descendent) of target_t
        """
        assert isinstance(target_t, (Type, Undefined)), f't must be a Type or undefined, not {target_t}'
        assert isinstance(obj_t, (Type, Undefined)), f'obj_t must be a Type or undefined, not {obj_t}'
        
        # undefined target always returns true
        if isinstance(target_t, Undefined):
            return True
        
        # since target is not undefined, undefined obj_t necessarily doesn't match
        if isinstance(obj_t, Undefined):
            return False
        
        
        #DEBUG/TODO
        if isinstance(target_t.name, AST) and isinstance(obj_t.name, AST):
            #TODO: need to make AST equality work properly (namely for binops and/or)
            #      will remove this if check, and let target.name == obj_t.name handle it
            pdb.set_trace()
        
        # check the type by name, and params
        if target_t.name == obj_t.name:
            if len(obj_t.params) < len(target_t.params):
                return False
            
            # check if any object parameters don't match the present target parameters
            # note: obj_t may have more params than target_t
            if any(obj_param != target_param for obj_param, target_param in zip(target_t.params, obj_t.params)):
                return False
                
            # names match, and obj_t has all params target_t has
            return True
        

        # for non-matching name, if atomic type, recursively check parent type in graph for compatibility
        if isinstance(obj_t.name, str):
            parent = Type.graph[obj_t.name]
            if parent is not None:
                #TODO: this is a poor way to check for parent types. Assumes parent parameters are same shape as child type...
                return Type.is_instance(Type(parent, obj_t.params), target_t)

        return False

# set the type class property for Type, and Undefined since class Type() exists now
Type.type = Type('type')
Undefined.type = Type('undefined')

class Identifier(PrototypeAST):
    # intermediate node, expected to be replaced with call or etc. during AST construction

    def __init__(self, name:str) -> None:
        self.name = name
    
    def __str__(self) -> str:
        return f'{self.name}'
    
    def __repr__(self) -> str:
        return f'Identifier({self.name})'


class Callable(AST):

    type:Type = Type('callable')

    def call(self, scope:'Scope'=None):
        """Call the callable in the given scope"""
        raise NotImplementedError(f'{self.__class__.__name__}.call')

class Orderable(AST):
    """An object that can be sorted relative to other objects of the same type"""
    def compare(self, other:'Orderable', scope:'Scope'=None) -> 'Number':
        """Return a value indicating the relationship between this value and another value"""
        raise NotImplementedError(f'{self.__class__.__name__}.compare')
    @staticmethod
    def max(self) -> 'Rangeable':
        """Return the maximum element from the set of all elements of this type"""
        raise NotImplementedError(f'{self.__class__.__name__}.max')
    @staticmethod
    def min(self) -> 'Rangeable':
        """Return the minimum element from the set of all elements of this type"""
        raise NotImplementedError(f'{self.__class__.__name__}.min')
    
#TODO: come up with a better name for this class... successor and predecessor are only used for range iterators, not ranges themselves
#        e.g. Incrementable, Decrementable, etc.
class Rangeable(Orderable):
    """An object that can be used to specify bounds of a range"""
    def successor(self, step=undefined, scope:'Scope'=None) -> 'Rangeable':
        """Return the next value in the range"""
        raise NotImplementedError(f'{self.__class__.__name__}.successor')
    def predecessor(self, step=undefined, scope:'Scope'=None) -> 'Rangeable':
        """Return the previous value in the range"""
        raise NotImplementedError(f'{self.__class__.__name__}.predecessor')

class Unpackable(AST):
    def len(self, scope:'Scope'=None) -> int:
        """Return the length of the unpackable"""
        raise NotImplementedError(f'{self.__class__.__name__}.len')
    def get(self, key:int|EllipsisType|slice|tuple[int|EllipsisType|slice], scope:'Scope'=None) -> AST:
        """Return the item at the given index"""
        raise NotImplementedError(f'{self.__class__.__name__}.get')
#TODO: make a type annotation for Unpackable[N] where N is the number of items in the unpackable?
#        would maybe replace the len property?

class Iter(AST):
    def next(self, scope:'Scope'=None) -> Unpackable: #TODO: TBD on the return type. need dewy tuple type...
        """Get the next item from the iterator"""
        raise NotImplementedError(f'{self.__class__.__name__}.next')

class Iterable(AST):
    #TODO: maybe don't need scope for this method...
    def iter(self, scope:'Scope'=None) -> Iter:
        """Return an iterator over the iterable"""
        raise NotImplementedError(f'{self.__class__.__name__}.iter')




class Arg:
    def __init__(self, name:str, type:Type=None, val:AST|None=None):
        self.name = name
        self.val = val
        self.type = type
    # @insert_tabs
    def __str__(self):
        s = f'{self.name}'
        if self.type is not None:
            s += f':{self.type}'
        if self.val is not None:
            s += f' = {self.val}'
        return s
    def __repr__(self):
        s = f'Arg({self.name}'
        if self.type is not None:
            s += f', {repr(self.type)}'
        if self.val is not None:
            s += f', {repr(self.val)}'
        s += ')'
        return s


class Function(Callable):
    
    type:Type = Type('function')

    def __init__(self, args:list[Arg], body:AST, scope:Scope=None):
        self.args = args
        self.body = body
        self.scope = scope #scope where the function was defined, which may be different from the scope where it is called
    
    def eval(self, scope:Scope=None):
        #TODO: maybe this should do self.scope=scope since this is the scope where the function is defined
        #        just probably problems when expressing the function without calling it, e.g. with handles
        #        f = {() => {...}} @f // the @f would set the scope to be the outer scope...
        return self
    
    def call(self, scope:Scope=None):
        #collect args from calling scope, and merge into function scope
        fscope = Scope(self.scope)

        # grab the args being passed in
        caller_args:list[AST] = []
        caller_kwargs:dict[str,AST] = {}
        if scope is not None and scope.args is not None:
            for arg in scope.args.vals:
                if not isinstance(arg, Bind):
                    caller_args.append(arg)
                else:
                    caller_kwargs[arg.name] = arg.value
        
        # grab the args and any default args from the function definition
        fn_args = [arg for arg in self.args if arg.val is None and arg.name not in caller_kwargs]
        fn_kwargs = [arg for arg in self.args if arg.val is not None and arg.name not in caller_kwargs]
        fn_arg_names = {arg.name for arg in self.args}

        # bind the positional arguments to the function scope
        assert len(fn_args) == len(caller_args), f'encountered different number of positional arguments than expected. Function is defined with {[a for a in self.args if a.val is None]}. Tried to call with {caller_args}'
        for fn_arg, caller_arg in zip(fn_args, caller_args):
            fscope.let(fn_arg.name, caller_arg)

        # bind keyward arguments to the function scope, first from default args, then from caller
        for fn_kwarg in fn_kwargs:
            fscope.let(fn_kwarg.name, fn_kwarg.val)
        for caller_kwarg_name, caller_kwarg_value in caller_kwargs.items():
            assert caller_kwarg_name in fn_arg_names, f"tried to bind unrecognized keyword argument '{caller_kwarg_name}' to function {self}"
            fscope.let(caller_kwarg_name, caller_kwarg_value)

        return self.body.eval(fscope)

    def treestr(self, indent=0):
        s = tab * indent + f'Function()\n'
        for arg in self.args:
            s += tab * (indent + 1) + f'Arg: {arg.name}\n'
            if arg.type is not None:
                s += arg.type.treestr(indent + 2) + '\n'
            if arg.val is not None:
                s += arg.val.treestr(indent + 2) + '\n'
        s += tab*(indent+1) + 'Body:\n' + self.body.treestr(indent + 2)
        return s

    @insert_tabs
    def __str__(self):
        s = ''
        if len(self.args) == 1:
            s += f'{self.args[0]}'
        else:
            s += f'({", ".join(map(str, self.args))})'
        s += f' => {self.body}'
        return s

    def __repr__(self):
        return f'Function(args:{self.args}, body:{self.body}, scope:{self.scope})'

class Builtin(Callable):
    funcs = {
        'print': partial(print, end=''),
        'printl': print,
        'readl': input
    }

    type:Type = Type('builtin')

    def __init__(self, name:str, args:list[Arg], cls:PyCallable, type:Type):
        self.name = name
        self.args = args
        self.cls = cls
        self.type = type
    
    def eval(self, scope:Scope=None):
        return self
    
    def call(self, scope:Scope=None):
        if self.name in Builtin.funcs:
            f = Builtin.funcs[self.name]
            #TODO: this doesn't handle differences in named vs unnamed args between the function definition and the call

            args = []
            kwargs = {}

            # insert positional and keyward args from the call
            if scope.args is not None:
                for ast in scope.args.vals:
                    if not isinstance(ast, Bind):
                        args.append(ast.eval(scope).topy(scope))
                    else:
                        kwargs[ast.name] = ast.value.eval(scope).topy(scope)

            # insert any default/named args (not already inserted by the call)
            for arg in self.args:
                if arg.val is not None and arg.name not in kwargs:
                    kwargs[arg.name] = arg.val.eval(scope).topy(scope)

            result = f(*args, **kwargs)
            if self.cls is not None:
                return self.cls(result)
        else:
            raise NameError(self.name, 'is not a builtin')

    def treestr(self, indent=0):
        s = tab * indent + f'Builtin({self.name})\n'
        for arg in self.args:
            s += tab * (indent + 1) + f'Arg: {arg.name}\n'
            if arg.type is not None:
                s += arg.type.treestr(indent + 2) + '\n'
            if arg.val is not None:
                s += arg.val.treestr(indent + 2) + '\n'
        return s

    # @insert_tabs
    def __str__(self):
        return f'{self.name}({", ".join(map(str, self.args))})'

    def __repr__(self):
        return f'Builtin({self.name}, {self.args})'


class Let(AST):
    def __init__(self, name:str, type:Type, value:AST=undefined, const=False):
        self.name = name
        self.type = type
        self.value = value
        self.const = const

    def eval(self, scope:Scope=None):
        scope.let(self.name, self.type, self.value, self.const)

    def treestr(self, indent=0):
        return f'{tab * indent}{"Const" if self.const else "Let"}: {self.name}\n{self.type.treestr(indent + 1)}'

    @insert_tabs
    def __str__(self):
        return f'{"const" if self.const else "let"} {self.name}:{self.type} = {self.value}'

    def __repr__(self):
        return f'{"Const" if self.const else "Let"}({self.name}, {self.type}, {self.value})'


class Bind(AST):
    #TODO: allow bind to take in an unpack structure
    def __init__(self, name:str, value:AST):
        self.name = name
        self.value = value
    def eval(self, scope:Scope=None):
        scope.bind(self.name, self.value.eval(scope))

    def treestr(self, indent=0):
        return f'{tab * indent}Bind: {self.name}\n{self.value.treestr(indent + 1)}'

    @insert_tabs
    def __str__(self):
        return f'{self.name} = {self.value}'

    def __repr__(self):
        return f'Bind({self.name}, {repr(self.value)})'


class PackStruct(list[Union[str,'PackStruct']]):
    """
    represents the type for left hand side of an unpack operation

    unpacking operations in dewy look like this:
    [a, b, c] = [1 2 3]                                         //a=1, b=2, c=3
    [a, [b, c]] = [1 [2 3]]                                     //a=1, b=2, c=3
    [a, [b, c], d] = [1 [2 3] 4]                                //a=1, b=2, c=3, d=4
    [a, ...b] = [1 2 3 4]                                       //a=1, b=[2 3 4]
    [a, ...b, c] = [1 2 3 4 5]                                  //a=1, b=[2 3 4], c=5
    [a, ...b, [c, [...d, e, f]]] = [1 2 3 4 [5 [6 7 8 9 10]]]   //a=1, b=[2 3 4], c=5, d=[6 7 8], e=9, f=10

    note that there may only be one ellipsis in a given level of the structure
    """
    ...



class Unpack(AST):
    def __init__(self, struct:PackStruct, value:Unpackable):
        #check that there is only one ellipsis in the top level of the structure (lower levels are checked recursively)
        #TODO: problem with checking lower levels recursively is that it is no longer a compile time check
        assert sum(1 for s in struct if isinstance(s, str) and s.startswith('...')) <= 1, 'only one ellipsis is allowed per level of the structure'
        self.struct = struct
        self.value = value

    def eval(self, scope:Scope=None):
        value = self.value.eval(scope)
        assert isinstance(value, Unpackable), f'{value} is not unpackable'
        value_len = value.len(scope)
        
        #check that the structure has a matching number of elements to the value
        #TODO: actually maybe allow this, and any extra elements are just undefined. complicated to handle if the number of elements is wrong though
        has_ellipsis = any(isinstance(s, str) and s.startswith('...') for s in self.struct)
        if has_ellipsis:
            assert value_len >= len(self.struct) - 1, f'cannot unpack {value} into {Unpack.str_helper(self.struct)}, expected more values to unpack'
        else:
            assert value_len == len(self.struct), f'cannot unpack {value} into {Unpack.str_helper(self.struct)}, ' + ('expected more' if value_len < len(self.struct) else 'expected less') + ' values to unpack'

        offset = 0 #offset for handling if an ellipsis was encountered during the unpack
        for i, s in enumerate(self.struct):
            if isinstance(s, str):
                if s.startswith('...'):
                    name = s[3:]
                    n = value_len - len(self.struct) + 1 #number of elements to fill the ellipsis with
                    scope.bind(name, value.get(slice(i,i+n), scope))
                    offset += n - 1
                else:
                    scope.bind(s, value.get(i+offset, scope))
            elif isinstance(s, list) or isinstance(s, tuple):
                Unpack(s, value.get(i+offset, scope)).eval(scope)
            else:
                raise TypeError(f'invalid type in unpack structure: `{s}` of type `{type(s)}`')

    def treestr(self, indent=0):
        return f'{tab * indent}Unpack: {self.struct}\n{self.value.treestr(indent + 1)}'

    @insert_tabs
    def __str__(self):
        return f'{Unpack.str_helper(self.struct)} = {self.value}'

    @staticmethod
    def str_helper(val):
        if isinstance(val, str):
            return val
        else:
            s = '['
            for i, v in enumerate(val):
                if isinstance(v, str):
                    s += v
                else:
                    s += Unpack.str_helper(v)
                if i != len(val) - 1:
                    s += ', '
            s += ']'
            return s

    def __repr__(self):
        return f'Unpack({self.struct}, {self.value})'

class Tuple(PrototypeAST):
    """
    A comma separated list of expressions (not wrapped in parentheses) e.g. 1, 2, 3
    There is no special in-memory representation of a tuple, it is literally just a const list
    """
    def __init__(self, exprs:list[AST]):
        self.exprs = exprs
    def __repr__(self):
        return f'Tuple({repr(self.exprs)})'
    

class Block(AST):
    def __init__(self, exprs:list[AST], newscope:bool=True):
        self.exprs = exprs
        self.newscope = newscope
    def eval(self, scope:Scope=None):
        #TODO: handle flow control from a block, e.g. return, break, continue, express, etc.
        if self.newscope:
            scope = Scope(scope)
        expressed = []
        for expr in self.exprs:
            res = expr.eval(scope)
            if res is not None and res is not void:
                expressed.append(res)
        if len(expressed) == 0:
            return void
        if len(expressed) == 1:
            return expressed[0]
        raise NotImplementedError('block with multiple expressions not yet supported')
        #TODO: this is actually a lot like `yield`! maybe `yield` should be instead of `express` or they are synonymous
        return Array(expressed)

    def treestr(self, indent=0):
        """print each expr on its own line, indented"""
        s = tab * indent + 'Block\n'
        for expr in self.exprs:
            s += expr.treestr(indent + 1)
        return s

    @insert_tabs
    def __str__(self):
        return f'{{{newline}{newline.join(map(str, self.exprs))}{newline}}}'

    def __repr__(self):
        return f'Block({repr(self.exprs)})'


class Call(AST):
    def __init__(self, expr:str|Callable, args:Union['Array',None]=None):
        assert isinstance(expr, str|Callable), f'invalid type for call expression: `{self.expr}` of type `{type(self.expr)}`'
        self.expr = expr
        self.args = args


    def eval(self, scope:Scope):
        scope.attach_args(self.args)

        #check if we need to resolve the name, or if it was an anonymous expression
        if isinstance(self.expr, AST):
            expr = self.expr.eval(scope)
        else:
            expr = scope.get(self.expr)
        
        #functions get called with args, while everything else just gets evaluated/returned
        if isinstance(expr, Callable):
            return expr.call(scope)
        else:
            return expr

    def treestr(self, indent=0):
        s = tab * indent + 'Call: '
        if isinstance(self.expr, AST):
            s += self.expr.treestr(indent + 1)
        else:
            s += self.expr
        if self.args is not None:
            s += '\n'
            s += self.args.treestr(indent+1)
        return s

    @insert_tabs
    def __str__(self):
        #TODO: not sure if expr of type Function should get () even if they don't have args
        argsstr = '' if self.args is None else f'({str(self.args)[1:-1]})' #strip off [] and replace with ()
        return f'{self.expr}' + (f'{self.args}' if self.args else '')

    def __repr__(self):
        return f'Call({repr(self.expr)}, {repr(self.args)})'

class String(Rangeable):
    type:Type=Type('string')
    
    def __init__(self, val:str):
        self.val = val
    def eval(self, scope:Scope=None):
        return self
    def typeof(self, scope:Scope=None):
        return self.type
    #TODO: implement rangable methods
    def topy(self, scope:Scope=None) -> str:
        return self.val
    def treestr(self, indent=0):
        return f'{tab * indent}String: `{self.val}`'
    # @insert_tabs
    def __str__(self):
        return f'"{self.val}"'
    def __repr__(self):
        return f'String({repr(self.val)})'

class IString(AST):
    def __init__(self, parts:list[AST]):
        self.parts = parts

    def eval(self, scope:Scope=None):
        #convert self into a String()
        return String(self.topy(scope))

    def topy(self, scope:Scope=None):
        return ''.join(str(part.eval(scope).topy(scope)) for part in self.parts)

    def treestr(self, indent=0):
        s = tab * indent + 'IString\n'
        for part in self.parts:
            s += part.treestr(indent + 1) + '\n'
        return s

    @insert_tabs
    def __str__(self):
        s = ''
        for part in self.parts:
            if isinstance(part, String):
                s += part.val
            else:
                s += f'{{{part}}}'
        return f'"{s}"'

    def __repr__(self):
        return f'IString({repr(self.parts)})'

class BinOp(AST):
    def __init__(self, left:AST, right:AST, op:PyCallable[[Any, Any],Any], outtype:PyType[AST]|None, opname:str, opsymbol:str):
        self.left = left
        self.right = right
        self.op = op
        self.outtype = outtype
        self.opname = opname
        self.opsymbol = opsymbol

    def eval(self, scope:Scope=None):
        left = self.left.eval(scope)
        right = self.right.eval(scope)
        outtype = self.outtype

        if outtype is None:
            #TODO: remove support for this later. type should be determined after parsing during type checking!
            # determine the outtype from the input types
            assert type(left) == type(right), f"For unspecified output type, both left and right must have the same type. Found {type(left)=}, {type(right)=}"
            outtype = type(left)

        return outtype(self.op(left.topy(), right.topy()))
    

    def treestr(self, indent=0):
        return f'{tab * indent}{self.opname}\n{self.left.treestr(indent + 1)}\n{self.right.treestr(indent + 1)}'
    @insert_tabs
    def __str__(self):
        return f'{self.left} {self.opsymbol} {self.right}'
    def __repr__(self):
        return f'{self.opname}({repr(self.left)}, {repr(self.right)})'

##################### Binary operators #####################
class Equal(BinOp):
    def __init__(self, left:AST, right:AST):
        super().__init__(left, right, operator.eq, Bool, 'Equal', '=?')

class NotEqual(BinOp):
    def __init__(self, left:AST, right:AST):
        super().__init__(left, right, operator.ne, Bool, 'NotEqual', 'not=?')

class Less(BinOp):
    def __init__(self, left:AST, right:AST):
        super().__init__(left, right, operator.lt, Bool, 'Less', '<?')

class LessEqual(BinOp):
    def __init__(self, left:AST, right:AST):
        super().__init__(left, right, operator.le, Bool, 'LessEqual', '<=?')

class Greater(BinOp):
    def __init__(self, left:AST, right:AST):
        super().__init__(left, right, operator.gt, Bool, 'Greater', '>?')

class GreaterEqual(BinOp):
    def __init__(self, left:AST, right:AST):
        super().__init__(left, right, operator.ge, Bool, 'GreaterEqual', '>=?')

class Add(BinOp):
    def __init__(self, left:AST, right:AST, outtype:PyType[AST]):
        super().__init__(left, right, operator.add, outtype, 'Add', '+')

class Sub(BinOp):
    def __init__(self, left:AST, right:AST, outtype:PyType[AST]):
        super().__init__(left, right, operator.sub, outtype, 'Sub', '-')

class Mul(BinOp):
    def __init__(self, left:AST, right:AST, outtype:PyType[AST]):
        super().__init__(left, right, operator.mul, outtype, 'Mul', '*')

class Div(BinOp):
    def __init__(self, left:AST, right:AST, outtype:PyType[AST]):
        super().__init__(left, right, operator.truediv, outtype, 'Div', '/')

class IDiv(BinOp):
    def __init__(self, left:AST, right:AST, outtype:PyType[AST]):
        super().__init__(left, right, operator.floordiv, outtype, 'IDiv', '//')

class Mod(BinOp):
    def __init__(self, left:AST, right:AST, outtype:PyType[AST]):
        super().__init__(left, right, operator.mod, outtype, 'Mod', '%')

class Pow(BinOp):
    def __init__(self, left:AST, right:AST, outtype:PyType[AST]):
        super().__init__(left, right, operator.pow, outtype, 'Pow', '**')

class And(BinOp):
    def __init__(self, left:AST, right:AST, outtype:PyType[AST]):
        super().__init__(left, right, operator.and_, outtype, 'And', 'and')

class Or(BinOp):
    def __init__(self, left:AST, right:AST, outtype:PyType[AST]):
        super().__init__(left, right, operator.or_, outtype, 'Or', 'or')

class Xor(BinOp):
    def __init__(self, left:AST, right:AST, outtype:PyType[AST]):
        super().__init__(left, right, operator.xor, outtype, 'Xor', 'xor')

class Nand(BinOp):
    def __init__(self, left:AST, right:AST, outtype:PyType[AST]):
        super().__init__(left, right, lambda l, r: not (l and r), outtype, 'Nand', 'nand')

class Nor(BinOp):
    def __init__(self, left:AST, right:AST, outtype:PyType[AST]):
        super().__init__(left, right, lambda l, r: not (l or r), outtype, 'Nor', 'nor')

class Xnor(BinOp):
    def __init__(self, left:AST, right:AST, outtype:PyType[AST]):
        super().__init__(left, right, lambda l, r: l == r, outtype, 'Xnor', 'xnor')



##################### Unary operators #####################
class UnaryOp(AST):
    def __init__(self, child:AST, op:PyCallable[[Any, Any],Any], outtype:PyType[AST]|None, opname:str, opsymbol:str):
        self.child = child
        self.op = op
        self.outtype = outtype
        self.opname = opname
        self.opsymbol = opsymbol

    def eval(self, scope:Scope=None):
        child = self.child.eval(scope)
        outtype = self.outtype

        if outtype is None:
            #TODO: remove support for this later. type should be determined after parsing during type checking!
            # determine the outtype from the input type
            outtype = type(child)

        return outtype(self.op(child.topy()))

    def treestr(self, indent=0):
        return f'{tab * indent}{self.opname}\n{self.child.treestr(indent + 1)}'
    @insert_tabs
    def __str__(self):
        return f'{self.opsymbol}{self.child}'
    def __repr__(self):
        return f'{self.opname}({repr(self.child)})'

class Neg(UnaryOp):
    def __init__(self, child:AST, outtype:PyType[AST]):
        super().__init__(child, operator.neg, outtype, 'Neg', '-')


class Inv(UnaryOp):
    def __init__(self, child:AST, outtype:PyType[AST]):
        super().__init__(child, lambda x: 1/x, outtype, 'Inv', '/')


class Bool(AST):
    def __init__(self, val:bool):
        self.val = val
    def eval(self, scope:Scope=None):
        return self
    def topy(self, scope:Scope=None):
        return self.val
    def typeof(self, scope:Scope=None):
        return Type('bool')
    def treestr(self, indent=0):
        return f'{tab * indent}Bool: {self.val}'
    # @insert_tabs
    def __str__(self):
        return f'{self.val}'
    def __repr__(self):
        return f'Bool({repr(self.val)})'

class Flowable(AST):
    def was_entered(self) -> bool:
        """Determine if the flowable branch was entered. Should reset before performing calls to flow and checking this."""
        raise NotImplementedError(f'flowables must implement `was_entered()`. No implementation found for {self.__class__}')

    def reset_was_entered(self) -> None:
        """reset the state of was_entered, in preparation for executing branches in a flow"""
        raise NotImplementedError(f'flowables must implement `reset_was_entered()`. No implementation found for {self.__class__}')

class If(Flowable):
    def __init__(self, cond:AST, body:AST):
        self.cond = cond
        self.body = body
        self._was_entered: bool = False
    
    def was_entered(self) -> bool:
        return self._was_entered
    
    def reset_was_entered(self) -> None:
        self._was_entered = False
    
    def eval(self, scope:Scope=None):
        child = Scope(scope) #if clause gets an anonymous scope
        if self.cond.eval(child).topy(child):
            self._was_entered = True
            return self.body.eval(child)

    def treestr(self, indent=0):
        s = tab * indent + 'If\n'
        s += self.cond.treestr(indent + 1) + '\n'
        s += self.body.treestr(indent + 1) + '\n'
        return s

    @insert_tabs
    def __str__(self):
        return f'if {self.cond} {self.body}'
        
    def __repr__(self):
        return f'If({repr(self.cond)}, {repr(self.body)})'

#TODO: maybe loop can work the say way as If, taking in a list of clauses?
class Loop(Flowable):
    def __init__(self, cond:AST, body:AST):
        self.cond = cond
        self.body = body
        self._was_entered: bool = False
    
    def was_entered(self) -> bool:
        return self._was_entered
    
    def reset_was_entered(self) -> None:
        self._was_entered = False

    def eval(self, scope:Scope=None):
        child = Scope(scope)
        while self.cond.eval(child).topy(child):
            self._was_entered = True
            self.body.eval(child)
        #TODO: handle capturing values from a loop
        #TODO: handle break and continue
        #TODO: also eventually handle return (problem for other ASTs as well)

        #for now just don't let loops return anything
        return void

    def treestr(self, indent=0):
        return f'{tab * indent}Loop\n{self.cond.treestr(indent + 1)}\n{self.body.treestr(indent + 1)}'

    @insert_tabs
    def __str__(self):
        return f'loop {self.cond} {self.body}'

    def __repr__(self):
        return f'Loop({repr(self.cond)}, {repr(self.body)})'

class Flow(AST):
    def __init__(self, branches:list[Flowable|AST]):
        
        #separate out the possible last branch which need not be a Flowable()
        if not isinstance(branches[-1], Flowable):
            branches, default = branches[:-1], branches[-1]
        else:
            branches, default = branches, None
        
        #verify all branches (not necessarily including last) are Flowable
        assert all(isinstance(branch, Flowable) for branch in branches), f'All branches in a flow (excluding the last one) must inherit `Flowable()`. Got {branches=}'
        
        self.branches: list[Flowable] = branches
        self.default: AST|None = default
        
    
    def eval(self, scope:Scope=None):
        shared = Scope(scope) #for now, all clauses share a common anonymous scope

        #reset was entered for this execution of the flow
        for expr in self.branches:
            expr.reset_was_entered()

        #execute branches in the flow until one is entered
        for expr in self.branches:
            res = expr.eval(shared)
            if expr.was_entered():
                return res
            
        #execute any default branch if it exists
        if self.default is not None:
            return self.default.eval(shared)
        
        return undefined

    def treestr(self, indent=0):
        s = tab * indent + 'Flow\n'
        for expr in self.branches:
            s += expr.treestr(indent + 1) + '\n'
        if self.default is not None:
            s += self.default.treestr(indent + 1) + '\n'
        return s

    @insert_tabs
    def __str__(self):
        s = ''
        for i, expr in enumerate(self.branches):
            if i == 0:
                s += f'{expr}'
            else:
                s += f' else {expr}'
        if self.default is not None:
            s += f' else {self.default}'
        return s

    def __repr__(self):
        return f'Flow({repr(self.branches)})'


#DEBUG example
"""
loop i in [0..10)
    printl(i)

//expanded version of above
iter = [0..10).iter()
let i
loop {(cond, i) = iter.next(); cond} 
    printl(i)

//but ideally the entire iterator could be contained in the condition expression of the loop
{
    loop
    (
        // idempotent initialization here
        #ifnotexists(iter) iter = [0..10).iter()
        (cond, i) = iter.next()
        cond
    )
    (
        // condition and body share the same scope
        printl(i)
    )
}
"""
#TODO: convert to class In()
#  in basically does this, but has the extra stuff with the var being set, and so forth
#class iter is the manager for things that can iterate, e.g. Range.iter()->RangeIter, Array.iter()->ArrayIter, etc.
class In(AST):
    #TODO: allow name to be an unpack structure as well
    def __init__(self, name:str|PackStruct, iterable:Iterable):#, init:AST, body:AST):
        self._id = f'.it_{id(self)}'
        self.name = name
        self.iterable = iterable

    def eval(self, scope:Scope=None) -> Bool:
        #idempotent initialization
        try:
            it = scope.get(self._id)
        except NameError:
            it = self.iterable.iter(scope)
            scope.let(self._id, value=it, const=True)

        # body gets the binds element and returns the resulting condition
        Unpack(['_', self.name], Next(Call(self._id))).eval(scope)
        cond = Call('_').eval(scope)
        assert isinstance(cond, Bool), f'loop condition must be a Bool, not {cond} of type {type(cond)}'
        return cond

    def treestr(self, indent=0):
        return f'{tab * indent}In: {self.name}\n{self.iterable.treestr(indent + 1)}'

    @insert_tabs
    def __str__(self):
        return f'{self.name} in {self.iterable}'

    def __repr__(self):
        return f'In({repr(self.name)}, {repr(self.iterable)})'

class Next(AST):
    """handle getting the next element in the iteration"""
    def __init__(self, iterable:AST):
        self.iterable = iterable

    def eval(self, scope:Scope=None) -> AST:
        it = self.iterable.eval(scope)
        assert isinstance(it, Iter), f'cannot call next on {it}, not an iterator'
        return it.next(scope)

    def __repr__(self):
        return f'Next({repr(self.iterable)})'

    @insert_tabs
    def __str__(self):
        return f'next({self.iterable})'

class Number(Rangeable):
    type:Type = Type('number')

    def __init__(self, val:int|float):
        self.val = val
    def eval(self, scope:Scope=None):
        return self
    def typeof(self):
        return Type('number')
    
    #Rangeable methods
    def compare(self, other:'Number', scope:Scope=None) -> 'Number':
        return Number(self.val - other.val)
    def successor(self, step:'Number'=undefined, scope:'Scope'=None) -> 'Number':
        if step is undefined:
            return Number(self.val + 1)
        else:
            return Number(self.val + step.val)
    def predecessor(self, step:'Number'=undefined, scope:'Scope'=None) -> 'Number':
        if step is undefined:
            return Number(self.val - 1)
        else:
            return Number(self.val - step.val)
    @staticmethod
    def max() -> 'Number':
        return Number(float('inf'))
    @staticmethod
    def min() -> 'Number':
        return Number(float('-inf'))
    
    def topy(self, scope:Scope=None):
        return self.val
    def treestr(self, indent=0):
        return f'{tab * indent}Number: {self.val}'
    # @insert_tabs
    def __str__(self):
        return f'{self.val}'
    def __repr__(self):
        return f'Number({repr(self.val)})'


#TODO: handling of different types of ranges (e.g. character ranges, vs number ranges). 
#   how to handle +inf/-inf in non-numeric case? implies some sort of max/min element for the range value...
#   i.e. class Rangeable(AST): where class Number(Rangable), Char(Rangeable), etc.
#   Rangable types should implement successor(step=1) and predecessor(step=1) methods
class Range(Iterable,Unpackable):
    """
    Inspired by Haskell syntax for ranges:
    [first..]               // first to inf
    [first,second..]        // step size is second-first
    [first..last]           // first to last
    [first,second..last]    // first to last, step size is second-first
    //[first..2ndlast,last] // this is explicitly NOT ALLOWED, as it is covered by the previous case, and can have unintuitive behavior
    [..2ndlast,last]        // -inf to last, step size is last-penultimate
    [..last]                // -inf to last
    [..]                    // -inf to inf

    open/closed ranges:
    [first..last]           // first to last including first and last
    [first..last)           // first to last including first, excluding last
    (first..last]           // first to last excluding first, including last
    (first..last)           // first to last excluding first and last
    first..last             // same as [first..last]. Note that parentheses are required if `second` is included in the expression
    """
    def __init__(self, first:Rangeable=undefined, second:Rangeable=undefined, last:Rangeable=undefined, include_first:bool=True, include_last:bool=True):
        range_type = type(first) if first is not undefined else type(second) if second is not undefined else type(last)
        if range_type is undefined:
            range_type = Number
        assert issubclass(range_type, Rangeable), f'Range type must be of type Rangeable, not {range_type}'
        #TODO: type checking to confirm that first, second, and last are all compatible types

        self.range_type = range_type
        self.first = first if first is not undefined else range_type.min()
        self.second = second
        self.last = last if last is not undefined else range_type.max()
        self.include_first = include_first
        self.include_last = include_last
        
        # DEBUG: for now, require that ranges were wrapped in [], [), (], or () to avoid ambiguity
        # this means at parse time, when the range is first made, was_wrapped will be false
        # and then when the enclosing block is parsed, was_wrapped can be set to true
        self.was_wrapped = False

    def eval(self, scope:Scope=None):
        return self
    
    def iter(self, scope:'Scope'=None) -> Iter:
        return RangeIter(self)

    # def typeof(self):
    #     return Type('Range') #TODO: this should maybe care about the type of data in it?

    def topy(self, scope:Scope=None):
        step_size = self.second.topy() - self.first.topy() if self.second is not undefined else 1
        return range(self.first.topy(scope), self.last.topy(scope), step_size)

    def treestr(self, indent=0):
        s = f'{tab * indent}Range\n'
        s += f'{tab * (indent + 1)}first:\n{self.first.treestr(indent + 2)}\n'
        s += f'{tab * (indent + 1)}second:\n{self.second.treestr(indent + 2)}\n'
        s += f'{tab * (indent + 1)}last:\n{self.last.treestr(indent + 2)}\n'
        return s

    @insert_tabs
    def __str__(self):
        s = ''
        s += '[' if self.include_first else '('
        if self.first is not undefined:
            s += str(self.first)
        if self.second is not undefined:
            s += ','
            s += str(self.second)
        s += '..'
        if self.last is not undefined:
            s += str(self.last)
        s += ']' if self.include_last else ')'
        return s

    def __repr__(self):
        interval = f'{"[" if self.include_first else "("}{"]" if self.include_last else ")"}'
        return f'Range({repr(self.first)},{repr(self.second)},{repr(self.last)},interval={interval})'


class RangeIter(Iter):
    def __init__(self, ast:AST):#range:Range): #TODO: want AST[Range] typing which means it evals to a range...
        # self._id = f'.iter_{id(self)}'
        self.ast = ast
    #     self.reset()

    # def reset(self):
        self.range = None
        self.i = None
        self.step = None

    def eval(self, scope:Scope=None):
        return self

    def next(self, scope:Scope=None) -> Unpackable:
        if self.range is None:
            self.range = self.ast.eval(scope)
            assert isinstance(self.range, Range), f'RangeIter must be initialized with an AST that evaluates to a Range, not {type(self.range)}' 
            self.i = self.range.first
            #set the stepsize (needed access to the scope)
            if self.range.second is not undefined:
                self.step = self.range.second.compare(self.range.first, scope)
            else:
                self.step = undefined
            
            #skip the first element if it's not included (closed interval)
            if not self.range.include_first:
                self.i = self.i.successor(self.step, scope)

        #check the stop condition and return the next element
        if (c:=self.i.compare(self.range.last).val) < 0 or (c==0 and self.range.include_last):
            ret = self.i
            self.i = self.i.successor(self.step, scope)
            return Array([Bool(True), ret])
        else:
            return Array([Bool(False), undefined])

    def typeof(self):
        return Type('RangeIter')

    def topy(self, scope:Scope=None):
        raise NotImplementedError

    def treestr(self, indent=0):
        return f'{tab * indent}RangeIter:\n{self.ast.treestr(indent + 1)}'
        
    @insert_tabs
    def __str__(self):
        return f'RangeIter({self.ast})'

    def __repr__(self):
        return f'RangeIter({repr(self.ast)})'


class Array(Iterable, Unpackable):
    def __init__(self, vals:list[AST]):
        self.vals = vals
    def eval(self, scope:Scope=None):
        return self
    def typeof(self, scope:Scope=None):
        #TODO: this should include the type of the data inside the vector...
        return Type('Array')
    
    #unpackable interface
    def len(self, scope:Scope=None):
        return len(self.vals)
    def get(self, key:int|EllipsisType|slice|tuple[int|EllipsisType|slice], scope:Scope=None):
        if isinstance(key, int):
            return self.vals[key]
        elif isinstance(key, EllipsisType):
            return self
        elif isinstance(key, slice):
            return Array(self.vals[key])
        elif isinstance(key, tuple):
            #probably only valid for N-dimensional/non-jagged vectors
            raise NotImplementedError('TODO: implement tuple indexing for Array')
        else:
            raise TypeError(f'invalid type for Array.get: `{key}` of type `{type(key)}`')


    #iterable interface
    #TODO...

    def topy(self, scope:Scope=None):
        return [v.eval(scope).topy(scope) for v in self.vals]
    def treestr(self, indent=0):
        s = tab * indent + 'Array\n'
        for v in self.vals:
            s += v.treestr(indent + 1) + '\n'
        return s
    @insert_tabs
    def __str__(self):
        return f'[{" ".join(map(str, self.vals))}]'
    def __repr__(self):
        return f'Array({repr(self.vals)})'



class Void(AST):
    """void singleton"""

    type:Type=Type('void')

    def __new__(cls):
        if not hasattr(cls, 'instance'):
            cls.instance = super(Void, cls).__new__(cls)
        return cls.instance
    def eval(self, scope:Scope=None):
        return self
    def topy(self, scope:Scope=None):
        return None
    def typeof(self, scope:Scope=None):
        return Type('void')
    def treesr(self, indent=0):
        return tab * indent + 'Void'
    def __str__(self):
        return 'void'
    def __repr__(self):
        return 'Void()'

#void shorthand, for convenience
void = Void()





############################################## EXAMPLE PROGRAMS #############################################

def hello(root:Scope) -> AST:
    """printl('Hello, World!')"""
    return Call('printl', Array([String('Hello, World!')]))


def hello_func(root:Scope) -> AST:
    """
    {
        main = () => {printl('Hello, World!')}
        main
    }
    """
    return Block([
        Bind(
            'main',
            Function(
                [],
                Call('printl', Array([String('Hello, World!')])),
                root
            )
        ),
        Call('main'),
    ])
   

def anonymous_func(root:Scope) -> AST:
    """
    {
        (() => printl('Hello, World!'))()
    }
    """
    return Block([
        Call(
            Function(
                [],
                Call('printl', Array([String('Hello, World!')])),
                root
            )
        ),
    ])

def hello_name(root:Scope) -> AST:
    """
    {
        print("What's your name? ")
        name = readl()
        printl('Hello {name}!')
    }
    """
    return Block([
        Call('print', Array([String("What's your name? ")])),
        Bind('name', Call('readl')),
        Call('printl', Array([IString([String('Hello '), Call('name'), String('!')])])),
    ])


def if_else(root:Scope) -> AST:
    """
    {
        print("What's your name? ")
        name = readl()
        if name =? 'Alice' printl('Hello Alice!')
        else printl('Hello stranger!')
    }
    """
    return Block([
        Call('print', Array([String("What's your name? ")])),
        Bind('name', Call('readl')),
        Flow([
            If(
                Equal(Call('name'), String('Alice')),
                Call('printl', Array([String('Hello Alice!')]))
            ),
            Call('printl', Array([String('Hello Stranger!')])),
        ])
    ])


def if_else_if(root:Scope) -> AST:
    """
    {
        print("What's your name? ")
        name = readl()
        if name =? 'Alice' printl('Hello Alice!')
        else if name =? 'Bob' printl('Hello Bob!')
        else printl('Hello stranger!')
    }
    """
    return Block([
        Call('print', Array([String("What's your name? ")])),
        Bind('name', Call('readl')),
        Flow([
            If(
                Equal(Call('name'), String('Alice')),
                Call('printl', Array([String('Hello Alice!')]))
            ),
            If(
                Equal(Call('name'), String('Bob')),
                Call('printl', Array([String('Hello Bob!')]))
            ),
            Call('printl', Array([String('Hello Stranger!')])),
        ])
    ])


def hello_loop(root:Scope) -> AST:
    """
    {
        print("What's your name? ")
        name = readl()
        i = 0
        loop i <? 10 {
            printl('Hello {name}!')
            i = i + 1
        }
    }
    """
    return Block([
        Call('print', Array([String("What's your name? ")])),
        Bind('name', Call('readl')),
        Bind('i', Number(0)),
        Loop(
            Less(Call('i'), Number(10)),
            Block([
                Call('printl', Array([IString([String('Hello '), Call('name'), String('!')])])),
                Bind('i', Add(Call('i'), Number(1), Number)),
            ])
        )
    ])


def unpack_test(root:Scope) -> AST:
    """
    {
        s = ['Hello' ['World' '!'] 5 10]
        printl('s={s}')
        a, b, c, d = s
        printl('a={a} b={b} c={c} d={d}')
        a, ...b = s
        printl('a={a} b={b}')
        ...a, b = s
        printl('a={a} b={b}')
        a, [b, c], ...d = s
        printl('a={a} b={b} c={c} d={d}')

        //error tests
        //a, b, c, d, e = s         //error: not enough values to unpack
        //a, b = s                  //error: too many values to unpack
        //a, ...b, c, d, e, f = s   //error: too many values to unpack

        //TBD how unpack would handle `a, ...b, c, d, e = s`. Probably b would be empty?
    }
    """

    return Block([
        Bind('s', Array([String('Hello'), Array([String('World'), String('!')]), Number(5), Number(10)])),
        Call('printl', Array([IString([String('s='), Call('s')])])),
        Unpack(['a', 'b', 'c', 'd'], Call('s')),
        Call('printl', Array([IString([String('a='), Call('a'), String(' b='), Call('b'), String(' c='), Call('c'), String(' d='), Call('d')])])),
        Unpack(['a', '...b'], Call('s')),
        Call('printl', Array([IString([String('a='), Call('a'), String(' b='), Call('b')])])),
        Unpack(['...a', 'b'], Call('s')),
        Call('printl', Array([IString([String('a='), Call('a'), String(' b='), Call('b')])])),
        Unpack(['a', ['b', 'c'], '...d'], Call('s')),
        Call('printl', Array([IString([String('a='), Call('a'), String(' b='), Call('b'), String(' c='), Call('c'), String(' d='), Call('d')])])),

        # Test unpacking too few/many values
        # Unpack(['a', 'b', 'c', 'd', 'e'], Call('s')),         # error: not enough values to unpack
        # Unpack(['a', 'b'], Call('s')),                        # error: too many values to unpack
        # Unpack(['a', '...b', 'c', 'd', 'e', 'f'], Call('s')), # error: too many values to unpack
    ])


def range_iter_test(root:Scope) -> AST:
    """
    {
        r = [0,2..20]
        it = iter(r)
        printl(next(it))
        printl(next(it))
        printl(next(it))
        printl(next(it))
        printl(next(it))
        printl(next(it))
        printl(next(it))
        printl(next(it))
        printl(next(it))
        printl(next(it))
        printl(next(it)) //last iteration. should return [true, 20]
        printl(next(it)) //should return [false, undefined]
        printl(next(it))
        printl(next(it))
    }
    """
    return Block([
        Bind('r', Range(Number(0), Number(2), Number(20))),
        Bind('it', RangeIter(Call('r'))),
        Call('printl', Array([Next(Call('it'))])),
        Call('printl', Array([Next(Call('it'))])),
        Call('printl', Array([Next(Call('it'))])),
        Call('printl', Array([Next(Call('it'))])),
        Call('printl', Array([Next(Call('it'))])),
        Call('printl', Array([Next(Call('it'))])),
        Call('printl', Array([Next(Call('it'))])),
        Call('printl', Array([Next(Call('it'))])),
        Call('printl', Array([Next(Call('it'))])),
        Call('printl', Array([Next(Call('it'))])),
        Call('printl', Array([Next(Call('it'))])),
        Call('printl', Array([Next(Call('it'))])), #should print [False, None] since the iterator is exhausted
        Call('printl', Array([Next(Call('it'))])),
        Call('printl', Array([Next(Call('it'))])),
    ])


def loop_iter_manual(root:Scope) -> AST:
    """
    {
        it = iter([0,2..10])
        [cond, i] = next(it)
        loop cond {
            printl(i)
            [cond, i] = next(it)
        }
    }
    """
    return Block([
        Bind('it', RangeIter(Range(Number(0), Number(2), Number(10)))),
        Unpack(['cond', 'i'], Next(Call('it'))),

        Loop(
            Call('cond'),
            Block([
                Call('printl', Array([Call('i')])),
                Unpack(['cond', 'i'], Next(Call('it'))),
            ])
        )
    ])



def loop_in_iter(root:Scope) -> AST:
    """
    {
        loop i in [0,2..10] printl(i)
    }
    """
    return Loop(
        In('i', Range(Number(0), Number(2), Number(10))),
        Call('printl', Array([Call('i')])),
    )
   

def nested_loop(root:Scope) -> AST:
    """    
    loop i in [0,2..10]
        loop j in [0,2..10]
            printl('{i},{j}')
    """
    return Loop(
        In('i', Range(Number(0), Number(2), Number(10))),
        Loop(
            In('j', Range(Number(0), Number(2), Number(10))),
            Call('printl', Array([IString([Call('i'), String(','), Call('j')])])),
        )
    )



def block_printing(root:Scope) -> AST:
    """
    {
        loop i in [0,2..5] {
            loop j in [0,2..5] {
                loop k in [0,2..5] {
                    loop l in [0,2..5] {
                        loop m in [0,2..5] {
                            printl('{i},{j},{k},{l},{m}')
                        }
                    }
                }
            }
        }
    }
    """
    return Block([
        Loop(
            In('i', Range(Number(0), Number(2), Number(5))),
            Block([
                Loop(
                    In('j', Range(Number(0), Number(2), Number(5))),
                    Block([
                        Loop(
                            In('k', Range(Number(0), Number(2), Number(5))),
                            Block([
                                Loop(
                                    In('l', Range(Number(0), Number(2), Number(5))),
                                    Block([
                                        Loop(
                                            In('m', Range(Number(0), Number(2), Number(5))),
                                            Block([
                                                Call('printl', Array([IString([Call('i'), String(','), Call('j'), String(','), Call('k'), String(','), Call('l'), String(','), Call('m')])])),
                                            ])
                                        )
                                    ])
                                )
                            ])
                        )
                    ])
                )
            ])
        )
    ])

def rule110(root:Scope) -> AST:
    """
    progress = world:vector<bit> => {
        update:bit = 0
        loop i in 0..world.length
        {
            if i >? 0 world[i-1] = update //TODO: #notfirst handled by compiler unrolling the loop into prelude, interludes, and postlude
            update = 0b01110110 << (world[i-1..i+1] .?? 0 .<< [2 1 0])
        }
        world.push(update)
    }

    world: vector<bit> = [1]
    loop true
    {
        printl(world)
        progress(world)
    }
    """

    #rule 110
    #TODO: handle type annotations in AST
    return Block([
        Bind(
            'progress', 
            Function(
                [Arg('world', Type('vector', [Type('bit')]))], 
                Block([
                    Bind('cell_update', Number(0)),
                    # loop i in 0..world.length
                    #     if i >? 0 world[i-1] = cell_update
                    #     update = (0b01110110 << (((world[i-1] ?? 0) << 2) or ((world[i] ?? 0) << 1) or (world[i+1] ?? 0)))
                    # world.push(update)
                    #etc....
                ]), 
                root
            ),
            # Type('function', [Type('vector', [Type('bit')]), Type('vector', [Type('bit')])]),
        ),
        Let('world', Type('vector', [Type('bit')])),
        Bind(
            'world',
            Array([Number(1)]),
        ),
        # loop true
        #     printl(world)
        #     update(world)
    ])




if __name__ == '__main__':
    show = True
    show_verbose = True
    run = True

    progs = [
        hello,
        hello_func,
        anonymous_func,
        hello_name,
        if_else,
        if_else_if,
        hello_loop,
        unpack_test,
        range_iter_test,
        loop_iter_manual,
        loop_in_iter,
        nested_loop,
        block_printing,
        # rule110,
    ]

    for prog in progs:
        #set up root scope with some functions
        root = Scope.default()

        # get the program AST
        ast = prog(root)

        # display and or run the program
        if show:
            print(ast)
        if show_verbose:
            print(repr(ast))
        if run:
            ast.eval(root)

        print('----------------------------------------')18:T69d,# llvm backend or interpreter
# all backends either compile+run the code or just run it directly

from argparse import ArgumentParser, REMAINDER
from backend import backends, get_backend, python_interpreter, llvm_compiler, get_version

import pdb

def main():
    arg_parser = ArgumentParser(description='Dewy Compiler')

    # positional argument for the file to compile
    arg_parser.add_argument('file', help='.dewy file to run')

    # mutually exclusive flags for specifying the backend to use
    group = arg_parser.add_mutually_exclusive_group()
    group.add_argument('-i', action='store_true', help='(DEFAULT) Run in interpreter mode with the python backend')
    group.add_argument('-c', action='store_true', help='Run in compiler mode with the llvm backend (not implemented yet)')
    group.add_argument('--backend', type=str, help=f'Specify a backend compiler/interpreter by name to use. Backends will include: {backends} (however currently only python is available).')

    arg_parser.add_argument('-v', '--version', action='version', version=f'Dewy {get_version()}', help='Print version information and exit')
    arg_parser.add_argument('args', nargs=REMAINDER, help='Arguments after the file are passed directly to program')

    args = arg_parser.parse_args()

    # default interpreter is python. default compiler is llvm. default with no args is python.
    if args.backend: backend = get_backend(args.backend)
    elif args.c:     backend = llvm_compiler
    elif args.i:     backend = python_interpreter
    else:            backend = python_interpreter
    
    # run with the selected backend
    backend(args.file, args.args)





if __name__ == '__main__':
    main()19:T9ed6,# from __future__ import annotations

from dewy import (
    AST, PrototypeAST,
    Undefined, undefined,
    Void, void,
    Identifier,
    Callable,
    Orderable,
    Rangeable,
    Unpackable,
    Iter,
    Iterable,
    Type,
    Arg,
    Tuple,
    Function,
    Builtin,
    Let,
    Bind,
    PackStruct,
    Unpack,
    Block,
    Call,
    String, IString,
    BinOp,
    Equal, NotEqual, Less, LessEqual, Greater, GreaterEqual,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    UnaryOp,
    Neg, Inv,
    Bool,
    Flow,
    Flowable,
    If,
    Loop,
    In,
    Next,
    Number,
    Range,
    RangeIter,
    Array,
    Scope,
)
from tokenizer import ( tokenize, tprint, traverse_tokens,                       
    unary_prefix_operators,
    unary_postfix_operators,
    binary_operators,
    
    Token,

    WhiteSpace_t,

    Escape_t,

    Identifier_t,
    Block_t,
    TypeParam_t,
    RawString_t,
    String_t,
    Integer_t,
    BasedNumber_t,
    Boolean_t,
    Hashtag_t,
    DotDot_t,

    Keyword_t,

    Juxtapose_t,
    Operator_t,
    ShiftOperator_t,
    Comma_t,
)

from postok import post_process, get_next_chain, is_op, Chain, Flow_t, Range_t

from utils import based_number_to_int, bool_to_bool
from dataclasses import dataclass
from typing import Generator
from itertools import groupby, chain as iterchain
from enum import Enum, auto

try:
    from rich import print, traceback; traceback.install(show_locals=True)
except:
    print('rich unavailable for import. using built-in printing')

import pdb




#compiler pipeline steps:
# 1. tokenize
# 2. post tokenization
#    -> invert whitespace to juxtapose
#    -> bundle conditional chains into a single token
#    -> chain operator sequences into a single compount operator
#    -> desugar things, e.g. empty range `..` to `()..()`
# 3. parse tokens to AST
# 4. post parse
#    -> convert PrototypeASTs to concrete AST
#    -> (maybe) set correct scope for exprs that use (e.g. Function)
# 5. type checking, other validation, etc.
# 6. high level optimizations/transformations
# 7. generate code via a backend (e.g. llvm, c, python)
#    -> llvm: convert ast to ssa form, then generate llvm ir from ssa form



@dataclass
class qint:
    """
    quantum int for dealing with precedences that are multiple values at the same time
    qint's can only be strictly greater or strictly less than other values. Otherwise it's ambiguous
    """
    values:set[int]
    def __gt__(self, other:'int|qint') -> bool:
        if isinstance(other, int):
            return all(v > other for v in self.values)
        return all(v > other for v in self.values)
    def __lt__(self, other:'int|qint') -> bool:
        if isinstance(other, int):
            return all(v < other for v in self.values)
        return all(v < other for v in self.values)
    def __ge__(self, other:'int|qint') -> bool: return self.__gt__(other)
    def __le__(self, other:'int|qint') -> bool: return self.__lt__(other)
    def __eq__(self, other:'int|qint') -> bool: return False
        


######### Operator Precedence Table #########
#TODO: class for compund operators, e.g. += -= .+= .-= not=? not>? etc.
#TODO: how to handle unary operators in the table? perhaps make PrefixOperator_t/PostfixOperator_t classes?
#TODO: add specification of associativity for each row
class Associativity(Enum):
    left = auto()    #left-to-right
    right = auto()   #right-to-left
    prefix = auto()
    postfix = auto()
    none = auto()
    fail = auto()

operator_groups: list[tuple[Associativity, list[Operator_t|ShiftOperator_t|Juxtapose_t|Comma_t]]] = list(reversed([
    (Associativity.prefix, [Operator_t('@')]),
    (Associativity.left, [Operator_t('.'), Juxtapose_t(None)]), #jux-call, jux-index
    (Associativity.right,  [Operator_t('^')]),
    (Associativity.left, [Juxtapose_t(None)]), #jux-multiply
    (Associativity.left, [Operator_t('*'), Operator_t('/'), Operator_t('%')]),
    (Associativity.left, [Operator_t('+'), Operator_t('-')]),
    (Associativity.left, [ShiftOperator_t('<<'), ShiftOperator_t('>>'), ShiftOperator_t('<<<'), ShiftOperator_t('>>>'), ShiftOperator_t('<<!'), ShiftOperator_t('!>>')]),
    (Associativity.none, [Operator_t('in')]), 
    (Associativity.left, [Operator_t('=?'), Operator_t('>?'), Operator_t('<?'), Operator_t('>=?'), Operator_t('<=?')]),
    (Associativity.left, [Operator_t('and'), Operator_t('nand'), Operator_t('&')]),
    (Associativity.left, [Operator_t('xor'), Operator_t('xnor')]),
    (Associativity.left, [Operator_t('or'), Operator_t('nor'), Operator_t('|')]),
    (Associativity.none,  [Comma_t(None)]),
    # (Associativity.none/fail?, [RangeJuxtapose_t(None)]), #jux-range
    (Associativity.right,  [Operator_t('=>')]), # () => () => () => 42
    (Associativity.fail,  [Operator_t('=')]),
    (Associativity.none,  [Operator_t('else')]),
]))
precedence_table: dict[Operator_t|ShiftOperator_t|Juxtapose_t|Comma_t, int|qint] = {}
associativity_table: dict[int, Associativity] = {}
for i, (assoc, group) in enumerate(operator_groups):

    #mark precedence level i as the specified associativity
    associativity_table[i] = assoc

    #insert all ops in the row into the precedence table at precedence level i
    for op in group:
        if op not in precedence_table:
            precedence_table[op] = i
            continue
        
        val = precedence_table[op]
        if isinstance(val, int):
            precedence_table[op] = qint({val, i})
        else:
            precedence_table[op] = qint(val.values|{i})


def operator_precedence(op:Operator_t|ShiftOperator_t|Juxtapose_t|Comma_t) -> int | qint:
    """
    precedence:
    [HIGHEST]
    (prefix) @
    . <jux call> <jux index access>
    (prefix) not ...
    (postfix) ? `
    ^                                   //right-associative
    <jux mul>
    / * %
    + -
    << >> <<< >>> <<! !>>
    in
    =? >? <? >=? <=? not=? <=> is? isnt? @?
    and nand &
    xor xnor                            //following C's precedence: and > xor > or
    or nor |
    ,                                   //tuple maker
    <jux range>                         //e.g. [first,second..last]
    =>
    = .= <op>= .<op>=  (e.g. += .+=)    //right-associative (but technically causes a type error since assignments can't be chained)
    else
    (postfix) ;
    <seq> (i.e. space)
    [LOWEST]

    TODO:
    - add operators: as transmute |> <| -> <-> <- :

    [Notes]
    .. for ranges is not an operator, it is an expression. it uses juxtapose to bind to left/right arguments (or empty), and type-checks left and right
    if-else-loop chain expr is more like a single unit, so it doesn't really have a precedence. but they act like they have the lowest precedence since the expressions they capture will be full chains only broken by space/seq
    the unary versions of + - * / % have the same precedence as their binary versions
    """

    #TODO: handling compound operators like +=, .+=, etc.
    # if isinstance(op, CompoundOperator_t):
    #     op = op.base

    try:
        return precedence_table[op]
    except:
        raise ValueError(f"ERROR: expected operator, got {op=}") from None

def operator_associativity(op:Operator_t|ShiftOperator_t|Juxtapose_t|Comma_t|int) -> Associativity:
    if not isinstance(op, int):
        i = operator_precedence(op)
        assert isinstance(i, int), f'Cannot determine associativity of operator ({op}) with multiple precedence levels ({i})'
    else:
        i = op
    try:
        return associativity_table[i]
    except:
        raise ValueError(f"Error: failed to determine associativity for operator {op}") from None

opname_map = {
    '@': 'reference',
    '.': 'access',
    '^': 'power',
    '*': 'multiply',
    '/': 'divide',
    '%': 'modulus',
    '+': 'add',
    '-': 'subtract',
    '<<': 'left shift',
    '>>': 'right shift',
    '>>>': 'rotate left no carry',
    '<<<': 'rotate right no carry',
    '<<!': 'rotate left with carry',
    '!>>': 'rotate right with carry',
    '>?': 'greater than',
    '<?': 'less than',
    '>=?': 'greater than or equal',
    '<=?': 'less than or equal',
    '=?': 'equal',
    'and': 'and',
    'nand': 'nand',
    '&': 'and',
    'xor': 'xor',
    'xnor': 'xnor',
    'or': 'or',
    'nor': 'nor',
    '|': 'or',
    '=>': 'function arrow',
    '=': 'bind',
    'else': 'flow alternate',
    ';': 'semicolon',
    'in': 'in',
    'as': 'as',
    'transmute': 'transmute',
    '|>': 'pipe',
    '<|': 'reverse pipe',
    '->': 'right pointer',
    '<->': 'bidir pointer',
    '<-': 'left pointer',
    ':': 'type annotation',

    Comma_t(None): 'comma',
    Juxtapose_t(None): 'unknown juxtapose',
}
def get_precedence_table_markdown() -> str:
    """return a string that is the markdown table for the docs containing all the operators"""
    header = '| Precedence | Operator | Name | Associativity |\n| --- | --- | --- | --- |'
    
    def get_ops_str(ops:list[Operator_t|ShiftOperator_t|Juxtapose_t|Comma_t]) -> str:
        return '<br>'.join(f'`{op.op if isinstance(op, (Operator_t, ShiftOperator_t)) else op.__class__.__name__[:-2].lower()}`' for op in ops)
    def get_opnames_str(ops:list[Operator_t|ShiftOperator_t|Juxtapose_t|Comma_t]) -> str:
        return '<br>'.join(f'{opname_map.get(op.op, None) if isinstance(op, (Operator_t, ShiftOperator_t)) else op.__class__.__name__[:-2].lower()}' for op in ops)
    def get_row_str(row:tuple[Associativity, list[Operator_t|ShiftOperator_t|Juxtapose_t|Comma_t]]) -> str:
        assoc, group = row
        return f'{get_ops_str(group)} | {get_opnames_str(group)} | {assoc.name}'

    rows = [
        f'| {i} | {get_row_str(row)} |'
        for i, row in reversed([*enumerate(operator_groups)])
    ]

    return header + '\n' + '\n'.join(rows)


#TODO: this isn't very well integrated into the type system...
# operator_result_type_table = {
#     (Number.type, Add, Number.type): Number,
#     (Number.type, Sub, Number.type): Number,
#     (Number.type, Mul, Number.type): Number,
#     (Number.type, Div, Number.type): Number,
#     (Inv, Number.type): Number,
#     (Neg, Number.type): Number,
#     #TODO: add anything else to the matrix
# }


# @cache
def split_by_lowest_precedence(tokens: Chain[Token], scope:Scope) -> tuple[Chain[Token], Token, Chain[Token]]:
    """
    return the integer index/indices of the lowest precedence operator(s) in the given list of tokens
    """
    assert isinstance(tokens, Chain), f"ERROR: `split_by_lowset_precedence()` may only be called on explicitly known Chain[Token], got {type(tokens)}"

    #collect all operators and their indices in the list of tokens
    idxs, ops = zip(*[(i,token) for i,token in enumerate(tokens) if is_op(token)])

    if len(ops) == 0:
        pdb.set_trace()
        #TODO: how to handle this case?
        return Chain(), None, Chain()
        raise ValueError()
    if len(ops) == 1:
        i, = idxs
        op, = ops
        return Chain(tokens[:i]), op, Chain(tokens[i+1:])
    
    # when more than one op present, find the lowest precedence one
    ranks = [operator_precedence(op) for op in ops]
    min_rank = min(ranks)

    # verify that the min is strictly less than or equal to all other ranks
    if not all(min_rank <= r for r in ranks):
        #TODO: probably enumerate out all permutations of the ambiguous operators and return all of them as a list of lists of indices
        #make use of scope/chain typeof to disambiguate if need be
        raise NotImplementedError(f"TODO: ambiguous precedence for {ops=} with {ranks=}, in token stream {tokens=}")


    # find operators with precedence equal to the current minimum
    op_idxs = [i for i,r in zip(idxs, ranks) if r == min_rank]

    if len(op_idxs) == 1:
        i, = op_idxs
        return Chain(tokens[:i]), tokens[i], Chain(tokens[i+1:])

    # handling when multiple ops have the same precedence, select based on associativity rules
    if isinstance(min_rank, qint):
        assocs = {operator_associativity(i) for i in min_rank.values}
        if len(assocs) > 1:
            raise NotImplementedError(f'TODO: need to type check to deal with multiple/ambiguous operator associativities: {assocs}')
        assoc, = assocs
    else:
        assoc = operator_associativity(min_rank)
    
    match assoc:
        case Associativity.left: i = op_idxs[-1]
        case Associativity.right: i = op_idxs[0]
        case Associativity.prefix: i = op_idxs[0]
        case Associativity.postfix: i = op_idxs[-1]
        case Associativity.none: i = op_idxs[-1] #default to left. handled later in parsing
        case Associativity.fail: raise ValueError(f'Cannot handle multiple given operators in chain {tokens}, as lowest precedence operator is marked as un-associable.')
    
    return Chain(tokens[:i]), tokens[i], Chain(tokens[i+1:])

# @cache
def typeof(chain: list[Token], scope:Scope) -> Type|None: #this should be the same type system`` used in the interpreter!
    # recursively determine the type of the sequence of tokens
    # return none if the sequence does not form a valid type
    # follow a similar process to parsing, breaking down the expressions, etc.

    pdb.set_trace()
    chain, remainder = get_next_chain(chain)
    assert len(remainder) == 0, 'typeof may only be called on single chains of tokens'

    if len(chain) == 1:
        token, = chain
        return typeof_single(token, scope)
        
    # TODO: handle type check for more complicated chains...
    pdb.set_trace()
    left, op, right = split_by_lowest_precedence(chain, scope)
    left_t, right_t = typeof(left, scope), typeof(right, scope)

    pdb.set_trace()



def typeof_single(token:Token, scope:Scope) -> Type|None:

    pdb.set_trace()
    ...

def to_callable(ast:AST) -> str|Callable:
    """Convert the ast to either a string (identifier name) or Callable"""

    if isinstance(ast, Identifier):
        return ast.name
    if isinstance(ast, Callable):
        return ast
    
    # hacky way of dealing with blocks
    if isinstance(ast, Block):
        if len(ast.exprs) == 1:
            return to_callable(ast.exprs[0])
        else:
            pdb.set_trace()
            ...

    raise ValueError(f'Tried to prepare callable expression with unrecognized type {type(ast)}')


def to_call_args(ast:AST) -> Array:
    if isinstance(ast, Void):
        return Array([])
    
    if isinstance(ast, Tuple):
        return Array(ast.exprs)

    #TODO: some sort of check that ast is a valid single type...?
    return Array([ast])
    
    

def is_callable(ast:AST, scope:Scope) -> bool:
    #TODO: make calling typeof on an AST more robust/handle this better

    if isinstance(ast, Identifier):
        #check the type of the identifier in the scope
        if (val:=scope.get(ast.name, undefined)) is not undefined:
            return Type.is_instance(val.type, Callable.type)
        raise ValueError(f'Could not check is_callable. Identifier `{ast}` was undefined in scope.')

    # check if directly callable
    if isinstance(ast, Callable):
        return True
    
    # Non-callable types
    if isinstance(ast, (Number, String, IString)):
        return False

    # TODO: hacky way of dealing with blocks...
    if isinstance(ast, Block):
        if len(ast.exprs) == 1:
            return is_callable(ast.exprs[0], scope)
        else:
            pdb.set_trace()
            ...
    
    if isinstance(ast, PrototypeAST):
        raise NotImplementedError(f"Currently haven't handled is_callable for case of {type(ast)}")
        
    

    #other cases, e.g. function literals. should be able to use the AST type checking method at this point
    pdb.set_trace()
    ...

# don't need is_multipliable, because that is just the default case.
# also don't have to worry about the user making custom callable types not being parsed correctly,
#    since they should inherit from Callable, making is_callable return true for them!

def parse(tokens:list[Token], scope:Scope) -> AST:

    asts = []
    while len(tokens) > 0:
        chain, tokens = get_next_chain(tokens)

        if len(chain) == 1:
            asts.append(parse_single(chain[0], scope))
        else:
            asts.append(parse_chain(chain, scope))
    
    if len(asts) == 0:
        #literally nothing was parsed
        return void
    
    if len(asts) == 1:
        ast, = asts
        return ast
    
    block = Block(asts)
    block.newscope = True
    return block


def parse_single(token:Token, scope:Scope) -> AST:
    """Parse a single token into an AST"""
    match token:
        case Identifier_t():    return Identifier(token.src)
        case Integer_t():       return Number(int(token.src))
        case Boolean_t():       return Bool(bool_to_bool(token.src))
        case BasedNumber_t():   return Number(based_number_to_int(token.src))
        case RawString_t():     return String(token.to_str())
        case String_t():        return parse_string(token, scope)
        case Block_t():         return parse_block(token, scope)
        case Flow_t():          return parse_flow(token, scope)
        case Range_t():         return parse_range(token, scope)
        
        case _:
            #TODO handle other types...
            pdb.set_trace()
            ...
    
    pdb.set_trace()
    raise NotImplementedError()
    ...


def parse_chain(chain:Chain[Token], scope:Scope) -> AST:
    assert isinstance(chain, Chain), f"ERROR: parse chain may only be called on explicitly known Chain[Token], got {type(chain)}"
    
    if len(chain) == 0: return void
    if len(chain) == 1: return parse_single(chain[0], scope)
    
    left, op, right = split_by_lowest_precedence(chain, scope)
    left, right = parse(left, scope), parse(right, scope)

    assert not isinstance(left, Void) or not isinstance(right, Void), f"Internal Error: both left and right returned Void during parse chain, implying chain was invalid: {chain}"

    # 3 cases are prefix expr, postfix expr, or binary expr
    if isinstance(left, Void): return build_unary_prefix_expr(op, right, scope)
    if isinstance(right, Void): return build_unary_postfix_expr(left, op, scope)
    return build_bin_expr(left, op, right, scope)


def build_bin_expr(left:AST, op:Token, right:AST, scope:Scope) -> AST:
    """create a unary prefix expression AST from the op and right AST"""

    match op:
        case Juxtapose_t():
            if is_callable(left, scope):
                fn = to_callable(left)
                args = to_call_args(right)
                return Call(fn, args)
            else:
                # assume left/right are multipliable
                return Mul(left, right, None)

        case Operator_t(op='='):
            if isinstance(left, Identifier):
                return Bind(left.name, right)
            else:
                #TODO: handle other cases, e.g. a.b, a[b], etc.
                #      probably make bind take str|AST as the left-hand-side target
                #      return Bind(left, right)
                pdb.set_trace()
                ...

        case Operator_t(op='=>'):
            if isinstance(left, Void):
                return Function([], right, scope) #TODO: scope needs to be set. not sure if should set here or on a post processing pass...
            elif isinstance(left, Identifier):
                pdb.set_trace()
                ...
            elif isinstance(left, Block):
                pdb.set_trace()
                ...
            else:
                raise ValueError(f'Unrecognized left-hand side for function literal: {left=}, {right=}')

        # a bunch of simple cases:
        # case ShiftOperator_t(op='<<'):  return LeftShift(left, right)
        # case ShiftOperator_t(op='>>'):  return RightShift(left, right)
        # case ShiftOperator_t(op='<<<'): return LeftRotate(left, right)
        # case ShiftOperator_t(op='>>>'): return RightRotate(left, right)
        # case ShiftOperator_t(op='<<!'): return LeftRotateCarry(left, right)
        # case ShiftOperator_t(op='!>>'): return RightRotateCarry(left, right)
        case Operator_t(op='+'): return Add(left, right, None)
        case Operator_t(op='-'): return Sub(left, right, None)
        case Operator_t(op='*'): return Mul(left, right, None)
        case Operator_t(op='/'): return Div(left, right, None)
        case Operator_t(op='%'): return Mod(left, right, None)
        case Operator_t(op='^'): return Pow(left, right, None)

        #comparison operators
        case Operator_t(op='=?'): return Equal(left, right)
        case Operator_t(op='>?'): return Greater(left, right)
        case Operator_t(op='<?'): return Less(left, right)
        case Operator_t(op='>=?'): return GreaterEqual(left, right)
        case Operator_t(op='<=?'): return LessEqual(left, right)
        # case Operator_t(op='in?'): return MemberIn(left, right)
        # case Operator_t(op='is?'): return Is(left, right)
        # case Operator_t(op='isnt?'): return Isnt(left, right)
        # case Operator_t(op='<=>'): return ThreewayCompare(left, right)

        # Logical Operators. TODO: outtype=Bool is not flexible enough...
        case Operator_t(op='and'):  return And(left, right, outtype=Bool)
        case Operator_t(op='or'):   return Or(left, right, outtype=Bool)
        case Operator_t(op='nand'): return Nand(left, right, outtype=Bool)
        case Operator_t(op='nor'):  return Nor(left, right, outtype=Bool)
        case Operator_t(op='xor'):  return Xor(left, right, outtype=Bool)
        case Operator_t(op='xnor'): return Xnor(left, right, outtype=Bool)

        # Misc Operators
        case Comma_t(): 
            #TODO: combine left or right tuples into a single tuple
            if isinstance(left, Tuple) and isinstance(right, Tuple):
                return Tuple([*left.exprs, *right.exprs])
            elif isinstance(left, Tuple):
                return Tuple([*left.exprs, right])
            elif isinstance(right, Tuple):
                return Tuple([left, *right.exprs])
            else:
                return Tuple([left, right])
        
        case Operator_t(op='else'):
            if isinstance(left, Flow) and isinstance(right, Flow):
                #merge left+right as single flow
                assert left.default is None, f'cannot merge left flow with default case. Got {left=}, {right=}'
                default = [right.default] if right.default else []
                return Flow([*left.branches, *right.branches, *default])
            elif isinstance(left, Flow):
                #append right to left
                assert left.default is None, f'cannot merge left flow with default case. Got {left=}, {right=}'
                return Flow([*left.branches, right])
            elif isinstance(right, Flow):
                #prepend left to right
                default = [right.default] if right.default else []
                return Flow([left, *right.branches, *default])
            else:
                #create a new flow out of the left and right
                return Flow([left, right])
        
        case Operator_t(op='in'):
            if isinstance(left, Identifier):
                return In(left.name, right)
            
            pdb.set_trace()
            #TODO: handle unpacking case where left is a PackStruct
            #TDB if post-tokenizer or parser handles. probably parser, which would build a PackStruct AST node 
            # elif isinstance(left, PackStruct):
            #     return In(left, right)
                
            raise NotImplementedError(f"Parsing of operator 'in' operator for non-identifiers on left, has not been implemented yet. Got {left=}, {right=}")
        
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'Parsing of operator {op} has not been implemented yet')


def build_unary_prefix_expr(op:Token, right:AST, scope:Scope) -> AST:
    """create a unary prefix expression AST from the op and right AST"""
    match op:
        # normal prefix operators
        case Operator_t(op='+'): return right
        case Operator_t(op='-'): return Neg(right, None)
        case Operator_t(op='*'): return right
        case Operator_t(op='/'): return Inv(right, None)
        case Operator_t(op='not'): raise NotImplementedError(f"TODO: prefix op: {op=}")
        case Operator_t(op='@'):   raise NotImplementedError(f"TODO: prefix op: {op=}")
        case Operator_t(op='...'): raise NotImplementedError(f"TODO: prefix op: {op=}")

        # binary operators that appear to be unary because the left can be void
        case Operator_t(op='=>'): return Function([], right, scope) # => called as unary prefix op means left was ()/void

        case _:
            raise ValueError(f"INTERNAL ERROR: {op=} is not a known unary prefix operator")


def build_unary_postfix_expr(left:AST, op:Token, scope:Scope) -> AST:
    """create a unary postfix expression AST from the left AST and op token"""
    match op:
        # normal postfix operators
        case Operator_t(op='!'): raise NotImplementedError(f"TODO: postfix op: {op=}") #return Fact(left)

        # binary operators that appear to be unary because the right can be void
        case Juxtapose_t(): return Call(to_callable(left), Array([])) # anything juxtaposed with void is treated as a zero-arg call()

        case _:
            raise NotImplementedError(f"TODO: {op=}")

def parse_string(token:String_t, scope:Scope) -> String | IString:
    """Convert a string token to an AST"""

    if len(token.body) == 1 and isinstance(token.body[0], str):
        return String(token.body[0])

    # else handle interpolation strings
    parts = []
    for chunk in token.body:
        if isinstance(chunk, str):
            parts.append(chunk)
        elif isinstance(chunk, Escape_t):
            parts.append(chunk.to_str())
        else:
            #put any interpolation expressions in a new scope
            ast = parse(chunk.body, scope)
            if isinstance(ast, Block):
                ast.newscope = True
            else:
                ast = Block([ast], newscope=True)
            parts.append(ast)

    # combine any adjacent Strings into a single string (e.g. if there were escapes)
    parts = iterchain(*((''.join(g),) if issubclass(t, str) else (*g,) for t, g in groupby(parts, type)))
    # convert any free strings to ASTs
    parts = [p if not isinstance(p, str) else String(p) for p in parts]

    return IString(parts)


def parse_block(block:Block_t, scope:Scope) -> AST:
    """Convert a block token to an AST"""

    # if new scope block, nest the current scope
    newscope =  block.left == '{' and block.right == '}'
    if newscope:
        scope = Scope(scope)
    
    #parse the inside of the block
    inner = parse(block.body, scope)

    delims = block.left + block.right
    match delims, inner:
        #types returned as is
        case '()'|'{}', String() | IString() | Call() | Function() | Identifier() | Number() | BinOp() | UnaryOp(): #TODO: more types
            return Block([inner], newscope=delims=='{}')
        case '()'|'{}', Void():
            return inner
        case '()'|'{}', Block():
            inner.newscope = delims == '{}'
            return inner
        case '()'|'{}', Flow() | Flowable():
            return Block([inner], newscope=delims=='{}')
        case '()'|'[]'|'(]'|'[)', Range():
            inner.include_first = block.left == '['
            inner.include_last = block.right == ']'
            inner.was_wrapped = True #TODO: look into removing this attribute (needs post-tokenization process to be able to separate the range (and any first,second..last expressions) from surrounding tokens)
            return inner
        
        # create class RawRange(PrototypeAST) for representing the inner part of a range without the block delimiters
        # case '()'|'(]'|'[)'|'[]', RawRange(): ...

        # array
        # case '[]', Block() | Tuple(): return Array(inner.exprs)

        case _:
            pdb.set_trace()
            raise NotImplementedError(f'block parse not implemented for {block.left+block.right}, {type(inner)}')


def parse_flow(flow:Flow_t, scope:Scope) -> If|Loop:

    # special case for closing else clause in a flow chain. Treat as `<if> <true> <clause>`
    if flow.keyword is None:
        return If(Bool(True), parse_chain(flow.clause, scope))

    cond = parse_chain(flow.condition, scope) 
    clause = parse_chain(flow.clause, scope)
    
    match flow.keyword:
        case Keyword_t(src='if'): return If(cond, clause)
        case Keyword_t(src='loop'): return Loop(cond, clause)
        case _:
            pdb.set_trace()
            ...
            raise NotImplementedError('TODO: other flow keywords, namely lazy')
    pdb.set_trace()
    ...


def parse_range(rng:Range_t, scope:Scope) -> Range:
    """
    Convert a range token to an AST
    
    Cases are:
        [first..]               // first to inf
        [first,second..]        // step size is second-first
        [first..last]           // first to last
        [first,second..last]    // first to last, step size is second-first
        //[first..2ndlast,last] // this is explicitly NOT ALLOWED, as it is covered by the previous case, and can have unintuitive behavior
        [..2ndlast,last]        // -inf to last, step size is last-penultimate
        [..last]                // -inf to last
        [..]                    // -inf to inf
    """
    left = parse(rng.left, scope) if rng.left is not None else None
    right = parse(rng.right, scope) if rng.right is not None else None


    match (left, right):
        case (None, None):                                                      return Range(undefined, undefined, undefined)
        case (None, AST() as last):                                             return Range(undefined, undefined, last)
        case (AST() as first, None):                                            return Range(first, undefined, undefined)
        case (Tuple(exprs=[AST() as first, AST() as second]), None):            return Range(first, second, undefined)
        case (Tuple(exprs=[AST() as first, AST() as second]), AST() as last):   return Range(first, second, last)
        # case (None, Tuple(exprs=[AST() as secondlast, AST() as last])):       return Range(undefined, secondlast=secondlast, last) #TODO: potentially modify Range() to also take a secondlast parameter
        case (AST() as first, AST() as last):                                   return Range(first, undefined, last)

    pdb.set_trace()
    raise ValueError(f'Unrecognized range case: {rng=}, {left=}, {right=}')

def top_level_parse(tokens:list[Token], scope:Scope=None) -> AST:
    """
    Parse the sequence of tokens into an AST

    Args:
        tokens (list[Token]): tokens to be parsed
        scope (Scope, optional): The scope to use when determining the type of identified values. Defaults to Scope.default()
    """

    #ensure there is a valid scope to do the parse with
    if scope is None:
        scope = Scope.default()
    
    # kick off the parser
    ast = parse(tokens, scope.copy())

    # post processing on the parsed AST 
    express_identifiers(ast)
    tuples_to_arrays(ast)
    ensure_no_prototypes(ast)
    ensure_no_unwrapped_ranges(ast)
    set_ast_scopes(ast, scope)

    return ast
    


def express_identifiers(root:AST) -> None:
    """
    Convert (in-place) any free floating Identifier AST nodes (PrototypeAST) to Call nodes
    """
    for ast in full_traverse_ast(root):
        if isinstance(ast, Identifier):
            #in place convert Identifier to Call
            call = Call(ast.name)
            ast.__dict__ = call.__dict__
            ast.__class__ = Call

def tuples_to_arrays(root:AST)  -> None:
    """Convert (in-place) any Tuple nodes (PrototypeAST) to Array nodes"""
    #TODO: should be able to specify that the array is const...
    for ast in full_traverse_ast(root):
        if isinstance(ast, Tuple):
            #in place convert Tuple to Array
            arr = Array(ast.exprs)
            ast.__dict__ = arr.__dict__
            ast.__class__ = Array

#TODO: if we make a third conversion function, make a meta conversion function that takes a lambda 
# for how to make the new instance from the old one, and then does the in-place conversion
# def in_place_type_conversion(root:AST, target:PyType[AST], converter: Function[[AST], AST]) -> None
#     for ast in full_traverse_ast(root):
#         if isinstance(ast, target):
#             new = converter(ast)
#             ast.__dict__ = new.__dict__
#             ast.__class__ = target

def ensure_no_prototypes(root:AST) -> None:
    """
    Raises an exception if there are any PrototypeAST nodes in the AST
    """
    for ast in full_traverse_ast(root):
        if isinstance(ast, PrototypeAST):
            raise ValueError(f'May not have any PrototypeASTs in a final AST. Found {ast} of type ({type(ast)})')


def ensure_no_unwrapped_ranges(root:AST) -> None:
    """
    Raises an exception, if any Range object has was_wrapped=False
    """
    for ast in full_traverse_ast(root):
        if isinstance(ast, Range):
            if not ast.was_wrapped:
                raise ValueError(f'Range AST node {ast} was not wrapped in brackets/parenthesis. Options are [], [), (], ().\nPotentially can relax this is the post-tokenizer process is able to separate the range from the surrounding tokens.')


def set_ast_scopes(root:AST, scope:Scope) -> None:
    #TODO: hacky, just setting function scopes to root scope!
    #      need to handle setting scope to where fn defined.
    #      probably have traverse keep track of scope for given node!
    for ast in full_traverse_ast(root):
        if isinstance(ast, Function):
            ast.scope = scope

#TODO: consider adding __iter__ to AST classes instead of manually specifying them here
#      though potentially good as is, as it forces you to implement it. 
#      if we use __iter__ we'd have to provide a default that raises, and then implement it on EVERY AST class            
def full_traverse_ast(root:AST) -> Generator[AST, None, None]:
    """
    Generator to recursively walk all nodes in the given AST.
    
    While traversing, the user can skip visiting the current node's children by calling `.send(True)`.
    Children nodes are visited after the current node (preorder traversal), so you may modify the children
      during iteration, and the iterator ought to handle it fine.

    e.g.
    ```python
    
    for ast in (gen := full_traverse_ast(root)):
        #do something with current ast node
        #...

        #maybe skip any children of this node
        if should_skip:
            gen.send(True)
    ```

    Do not call `.send()` twice in a row without calling `next()` in between. This will cause unexpected behavior.

    Args:
        root: the ast node to start traversing from

    Yields:
        ast: the current ast node being looked at (and recursively all children nodes)
    """
    
    skip = yield root
    if skip is not None: assert (yield) is None, ".send() may only be called once per iteration"
    if skip is not None: return
    
    match root:
        case Block(exprs=list(exprs)) | Tuple(exprs=list(exprs)):
            for expr in exprs:
                yield from full_traverse_ast(expr)

        case Array(vals=list(vals)):
            for val in vals:
                yield from full_traverse_ast(val)

        case Call():
            #handle expr being called
            if isinstance(root.expr, AST):
                yield from full_traverse_ast(root.expr)
            # else str identifier, which doesn't need to be visited

            #handle any arguments
            if root.args is not None:
                for arg in root.args.vals:
                    yield from full_traverse_ast(arg)
        
        case IString(parts=list(parts)):
            for ast in parts:
                yield from full_traverse_ast(ast)

        case Bind():
            yield from full_traverse_ast(root.value)
        
        case Function():
            for arg in root.args:
                yield from full_traverse_ast(arg.type)
                if arg.val is not None:
                    yield from full_traverse_ast(arg.val)
            yield from full_traverse_ast(root.body)

        case BinOp():
            yield from full_traverse_ast(root.left)
            yield from full_traverse_ast(root.right)

        case UnaryOp():
            yield from full_traverse_ast(root.child)


        case Flow():
            for expr in root.branches:
                yield from full_traverse_ast(expr)
            if root.default is not None:
                yield from full_traverse_ast(root.default)

        case If():
            yield from full_traverse_ast(root.cond)
            yield from full_traverse_ast(root.body)

        case Loop():
            yield from full_traverse_ast(root.cond)
            yield from full_traverse_ast(root.body)

        case In():
            #TODO: probably going to change pack struct to be a proper AST, which can then have contents yielded from...
            # if isinstance(root.name, PackStruct):
            #     yield from full_traverse_ast(root.name)
            yield from full_traverse_ast(root.iterable)

        case Range():
            if root.first is not None:
                yield from full_traverse_ast(root.first)
            if root.second is not None:
                yield from full_traverse_ast(root.second)
            if root.last is not None:
                yield from full_traverse_ast(root.last)

        # do nothing cases
        case String(): ...
        case Identifier(): ...
        case Number(): ...
        case Bool(): ...
        case Void(): ...
        case Undefined(): ...
        
        case _:
            #TODO: unhandled ast type
            pdb.set_trace()
            raise NotImplementedError(f'traversal not implemented for ast type {type(root)}')






def test_file(path:str):
    """Run a given file"""

    with open(path) as f:
        src = f.read()
    
    tokens = tokenize(src)
    post_process(tokens)

    root = Scope.default()
    ast = top_level_parse(tokens, root)
    res = ast.eval(root)
    if res: print(res)




#TODO: broken. probably set up scope with some default values
def test_many_lines():
    """
    Parse each line of syntax3.dewy one at a time for testing
    """
    #load the syntax3 file and split the lines
    with open('../../examples/syntax3.dewyl') as f:
        lines = f.read().splitlines()


    #set up a scope with declarations for all of the variables used in the example file    
    root = Scope.default()
    root.let('x', Number.type)
    root.let('y', Number.type)
    root.let('z', Number.type)

    for line in lines:
        tokens = tokenize(line)
        post_process(tokens)

        # skip empty lines
        if len(tokens) == 0:
            continue

        #print the line, and run it
        print('-'*80)
        print(tokens)

        ast = top_level_parse(tokens)
        print(ast)

        #TODO: maybe later we can run the file. potentially declare all the values used at the top?
        # res = ast.eval(root)
        # if res: print(res)


def test_hello():
    # line = "'Hello, World!'"
    line = r"""
print'What is your name? '
name = readl
printl'Hello {name}'
a = 4(5)
b = -5
c = /4
d = 1,2,3,4,5
printl'a={a}, b={b}, c={c} d={d}'
"""

    tokens = tokenize(line)
    post_process(tokens)

    #DEBUG
    # tokens = [Identifier_t('printl'), Juxtapose_t(''), Identifier_t('readl')]

    ast = top_level_parse(tokens)
    root = Scope.default()
    ast.eval(root)


def test_example_progs():
    from dewy import hello, hello_func, anonymous_func, hello_name, if_else, if_else_if, hello_loop, unpack_test, range_iter_test, loop_iter_manual, loop_in_iter, nested_loop, block_printing

    funcs = [hello, hello_func, anonymous_func, hello_name, if_else, if_else_if, hello_loop, unpack_test, range_iter_test, loop_iter_manual, loop_in_iter, nested_loop, block_printing]

    for func in funcs:
        src = func.__doc__
        print(f'Parsing source:\n{src}\n')
        tokens = tokenize(src)
        post_process(tokens)

        ast = top_level_parse(tokens)
        root = Scope.default()
        ast.eval(root)


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        test_file(sys.argv[1])
    else:
        # test_hello()
        # test_example_progs()
        test_many_lines()

    # print("Usage: `python parser.py [path/to/file.dewy>]`")






1a:T499b,from tokenizer import ( tokenize, tprint, full_traverse_tokens,
    unary_prefix_operators,
    unary_postfix_operators,
    binary_operators,
    opchain_starters,

    Token,

    WhiteSpace_t,

    Escape_t,

    Identifier_t,
    Block_t,
    TypeParam_t,
    RawString_t,
    String_t,
    Integer_t,
    Boolean_t,
    BasedNumber_t,
    Hashtag_t,
    DotDot_t,

    Keyword_t,

    Juxtapose_t,
    Operator_t,
    ShiftOperator_t,
    Comma_t,
)

from typing import Generator, overload
from abc import ABC


import pdb


# There is no chain class
# A chain is just a list of tokens that is directly parsable as an expression without any other syntax
# all other syntax is wrapped up into compound tokens
# it should literally just be a sequence of atoms and operators
    

#TODO: replace with 3.12 syntax when released: class Chain[T](list[T]): ...
from typing import TypeVar
T = TypeVar('T')
class Chain(list[T]):
    """class for explicitly annotating that a token list is a single chain"""


############### NEW TOKENS CREATED BY POST-TOKENIZATION PROCESS ###############

class Flow_t(Token):
    @overload
    def __init__(self, keyword:None, condition:None, clause:Chain[Token]): ... # closing else
    @overload
    def __init__(self, keyword:Keyword_t, condition:Chain[Token], clause:Chain[Token]): ... # if, loop, lazy

    def __init__(self, keyword:Keyword_t|None, condition:Chain[Token]|None, clause:Chain[Token]):
        if keyword is None and condition is not None:
            raise ValueError("closing else should have no condition. `keyword` and `condition` should both be None")
        self.keyword = keyword
        self.condition = condition
        self.clause = clause

    def __repr__(self) -> str:
        return f"<Flow_t: {self.keyword}: {self.condition} {self.clause}>"

    def __iter__(self) -> Generator[Token, None, None]:
        if self.condition is not None: 
            yield self.condition
        yield self.clause


# class Do_t(Token):...
# class Return_t(Token):...
# class Express_t(Token):...
# class Declare_t(Token):...


# class RangeJux_t(Token):
#     def __init__(self, _): ...
#     def __repr__(self) -> str:
#         return "<RangeJux_t>"
#     def __hash__(self) -> int:
#         return hash(RangeJux_t)
#     def __eq__(self, other) -> bool:
#         return isinstance(other, RangeJux_t)



class Range_t(Token):
    def __init__(self, left:Chain[Token]|None, right:Chain[Token]|None):
        self.left = left
        self.right = right
    def __repr__(self) -> str:
        return f"<Range_t: {self.left} .. {self.right}>"
    def __iter__(self) -> Generator[Chain[Token], None, None]:
        if self.left is not None:
            yield self.left
        if self.right is not None:
            yield self.right


# class Inf_t(Token):
#     def __init__(self, _): ...
#     def __repr__(self) -> str:
#         return "<Inf_t>"
#     def __hash__(self) -> int:
#         return hash(Inf_t)
#     def __eq__(self, other) -> bool:
#         return isinstance(other, Inf_t)


atom_tokens = (
    Identifier_t,
    Integer_t,
    Boolean_t,
    BasedNumber_t,
    RawString_t,
    String_t,
    Block_t,
    Range_t,
    TypeParam_t,
    Hashtag_t,
    DotDot_t,
    Flow_t,
)


class ShouldBreakTracker(ABC):
    def op_breaks_chain(self, token:Token) -> bool:
        raise NotImplementedError("op_breaks_chain must be implemented by subclass")
    def view(self, tokens:list[Token]) -> None:
        raise NotImplementedError("view must be implemented by subclass")

class ShouldBreakFlowTracker(ShouldBreakTracker):
    def __init__(self):
        self.flows_seen = 0
    def op_breaks_chain(self, token:Token) -> bool:
        #should only be operators
        if isinstance(token, Operator_t) and token.op == 'else':
            if self.flows_seen == 0:
                return True
            self.flows_seen -= 1
        
        return False
    
    def view(self, tokens:list[Token]) -> None:
        # view each token without any ability to do anything
        # keep track of how many flows we've seen
        for token in tokens:
            if isinstance(token, Flow_t) and token.keyword is not None:
                self.flows_seen += 1
            if isinstance(token, Operator_t) and token.op == 'else':
                raise ValueError("should not be seeing else here")
            if isinstance(token, Keyword_t) and token.src in ('if', 'loop', 'lazy'):
                raise ValueError("should not be seeing if/loop/lazy here. Everything should be bundled up into a flow")



def invert_whitespace(tokens: list[Token]) -> None:
    """
    removes all whitespace tokens, and insert juxtapose tokens between adjacent pairs (i.e. not separated by whitespace)

    Args:
        tokens (list[Token]): list of tokens to modify. This is modified in place.
    """
    
    #juxtapose singleton token so we aren't wasting memory
    jux = Juxtapose_t(None)
    
    i = 0
    while i < len(tokens):
        # delete whitespace if it comes up
        if isinstance(tokens[i], WhiteSpace_t):
            tokens.pop(i)
            continue

        # recursively handle inverting whitespace for blocks
        if isinstance(tokens[i], (Block_t, TypeParam_t)):
            invert_whitespace(tokens[i].body)
        elif isinstance(tokens[i], String_t):
            for child in tokens[i].body:
                if isinstance(child, Block_t):
                    invert_whitespace(child.body)

        # insert juxtapose if no whitespace between tokens
        if i + 1 < len(tokens) and not isinstance(tokens[i + 1], WhiteSpace_t):
            tokens.insert(i + 1, jux)
            i += 1
        i += 1

    #finally, remove juxtapose tokens next to operators that are not whitespace sensitive
    i = 1
    while i < len(tokens) - 1:
        left,middle,right = tokens[i-1:i+2]
        if isinstance(middle, Juxtapose_t) and (isinstance(left, (Operator_t, ShiftOperator_t, Comma_t)) or isinstance(right, (Operator_t, ShiftOperator_t, Comma_t))):
            tokens.pop(i)
            continue
        i += 1




def _get_next_prefixes(tokens:list[Token]) -> tuple[list[Token], list[Token]]:
    prefixes = []
    while len(tokens) > 0 and is_unary_prefix_op(tokens[0]): #isinstance(tokens[0], Operator_t) and tokens[0].op in unary_prefix_operators:
        prefixes.append(tokens.pop(0))
    return prefixes, tokens
def _get_next_postfixes(tokens:list[Token]) -> tuple[list[Token], list[Token]]:
    postfixes = []
    while len(tokens) > 0 and is_unary_postfix_op(tokens[0], exclude_semicolon=True):#isinstance(tokens[0], Operator_t) and tokens[0].op in unary_postfix_operators - {';'}:
        postfixes.append(tokens.pop(0))
    return postfixes, tokens
def _get_next_atom(tokens:list[Token]) -> tuple[Token, list[Token]]:
    if len(tokens) == 0:
        raise ValueError(f"ERROR: expected atom, got {tokens=}")

    #TODO: this is going to be unnecessary as expressions will have been bundled up into single tokens
    if isinstance(tokens[0], Keyword_t):
        return _get_next_keyword_expr(tokens)

    if isinstance(tokens[0], atom_tokens):#(Integer_t, BasedNumber_t, String_t, RawString_t, Identifier_t, Hashtag_t, Block_t, TypeParam_t, DotDot_t)):
        return tokens[0], tokens[1:]

    raise ValueError(f"ERROR: expected atom, got {tokens[0]=}")

def _get_next_chunk(tokens:list[Token]) -> tuple[list[Token], list[Token]]:
    chunk = []
    t, tokens = _get_next_prefixes(tokens)
    chunk.extend(t)

    t, tokens = _get_next_atom(tokens)
    if t is None:
        raise ValueError(f"ERROR: expected atom, got {tokens[0]=}")
    chunk.append(t)

    t, tokens = _get_next_postfixes(tokens)
    chunk.extend(t)

    return chunk, tokens

def is_unary_prefix_op(token:Token) -> bool:
    """
    Determines if a token could be a unary prefix operator.
    Note that this is not mutually exclusive with being a postfix operator or a binary operator.
    """
    return isinstance(token, Operator_t) and token.op in unary_prefix_operators

def is_unary_postfix_op(token:Token, exclude_semicolon:bool=False) -> bool:
    """
    Determines if a token could be a unary postfix operator. 
    Optionally can exclude semicolon from the set of operators.
    Note that this is not mutually exclusive with being a prefix operator or a binary operator.
    """
    if exclude_semicolon:
        return isinstance(token, Operator_t) and token.op in unary_postfix_operators - {';'}
    return isinstance(token, Operator_t) and token.op in unary_postfix_operators

def is_binop(token:Token) -> bool:
    """
    Determines if a token could be a binary operator.
    Note that this is not mutually exclusive with being a prefix operator or a postfix operator.
    """
    return isinstance(token, Operator_t) and token.op in binary_operators or isinstance(token, (ShiftOperator_t, Comma_t, Juxtapose_t))

def is_op(token:Token) -> bool:
    return is_binop(token) or is_unary_prefix_op(token) or is_unary_postfix_op(token)

def is_opchain_starter(token:Token) -> bool:
    return isinstance(token, Operator_t) and token.op in opchain_starters

def _get_next_keyword_expr(tokens:list[Token]) -> tuple[Token, list[Token]]:
    """package up the next keyword expression into a single token"""
    if len(tokens) == 0:
        raise ValueError(f"ERROR: expected keyword expression, got {tokens=}")
    t, tokens = tokens[0], tokens[1:]

    if not isinstance(t, Keyword_t):
        raise ValueError(f"ERROR: expected keyword expression, got {t=}")

    match t:
        case Keyword_t(src='if'|'loop'|'lazy'):#|'else_if'|'else_loop'|'else_lazy'):
            cond, tokens = get_next_chain(tokens)
            # pdb.set_trace()
            clause, tokens = get_next_chain(tokens, tracker=ShouldBreakFlowTracker())
            return Flow_t(t, cond, clause), tokens
        case Keyword_t(src='closing_else'):
            clause, tokens = get_next_chain(tokens, tracker=ShouldBreakFlowTracker())
            return Flow_t(None, None, clause), tokens
        case Keyword_t(src='do'):
            clause, tokens = get_next_chain(tokens)
            #assert next token is a do_keyward
            #depending on the keyward, get a condition, or condition+clause
            pdb.set_trace()
            ...
        case Keyword_t(src='return'):
            #TBD how to do this one...
            pdb.set_trace()
            ...
        case Keyword_t(src='express'):
            pdb.set_trace()
            ...
        case Keyword_t(src='let'|'const'):
            expr, tokens = get_next_chain(tokens)
            pdb.set_trace()
            ...
    
    raise NotImplementedError("TODO: handle keyword based expressions")
    # (if | loop) #chain #chain (else (if | loop) #chain #chain)* (else #chain)?
    # return #chain?
    # express #chain
    # (break | continue) #hashtag? //note the hashtag should be an entire chain if present
    # (let | const) #chain


def get_next_chain(tokens:list[Token], *, tracker:ShouldBreakTracker=None, op_blacklist:set[Token]=None) -> tuple[Chain[Token], list[Token]]:
    """
    grab the next single expression chain of tokens from the given list of tokens

    Also wraps up keyword-based expressions (if loop etc.) into a single token

    A chain is represented by the following grammar:
        #chunk = #prefix_op* #atom_expr (#postfix_op - ';')*
        #chain = #chunk (#binary_op #chunk)* ';'?

    Args:
        tokens (list[Token]): list of tokens to grab the next chain from
        tracker (ShouldBreakTracker, optional): tracker for complex analysis to determine if an operator should break the chain. Defaults to None.
        op_blacklist (set[Token], optional): simpler handler for operators that should break the chain. Defaults to None.

    Returns:
        next, rest (list[Token], list[Token]): the next chain of tokens, and the remaining tokens
    """

    if op_blacklist is None:
        op_blacklist = set()

    chain = []

    # grab the first chunk and let the tracker view it
    chunk, tokens = _get_next_chunk(tokens)
    chain.extend(chunk)
    if tracker is not None:
        tracker.view(chunk)

    while len(tokens) > 0 and is_binop(tokens[0]) and (tracker is None or not tracker.op_breaks_chain(tokens[0])) and tokens[0] not in op_blacklist:
        # get the operator, and continuing chunk, then let the tracker view it
        chain.append(tokens.pop(0))
        chunk, tokens = _get_next_chunk(tokens)
        chain.extend(chunk)
        if tracker is not None:
            tracker.view(chunk)

    # if there's a semicolon, it ends the chain
    if len(tokens) > 0 and isinstance(tokens[0], Operator_t) and tokens[0].op == ';':
        chain.append(tokens.pop(0))

    return Chain(chain), tokens




def regularize_ranges(tokens: list[Token]) -> None:
    """
    convert [<token>, <jux>, <..>] into [<token>, <range_jux>, <..>]
    convert [<..>, <jux>, <token>] into [<..>, <range_jux>, <token>]
    if .. doesn't connect to anything on the left or right, connect it to -/+ infinity
    if range expression chain is not wrapped in a block, wrap it in a block
    """
    # op_in = Operator_t('in')
    #TODO: also maybe put range in a group with []
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if isinstance(token, DotDot_t):
            
            # find the start and end bounds of the chain that contains the dotdot
            j0, j1 = 0, 0
            remainder = stream
            while i > j1:
                _, remainder = get_next_chain(remainder)
                j0 = j1
                j1 = len(stream) - len(remainder)
            
            # TODO: can we completely separate the range and it's expressions here, even if unwrapped?
            # we need the chain to not include any operators that have lower precedence than range_jux
            # # if there is an `in` to the left of the dotdot, do not include it in the expression
            # in_indices = [j for j, t in enumerate(stream[j0:i]) if op_in == t]
            # if len(in_indices) > 0:
            #     raise NotImplementedError("Currently don't support unwrapped ranges.")
            #     # j0 = in_indices[-1] + j0 + 1

            left = Chain(stream[j0:i-1]) if i-1-j0 > 0 else None
            right = Chain(stream[i+2:j1]) if j1-i-2 > 0 else None

            stream[j0:j1] = [Range_t(left, right)]
            gen.send(j0+1)


def convert_bare_else(tokens: list[Token]) -> None:
    """
    convert any instances of `else` without a flow keyword after, and convert to `else` `if` `true`
    """
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if isinstance(token, Operator_t) and token.op == 'else':
            if i+1 < len(stream) and isinstance(stream[i+1], Keyword_t) and stream[i+1].src in ('if', 'loop', 'lazy'):
                continue
            # stream[i+1:i+1] = [Keyword_t('if'), Boolean_t('true')]
            stream.insert(i+1, Keyword_t('closing_else'))

       
def bundle_conditionals(tokens: list[Token]) -> None:
    """Convert sequences of tokens that represent conditionals (if, loop, etc.) into a single expression token"""
    
    #TODO: need to check that nested conditionals as well as chained conditionals are handled properly
    #      e.g. `if a b`, `if a b else if c d else f`, `if a if b c else d`
    
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if isinstance(token, Keyword_t) and token.src in ('if', 'loop', 'lazy'):
            flow_chain, tokens = get_next_chain(stream[i:])
            stream[i] = flow_chain[0]
            stream[i+1:] = [*flow_chain[1:], *tokens]




def chain_operators(tokens: list[Token]) -> None:
    """Convert consecutive operator tokens into a single opchain token"""
    """
    A chain is represented by the following grammar:
        #chunk = #prefix_op* #atom_expr (#postfix_op - ';')*
        #chain = #chunk (#binary_op #chunk)* ';'?

        #prefix_op = '+' | '-' | '*' | '/' | 'not' | '@' | '...'
        #postfix_op = '?' | '`' | ';'
        #binary_op = '+' | '-' | '*' | '/' | '%' | '^'
          | '=?' | '>?' | '<?' | '>=?' | '<=?' | 'in?' | 'is?' | 'isnt?' | '<=>'
          | '|' | '&'
          | 'and' | 'or' | 'nand' | 'nor' | 'xor' | 'xnor' | '??'
          | '=' | ':=' | 'as' | 'in' | 'transmute'
          | '@?'
          | '|>' | '<|' | '=>'
          | '->' | '<->' | '<-'
          | '.' | ':'
    """

    # TODO: skip for now. not needed by hello world
    # also may not be necessary if we use a pratt parser. was necessary for split by lowest precedence parser

    for i, token, stream in (gen := full_traverse_tokens(tokens)):

        #TODO: this is not a correct way to detect these. need to verify that the operators are in between two #chunks
        #   this will be conservative, but for now it will let us do a hello world happy path
        if is_opchain_starter(token):
            j = 1
            while i+j < len(stream) and is_unary_prefix_op(stream[i+j]):
                j+=1
            if j > 1:
                pdb.set_trace()
                raise NotImplementedError('opchaining has not been implemented yet')


def post_process(tokens: list[Token]) -> None:
    """post process the tokens to make them ready for parsing"""

    # remove whitespace, and insert juxtapose tokens
    invert_whitespace(tokens)

    if len(tokens) == 0: return

    # find any instances of <else> without a flow keyword after, and convert to <else> <if> <true>
    convert_bare_else(tokens)

    # bundle up conditionals into single token expressions
    bundle_conditionals(tokens)

    # combine operator chains into a single operator token
    chain_operators(tokens)

    # desugar ranges
    regularize_ranges(tokens)


    # make the actual list of chains

    # based on types, replace jux with jux_mul or jux_call
    # TODO: actually this probably would need to be done during parsing, since we can't get a type for a complex/compound expression...




def test():
    with open('../../../examples/hello.dewy') as f:
        src = f.read()

    tokens = tokenize(src)

    #chainer process
    post_process(tokens)

    pdb.set_trace()
    ...
    

def test2():
    """gauntlet of multiple tests from example file"""
    with open('../../../examples/syntax3.dewyl') as f:
        lines = f.readlines()

    # filter out empty lines
    lines = [l for line in lines if (l := line.strip())]

    for line in lines:
        tokens = tokenize(line)

        #chainer process
        post_process(tokens)

        #other stuff? pass to the parser? etc.

    pdb.set_trace()
    ...


def test_hello():
    line = "printl'Hello, World!'"

    tokens = tokenize(line)
    post_process(tokens)

    pdb.set_trace()
    ...


if __name__ == '__main__':
    # test()
    # test2()
    test_hello()1b:T922f,from abc import ABC
import inspect
from typing import Callable, Type, Generator
from types import UnionType
from functools import lru_cache
from utils import CoordString

import pdb

#### DEBUG rich traceback printing ####
try:
    from rich import traceback, print
    traceback.install(show_locals=True)
except:
    print('rich unavailable for import. using built-in printing')



"""
[tasks]
- clean up eat_block
    - general cleanup
    - break out eat_ matching into smaller functions?
    - figure out tie breaking process:
        1. prefer @full_eat over @peek_eat ---> TODO: implement
        2. prefer longest matches
        3. prefer higher precedence
        4. error
- make all tokens keep the source they come from (for error reporting/keeping track of row/col of the token)
"""




class Token(ABC):
    def __repr__(self) -> str:
        """default repr for tokens is just the class name"""
        return f"<{self.__class__.__name__}>"
    def __hash__(self) -> int:
        raise NotImplementedError(f'hash is not implemented for token type {type(self)}')
    def __eq__(self, __value: object) -> bool:
        raise NotImplementedError(f'equals is not implemented for token type {type(self)}')
    def __iter__(self) -> Generator['list[Token]', None, None]:
        """
        Iter is used by full_traverse_tokens for iterating over any contained tokens.
        e.g. Block_t.body TypeParam_t.body, String_t.body (interpolation blocks only), etc.
        """
        raise NotImplementedError(f'iter is not implemented for token type {type(self)}')

class WhiteSpace_t(Token):
    def __init__(self, _): ...

class Juxtapose_t(Token):
    def __init__(self, _): ...
    def __hash__(self) -> int:
        return hash(Juxtapose_t)
    def __eq__(self, other) -> bool:
        return isinstance(other, Juxtapose_t)

class Keyword_t(Token):
    def __init__(self, src:str):
        self.src = src.lower()
    def __repr__(self) -> str:
        return f"<Keyword_t: {self.src}>"
    def __hash__(self) -> int:
        return hash((Keyword_t, self.src))
    def __eq__(self, other) -> bool:
        return isinstance(other, Keyword_t) and self.src == other.src

class Identifier_t(Token):
    def __init__(self, src:str):
        self.src = src
    def __repr__(self) -> str:
        return f"<Identifier_t: {self.src}>"
    
class Hashtag_t(Token):
    def __init__(self, src:str):
        self.src = src
    def __repr__(self) -> str:
        return f"<Hashtag_t: {self.src}>"

class Block_t(Token):
    def __init__(self, body:list[Token], left:str, right:str):
        self.body = body
        self.left = left
        self.right = right
    def __repr__(self) -> str:
        body_str = ', '.join(repr(token) for token in self.body)
        return f"<Block_t: {self.left}{body_str}{self.right}>"
    def __iter__(self) -> Generator[list[Token], None, None]:
        yield self.body
    
class TypeParam_t(Token):
    def __init__(self, body:list[Token]):
        self.body = body
    def __repr__(self) -> str:
        body_str = ', '.join(repr(token) for token in self.body)
        return f"<TypeParam_t: `<{body_str}>`>"
    def __iter__(self) -> Generator[list[Token], None, None]:
        yield self.body

class Escape_t(Token):
    escape_map = {
        '\\n': '\n', '\\r': '\r', '\\t': '\t', '\\b': '\b', '\\f': '\f', '\\v': '\v', '\\a': '\a', '\\0': '\0', '\\\\': '\\'
    }
    def __init__(self, src:str):
        self.src = src
    def __repr__(self) -> str:
        return f"<Escape_t: {self.src}>"
    def to_str(self) -> str:
        """Convert the escape sequence to the character it represents"""

        #unicode escape (may be several characters long)
        if self.src.startswith('\\U') or self.src.startswith('\\u'):
            return chr(int(self.src[2:], 16))
        assert len(self.src) == 2 and self.src[0] == '\\', "internal error. Ill-posed escape sequence"

        #known escape sequence
        if self.src in self.escape_map:
            esc = self.escape_map[self.src]
            #construct a CoordString at the position of the original escape
            return CoordString.from_existing(esc, self.src[:len(esc)].row_col_map)

        #unknown escape sequence (i.e. just replicate the character)
        return self.src[1]



class RawString_t(Token):
    def __init__(self, body:str):
        self.body = body
    def __repr__(self) -> str:
        return f"<RawString_t: {self.body}>"
    def to_str(self) -> str:
        body = self.body
        if body.startswith('r"""') or body.startswith("r'''"):
            body = body[4:-3]
        elif body.startswith('r"') or body.startswith("r'"):
            body = body[2:-1]
        else:
            raise ValueError(f"Internal Error: unrecognized delimiters on raw string: {repr(self)}")
        return body

class String_t(Token):
    def __init__(self, body:list[str|Escape_t|Block_t]):
        self.body = body
    def __repr__(self) -> str:
        return f"<String_t: {self.body}>"
    def __iter__(self) -> Generator[list[Token], None, None]:
        for token in self.body:
            if isinstance(token, Block_t):
                yield token.body
    
# class Number_t(Token, ABC):...
    
class Integer_t(Token):
    def __init__(self, src:str):
        self.src = src
    def __repr__(self) -> str:
        return f"<Integer_t: {self.src}>"
    
class BasedNumber_t(Token):
    def __init__(self, src:str):
        self.src = src
    def __repr__(self) -> str:
        return f"<BasedNumber_t: {self.src}>"

class Boolean_t(Token):
    def __init__(self, src:str):
        self.src = src
    def __repr__(self) -> str:
        return f"<Boolean_t: {self.src}>"

class Operator_t(Token):
    def __init__(self, op:str):
        self.op = op
    def __repr__(self) -> str:
        return f"<Operator_t: `{self.op}`>"
    def __hash__(self) -> int:
        return hash((Operator_t, self.op))
    def __eq__(self, other) -> bool:
        return isinstance(other, Operator_t) and self.op == other.op
    
class ShiftOperator_t(Token):
    def __init__(self, op:str):
        self.op = op
    def __repr__(self) -> str:
        return f"<ShiftOperator_t: `{self.op}`>"
    def __hash__(self) -> int:
        return hash((ShiftOperator_t, self.op))
    def __eq__(self, other) -> bool:
        return isinstance(other, ShiftOperator_t) and self.op == other.op
    

class Comma_t(Token):
    def __init__(self, src:str):
        self.src = src
    def __hash__(self) -> int:
        return hash(Comma_t)
    def __eq__(self, other) -> bool:
        return isinstance(other, Comma_t)

class DotDot_t(Token):
    def __init__(self, src:str):
        self.src = src


# #TODO: these should probably each be their own class/token, or a single class..
# these should all be case insensitive
# reserved_values = ['true', 'false', 'void', 'undefined', 'end'] 


    


# identify token classes that should take precedence over others when tokenizing
# each row is a list of token types that are confusable in their precedence order. e.g. [Keyword, Unit, Identifier] means Keyword > Unit > Identifier
# only confusable token classes need to be included in the table
precedence_table = [
    [Keyword_t, Boolean_t, Operator_t, DotDot_t, Identifier_t],
]
precedence = {cls: len(row)-i for row in precedence_table for i, cls in enumerate(row)}

# mark which tokens cannot be repeated in a list of tokens. E.g. whitespace should always be merged into a single token
idempotent_tokens = {
    WhiteSpace_t
}

# paired delimiters for blocks, ranges, groups, etc.
pair_opening_delims = '{(['
pair_closing_delims = '})]'

# which closing delimiters are allowed for each opening delimiter
valid_delim_closers = {
    '{': '}',
    '(': ')]',
    '[': '])',
    # '<': '>'
}

#list of all operators sorted from longest to shortest
unary_prefix_operators = {'+', '-', '*', '/', 'not', '@', '...'}
unary_postfix_operators = {'?', '`', ';'}
binary_operators = {
        '+', '-', '*', '/', '%', '^',
        '=?', '>?', '<?', '>=?', '<=?', 'in?', 'is?', 'isnt?', '<=>',
        '|', '&',
        'and', 'or', 'nand', 'nor', 'xor', 'xnor', '??',
        'else',
        '=', ':=', 'as', 'in', 'transmute',
        '@?',
        '|>', '<|', '=>',
        '->', '<->', '<-',
        '.', ':'
}
opchain_starters = {'+', '-', '*', '/', '%', '^'}
operators = sorted(
    [*(unary_prefix_operators | unary_postfix_operators | binary_operators)],
    key=len,
    reverse=True
)
#TODO: may need to separate |> from regular operators since it may confuse type param
shift_operators = sorted(['<<', '>>', '<<<', '>>>', '<<!', '!>>'], key=len, reverse=True)
keywords = ['loop', 'lazy', 'do', 'if', 'return', 'express', 'import', 'let', 'const']
#TODO: what about language values, e.g. void, undefined, end, units, etc.? probably define at compile time, rather than in the compiler

# note that the prefix is case insensitive, so call .lower() when matching the prefix
# numbers may have _ as a separator (if _ is not in the set of digits)
number_bases = {
    '0b': {*'01'},                      #binary
    '0t': {*'012'},                     #ternary
    '0q': {*'0123'},                    #quaternary
    '0s': {*'012345'},                  #seximal
    '0o': {*'01234567'},                #octal
    '0d': {*'0123456789'},              #decimal
    '0z': {*'0123456789xeXE'},          #dozenal
    '0x': {*'0123456789abcdefABCDEF'},  #hexadecimal 
    '0u': {*'0123456789abcdefghijklmnopqrstuvABCDEFGHIJKLMNOPQRSTUV'},              #base 32 (duotrigesimal)
    '0r': {*'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'},      #base 36 (hexatrigesimal)
    '0y': {*'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!$'},    #base 64 (tetrasexagesimal)
}


# units = #actually units should probably not be specific tokens, but recognized identifiers since the user can make their own units



def peek_eat(cls:Type[Token], whitelist:list[Type[Token]]|None=None, blacklist:list[Type[Token]]|None=None):
    """
    Decorator for functions that eat tokens, but only return how many characters would make up the token. 
    Makes function return include constructor for token class that it tries to eat, in tupled with return.

    whitelist and blacklist can be used to specify parent token contexts that may or may not consume this type as a child
    """
    assert issubclass(cls, Token), f"cls must be a subclass of Token, but got {cls}"
    if whitelist is not None and blacklist is not None:
        raise ValueError("cannot specify both whitelist and blacklist")
    def decorator(eat_func:Callable[[str], int|None]):
        def wrapper(src:str) -> tuple[int|None, Type[Token]]:
            return eat_func(src), cls
        wrapper._is_peek_eat_decorator = True  # make it easy to check if a function has this decorator
        wrapper._eat_func = eat_func
        wrapper._token_cls = cls
        wrapper._whitelist = whitelist
        wrapper._blacklist = blacklist
        return wrapper
    return decorator

#TODO: full eat probably won't need to take the class as an argument, since the function will know how to construct the token itself
def full_eat(whitelist:list[Type[Token]]|None=None, blacklist:list[Type[Token]]|None=None):
    def decorator(eat_func:Callable[[str], tuple[int, Token] | None]):
        """
        Decorator for functions that eat tokens, and return the token itself if successful.
        TBD what this actually does...for now, largely keep unmodified, but attach the metadata to the wrapped function
        """
        # pull cls it from the return type of eat_func (which should be a Union[tuple[int, Token], None])
        cls = inspect.signature(eat_func).return_annotation.__args__[0].__args__[1]
        assert issubclass(cls, Token), f"cls must be a subclass of Token, but got {cls}"
        if whitelist is not None and blacklist is not None:
            raise ValueError("cannot specify both whitelist and blacklist")
        def wrapper(*args, **kwargs):
            return eat_func(*args, **kwargs), cls
        wrapper._is_full_eat_decorator = True  # make it easy to check if a function has this decorator
        wrapper._eat_func = eat_func
        wrapper._token_cls = cls
        wrapper._whitelist = whitelist
        wrapper._blacklist = blacklist

        return wrapper
    return decorator


def get_peek_eat_funcs_with_name() -> tuple[tuple[str, Callable]]:
    return tuple((name, func) for name, func in globals().items() if callable(func) and getattr(func, '_is_peek_eat_decorator', False))
def get_full_eat_funcs_with_name() -> tuple[tuple[str, Callable]]:
    return tuple((name, func) for name, func in globals().items() if callable(func) and getattr(func, '_is_full_eat_decorator', False))

def get_eat_funcs() -> tuple[Callable]:
    return tuple(func for name, func in get_peek_eat_funcs_with_name() + get_full_eat_funcs_with_name())

@lru_cache()
def get_contextual_eat_funcs(context:Type[Token]) -> tuple[Callable]:
    """Get all the eat functions that are valid in the given context"""
    return tuple(func for func in get_eat_funcs() if (func._whitelist is None or context in func._whitelist) and (func._blacklist is None or context not in func._blacklist))

@lru_cache()
def get_func_precedences(funcs:tuple[Callable]) -> tuple[int]:
    assert isinstance(funcs, tuple)
    return tuple(precedence.get(func._token_cls, 0) for func in funcs)


@peek_eat(WhiteSpace_t)
def eat_line_comment(src:str) -> int|None:
    """eat a line comment, return the number of characters eaten"""
    if src.startswith('//'):
        try:
            return src.index('\n') + 1
        except ValueError:
            return len(src)
    return None

@peek_eat(WhiteSpace_t)
def eat_block_comment(src:str) -> int|None:
    """
    Eat a block comment, return the number of characters eaten
    Block comments are of the form /{ ... }/ and can be nested.
    """
    if not src.startswith("/{"):
        return None

    nesting_level = 0
    i = 0

    while i < len(src):
        if src[i:].startswith('/{'):
            nesting_level += 1
            i += 2
        elif src[i:].startswith('}/'):
            nesting_level -= 1
            i += 2

            if nesting_level == 0:
                return i
        else:
            i += 1

    raise ValueError("unterminated block comment")
    # return None

@peek_eat(WhiteSpace_t)
def eat_whitespace(src:str) -> int|None:
    """Eat whitespace, return the number of characters eaten"""
    i = 0
    while i < len(src) and src[i].isspace():
        i += 1
    return i if i > 0 else None

@peek_eat(Keyword_t)
def eat_keyword(src: str) -> int | None:
    """
    Eat a reserved keyword, return the number of characters eaten

    #keyword = {in} | {as} | {loop} | {lazy} | {if} | {and} | {or} | {xor} | {nand} | {nor} | {xnor} | {not};# | {true} | {false}; 
    
    noting that keywords are case insensitive
    """

    max_len = max(len(keyword) for keyword in keywords)
    
    lower_src = src[:max_len].lower()
    for keyword in keywords:
        if lower_src.startswith(keyword):
            #TBD if we need to check that the next character is not an identifier character
            return len(keyword)

    return None



#TODO: expand the list of valid identifier characters
digits = set('0123456789')
alpha = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz')
greek = set('ΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩαβγδεζηθικλμνξοπρςστυφχψω')
misc = set('_?!$&°')

start_characters = (alpha | greek | misc) - {'?'}
continue_characters = (alpha | digits | greek | misc)


@peek_eat(Identifier_t)
def eat_identifier(src:str) -> int|None:
    """
    Eat an identifier, return the number of characters eaten

    Identifiers:
    - may not start with a number or a question mark
    - may not end with a question mark
    - may use (TODO enumerate the full chars list somewhere. for now copying from python)
    
    """
    if not src[0] in start_characters:
        return None

    i = 1
    while i < len(src) and src[i] in continue_characters:
        i += 1

    # while last character is ?, remove it
    while i > 1 and src[i-1] == '?':
        i -= 1

    return i



@peek_eat(Hashtag_t)
def eat_hashtag(src:str) -> int|None:
    """
    Eat a hashtag, return the number of characters eaten
    
    hashtags are special identifiers that start with #
    """

    if src.startswith('#'):
        i,_ = eat_identifier(src[1:])
        if i is not None:
            return i + 1
        
    return None


@peek_eat(Escape_t, whitelist=[String_t])
def eat_escape(src:str) -> int|None:
    r"""
    Eat an escape sequence, return the number of characters eaten
    Escape sequences must be either a known escape sequence:
    - \n newline 
    - \r carriage return
    - \t tab
    - \b backspace
    - \f form feed
    - \v vertical tab
    - \a alert
    - \0 null
    - \u##..# or \U##..# for an arbitrary unicode character. May have any number of hex digits
    
    or a \ followed by an unknown character. In this case, the escape converts to just the unknown character
    This is how to insert characters that are otherwise illegal inside a string, e.g. 
    - \' converts to just a single quote '
    - \{ converts to just a single open brace {
    - \\ converts to just a single backslash \
    - \m converts to just a single character m
    - etc.
    """
    if not src.startswith('\\'):
        return None

    if len(src) == 1:
        raise ValueError("unterminated escape sequence")

    if src[1] in 'uU':
        i = 2
        while i < len(src) and src[i].isxdigit():
            i += 1
        if i == 2:
            raise ValueError("invalid unicode escape sequence")
        return i

    # if src[1] in 'nrtbfva0':
    #     return 2

    #all other escape sequences (known or unknown) are just a single character
    return 2


@full_eat()
def eat_string(src:str) -> tuple[int, String_t] | None:
    r"""
    strings are delimited with either single (') or double quotes (")
    the character portion of a string may contain any character except the delimiter, \, or {.
    strings may be multiline
    strings may contain escape sequences of the form \s where s is either a known escape sequence or a single character
    strings may interpolation blocks which open with { and close with }

    Tokenizing of escape sequences and interpolation blocks is handled as sub-tokenization task via eat_block and eat_escape

    returns the number of characters eaten and an instance of the String token, containing the list of tokens/string chunks/escape sequences
    """
    
    #determine the starting delimiter, or exit if there is none
    if src.startswith('"""') or src.startswith("'''"):
        delim = src[:3]
        i = 3
    elif src.startswith('"') or src.startswith("'"):
        delim = src[0]
        i = 1
    else:
        return None
    
    #keep track of chunks, and the start index of the current chunk
    chunk_start = i
    body = []

    # add character sequences, escapes, and block sections until the end of the string
    while i < len(src) and not src[i:].startswith(delim):
        
        #regular characters
        if src[i] not in '\\{':
            i += 1
            continue

        #add the previous chunk before handling the escape/interpolation block
        if i > chunk_start:
            body.append(src[chunk_start:i])

        if src[i] == '\\':
            res, _ = eat_escape(src[i:])
            if res is None:
                raise ValueError("invalid escape sequence")
            body.append(Escape_t(src[i:i+res]))
            i += res

        else: # src[i] == '{':
            assert src[i] == '{', "internal error"
            res, _ = eat_block(src[i:])
            if res is None:
                raise ValueError("invalid block")
            n_eaten, block = res
            body.append(block)
            i += n_eaten
        
        #update the chunk start
        chunk_start = i
            

    if i == len(src):
        raise ValueError("unterminated string")
    
    #add the final chunk
    if i > chunk_start:
        body.append(src[chunk_start:i])
    
    return i + len(delim), String_t(body)



@peek_eat(RawString_t)
def eat_raw_string(src:str) -> int|None:
    """
    raw strings start with `r`, followed by a delimiter, one of ' " ''' or \"""
    raw strings may contain any character except the delimiter.
    Escapes and interpolations are ignored.
    The string ends at the first instance of the delimiter
    """
    if not src.startswith('r'):
        return None
    i = 1

    if src[i:].startswith('"""') or src[i:].startswith("'''"):
        delim = src[i:i+3]
        i += 3
    elif src[i:].startswith('"') or src[i:].startswith("'"):
        delim = src[i]
        i += 1
    else:
        return None

    while i < len(src) and not src[i:].startswith(delim):
        i += 1

    if i == len(src):
        raise ValueError("unterminated raw string")

    return i + len(delim)


@peek_eat(Integer_t)
def eat_integer(src:str) -> int|None:
    """
    eat an integer, return the number of characters eaten
    integers are of the form [0-9]+
    """
    i = 0
    while i < len(src) and src[i].isdigit():
        i += 1
    return i if i > 0 else None


@peek_eat(BasedNumber_t)
def eat_based_number(src:str) -> int|None:
    """
    eat a based number, return the number of characters eaten

    based numbers have a (case-insensitive) prefix (0p) identifying the base, and (case-sensitive) allowed digits
    """
    try:
        digits = number_bases[src[:2].lower()]
    except KeyError:
        return None
    
    i = 2
    while i < len(src) and src[i] in digits or src[i] == '_':
        i += 1

    return i if i > 2 else None


@peek_eat(Boolean_t)
def eat_boolean(src:str) -> int|None:
    """
    eat a boolean, return the number of characters eaten

    booleans are either true or false (case-insensitive)
    """
    sample = src[:5].lower()
    if sample.startswith('true'):
        return 4
    elif sample.startswith('false'):
        return 5

    return None


@peek_eat(Operator_t)
def eat_operator(src:str) -> int|None:
    """
    eat a unary or binary operator, return the number of characters eaten

    picks the longest matching operator

    see `operators` for full list of operators
    """
    for op in operators:
        if src.startswith(op):
            return len(op)
    return None

@peek_eat(ShiftOperator_t, blacklist=[TypeParam_t])
def eat_shift_operator(src:str) -> int|None:
    """
    eat a shift operator, return the number of characters eaten

    picks the longest matching operator. 
    Shift operators are not allowed in type parameters, e.g. `>>` is not recognized in `Foo<Bar<Baz<T>>, U>`

    see `shift_operators` for full list of operators
    """
    for op in shift_operators:
        if src.startswith(op):
            return len(op)
    return None

@peek_eat(Comma_t)
def eat_comma(src:str) -> int|None:
    """
    eat a comma, return the number of characters eaten
    """
    return 1 if src.startswith(',') else None


@peek_eat(DotDot_t)
def eat_dotdot(src:str) -> int|None:
    """
    eat a dotdot, return the number of characters eaten
    """
    return 2 if src.startswith('..') else None



class EatTracker:
    i: int
    tokens: list[Token]


@full_eat()
def eat_type_param(src:str) -> tuple[int, TypeParam_t] | None:
    """
    eat a type parameter, return the number of characters eaten and an instance of the TypeParam token

    type parameters are of the form <...> where ... is a sequence of tokens. 
    Type parameters may not start with `<<` or contain any shift operators (`<<`, `<<<`, `>>`, `>>>`)
    Internally encountered shift operators are considered to be delimiters for the type parameter
    """
    if not src.startswith('<') or src.startswith('<<'):
        return None
    
    i = 1
    body: list[Token] = []

    while i < len(src) and src[i] != '>':
        
        funcs = get_contextual_eat_funcs(TypeParam_t)
        precedences = get_func_precedences(funcs)
        res = get_best_match(src[i:], funcs, precedences)

        if res is None:
            return None        
        n_eaten, token = res

        if isinstance(token, Token):
            #add the already-eaten token to the list of tokens
            body.append(token)
        else:
            #add a new instance of the token to the list of tokens (handling idempotent token cases)
            token_cls = token
            if not body or token_cls not in idempotent_tokens or not isinstance(body[-1], token_cls):
                body.append(token_cls(src[i:i+n_eaten]))

        #increment the index
        i += n_eaten


    if i == len(src):
        return None
    
    return i + 1, TypeParam_t(body)
    



@full_eat()
def eat_block(src:str, tracker:EatTracker|None=None) -> tuple[int, Block_t] | None:
    """
    Eat a block, return the number of characters eaten and an instance of the Block token

    blocks are { ... } or ( ... ) and may contain sequences of any other tokens including other blocks

    if return_partial is True, then returns (i, body) in the case where the eat process fails, instead of None
    """
    
    if not src or src[0] not in pair_opening_delims:
        return None
    
    # save the opening delimiter
    left = src[0]
    
    i = 1
    body: list[Token] = []

    if tracker:
        tracker.i = i
        tracker.tokens = body

    while i < len(src) and src[i] not in pair_closing_delims:
        #run all root eat functions
        #if multiple, resolve for best match (TBD... current is longest match + precedence)
        #if no match, return None


        ########### TODO: probably break this inner part into a function that eats the next token, given a list of eat functions
        ###########       could also think about ways to specify other multi-match resolutions, other than longest match + precedence...
        #run all the eat functions on the current src
        funcs = get_contextual_eat_funcs(Block_t)
        precedences = get_func_precedences(funcs)
        res = get_best_match(src[i:], funcs, precedences)

        #if we didn't match anything, return None
        if res is None:
            return None
        
        n_eaten, token = res
        
        if isinstance(token, Token):
            #add the already-eaten token to the list of tokens
            body.append(token)
        else:
            #add a new instance of the token to the list of tokens (handling idempotent token cases)
            token_cls = token
            if not body or token_cls not in idempotent_tokens or not isinstance(body[-1], token_cls):
                body.append(token_cls(src[i:i+n_eaten]))

        #increment the index
        i += n_eaten
        if tracker:
            tracker.i = i

    if i == len(src):
        if tracker: #only return an exception for the top level block. nested blocks can return None
            raise ValueError("unterminated block") 
        return None
    
    # closing delim (doesn't need to match opening delim)
    right = src[i]
    assert left in pair_opening_delims and right in pair_closing_delims, f"invalid block delimiters: {left} {right}"

    #include closing delimiter in character count
    i += 1
    if tracker:
        tracker.i = i

    return i, Block_t(body, left=left, right=right)



def get_best_match(src:str, eat_funcs:list, precedences:list[int]) -> tuple[int, Type[Token]|Token]|None:
    #TODO: handle selecting between full_eat and peek_eat functions that were successful...
    #      general, just need to clarify the selection order precedence

    #may return none if no match
    #may return (i, token_cls) if peek match
    #may return (i, token) if full match

    matches = [eat_func(src) for eat_func in eat_funcs]

    #find the longest token that matched. if multiple tied for longest, use the one with the highest precedence.
    #raise an error if multiple tokens tied, and they have the same precedence
    def key(x):
        (res, _cls), precedence = x
        if res is None:
            return 0, precedence
        if isinstance(res, tuple):
            res, _token = res #full_eat functions return a tuple of (num_chars_eaten, token)
        return res, precedence

    matches = [*zip(matches, precedences)]
    best = max(matches, key=key)
    ties = [match for match in matches if key(match) == key(best)]
    if len(ties) > 1:
        raise ValueError(f"multiple tokens matches tied {[match[0][1].__name__ for match in ties]}: {repr(src)}\nPlease disambiguate by providing precedence levels for these tokens.")

    (res, token_cls), _ = best
    
    # force the type annotations
    res: tuple[int, Token]|int|None 
    token_cls: type[Token]

    if res is None:
        return None
    
    if isinstance(res, int):
        return res, token_cls
    
    if isinstance(res, tuple):
        return res
    
    raise ValueError(f"Internal Error: invalid return type from eat function: {res}")



def tokenize(src:str) -> list[Token]:

    # insert src into a block
    src = f'{{\n{src}\n}}'

    #convert string to a coordinate string (for keeping track of row/col numbers)
    src = CoordString(src, anchor=(-1, 0))

    # eat tokens for a block
    tracker = EatTracker()
    try:
        res, _cls = eat_block(src, tracker=tracker)
    except Exception as e:
        raise ValueError(f"failed to tokenize: ```{escape_whitespace(src[tracker.i:])}```.\nCurrent tokens: {tracker.tokens}") from e

    # check if the process failed
    if res is None:
        raise ValueError(f"failed to tokenize: ```{escape_whitespace(src[tracker.i:])}```.\nCurrent tokens: {tracker.tokens}")

    (i, block) = res
    tokens = block.body

    # ensure that all blocks have valid open/close pairs
    validate_block_braces(tokens)

    return tokens

def full_traverse_tokens(tokens:list[Token]) -> Generator[tuple[int, Token, list[Token]], None, None]:
    """
    Walk all tokens recursively, allowing for modification of the tokens list as it is traversed.
    
    So long as modifications do not occur before the current token, this will safely iterate over all tokens.
    This will not yield string or escape chunks in strings, but will yield interpolated blocks.

    While traversing, the user can overwrite the current index by calling .send(new_index).

    e.g.
    ```python
    gen = full_traverse_tokens(tokens)
    for i, token, stream in gen:
        #do something with current token
        #...

        #maybe overwrite the current index
        if should_overwrite:
            gen.send(new_index)
    ```

    Do not call .send() twice in a row without calling next() in between. This will cause unexpected behavior.

    Args:
        tokens: the list of tokens to traverse

    Yields:
        i: the index of the current token in the current token list
        token: the current token
        stream: the current token list
    """

    i = 0

    while i < len(tokens):
        """
        1. get next token
        2. send current to user
        3. increment index (or overwrite it)
        4. recurse into blocks
        """

        # get the current token
        token = tokens[i]

        # send the current index to the user. possibly receive a new index to continue from
        overwrite_i = yield i, token, tokens

        # only calls to next() will continue execution. calls to .send do nothing wait
        if overwrite_i is not None:
            assert (yield) is None, ".send() may only be called once per iteration."
            i = overwrite_i
        else:
            i += 1

        # for tokens that have defined __iter__ methods, yield their contents
        try:
            for children in token:
                yield from full_traverse_tokens(children)
        except NotImplementedError:
            pass


def traverse_tokens(tokens:list[Token]) -> Generator[Token, None, None]:
    """
    Convenience function over full_traverse_tokens. Walk all tokens recursively
    
    Does not allow for modification of the tokens list as it is traversed.
    To modify during traversal, use `full_traverse_tokens` instead.

    Args:
        tokens: the list of tokens to traverse

    Yields:
        token: the current token
    """
    for _, token, _ in full_traverse_tokens(tokens):
        yield token



def validate_block_braces(tokens:list[Token]) -> None:
    """
    Checks that all blocks have valid open/close pairs.

    For example, ranges may have differing open/close pairs, e.g. [0..10), (0..10], etc.
    But regular blocks must have matching open/close pairs, e.g. { ... }, ( ... ), [ ... ]
    Performs some validation, without knowing if the block is a range or a block. 
    So more validation is needed when the actual block type is known.

    Raises:
        AssertionError: if a block is found with an invalid open/close pair
    """
    for token in traverse_tokens(tokens):
        if isinstance(token, Block_t):
            assert token.left in valid_delim_closers, f'INTERNAL ERROR: left block opening token is not a valid token. Expected one of {[*valid_delim_closers.keys()]}. Got \'{token.left}\''
            assert token.right in valid_delim_closers[token.left], f'ERROR: mismatched opening and closing braces. For opening brace \'{token.left}\', expected one of \'{valid_delim_closers[token.left]}\''
        

def validate_functions():

    # Validate the @peek_eat function signatures
    peek_eat_functions = get_peek_eat_funcs_with_name()
    for name, wrapper_func in peek_eat_functions:
        func = wrapper_func._eat_func
        signature = inspect.signature(func)
        param_types = [param.annotation for param in signature.parameters.values() if param.default is inspect.Parameter.empty]
        return_type = signature.return_annotation

        # Check if the function has the correct signature
        if len(param_types) != 1 or param_types[0] != str or return_type != int|None:
            pdb.set_trace()
            raise ValueError(f"{func.__name__} has an invalid signature: `{signature}`. Expected `(src: str) -> int | None`")

    # Validate the @full_eat function signatures
    full_eat_functions = get_full_eat_funcs_with_name()
    for name, wrapper_func in full_eat_functions:
        func = wrapper_func._eat_func
        signature = inspect.signature(func)
        param_types = [param.annotation for param in signature.parameters.values() if param.default is inspect.Parameter.empty]
        return_type = signature.return_annotation

        # Check if the function has the correct signature
        error_message = f"{func.__name__} has an invalid signature: `{signature}`. Expected `(src: str) -> tuple[int, Token] | None`"
        if not (isinstance(return_type, UnionType) and len(return_type.__args__) == 2 and type(None) in return_type.__args__):
            raise ValueError(error_message)
        A, B = return_type.__args__
        if B is not type(None):
            B, A = A, B
        if not (isinstance(A, type(tuple)) and len(A.__args__) == 2 and A.__args__[0] is int and issubclass(A.__args__[1], Token)):
            raise ValueError(error_message)        
        if len(param_types) != 1 or param_types[0] != str:
            pdb.set_trace()
            raise ValueError(error_message)

    # check for any functions that start with eat_ but are not decorated with @eat
    peek_eat_func_names = {name for name, _ in peek_eat_functions}
    full_eat_func_names = {name for name, _ in full_eat_functions}
    for name, func in globals().items():
        if name.startswith("eat_") and callable(func) and name not in peek_eat_func_names and name not in full_eat_func_names:
            raise ValueError(f"`{name}()` function is not decorated with @peek_eat or @full_eat")


def escape_whitespace(s:str):
    """convert a string to one where all non-space whitespace is escaped"""
    escape_map = {
        '\t': '\\t',
        '\r': '\\r',
        '\f': '\\f',
        '\v': '\\v',
        '\n': '\\n',
    }
    return ''.join(escape_map.get(c, c) for c in s)


def tprint(token:Token, level=0):
    """
    print a token with a certain indentation level.
    
    If tokens contain nested tokens, they will be printed recursively with an increased indentation level
    """
    print(f'{"    "*level}', end='')
    if isinstance(token, Block_t):
        print(f'<Block {token.left}{token.right}>')
        for t in token.body:
            tprint(t, level=level+1)
    elif isinstance(token, String_t):
        print(f'<String>')
        for t in token.body:
            tprint(t, level=level+1)
    elif isinstance(token, TypeParam_t):
        print(f'<TypeParam>')
        for t in token.body:
            tprint(t, level=level+1)
    else:
        print(token)
        


def test():
    import sys
    """simple test dewy program"""

    try:
        path = sys.argv[1]
    except IndexError:
        raise ValueError("Usage: `python tokenizer.py path/to/file.dewy>`")


    with open(path) as f:
        src = f.read()

    tokens = tokenize(src)
    print(f'matched tokens:')
    tprint(Block_t(left='{', right='}', body=tokens))
    # for t in tokens:
    #     tprint(t, level=1)




if __name__ == "__main__":
    validate_functions()
    test()1c:T14ae,from typing import Callable



def wrap_coords(method:Callable):
    def wrapped_method(self, *args, **kwargs):
        result = method(self, *args, **kwargs)
        if isinstance(result, str) and len(result) == len(self):
            custom_str = CoordString(result)
            custom_str.row_col_map = self.row_col_map
            return custom_str
        else:
            raise ValueError("coord_string_method must return a string of the same length as the original string")
        return result

    return wrapped_method

def fail_coords(method:Callable):
    def wrapped_method(self, *args, **kwargs):
        raise ValueError(f"coord_string_method {method} cannot be called on a CoordString, as it will not return a CoordString")
    return wrapped_method


class CoordString(str):
    """
    Drop-in replacement for str that keeps track of the coordinates of each character in the string

    Identical to normal strings, but attaches the `row_col(i:int) -> tuple[int, int]` method
    which returns the (row, column) of the character at index i

    Args:
        anchor (tuple[int,int], optional): The row and column of the top left of the string. Defaults to (0, 0).
    """
    def __new__(cls, *args, anchor:tuple[int,int]=(0, 0), **kwargs):
        self = super().__new__(cls, *args, **kwargs)
        row, col = anchor
        self.row_col_map = self._generate_row_col_map(row, col)

        return self

    #TODO: make init so that class recognized .row_col_map as property on instances
    # def __init__(self, s:str, row_col_map:list[tuple[int,int]]):


    def _generate_row_col_map(self, row=0, col=0) -> list[tuple[int, int]]:
        row_col_map = []
        for c in self:
            if c == '\n':
                row_col_map.append((row, col))
                row += 1
                col = 0
            else:
                row_col_map.append((row, col))
                col += 1
        return row_col_map

    def __getitem__(self, key):
        if isinstance(key, slice):
            sliced_str = super().__getitem__(key)
            sliced_row_col_map = self.row_col_map[key]
            custom_str = CoordString(sliced_str)
            custom_str.row_col_map = sliced_row_col_map
            return custom_str
        return super().__getitem__(key)

    def loc(self, index):
        return self.row_col_map[index]

    @staticmethod
    def from_existing(new_str:str, old_coords:list[tuple[int,int]]) -> 'CoordString':
        new_coord_str = CoordString(new_str)
        new_coord_str.row_col_map = old_coords
        return new_coord_str

    #wrappers for string methods that should return CoordStrings
    def lstrip(self, *args, **kwargs):
        result = super().lstrip(*args, **kwargs)
        custom_str = CoordString(result)
        custom_str.row_col_map = self.row_col_map[len(self)-len(result):]
        return custom_str

    def rstrip(self, *args, **kwargs):
        result = super().rstrip(*args, **kwargs)
        custom_str = CoordString(result)
        custom_str.row_col_map = self.row_col_map[:len(result)]
        return custom_str
    
    def strip(self, *args, **kwargs):
        return self.lstrip(*args, **kwargs).rstrip(*args, **kwargs)


    @wrap_coords
    def capitalize(self): return super().capitalize()

    @wrap_coords
    def casefold(self): return super().casefold()

    @wrap_coords
    def lower(self): return super().lower()

    @wrap_coords
    def upper(self): return super().upper()

    @wrap_coords
    def swapcase(self): return super().swapcase()

    @wrap_coords
    def title(self): return super().title()

    @wrap_coords
    def translate(self, table): return super().translate(table)

    @wrap_coords
    def replace(self, old, new, count=-1): return super().replace(old, new, count)

    @fail_coords
    def center(self, *args, **kwargs): ...

    @fail_coords
    def expandtabs(self, *args, **kwargs): ...

    @fail_coords
    def ljust(self, *args, **kwargs): ...

    @fail_coords
    def zfill(self, *args, **kwargs): ...


#TODO: maybe make adding this string with other regular str illegal





int_parsable_base_prefixes = {
    '0b':2,  '0B':2,
    '0t':3,  '0T':3,
    '0q':4,  '0Q':4,
    '0s':6,  '0S':6,
    '0o':8,  '0O':8,
    '0d':10, '0D':10,
    # '0z':12, #uses different digits Z/z and X/x (instead of A/a and B/b expected by int())
    '0x':16, '0X':16,
    '0u':32, '0U':32,
    '0r':36, '0R':36,
    # '0y':64, #more than int's max parsable base (36)
}


def based_number_to_int(src:str) -> int:
    """
    convert a number in a given base to an int
    """
    prefix, digits = src[:2], src[2:]
    if prefix in int_parsable_base_prefixes:
        return int(digits, int_parsable_base_prefixes[prefix])
    elif prefix == '0z':
        raise NotImplementedError(f"base {prefix} is not supported")
    elif prefix == '0y':
        raise NotImplementedError(f"base {prefix} is not supported")
    else:
        raise ValueError(f"INTERNAL ERROR: base {prefix} is not a valid base")


def bool_to_bool(src:str) -> bool:
    """
    convert a (case-insensitive) bool literal to a bool
    """
    try:
        return bool(['false', 'true'].index(src.lower()))
    except ValueError:
        raise ValueError(f"INTERNAL ERROR: bool {src} is not a valid bool") from None1d:T8e5,//TODO: uncomment when types are supported
/{
// simple xorshift+ generator
state:uint64 = 123456789
rand = ():uint64 => {
    state xor= state >> 21
    state xor= state << 35
    state xor= state >> 4
    
    return state * 2_685821_657736_338717 //TODO: divide by uint64.MAX (18_446744_073709_551615)
}
half = 9_223372_036854_775807
a = rand <? half
b = rand <? half
c = rand <? half
d = rand <? half
e = rand <? half
f = rand <? half
g = rand <? half
h = rand <? half
i = rand <? half
j = rand <? half
k = rand <? half
l = rand <? half
m = rand <? half
n = rand <? half
o = rand <? half
p = rand <? half
q = rand <? half
r = rand <? half
s = rand <? half
t = rand <? half
u = rand <? half
v = rand <? half
w = rand <? half
x = rand <? half
y = rand <? half
z = rand <? half
}/

//manually specify bools
a = false
b = true
c = true
d = false
e = true
f = false
g = true
h = false
i = true
j = false
k = true
l = false
m = true
n = false
o = true
p = false
q = true
r = false
s = true
t = false
u = true
v = false
w = true
x = false
y = true
z = false



if a
    printl'a'
else if b
    if c
        if d
            if e
                printl'bcde'
            else if f
                if g
                    if h
                        printl'bcdfgh'
                    else if i
                        printl'bcdfgi'
                    else if j
                        printl'bcdfgj'
                    else
                        printl'bcdfg[]'
                else if k
                    printl'bcdfk'
                else if l
                    if m
                        printl'bcdflm'
                    else
                        printl'bcdfl[]'
                else
                    printl'bcdf[]'
            else if n
                printl'bcdn'
            else
                printl'bcd[]'
        else if o
            printl'bco'
        else if p
            if q
                printl'bcpq'
            else
                printl'bcp[]'
        else
            printl'bc[]'
    else
        printl'b[]'
else if r
    printl'r'
else if s
    if t
        printl'st'
    else if u
        printl'su'
if v
    if w
        printl'vw'
    else if x
        printl'x'
if y
    printl'y'
else if z
    printl'z'
else
    printl'[]'
1e:T1092,//simple, fast, high quality, dependency-free RNG generation
//uniform distribution via XORSHIFT*
//normal distribution via PPND16


RNG = (s:uint64) => [
    next_u64 = () => {
        s ^= s >> 21
        s ^= s << 35
        s ^= s >> 4
        s * 2685821657736338717
    }
    next_uniform = () => fast_to_uniform(next_u64())
    next_normal = () => ppnd16(fast_to_uniform(next_u64()))
    
    /{
        Use bit hacks to quickly convert a 64-bit number to a double in the range [0, 1)

        @param x the number to convert. Only the lowest 23 bits are used.
        @return the number in the range [0, 1)
    }/
    fast_to_uniform = (x:uint64):float64 => {
        const mask1 = 0x3FF0_0000_0000_0000
        const mask2 = 0x3FFF_FFFF_FFFF_FFFF
        out: uint64 = (x | mask1) & mask2
        (out as float64) - 1
    }

    full_to_uniform = (x:uint64):float64 => truediv(x, uint64.max, float64)

    /{
        Convert a uniformly distributed double in the range (0, 1) to a normally distributed double
        Uses the PPND16 algorithm from Algorithm AS241: The Percentage Points of the Normal Distribution

        @param x the uniformly distributed double in the range (0, 1)
        @return the normally distributed double
    }/
    ppnd16 = (x:float64) => {

        // zero area at x=0/x=1
        if x <=? 0 or x >=? 1
            return 0

        const split1:float64 = 0.425
        const split2:float64 = 5.0
        const C1:float64 = 0.180625
        const C2:float64 = 1.6

        // Cofficients for x close to 0.5
        const A:float64[] =
        [
            3.3871328727963665
            133.14166789178438
            1971.5909503065514
            13731.693765509461
            45921.95393154987
            67265.7709270087
            33430.575583588128
            2509.0809287301227            
        ]
        const B:float64[] =
        [
            1.0
            42.313330701600911
            687.18700749205789
            5394.1960214247511
            21213.794301586597
            39307.895800092709
            28729.085735721943
            5226.4952788528544
        ]
        
        // Coefficients for x not close to 0, 0.5 or 1
        const C:float64[] =
        [
            1.4234371107496835
            4.6303378461565456
            5.769497221460691
            3.6478483247632045
            1.2704582524523684
            0.24178072517745061
            0.022723844989269184
            0.00077454501427834139
        ]
        const D:float64[] =
        [
            1.0
            2.053191626637759
            1.6763848301838038
            0.6897673349851
            0.14810397642748008
            0.015198666563616457
            0.00054759380849953455
            0.0000000010507500716444169
        ]

        // Coefficients for x near 0 or 1
        const E:float64[] =
        [
            6.6579046435011033
            5.4637849111641144
            1.7848265399172913
            0.29656057182850487
            0.026532189526576124
            0.0012426609473880784
            0.000027115555687434876
            0.00000020103343992922882
        ]
        const F:float64[] =
        [
            1.0
            0.599832206555888
            0.13692988092273581
            0.014875361290850615
            0.00078686913114561329
            0.000018463183175100548
            0.0000001421511758316446
            0.0000000000000020442631033899397
        ]

        let r:float64

        // shift x to the range (-0.5, 0.5)
        const q = x - 0.5
        if abs(q) <=? split1
        {
            r = C1 - q^2
            powers = r.^[0..7]
            return q * (A .* powers).sum / (B .* powers).sum
        }
        
        // shift x back to (0,1) and invert if it was positive
        r = if q <? 0 x else 1 - x
        r = sqrt(-log(r))
        
        if r <=? split2
        {
            r -= C2
            powers = r.^[0..7]
            return sign(q) * (C .* powers).sum / (D .* powers).sum
        }
        else
        {
            r -= split2
            powers = r.^[0..7]
            return sign(q) * (E .* powers).sum / (F .* powers).sum
        }
    }
]



r = RNG(42)
loop i in 0..100 printl(r.next_normal())1f:Tc2b,// examples of syntax used in dewy
// line comments
/{ block/multiline comments }/

// typed declaration
apple: uint64
banana: map<int, string>
peach: array<int, length=N>  //array of ints with length N...
let pear: set<range>  // let indicates that this is definitely a new declaration, even if the identifier already exists



// unpack assignment examples
A = 1..10
B = [loop a in A -a]
loop [a, b] in [A B] {}

// object with nested objects to unpack
my_obj = [
    apple = [1 2 3 4 [
        ultimate_answer = 42
    ]]
    banana = 10
    peach = [
        purple = 23
        blue = 12
        orange = 'orange'
    ]
]

// nested unpack assignment. tbd if the top level is `[unpack, params, etc] = obj`, or `obj as [unpack, params, etc]`
[
    apple as [
        a1, 
        a2, 
        a3, 
        a4, 
        a5 as [ultimate_answer as answer]
    ], 
    banana as renamed_banana, 
    peach as [purple, blue, orange]
] = my_obj
// unpacked variables are:
//   a1 = 1, a2 = 2, a3 = 3, a4 = 4
//   answer = 42
//   renamed_banana = 10
//   purple = 23, blue = 12, orange = 'orange'

// unpacking dictionaries probably treats them as just the list of key -> value pairs
// unpacking sets, probably just treats the elements like a normal array
// unpacking ranges treats them as a normal array

// `...` can be used in unpack to coalesce extra elements for list-like containers
// there may only be 1 `...` in an unpack (otherwise it would be ambiguous which elements to collect)
// the variable receiving the `...` will be of the same type as the original object being unpacked
my_arr = [1 2 3 4 5 6 7 8 9]
[a1, a2, a3, ...my_arr, a8, a9] = my_arr  //a1 = 1, a2 = 2, a3 = 3, my_arr = [4, 5, 6, 7], a8 = 8, a9 = 9

my_dict = ['apple' -> 1 'banana' -> 2 'peach' -> 3 'pie' -> 4]
[d1, ...dict_left] = my_dict //d1 = ('apple' -> 1), dict_left = ['banana' -> 2 'peach' -> 3 'pie' -> 4]

// random range note: for step sizes other than +1, use range_iter constructor e.g. range_iter(start to stop, step=5)
my_range = 1..inf
loop my_range.length >? 0 ( [i, ...my_range] = my_range )
// first iteration: i = 1, my_range = 2..inf
// second iteration: i = 2, my_range = 3..inf
// third iteration: i = 3, my_range = 4..inf
// ...
// for forever

//[...my_range, i] = my_range //will probably set my_range = 1 to inf, i = inf

// unpacking too many values, or named values that don't exist just sets them to undefined



// assignment expressions (i.e. python's walrus operator from https://www.python.org/dev/peps/pep-0572/)
// Handle a matched regex
if (match = pattern.search(data); match) not =? undefined
{
    // Do something with match
}

// A loop that can't be trivially rewritten using 2-arg iter()
loop (chunk = file.read(8192); chunk.length >? 0)
{
   process(chunk)
}

// Reuse a value that's expensive to compute
[(y = f(x); y) y**2 y**3]

// Share a subexpression between a comprehension filter clause and its output
filtered_data = [for x in data if (y = f(x); y) not =? undefined y]
//though you could also just write this like so
filtered_data = [for x in data {y = f(x) if y not =? undefined y}]
20:T1536,///////////////////// STRING INTERPOLATION /////////////////////

/{Todo: probably break each section for different syntaxes into different files?}/

//silly example with keyword vs identifier
loop i i

r'this is a raw string \'  expr  'a separate string later'

// simple blocks
{   }
( /{comment inside}/ )
{ 2+2 }
( 2+2 )


//string interpolation
my_string = '2 + 2 = {2+2}'

//complex interpolation
s = "first 10 primes are: {
    primes = [2]
    loop i in [3, 5..) and primes.length <? 10
        if i .% primes |> product not =? 0 
            primes.push(i)
    primes
}"


//alternative prime generator + getting first 10 primes
#ctx
primes = [
    2
    lazy i in [3, 5..)
        if i .% #ctx.primes .=? 0 |> @reduce(, (prev, v) => prev and v)
            i
][..10)

//TBD if there is a parallel way to do this where the i .% primes dispatches each operation, and fails immediately on any returning false
#label
primes = [
    2
    lazy i in [3, 5..)
        if not parallel_or(p => i % p =? 0, #label.primes)
            i
][..10)
//parallel or is like goroutines with cancel once any is true...should have it be more flexible, e.g. able to use any of the boolean keywords that can short circuit
//actually probably don't want to need to specify that it's parallel. Instead if there's an operation over a vector, it gets parallelized if possible.

//nested interpolation
s2 = 'this is an outer string, and {'this is an interior string with "{my_string}" in it'}'





const add: (int, int) => int = (a:int, b:int) => { /{return sum of a and b}/ }
let div: (real, real) => real? = (a:real, b:real) => { /{return a / b}/ }

// function type with named default argument
my_func: (string, kwarg:bool) => () = (s:string, kwarg:bool=false) => {}

//you probably can do the verbose version as well (probably useful for when you're just defining the interface without the implementation)
my_func: (s:string, kwarg:bool=false) => void



// example annotations for function types
() => ()
() => void
() => bool
int => bool
a: int => bool
(int, int) => int
<T>(T, T) => T
<T>(a:T, b:T) => T

// object type
[a:int? b:string]

//? (optional) is sugar for |void
[a:int|void b:string]

// operators juxtaposed to identifiers
aorb
a or b
a+b


//based number literals
0b1010_0011_0101_0110_1001_1010_1100_1111
0B0101_1111_1010_1110_0011_0101_1001_1100

0t012_221_012_221_012_221_012_221
0t211_001_211_001_211_001_211_001

0q331_231_223_131_331_231_223_131
0Q123_321_123_321_123_321_123_321

0s123_450_123_450_123_450_123_450
0S543_210_543_210_543_210_543_210

0o123_456_701_234_567_012_345_670
0O012_345_670_123_456_701_234_567

0d123_456_789_012_345_678_901_234
0D987_654_321_098_765_432_109_876

0z123_456_789_xe0_123_456_789_xe0
0ZEX9_876_543_210_987_654_321_09E

0x1234_5678_9abc_def0_1234_5678_9abc_def0
0XFEDC_BA98_7654_3210_fedc_ba98_7654_3210

0u0123456789abcdefghijklmnopqrstuv0123456
0UVUTSRQPONMLKJIHGFEDCBA9876543210vutsrq

0r0123456789abcdefghijklmnopqrstuvwxyz012345
0RZYXWVUTSRQPONMLKJIHGFEDCBA9876543210zyxwv

0y0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!$
0Y$!ZYXWVUTSRQPONMLKJIHGFEDCBA9876543210zyxwvutsrqponmlkjihgfedcba

//Units TODO




[a, b, c] = [1 2 3]                                         //a=1, b=2, c=3
[a, [b, c]] = [1 [2 3]]                                     //a=1, b=2, c=3
[a, [b, c], d] = [1 [2 3] 4]                                //a=1, b=2, c=3, d=4
[a, ...b] = [1 2 3 4]                                       //a=1, b=[2 3 4]
[a, ...b, c] = [1 2 3 4 5]                                  //a=1, b=[2 3 4], c=5
[a, ...b, [c, [...d, e, f]]] = [1 2 3 4 [5 [6 7 8 9 10]]]   //a=1, b=[2 3 4], c=5, d=[6 7 8], e=9, f=10



// silly things that are technically valid
x = loop i in [0..10] i //x=10

y = [
    if something 
        x
    else loop i in something_else
        y
    else if z
        z
    else loop i in last_thing
        w
    else
        ()
]


apple & banana
apple&banana
apple | banana
apple|banana





///////////// String prefixes ////////////////
p = s:string => [
    //process s based on / and \ separators
    //store result in this object
    route:array<string> = ...
    filename:string? = ...
    extension:string? = ...
]

p"this/is/a/file/path.ext"

//other prefixes
re"[^i*&2@]"                            // regex literal
t'my_token'                             //token literal. probably my version of enums
r'this is a raw string'                 //raw string. technically handled during tokenizing, there is no r function
(dewy)r'''printl("Hello, World!")'''    //dewy source code literal. uses raw string so that we don't have to worry about {}.

ipa"ɛt vɔkavit dɛus aɾidam tɛɾam kɔngɾɛgatsiɔnɛskwɛ akwaɾum apɛlavit maɾia ɛt vidit dɛus kwɔd ɛsɛt bɔnum" //international phonetic alphabet literal
(apl)r"life ← {⊃1 ⍵ ∨.∧ 3 4 = +/ +⌿ ¯1 0 1 ∘.⊖ ¯1 0 1 ⌽¨ ⊂⍵}"  //apl expression literal
apl<|r"life ← {⊃1 ⍵ ∨.∧ 3 4 = +/ +⌿ ¯1 0 1 ∘.⊖ ¯1 0 1 ⌽¨ ⊂⍵}"  //same as above

'''this is a regular string with triple quotes'''
"""this is a regular string with triple quotes"""

///////////// object prefixes ////////////////
//doubly linked list
dll[1 2 3 4 6 5 3 6 3 2]

//set literal syntax
set[4 3 6 4 6 4 2 2 4 5]



// silly example for generating a list of ones
ones = n => {l = [...[1..n]] l.=1 l}
ones(10) // [1 1 1 1 1 1 1 1 1 1]
//alternate
ones = n => [loop i in 1..n 1]11:[["$","$13",null,{"fallback":null,"children":["$","$L14",null,{"children":["$","$L15",null,{"dewy_interpreter_source":[{"name":"backend","code":"$16"},{"name":"dewy","code":"$17"},{"name":"frontend","code":"$18"},{"name":"parser","code":"$19"},{"name":"postok","code":"$1a"},{"name":"tokenizer","code":"$1b"},{"name":"utils","code":"$1c"}],"dewy_examples":{"good_examples":[{"name":"hello.dewy","code":"printl'Hello, World!'"},{"name":"hello_func.dewy","code":"main = () => printl'Hello, World!'\nmain"},{"name":"hello_name.dewy","code":"print\"What's your name? \"\nname = readl\nprintl'Hello {name}!'"},{"name":"hello_loop.dewy","code":"print\"What's your name? \"\nname = readl\ni = 0\nloop i <? 10 {\n    printl'Hello {name}!'\n    i = i + 1\n}"},{"name":"anonymous_func.dewy","code":"(() => printl'Hello, World!')()"},{"name":"if_else.dewy","code":"print\"What's your name? \"\nname = readl\nif name =? 'Alice' printl'Hello Alice!'\nelse printl'Hello stranger!'"},{"name":"if_else_if.dewy","code":"print\"What's your name? \"\nname = readl\nif name =? 'Alice' printl'Hello Alice!'\nelse if name =? 'Bob' printl'Hello Bob!'\nelse printl'Hello stranger!'"},{"name":"dangling_else.dewy","code":"// if a then if b then s else s2\n// See: https://en.wikipedia.org/wiki/Dangling_else\n\na = false\nb = true\n\nif a\n    if b\n        printl's'\n    else\n        printl's2'\nelse\n    printl's3'"},{"name":"if_tree.dewy","code":"$1d"},{"name":"loop_in_iter.dewy","code":"loop i in [0,2..20)\n    printl(i)"},{"name":"loop_and_iters.dewy","code":"loop i in [0..] and j in [0,2..20]\n    printl'{i} and {j}'"},{"name":"loop_or_iters.dewy","code":"loop i in [0..20] or j in [0,2..10] \n    printl'{i} or {j}'"},{"name":"nested_loop.dewy","code":"loop i in [0,2..10]\n    loop j in [0,2..10]\n        printl'{i},{j}'"},{"name":"block_printing.dewy","code":"loop i in [0,2..5] {\n    loop j in [0,2..5] {\n        loop k in [0,2..5] {\n            loop l in [0,2..5] {\n                loop m in [0,2..5] {\n                    printl'{i},{j},{k},{l},{m}'\n                }\n            }\n        }\n    }\n}"}],"bad_examples":[{"name":"loop_iter_manual.dewy","code":"it = iter[0,2..10]\n[cond, i] = next(it)\nloop cond {\n    printl(i)\n    [cond, i] = next(it)\n}"},{"name":"range_iter_test.dewy","code":"r = [0,2..20]\nit = iter(r)\nprintl(next(it))\nprintl(next(it))\nprintl(next(it))\nprintl(next(it))\nprintl(next(it))\nprintl(next(it))\nprintl(next(it))\nprintl(next(it))\nprintl(next(it))\nprintl(next(it))\nprintl(next(it)) //last iteration. should return [true, 20]\nprintl(next(it)) //should return [false, undefined]\nprintl(next(it))\nprintl(next(it))"},{"name":"unpack_test.dewy","code":" s = ['Hello' ['World' '!'] 5 10]\nprintl's={s}'\na, b, c, d = s\nprintl'a={a} b={b} c={c} d={d}'\na, ...b = s\nprintl'a={a} b={b}'\n...a, b = s\nprintl'a={a} b={b}'\na, [b, c], ...d = s\nprintl'a={a} b={b} c={c} d={d}'\n\n//error tests\n//a, b, c, d, e = s         //error: not enough values to unpack\n//a, b = s                  //error: too many values to unpack\n//a, ...b, c, d, e, f = s   //error: too many values to unpack\n\n//TBD how unpack would handle `a, ...b, c, d, e = s`. Probably b would be empty?\n"},{"name":"fizzbuzz0.dewy","code":"taps = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nloop i in [0..100)\n{\n    printed_words = false\n    loop [tap string] in taps \n    {\n        if i % tap =? 0 \n        { \n            print(tap)\n            printed_words = true\n        }\n    }\n    if not printed_words print(i)\n    printl()\n}"},{"name":"fizzbuzz1.dewy","code":"taps = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nrange = [0..100)\n\n//indexing at [, ..] and [..,] adds singleton dimensions\nword_bools = range[, ..] .% taps.keys[..,] .=? 0\n\n// ` means transpose, which behaves like python's zip()\nwords_grid = [taps.values word_bools]`.map(\n    [word bools] => bools.map(b => if b word else '')\n)\n\nraw_lines = word_grid`.map(line_words => line_words.join(''))\n\nlines = [raw_lines range]`.map(\n    (raw_line, i) => if raw_line.length =? 0 '{i}' else raw_line\n)\n\nlines.join('\\n') |> printl"},{"name":"random.dewy","code":"$1e"},{"name":"fast_inverse_sqrt.dewy","code":"\n\n// fast inverse square-root. see: https://en.wikipedia.org/wiki/Fast_inverse_square_root#Overview_of_the_code\nfast_isqrt = (x:f32) => {\n    let y:f32, i:u32\n    \n    i = 0.5x transmute u32      // evil floating point bit level hacking\n    i = 0x5f3759df - (i >> 1)   // what the fuck?\n    y = i transmute f32\n    y *= 1.5 - (0.5x)y^2        // 1st iteration of newton's method\n    //y *= 1.5 - (0.5x)y^2      // 2nd iteration (optional)\n\n    return y\n}\n\n\n//TODO: use autodiff to calculate the derivative automatically?\n//isqrt = (x:number) => 1/x^0.5\n//diff(isqrt)"},{"name":"rule110.dewy","code":"// proof that dewy is turing complete\n// rule 110 would grow the vector from the front, so instead we reverse everything for efficiency\n// for now use parenthesis where precedence filter needed. eventually should be able to remove with precedence filter\n\nprogress = world:vector<bit> => {\n    update:bit = 0\n    loop i in 0..world.length\n    {\n        if i >? 0 world[i-1] = update //TODO: #notfirst handled by compiler unrolling the loop into prelude, interludes, and postlude\n        update = 0b01110110 << (world[i-1..i+1] .?? 0 .<< [2 1 0])\n    }\n    world.push(update)\n}\n\nworld: vector<bit> = [1]\nloop true\n{\n    printl(world)\n    progress(world)\n}"},{"name":"dewy_syntax_examples.dewy","code":"$1f"},{"name":"syntax.dewy","code":"$20"},{"name":"tokenizer.dewy","code":"//demo of manual dewy tokenizer written in dewy\n\n// (template) instance of the token class\nTokenBase = [\n    name = 'Token'\n    __repr__ = () => '<{name}>'\n]\nToken = type(TokenBase)\n\n\n// class constructor for Keyword token type\nKeyword = src:string => [\n    ...TokenBase\n    name = 'Keyword'\n    __repr__ = () => '<{name}: {value}>'\n]\n\n\neat_fn_type = func<src:string> => int? \n\n\n/{\n    Eat a reserved keyword, return the number of characters eaten\n\n    #keyword = {in} | {as} | {loop} | {lazy} | {if} | {and} | {or} | {xor} | {nand} | {nor} | {xnor} | {not}; \n\n    noting that keywords are case insensitive\n}/\neat_keyword: func<src:string> => int? = src:string => {\n    keywords = ['in' 'as' 'loop' 'lazy' 'if' 'and' 'or' 'xor' 'nand' 'nor' 'xnor' 'not']\n    max_len = [loop k in keywords k.length].max\n    lower_src = src[..max_len].lowercase()\n    loop k in keywords\n        if lower_src.startswith(k)\n            return k.length\n    return undefined // tbd if this is optional\n}\n"}]}}]}]}],["$","h3",null,{"className":"text-2xl mt-6 mb-2 font-quadon","children":"About"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"Dewy is a programming language I have been developing off and on since 2016. It is a general purpose language, designed with engineering applications in mind. Think the functionality and ease of use of matlab or python combined with the speed of a compiled language like C or Rust, but with its own unique flare."}],["$","p",null,{"className":"text-xl font-gentona text-justify mb-2","children":"Some key planned features include:"}],["$","ul",null,{"className":"list-disc mb-6 pl-10 text-xl font-gentona","children":[["$","li",null,{"children":[["$","strong",null,{"children":"Functional and Imperative"}]," - Dewy is an imperative language with strong support for functional programming. This allows for a very flexible programming style, where you can use the best tool for the job."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Expression based syntax"}]," - Dewy uses an expression based syntax, meaning that everything is an expression. This allows for a very simple yet powerful syntax, where common language features often are just a free consequence of the syntax"]}],["$","li",null,{"children":[["$","strong",null,{"children":"Garbage-collector-free memory management"}]," - Dewy uses a unique memory management system, allowing for fast and efficient memory management without the need for a garbage collector."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Strong type system"}]," - Dewy has a powerful static type system with inference, reminiscent of those in Typescript and Julia."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Built in unit system"}]," - Dewy has a built in unit system, allowing you to easily work with units and convert between them. This is especially useful for engineering applications."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Strong math support"}]," - Dewy has strong support many math features, including complex numbers, quaternions, vectors, matrices, and more. This is especially useful for engineering applications."]}]]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"An example of the common FizzBuzz program implemented in Dewy might look like this:"}],["$","$L21",null,{"src":"multiples = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nloop i in [0..100)\n{\n    printed_words = false\n    loop [multiple word] in multiples \n    {\n        if i % multiple =? 0 \n        { \n            print(multiple)\n            printed_words = true\n        }\n    }\n    if not printed_words print(i)\n    printl()\n}"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"Or a more functional style implementation might look like this:"}],["$","$L21",null,{"src":"multiples = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nrange = [0..100)\n\n//indexing at [, ..] and [..,] adds singleton dimensions\nword_bools = range[, ..] .% multiples.keys[..,] .=? 0\n\n// ` means transpose, which behaves like python's zip()\nword_grid = [multiples.values word_bools]`.map(\n[word bools] => bools.map(b => if b word else '')\n)\n\nraw_lines = word_grid`.map(line_words => line_words.join(''))\n\nlines = [raw_lines range]`.map(\n    (raw_line, i) => if raw_line.length =? 0 '{i}' else raw_line\n)\n\nlines.join('\\n') |> printl"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"For clarity, the variables at each step look like so:"}],["$","$L21",null,{"src":"word_bools = [[true false false true false false true false ...]\n             [true false false false false true false false ...]]\n\nword_grid = [['Fizz' '' '' 'Fizz' '' '' 'Fizz' '' '' 'Fizz' '' '' ...]\n            ['Buzz' '' '' '' '' 'Buzz' '' '' '' '' 'Buzz' '' '' ...]]\n\nraw_lines = ['FizzBuzz' '' '' 'Fizz' '' 'Buzz' 'Fizz' '' '' 'Fizz' 'Buzz' '' ...]\n\nlines = ['FizzBuzz' '1' '2' 'Fizz' '4' 'Buzz' 'Fizz' '7' '8' 'Fizz' 'Buzz' '11' ...]"}],["$","h3",null,{"className":"text-2xl mt-6 mb-2 font-quadon","children":"Current Status"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"Currently I'm working through a simple interpreter for the language (powering the demo above). I've got a tokenizer, a basic interpreter backend, and handling of a few basic types of syntaxes via the parser. But the majority of the syntax is still unimplemented, hence the long list of \"Broken Examples\". Thus the current focus is finishing parser support for the rest of the syntax features."}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["Previously I had been doing a lot of development on bleeding edge"," ",["$","$L22",null,{"href":"/projects/dewy_old","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"parser generators"}],", but that ended up being too big of a time sink for not much visible progress. Instead, for the time being, I ended up just hand rolling a parser in python, which has led to actually runnable code! I'll definitely revisit parser generators in the future when the language is further along."]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["After the parser is complete, the next steps will be working on compiling to"," ",["$","$L22",null,{"href":"https://en.wikipedia.org/wiki/LLVM#Intermediate_representation","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"LLVM IR"}]," (and possibly to C as a portable alternative), as well as starting to build out the standard library, and then bootstrapping the compiler to be able to compile itself."]}],["$","h3",null,{"className":"text-2xl mt-6 mb-2 font-quadon","children":"About the Demo"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["The demo above was actually pretty complex to put together. The current interpreter is written in python, and this website is statically hosted, which meant the demo required some way to statically run python code without a server. For this, I used ",["$","$L22",null,{"href":"https://pyodide.org/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Pyodide"}],", which is basically ",["$","$L22",null,{"href":"https://github.com/python/cpython","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"CPython"}]," compiled to"," ",["$","$L22",null,{"href":"https://webassembly.org/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"WebAssembly"}]," via"," ",["$","$L22",null,{"href":"https://emscripten.org/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Emscripten."}]]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["Pyodide itself isn't too difficult to use, except for the fact that it doesn't have good support for asynchronous standard input—it really wants to halt the entire UI while you type input into a stock browser popup prompt. To get around this, I found a"," ",["$","$L22",null,{"href":"https://www.npmjs.com/package/sync-message","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"handy library"}]," where you run pyodide in a web worker, and then any time it wants to read input, the worker makes a synchronous XHR request to a service worker, blocking the pyodide web worker until the service worker receives a response from the main thread with the input, which the service worker can then pass back to the pyodide worker. Suffice it to say, I don't think I ever want to deal with service workers again."]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["Now that python is handled, the next aspect is getting the Dewy interpreter itself to run. For this, I fetch (at website build time) the source code directly from"," ",["$","$L22",null,{"href":"https://github.com/david-andrew/dewy-lang/tree/master/src/compiler","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"github"}],". I then abuse the python import lib to allow loading \"modules\" directly from strings, and then pass all of the dewy source in as modules. Then I have a little wrapper function for the entry point which receives the source code string, and runs the program. The entry point can then be called from the browser via a javascript wrapper function."]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["The final piece of the puzzle is the text entry, and terminal emulator. For text input, I'm using the ",["$","$L22",null,{"href":"https://codemirror.net/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Code Mirror Library"}],"with a custom syntax highlighter (actually the current syntax highlighter was for the"," ",["$","$L22",null,{"href":"/projects/dewy_old","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Dewy meta language"}],", but it works well enough in the meantime). For the terminal, I use the ",["$","$L22",null,{"href":"https://xtermjs.org/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"xterm.js"}]," library. I then hooked up stdin and stdout from pyodide to interact with the terminal, and voila! A Dewy interpreter running in the browser."]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"There are definitely some rough edges, and the parser only supports a small handful of features, but it runs! It's probably the easiest way to try out the language, and I'm looking forward to getting all of the broken example programs working!"}],["$","h3",null,{"className":"text-2xl mt-6 mb-2 font-quadon","children":"Links"}],["$","div",null,{"className":"flex flex-col space-y-3","children":[["$","div",null,{"className":"flex flex-row text-sm items-center","children":[["$","$L23",null,{"src":{"src":"/_next/static/media/github.8d24eda3.svg","height":484,"width":496,"blurWidth":0,"blurHeight":0},"alt":"github icon","className":"inline-block w-8 h-8 mr-2  pointer-events-none select-none","draggable":false}],["$","span",null,{"className":"align-middle","children":["$","$L22",null,{"href":"https://github.com/david-andrew/dewy-lang","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Github Repo"}]}]]}],["$","div",null,{"className":"flex flex-row text-sm items-center","children":[["$","$L23",null,{"src":{"src":"/_next/static/media/docs.71257e0f.svg","height":512,"width":384,"blurWidth":0,"blurHeight":0},"alt":"docs icon","className":"inline-block w-8 h-8 mr-2  pointer-events-none select-none","draggable":false}],["$","span",null,{"className":"align-middle","children":["$","$L22",null,{"href":"https://david-andrew.github.io/dewy-lang/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Language Docs"}]}]]}]]}]]
