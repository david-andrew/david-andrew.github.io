1:HL["/_next/static/media/a34f9d1faa5f3315-s.p.woff2",{"as":"font","type":"font/woff2"}]
2:HL["/_next/static/css/5dd63daf860ee956.css",{"as":"style"}]
0:["QGc1ptBJMt9C2r7px7Yxc",[[["",{"children":["projects",{"children":["dewy",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],"$L3",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/5dd63daf860ee956.css","precedence":"next"}]],"$L4"]]]]
5:HL["/_next/static/css/85fa6dafca566008.css",{"as":"style"}]
3:[null,"$L6",null]
4:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"David Samson"}],["$","meta","2",{"name":"description","content":"Generated by create next app"}],["$","meta","3",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","link","4",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","meta","5",{"name":"next-size-adjust"}]]
7:I{"id":27250,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","5570:static/chunks/app/projects/layout-2b3afbf44649a362.js"],"name":"Navbar","async":false}
8:I{"id":19885,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","5570:static/chunks/app/projects/layout-2b3afbf44649a362.js"],"name":"GithubTimestampsProvider","async":false}
9:I{"id":19885,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","5570:static/chunks/app/projects/layout-2b3afbf44649a362.js"],"name":"ProjectsContextProvider","async":false}
a:I{"id":47767,"chunks":["2272:static/chunks/webpack-c3ade57f7d74aaca.js","2971:static/chunks/fd9d1056-bc91c3b3fa0b78fd.js","596:static/chunks/596-fc24fa4abd14b1c2.js"],"name":"default","async":false}
b:I{"id":57920,"chunks":["2272:static/chunks/webpack-c3ade57f7d74aaca.js","2971:static/chunks/fd9d1056-bc91c3b3fa0b78fd.js","596:static/chunks/596-fc24fa4abd14b1c2.js"],"name":"default","async":false}
d:I{"id":49488,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","8861:static/chunks/8861-dd878d5de052195d.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","3185:static/chunks/app/layout-aa0d19882be48f7d.js"],"name":"ColorPicker","async":false}
6:["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","script",null,{"src":"/pyodideCommsService.js","async":true}]}],["$","body",null,{"className":"__className_d65c78 overflow-hidden bg-slate-900","children":["$","div",null,{"className":"w-screen h-screen bg-black overflow-hidden","children":[["$","$L7",null,{}],["$","div",null,{"style":{"height":"calc(100vh - var(--navbar-height))"},"className":"overflow-x-hidden","children":["$","$L8",null,{"children":["$","$L9",null,{"children":["$","$La",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":["$","div",null,{"className":"fixed w-screen h-screen bg-black flex justify-center items-center","children":["$","div",null,{"role":"status","children":[["$","svg",null,{"aria-hidden":"true","className":"w-32 h-32 mr-2 text-gray-200 animate-spin dark:text-gray-600 fill-accent","viewBox":"0 0 100 101","fill":"none","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M100 50.5908C100 78.2051 77.6142 100.591 50 100.591C22.3858 100.591 0 78.2051 0 50.5908C0 22.9766 22.3858 0.59082 50 0.59082C77.6142 0.59082 100 22.9766 100 50.5908ZM9.08144 50.5908C9.08144 73.1895 27.4013 91.5094 50 91.5094C72.5987 91.5094 90.9186 73.1895 90.9186 50.5908C90.9186 27.9921 72.5987 9.67226 50 9.67226C27.4013 9.67226 9.08144 27.9921 9.08144 50.5908Z","fill":"currentColor"}],["$","path",null,{"d":"M93.9676 39.0409C96.393 38.4038 97.8624 35.9116 97.0079 33.5539C95.2932 28.8227 92.871 24.3692 89.8167 20.348C85.8452 15.1192 80.8826 10.7238 75.2124 7.41289C69.5422 4.10194 63.2754 1.94025 56.7698 1.05124C51.7666 0.367541 46.6976 0.446843 41.7345 1.27873C39.2613 1.69328 37.813 4.19778 38.4501 6.62326C39.0873 9.04874 41.5694 10.4717 44.0505 10.1071C47.8511 9.54855 51.7191 9.52689 55.5402 10.0491C60.8642 10.7766 65.9928 12.5457 70.6331 15.2552C75.2735 17.9648 79.3347 21.5619 82.5849 25.841C84.9175 28.9121 86.7997 32.2913 88.1811 35.8758C89.083 38.2158 91.5421 39.6781 93.9676 39.0409Z","fill":"currentFill"}]]}],["$","span",null,{"className":"sr-only","children":"Loading..."}]]}]}],"loadingStyles":[],"hasLoading":true,"error":"$undefined","errorStyles":"$undefined","template":["$","$Lb",null,{}],"templateStyles":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"childProp":{"current":[null,"$Lc",null],"segment":"projects"},"styles":[]}]}]}]}],["$","$Ld",null,{}]]}]}]]}]
e:I{"id":19885,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","5570:static/chunks/app/projects/layout-2b3afbf44649a362.js"],"name":"GithubTimestampsFetcher","async":false}
f:I{"id":52160,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","5570:static/chunks/app/projects/layout-2b3afbf44649a362.js"],"name":"Heading","async":false}
12:I{"id":27250,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","5570:static/chunks/app/projects/layout-2b3afbf44649a362.js"],"name":"NavbarDummy","async":false}
c:[["$","$Le",null,{"projects":[{"title":"Blob Opera Performances","imgSrc":{"src":"/_next/static/media/blob_opera_nox.e6f3aa2a.png","height":414,"width":512,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAJ1BMVEU4EgowEQlEIRV0Vzx9ZEo+JhlXOyBCT1RTMx0+LyVEUlxuTS5AS1HBZ0aTAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALklEQVR4nC3IuREAIAwDMMcOeYD95+XgKNQIhpahHYwUawuGoQuM6az0OwvPnwMUSwDO9qMceAAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":6},"summary":"Virtual choir performances leveraging the blob opera as a front end for voice synthesis","lastUpdated":"February 2021","tags":["Python","Blob Opera","choir","music","synthesis"],"route":"blob_opera"},{"title":"Boat Simulator","imgSrc":{"src":"/_next/static/media/boat_simulator.7cc391fd.jpg","height":1280,"width":2048,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAb/xAAdEAABAgcAAAAAAAAAAAAAAAAAAQQCAxESFSFT/8QAFAEBAAAAAAAAAAAAAAAAAAAABf/EABYRAAMAAAAAAAAAAAAAAAAAAAABEf/aAAwDAQACEQMRAD8AkFks7K49tuLmABWIOrP/2Q==","blurWidth":8,"blurHeight":5},"summary":"Spring 2017 HopHacks submission","lastUpdated":"March 2017","tags":["Unity","C#","3D game"],"route":"boat_simulator"},{"title":"Bueller Board","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Fall 2015 HopHacks submission: Midi keyboard that used user provided audio samples, a.k.a. the 'goat keyboard'","lastUpdated":"September 2015","tags":["midi","music"],"route":"bueller_board"},{"title":"Composer","imgSrc":{"src":"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":8},"summary":"React based composing software that acts as a front-end for LilyPond","lastUpdated":"January 2021","tags":["React","TypeScript","SMuFL","LilyPond","music","composition"],"route":"composer"},{"title":"Choir Compositions","imgSrc":{"src":"/_next/static/media/music_staff.3145785a.png","height":1616,"width":2745,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAHlBMVEUGBgZISEhSUlIeHh5paWk+Pj4vLy9cXFx7e3ufn5/bdycMAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAJ0lEQVR4nCXBtwEAIAwDMLe0/x9mQIKFjwN3jZBwxGowpFZGm7cpPAkxAH3f0FE4AAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":5},"summary":"","lastUpdated":"May 2015","tags":["music","choral","composition"],"route":"compositions"},{"title":"Dewy Programming Language","github":"dewy","imgSrc":{"src":"/_next/static/media/dewy_dandelion.e2efa7ee.jpg","height":1600,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z","blurWidth":8,"blurHeight":8},"summary":"An engineering focused programming language I am developing","tags":["Python","compilers","parsers","LLVM","Programming Languages"],"route":"dewy"},{"title":"Generalized Parsing","lastUpdated":"2022-02-06","imgSrc":{"src":"/_next/static/media/dewy_dandelion.e2efa7ee.jpg","height":1600,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z","blurWidth":8,"blurHeight":8},"summary":"Previous work on the Dewy Programming Language, namely a custom SRNGLR parser written entirely in C","tags":["C","compilers","parsers","SRNGLR","LLVM"],"route":"dewy_old"},{"title":"UR5 Draw Robot","imgSrc":{"src":"/_next/static/media/mona_lisa_contour.9bc5cbc9.jpg","height":1016,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAeEAACAgEFAQAAAAAAAAAAAAABAgAEAwUHEiFRsf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAExAv/aAAwDAQACEQMRAD8AtbT3HvV9WGZVPB8RHXqsT8iIgoVqn//Z","blurWidth":8,"blurHeight":5},"summary":"UR5 robot arm project","lastUpdated":"December 2017","tags":["Matlab","UR5 robot","ROS"],"route":"drawbot"},{"title":"EasyREPL","imgSrc":{"src":"/_next/static/media/pypi_logo.f6536a80.svg","height":58,"width":66,"blurWidth":0,"blurHeight":0},"summary":"Python package for easily creating Read-Eval-Print Loops (REPLs)","github":"easyrepl","tags":["Python","PyPI","REPL"],"route":"easyrepl"},{"title":"Hacking Harmony or The Demon Chipmunk Choir","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"Ensemble","summary":"2019 Peabody Hackathon Submission. Choral music synthesis via autotuned google text-to-speech, AKA the demon chipmunk choir","tags":["Google text-to-speech API","matlab","python"],"route":"ensemble_peabody"},{"title":"Escort Mission 2020","imgSrc":{"src":"/_next/static/media/escort_mission_lamb.4c525bc4.png","height":128,"width":128,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAHlBMVEUA/gAAugD/+//h9+EAdAAirCKG/YZf2l9ri2uA/4BNpwkIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAK0lEQVR4nE2KuREAMAyDbPnff+HcSU1ooMDsI1P2XWdURDEOOMYAo6dbj3gNJgBhtLya0gAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":8},"github":"escort_mission_2020","summary":"Submission for the 2020 GMTK Game Jam","tags":["Godot","GDScript","2D game"],"route":"escort_mission"},{"title":"Foxing Animatronic","imgSrc":{"src":"/_next/static/media/foxing_animatronic.91d20002.png","height":1252,"width":1540,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAJ1BMVEUGFwsJIxEILhUiJQkdSRwWIQoUORYULA8RRSArMAYZEwYyVzM6OglI+AgfAAAACXBIWXMAAAsTAAALEwEAmpwYAAAANElEQVR4nBXKuREAIAwEsT2/GOi/XgbFoqs9I6DvdY8CW7O9DBSzvQ3OKFOCE5Vp+mdJBg8iegD5GB/ChgAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":7},"summary":"Manually actuated animatronic robot featured in the Foxing music video 'Slapstick'","lastUpdated":"June 2018","tags":["Solidworks","mechanical design","animatronic","Foxing","music"],"route":"foxing_animatronic"},{"title":"Mechatronics Robots","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Robots from mechatronics","lastUpdated":"May 2019","tags":["Arduino","C++","SolidWorks","mechanical design"],"route":"mechatronics"},{"title":"Mehve (Working Title)","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"mehve","summary":"3D adventure game inspire by \"Nausicaa of the Valley of the Wind\"","tags":["Godot","GDScript","3D game"],"route":"mehve"},{"title":"Musical DL","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"MusicalDL","summary":"Using deep learning to generate choral music in the style of JS Bach","tags":["Python","Pytorch","AI/ML","choral","music","generation"],"route":"musical_dl"},{"title":"NAND 2 Tetris","imgSrc":{"src":"/_next/static/media/nand2tetris.660feb3d.png","height":346,"width":396,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAPFBMVEUAAABvKgIhHhw+KR0uJAFlRBU2GAYJBAENDw/TVQF/ZRBuWwrEoxytlRBiVBJJHQC7SwG5YCWSOwDqziPUksrRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAN0lEQVR4nCWKWw7AIAyA0Fbbqnvo7n/XZZMfEgKAscllu870J7tj2Tk6NSK151BcLilNwd3z974dVQEdrZJh0wAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":7},"github":"nand2tetris","summary":"A 16-bit computer built from scratch, starting with NAND gates","tags":["Computer Architecture","HDL","Assembly","Hack","Jack","Compilers","Operating Systems","Virtual Machines"],"route":"nand2tetris"},{"title":"PDF Chatter","imgSrc":{"src":"/_next/static/media/pypi_logo.f6536a80.svg","height":58,"width":66,"blurWidth":0,"blurHeight":0},"summary":"LLM powered Q&A over extracted PDF text","github":"pdf-chatter","tags":["Python","Optical Character Recognition (OCR)","Large Language Models (LLMs)","Nougat-OCR","GPT-4"],"route":"pdf_chatter"},{"title":"pOngBot","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Autonomous beer pong playing robot","lastUpdated":"June 2020","tags":["Arduino","C++","computer vision","Viola-Jones","mechanical design"],"route":"pongbot"},{"title":"PRS19: Fret Press Robot","imgSrc":{"src":"/_next/static/media/prs2019_preview.aed05a2a.png","height":2093,"width":3061,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAPFBMVEVJPTmhmIiCa1SBfXdBQD1IOzB3bGA6My5dUkyXlpNPRTyio6ChpadSTUm0q5e+taCun2xwYl2qrq5mXU/I9hK/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAL0lEQVR4nAXBhwEAIAjAsKoo4B7//2rCXFZLjoGwh4poIj5TJRfOsAok3G/v3toHGpkBOY6KaOIAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":5},"github":"PRS_robot","summary":"Automatic guitar fret press robot. Mechanical Engineering Master's Design captsone project","tags":["C++","Arduino","mechanical design"],"route":"prs19"},{"title":"Rewind","imgSrc":{"src":"/_next/static/media/rewind_title.a1e43a09.png","height":540,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAElBMVEUqHjguITs4J0JmWW1PRFo/MUqO6aYhAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAH0lEQVR4nGNggAMmJkYmRhCDhZmFmZUBxGRkgIhAAQADKgAhbD4/MgAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":5},"summary":"2018 Video Game Desgn (EN.601.355) capstone project","lastUpdated":"May 2018","tags":["Unity","C#","2D game"],"route":"rewind"},{"title":"RoboJay","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"lastUpdated":"May 2018","summary":"A balancing robot designed to give campus tours to incoming JHU freshmen","tags":["robotics","feedback control","navigation","BeagleBone","ROS"],"route":"robojay"},{"title":"High Power Rocketry","imgSrc":{"src":"/_next/static/media/rebel_scum.34e7823c.jpg","height":1365,"width":2048,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAaEAACAgMAAAAAAAAAAAAAAAAAARESIjFB/8QAFQEBAQAAAAAAAAAAAAAAAAAAAgP/xAAXEQADAQAAAAAAAAAAAAAAAAAAAhNR/9oADAMBAAIRAxEAPwC6o4w5OwATo+immH//2Q==","blurWidth":8,"blurHeight":5},"summary":"Level 1 & 2 High Powered Rocket built with the Johns Hopkins Rocketry Club, and Spaceport America Cup 2018","lastUpdated":"January 2018","tags":["High Power Rocketry","Arduino","C++","mechanical design","Tripoli"],"route":"rocketry"},{"title":"Silver Void","imgSrc":{"src":"/_next/static/media/silver_void_cover_art.cc703812.png","height":1439,"width":1823,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAIVBMVEUCAgEWGhAMDQgiFiEnJyYwMC4aCRZHJygZDgQ9EDxLU0cSomUyAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALUlEQVR4nC3KyQkAMBDDQHntzdV/wSEQ/QaEYkeAutPhJ0NBFdv0kWDM57euCwrsAGo10HJLAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":6},"github":"SilverVoid","summary":"Submission for Acerola Jam 0","tags":["Godot","GDScript","3D Game","Space Simulator","Bullet Hell"],"route":"silver_void"},{"title":"so voice!","imgSrc":{"src":"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":8},"lastUpdated":"December 2022","summary":"Choral music synthesis with deep learning (Continuation of Musical DL)","tags":["Python","Pytorch","AI/ML","choral","music","synthesis"],"route":"so_voice"},{"title":"Terminal Ray Tracer","imgSrc":{"src":"/_next/static/media/terminal_ray_tracer.00db7e4a.png","height":2043,"width":2881,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAbFBMVEWSamvNrLWWP0yUKVeYinmmnKTsEheTcnnmh4+Xfo6pV2GMMDasvsa5aXiPRVFVs2uOWF9WnGxXtVObh4ypREzbYmnsPkXIba/y/P10bHmfbcouPtDAMDdJbZOtmERQbkXfwcqTb3g+yNdOcdnEQLLCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAOklEQVR4nAXBBQKAIAAAsSMl7MZG//9HN/Igu6YtYDT+7qnhSK++tI+IcH5BViXZqn0Tk8M8Ti3rbH9KhwLIHy1s0QAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":6},"github":"TerminalRayTracer","summary":"A dependency-free ray tracer written in C that runs directly in a linux terminal","tags":["C","ray tracing","CLI","linux"],"route":"terminal_ray_tracer"},{"title":"Cloud Timelapse","imgSrc":{"src":"/_next/static/media/timelapse.b57dd258.jpg","height":3468,"width":4624,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAGAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAcEAACAgIDAAAAAAAAAAAAAAAAAQIFAxETYdH/xAAVAQEBAAAAAAAAAAAAAAAAAAADBP/EABYRAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAwDAQACEQMRAD8AlG4r56fBlT6ivQAUqwYX/9k=","blurWidth":8,"blurHeight":6},"github":"timelapse","summary":"A simple python project for taking timelapses of clouds from a webcam","tags":["Python","OpenCV","Raspberry Pi","timelapse","clouds"],"route":"timelapse"},{"title":"uSkipSpoilers","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"uSkipSpoilers","summary":"A small chrome extension for blocking spoilers in YouTube videos","tags":["React","TypeScript","Chrome","Extension"],"route":"uskipspoilers"},{"title":"This Website","github":"david-andrew.github.io","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"externalLink":"https://github.com/david-andrew/david-andrew.github.io","summary":"This website, written in react/typescript","tags":["Next.js","React","TypeScript","Tailwind CSS","WebAssembly"],"route":"website"},{"title":"WSE18: Machine Shop Biometric Interlock","imgSrc":{"src":"/_next/static/media/wse18.7405315e.png","height":2054,"width":2456,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAUVBMVEU8OTSVjYNlX1nIwbpLQjaqnpBOSUOJgHU8MCsyKSXo4dUfHh1iU0R8aWMUDAdoYlull4ialI2BZkzBuKyNdleyrKaQosO9iGCNY0PTj19hbYD9ZLzaAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAO0lEQVR4nAXBhQHAMAwDMBdCZRj/f+gkiLgdPDJcpJS6MbxKOJEBa1pH5whYpXl9DfEoc2A90MJC671/OekCOef5C1cAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":7},"summary":"Biometric security interlock system. Mechanical Engineering Senior Design capstone project","lastUpdated":"May 2018","tags":["Raspberry Pi","Python","C++","Qt","interlock","fingerprint","biometric"],"route":"wse18"},{"title":"Ziggy V (Working Title)","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Concept for a Real-Time-Strategy crossed with First-Person-Shooter","lastUpdated":"January 2021","tags":["Godot","GDScript","FPS x RTS","3D game"],"route":"ziggy_v"}]}],["$","div",null,{"className":"mx-auto px-4 sm:px-6 lg:px-8 max-w-[1190px]","children":[["$","$Lf",null,{"projects":[{"title":"Blob Opera Performances","imgSrc":{"src":"/_next/static/media/blob_opera_nox.e6f3aa2a.png","height":414,"width":512,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAJ1BMVEU4EgowEQlEIRV0Vzx9ZEo+JhlXOyBCT1RTMx0+LyVEUlxuTS5AS1HBZ0aTAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALklEQVR4nC3IuREAIAwDMMcOeYD95+XgKNQIhpahHYwUawuGoQuM6az0OwvPnwMUSwDO9qMceAAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":6},"summary":"Virtual choir performances leveraging the blob opera as a front end for voice synthesis","lastUpdated":"February 2021","tags":["Python","Blob Opera","choir","music","synthesis"],"route":"blob_opera"},{"title":"Boat Simulator","imgSrc":{"src":"/_next/static/media/boat_simulator.7cc391fd.jpg","height":1280,"width":2048,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAb/xAAdEAABAgcAAAAAAAAAAAAAAAAAAQQCAxESFSFT/8QAFAEBAAAAAAAAAAAAAAAAAAAABf/EABYRAAMAAAAAAAAAAAAAAAAAAAABEf/aAAwDAQACEQMRAD8AkFks7K49tuLmABWIOrP/2Q==","blurWidth":8,"blurHeight":5},"summary":"Spring 2017 HopHacks submission","lastUpdated":"March 2017","tags":["Unity","C#","3D game"],"route":"boat_simulator"},{"title":"Bueller Board","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Fall 2015 HopHacks submission: Midi keyboard that used user provided audio samples, a.k.a. the 'goat keyboard'","lastUpdated":"September 2015","tags":["midi","music"],"route":"bueller_board"},{"title":"Composer","imgSrc":{"src":"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":8},"summary":"React based composing software that acts as a front-end for LilyPond","lastUpdated":"January 2021","tags":["React","TypeScript","SMuFL","LilyPond","music","composition"],"route":"composer"},{"title":"Choir Compositions","imgSrc":{"src":"/_next/static/media/music_staff.3145785a.png","height":1616,"width":2745,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAHlBMVEUGBgZISEhSUlIeHh5paWk+Pj4vLy9cXFx7e3ufn5/bdycMAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAJ0lEQVR4nCXBtwEAIAwDMLe0/x9mQIKFjwN3jZBwxGowpFZGm7cpPAkxAH3f0FE4AAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":5},"summary":"","lastUpdated":"May 2015","tags":["music","choral","composition"],"route":"compositions"},{"title":"Dewy Programming Language","github":"dewy","imgSrc":{"src":"/_next/static/media/dewy_dandelion.e2efa7ee.jpg","height":1600,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z","blurWidth":8,"blurHeight":8},"summary":"An engineering focused programming language I am developing","tags":["Python","compilers","parsers","LLVM","Programming Languages"],"route":"dewy"},{"title":"Generalized Parsing","lastUpdated":"2022-02-06","imgSrc":{"src":"/_next/static/media/dewy_dandelion.e2efa7ee.jpg","height":1600,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z","blurWidth":8,"blurHeight":8},"summary":"Previous work on the Dewy Programming Language, namely a custom SRNGLR parser written entirely in C","tags":["C","compilers","parsers","SRNGLR","LLVM"],"route":"dewy_old"},{"title":"UR5 Draw Robot","imgSrc":{"src":"/_next/static/media/mona_lisa_contour.9bc5cbc9.jpg","height":1016,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAeEAACAgEFAQAAAAAAAAAAAAABAgAEAwUHEiFRsf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAExAv/aAAwDAQACEQMRAD8AtbT3HvV9WGZVPB8RHXqsT8iIgoVqn//Z","blurWidth":8,"blurHeight":5},"summary":"UR5 robot arm project","lastUpdated":"December 2017","tags":["Matlab","UR5 robot","ROS"],"route":"drawbot"},{"title":"EasyREPL","imgSrc":{"src":"/_next/static/media/pypi_logo.f6536a80.svg","height":58,"width":66,"blurWidth":0,"blurHeight":0},"summary":"Python package for easily creating Read-Eval-Print Loops (REPLs)","github":"easyrepl","tags":["Python","PyPI","REPL"],"route":"easyrepl"},{"title":"Hacking Harmony or The Demon Chipmunk Choir","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"Ensemble","summary":"2019 Peabody Hackathon Submission. Choral music synthesis via autotuned google text-to-speech, AKA the demon chipmunk choir","tags":["Google text-to-speech API","matlab","python"],"route":"ensemble_peabody"},{"title":"Escort Mission 2020","imgSrc":{"src":"/_next/static/media/escort_mission_lamb.4c525bc4.png","height":128,"width":128,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAHlBMVEUA/gAAugD/+//h9+EAdAAirCKG/YZf2l9ri2uA/4BNpwkIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAK0lEQVR4nE2KuREAMAyDbPnff+HcSU1ooMDsI1P2XWdURDEOOMYAo6dbj3gNJgBhtLya0gAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":8},"github":"escort_mission_2020","summary":"Submission for the 2020 GMTK Game Jam","tags":["Godot","GDScript","2D game"],"route":"escort_mission"},{"title":"Foxing Animatronic","imgSrc":{"src":"/_next/static/media/foxing_animatronic.91d20002.png","height":1252,"width":1540,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAJ1BMVEUGFwsJIxEILhUiJQkdSRwWIQoUORYULA8RRSArMAYZEwYyVzM6OglI+AgfAAAACXBIWXMAAAsTAAALEwEAmpwYAAAANElEQVR4nBXKuREAIAwEsT2/GOi/XgbFoqs9I6DvdY8CW7O9DBSzvQ3OKFOCE5Vp+mdJBg8iegD5GB/ChgAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":7},"summary":"Manually actuated animatronic robot featured in the Foxing music video 'Slapstick'","lastUpdated":"June 2018","tags":["Solidworks","mechanical design","animatronic","Foxing","music"],"route":"foxing_animatronic"},{"title":"Mechatronics Robots","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Robots from mechatronics","lastUpdated":"May 2019","tags":["Arduino","C++","SolidWorks","mechanical design"],"route":"mechatronics"},{"title":"Mehve (Working Title)","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"mehve","summary":"3D adventure game inspire by \"Nausicaa of the Valley of the Wind\"","tags":["Godot","GDScript","3D game"],"route":"mehve"},{"title":"Musical DL","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"MusicalDL","summary":"Using deep learning to generate choral music in the style of JS Bach","tags":["Python","Pytorch","AI/ML","choral","music","generation"],"route":"musical_dl"},{"title":"NAND 2 Tetris","imgSrc":{"src":"/_next/static/media/nand2tetris.660feb3d.png","height":346,"width":396,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAPFBMVEUAAABvKgIhHhw+KR0uJAFlRBU2GAYJBAENDw/TVQF/ZRBuWwrEoxytlRBiVBJJHQC7SwG5YCWSOwDqziPUksrRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAN0lEQVR4nCWKWw7AIAyA0Fbbqnvo7n/XZZMfEgKAscllu870J7tj2Tk6NSK151BcLilNwd3z974dVQEdrZJh0wAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":7},"github":"nand2tetris","summary":"A 16-bit computer built from scratch, starting with NAND gates","tags":["Computer Architecture","HDL","Assembly","Hack","Jack","Compilers","Operating Systems","Virtual Machines"],"route":"nand2tetris"},{"title":"PDF Chatter","imgSrc":{"src":"/_next/static/media/pypi_logo.f6536a80.svg","height":58,"width":66,"blurWidth":0,"blurHeight":0},"summary":"LLM powered Q&A over extracted PDF text","github":"pdf-chatter","tags":["Python","Optical Character Recognition (OCR)","Large Language Models (LLMs)","Nougat-OCR","GPT-4"],"route":"pdf_chatter"},{"title":"pOngBot","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Autonomous beer pong playing robot","lastUpdated":"June 2020","tags":["Arduino","C++","computer vision","Viola-Jones","mechanical design"],"route":"pongbot"},{"title":"PRS19: Fret Press Robot","imgSrc":{"src":"/_next/static/media/prs2019_preview.aed05a2a.png","height":2093,"width":3061,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAPFBMVEVJPTmhmIiCa1SBfXdBQD1IOzB3bGA6My5dUkyXlpNPRTyio6ChpadSTUm0q5e+taCun2xwYl2qrq5mXU/I9hK/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAL0lEQVR4nAXBhwEAIAjAsKoo4B7//2rCXFZLjoGwh4poIj5TJRfOsAok3G/v3toHGpkBOY6KaOIAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":5},"github":"PRS_robot","summary":"Automatic guitar fret press robot. Mechanical Engineering Master's Design captsone project","tags":["C++","Arduino","mechanical design"],"route":"prs19"},{"title":"Rewind","imgSrc":{"src":"/_next/static/media/rewind_title.a1e43a09.png","height":540,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAElBMVEUqHjguITs4J0JmWW1PRFo/MUqO6aYhAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAH0lEQVR4nGNggAMmJkYmRhCDhZmFmZUBxGRkgIhAAQADKgAhbD4/MgAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":5},"summary":"2018 Video Game Desgn (EN.601.355) capstone project","lastUpdated":"May 2018","tags":["Unity","C#","2D game"],"route":"rewind"},{"title":"RoboJay","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"lastUpdated":"May 2018","summary":"A balancing robot designed to give campus tours to incoming JHU freshmen","tags":["robotics","feedback control","navigation","BeagleBone","ROS"],"route":"robojay"},{"title":"High Power Rocketry","imgSrc":{"src":"/_next/static/media/rebel_scum.34e7823c.jpg","height":1365,"width":2048,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAaEAACAgMAAAAAAAAAAAAAAAAAARESIjFB/8QAFQEBAQAAAAAAAAAAAAAAAAAAAgP/xAAXEQADAQAAAAAAAAAAAAAAAAAAAhNR/9oADAMBAAIRAxEAPwC6o4w5OwATo+immH//2Q==","blurWidth":8,"blurHeight":5},"summary":"Level 1 & 2 High Powered Rocket built with the Johns Hopkins Rocketry Club, and Spaceport America Cup 2018","lastUpdated":"January 2018","tags":["High Power Rocketry","Arduino","C++","mechanical design","Tripoli"],"route":"rocketry"},{"title":"Silver Void","imgSrc":{"src":"/_next/static/media/silver_void_cover_art.cc703812.png","height":1439,"width":1823,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAIVBMVEUCAgEWGhAMDQgiFiEnJyYwMC4aCRZHJygZDgQ9EDxLU0cSomUyAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALUlEQVR4nC3KyQkAMBDDQHntzdV/wSEQ/QaEYkeAutPhJ0NBFdv0kWDM57euCwrsAGo10HJLAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":6},"github":"SilverVoid","summary":"Submission for Acerola Jam 0","tags":["Godot","GDScript","3D Game","Space Simulator","Bullet Hell"],"route":"silver_void"},{"title":"so voice!","imgSrc":{"src":"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":8},"lastUpdated":"December 2022","summary":"Choral music synthesis with deep learning (Continuation of Musical DL)","tags":["Python","Pytorch","AI/ML","choral","music","synthesis"],"route":"so_voice"},{"title":"Terminal Ray Tracer","imgSrc":{"src":"/_next/static/media/terminal_ray_tracer.00db7e4a.png","height":2043,"width":2881,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAbFBMVEWSamvNrLWWP0yUKVeYinmmnKTsEheTcnnmh4+Xfo6pV2GMMDasvsa5aXiPRVFVs2uOWF9WnGxXtVObh4ypREzbYmnsPkXIba/y/P10bHmfbcouPtDAMDdJbZOtmERQbkXfwcqTb3g+yNdOcdnEQLLCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAOklEQVR4nAXBBQKAIAAAsSMl7MZG//9HN/Igu6YtYDT+7qnhSK++tI+IcH5BViXZqn0Tk8M8Ti3rbH9KhwLIHy1s0QAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":6},"github":"TerminalRayTracer","summary":"A dependency-free ray tracer written in C that runs directly in a linux terminal","tags":["C","ray tracing","CLI","linux"],"route":"terminal_ray_tracer"},{"title":"Cloud Timelapse","imgSrc":{"src":"/_next/static/media/timelapse.b57dd258.jpg","height":3468,"width":4624,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAGAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAcEAACAgIDAAAAAAAAAAAAAAAAAQIFAxETYdH/xAAVAQEBAAAAAAAAAAAAAAAAAAADBP/EABYRAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAwDAQACEQMRAD8AlG4r56fBlT6ivQAUqwYX/9k=","blurWidth":8,"blurHeight":6},"github":"timelapse","summary":"A simple python project for taking timelapses of clouds from a webcam","tags":["Python","OpenCV","Raspberry Pi","timelapse","clouds"],"route":"timelapse"},{"title":"uSkipSpoilers","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"uSkipSpoilers","summary":"A small chrome extension for blocking spoilers in YouTube videos","tags":["React","TypeScript","Chrome","Extension"],"route":"uskipspoilers"},{"title":"This Website","github":"david-andrew.github.io","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"externalLink":"https://github.com/david-andrew/david-andrew.github.io","summary":"This website, written in react/typescript","tags":["Next.js","React","TypeScript","Tailwind CSS","WebAssembly"],"route":"website"},{"title":"WSE18: Machine Shop Biometric Interlock","imgSrc":{"src":"/_next/static/media/wse18.7405315e.png","height":2054,"width":2456,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAUVBMVEU8OTSVjYNlX1nIwbpLQjaqnpBOSUOJgHU8MCsyKSXo4dUfHh1iU0R8aWMUDAdoYlull4ialI2BZkzBuKyNdleyrKaQosO9iGCNY0PTj19hbYD9ZLzaAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAO0lEQVR4nAXBhQHAMAwDMBdCZRj/f+gkiLgdPDJcpJS6MbxKOJEBa1pH5whYpXl9DfEoc2A90MJC671/OekCOef5C1cAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":7},"summary":"Biometric security interlock system. Mechanical Engineering Senior Design capstone project","lastUpdated":"May 2018","tags":["Raspberry Pi","Python","C++","Qt","interlock","fingerprint","biometric"],"route":"wse18"},{"title":"Ziggy V (Working Title)","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Concept for a Real-Time-Strategy crossed with First-Person-Shooter","lastUpdated":"January 2021","tags":["Godot","GDScript","FPS x RTS","3D game"],"route":"ziggy_v"}]}],["$","$La",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$Lb",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$","$La",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children","dewy","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$Lb",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$L10","$L11",null],"segment":"__PAGE__"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/85fa6dafca566008.css","precedence":"next"}]]}],"segment":"dewy"},"styles":[]}],["$","$L12",null,{}]]}]]
10:null
13:"$Sreact.suspense"
14:I{"id":33699,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","4769:static/chunks/4769-afbb25c509513e18.js","9659:static/chunks/9659-468d9b7f9560cf55.js","6413:static/chunks/6413-9fe1335479bb3ecd.js","6896:static/chunks/6896-a56055cb12157616.js","3215:static/chunks/3215-a5826776a257bdd1.js","1134:static/chunks/app/projects/dewy/page-f861bb2ba8d69ab6.js"],"name":"NoSSR","async":false}
15:I{"id":43215,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","4769:static/chunks/4769-afbb25c509513e18.js","9659:static/chunks/9659-468d9b7f9560cf55.js","6413:static/chunks/6413-9fe1335479bb3ecd.js","6896:static/chunks/6896-a56055cb12157616.js","3215:static/chunks/3215-a5826776a257bdd1.js","1134:static/chunks/app/projects/dewy/page-f861bb2ba8d69ab6.js"],"name":"","async":false}
27:I{"id":53123,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","4769:static/chunks/4769-afbb25c509513e18.js","9659:static/chunks/9659-468d9b7f9560cf55.js","6413:static/chunks/6413-9fe1335479bb3ecd.js","6896:static/chunks/6896-a56055cb12157616.js","3215:static/chunks/3215-a5826776a257bdd1.js","1134:static/chunks/app/projects/dewy/page-f861bb2ba8d69ab6.js"],"name":"DewyCodeBlock","async":false}
28:I{"id":46685,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","4769:static/chunks/4769-afbb25c509513e18.js","9659:static/chunks/9659-468d9b7f9560cf55.js","6413:static/chunks/6413-9fe1335479bb3ecd.js","6896:static/chunks/6896-a56055cb12157616.js","3215:static/chunks/3215-a5826776a257bdd1.js","1134:static/chunks/app/projects/dewy/page-f861bb2ba8d69ab6.js"],"name":"","async":false}
29:I{"id":63222,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","4769:static/chunks/4769-afbb25c509513e18.js","9659:static/chunks/9659-468d9b7f9560cf55.js","6413:static/chunks/6413-9fe1335479bb3ecd.js","6896:static/chunks/6896-a56055cb12157616.js","3215:static/chunks/3215-a5826776a257bdd1.js","1134:static/chunks/app/projects/dewy/page-f861bb2ba8d69ab6.js"],"name":"Image","async":false}
16:T57ab,from dataclasses import dataclass, field
from functools import cache
from typing import Protocol, TypeVar, Generator, Sequence, cast, overload, Literal as TypingLiteral


from .syntax import (
    AST,
    Type,
    PointsTo, BidirPointsTo,
    ListOfASTs, Block, Array, Group, Range, ObjectLiteral, Dict, BidirDict, UnpackTarget,
    TypedIdentifier,
    Void, void, Undefined, undefined, untyped,
    String, IString,
    Flowable, Flow, If, Loop, Default,
    Identifier, Express, Declare,
    PrototypeBuiltin, Call, Access, Index,
    Assign,
    Int, Bool,
    Range, IterIn,
    BinOp,
    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,
    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    UnaryPrefixOp, UnaryPostfixOp,
    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv, AtHandle,
    CycleLeft, CycleRight, Suppress,
    BroadcastOp,
    CollectInto, SpreadOutFrom,
    DeclarationType,
)
from .postparse import FunctionLiteral, Signature



import pdb




class Literal(AST):
    value: AST
    def __str__(self) -> str:
        return f'{self.value}'

class TBD(AST):
    """For representing values where the type is underconstrained"""
    def __str__(self) -> str:
        return '<TBD>'

class Fail(AST):
    """For representing values that typechecking fails on"""
    reason: str|None = None
    def __str__(self) -> str:
        return '<Fail>'

TypeExpr = Type | And | Or | Not | Literal | TBD | Fail



# Scope class only used during parsing to keep track of callables
@dataclass
class Scope:
    @dataclass
    class _var():
        # name:str #name is stored in the dict key
        decltype: DeclarationType
        type: TypeExpr
        value: AST

    parent: 'Scope | None' = None
    # callables: dict[str, AST | None] = field(default_factory=dict) #TODO: maybe replace str->AST with str->signature (where signature might be constructed based on the func structure)
    vars: 'dict[str, Scope._var]' = field(default_factory=dict)

    @overload
    def get(self, name:str, throw:TypingLiteral[True]=True, search_parents:bool=True) -> 'Scope._var': ...
    @overload
    def get(self, name:str, throw:TypingLiteral[False], search_parents:bool=True) -> 'Scope._var|None': ...
    def get(self, name:str, throw:bool=True, search_parents:bool=True) -> 'Scope._var|None':
        for s in self:
            if name in s.vars:
                return s.vars[name]
            if not search_parents:
                break

        if throw:
            raise KeyError(f'variable "{name}" not found in scope')
        return None

    def assign(self, name:str, value:AST):
        assert len(DeclarationType.__members__) == 2, f'expected only 2 declaration types: let, const. found {DeclarationType.__members__}'

        # var is already declared in current scope
        if name in self.vars:
            var = self.vars[name]
            assert var.decltype != DeclarationType.CONST, f"Attempted to assign to constant variable: {name=}{var=}. {value=}"
            var.value = value
            return

        var = self.get(name, throw=False)

        # var is not declared in any scope
        if var is None:
            self.let(name, value, untyped)
            return

        # var was declared in a parent scope
        if var.decltype == DeclarationType.LET:
            var.value = value
            return

        raise ValueError(f'Attempted to assign to constant variable: {name=}{var=}. {value=}')

    def declare(self, name:str, value:AST, type:Type, decltype:DeclarationType):
        if name in self.vars:
            var = self.vars[name]
            assert var.decltype != DeclarationType.CONST, f"Attempted to {decltype.name.lower()} declare a value that is const in this current scope. {name=}{var=}. {value=}"

        self.vars[name] = Scope._var(decltype, type, value)

    def let(self, name:str, value:AST, type:Type):
        self.declare(name, value, type, DeclarationType.LET)

    def const(self, name:str, value:AST, type:Type):
        self.declare(name, value, type, DeclarationType.CONST)

    def __iter__(self) -> Generator['Scope', None, None]:
        """return an iterator that walks up each successive parent scope. Starts with self"""
        s = self
        while s is not None:
            yield s
            s = s.parent

    #TODO: these should actually be defined in python.py. There should maybe only be stubs here..
    @classmethod
    def default(cls: type['Scope']) -> 'Scope':
        return cls(vars={
            'printl': Scope._var(
                DeclarationType.CONST,
                Type(PrototypeBuiltin),
                PrototypeBuiltin(
                    Group([Assign(TypedIdentifier(Identifier('s'), Type(String)), String(''))]),
                    Type(Void)
                )
            ),
            'print': Scope._var(
                DeclarationType.CONST,
                Type(PrototypeBuiltin),
                PrototypeBuiltin(
                    Group([Assign(TypedIdentifier(Identifier('s'), Type(String)), String(''))]),
                    Type(Void)
                )
            ),
            'readl': Scope._var(
                DeclarationType.CONST,
                Type(PrototypeBuiltin),
                PrototypeBuiltin(
                    Group([]),
                    Type(String)
                )
            )
        })


class TypeofFunc(Protocol):
    def __call__(self, ast: AST, scope: Scope, params:bool=False) -> TypeExpr:
        """
        Return the type of the given AST node.

        Args:
            ast (AST): the AST node to determine the type of
            scope (Scope): the scope in which the AST node is being evaluated
            params (bool, optional): indicates if full type checking including parameterization should be done. Defaults to False.

        Returns:
            Type: the type of the AST node
        """

def identity(ast: AST, scope: Scope, params:bool=False) -> Type:
    return Type(type(ast))

def short_circuit(ret: type[AST], param_fallback:TypeofFunc|None=None) -> TypeofFunc:
    def inner(ast: AST, scope: Scope, params:bool=False) -> Type:
        if params and param_fallback is not None:
            return param_fallback(ast, scope, params)
        return Type(ret)
    return inner

def cannot_typeof(ast: AST, scope: Scope, params:bool=False):
    raise ValueError(f'INTERNAL ERROR: determining the type of `({type(ast)}) {ast}` is not possible')


_external_typeof_fn_map: dict[type[AST], TypeofFunc] = {}
def register_typeof(cls: type[AST], fn: TypeofFunc):
    _external_typeof_fn_map[cls] = fn

@cache
def get_typeof_fn_map() -> dict[type[AST], TypeofFunc]:
    return {
        Declare: short_circuit(Void),
        Call: typeof_call,
        Block: typeof_block,
        Group: typeof_group,
        Array: typeof_array,
        # Dict: typeof_dict,
        # PointsTo: typeof_points_to,
        # BidirDict: typeof_bidir_dict,
        # BidirPointsTo: typeof_bidir_points_to,
        # ObjectLiteral: typeof_object_literal,
        # # Object: no_op,
        Access: typeof_access,
        Assign: short_circuit(Void),
        # IterIn: typeof_iter_in,
        # FunctionLiteral: typeof_function_literal,
        # # Closure: typeof_closure,
        # # Builtin: typeof_builtin,
        String: identity,
        IString: short_circuit(String),
        Identifier: typeof_identifier,
        Express: typeof_express,
        Int: identity,
        # # Float: no_op,
        Bool: identity,
        # Range: no_op,
        CycleLeft: lambda ast, scope, params: typeof(ast.operand, scope, params),
        CycleRight: lambda ast, scope, params: typeof(ast.operand, scope, params),
        # Flow: typeof_flow,
        # Default: typeof_default,
        # If: typeof_if,
        # Loop: typeof_loop,
        # UnaryPos: typeof_unary_dispatch,
        # UnaryNeg: typeof_unary_dispatch,
        # UnaryMul: typeof_unary_dispatch,
        # UnaryDiv: typeof_unary_dispatch,
        # Not: typeof_unary_dispatch,
        Greater: short_circuit(Bool),
        GreaterEqual: short_circuit(Bool),
        Less: short_circuit(Bool),
        LessEqual: short_circuit(Bool),
        Equal: short_circuit(Bool),
        # And: typeof_binary_dispatch,
        # Or: typeof_binary_dispatch,
        # Xor: typeof_binary_dispatch,
        # Nand: typeof_binary_dispatch,
        # Nor: typeof_binary_dispatch,
        # Xnor: typeof_binary_dispatch,
        # Add: typeof_binary_dispatch,
        # Sub: typeof_binary_dispatch,
        Mul: typeof_binary_dispatch,
        # Div: typeof_binary_dispatch,
        # Mod: typeof_binary_dispatch,
        # Pow: typeof_binary_dispatch,
        AtHandle: typeof_at_handle,
        Undefined: identity,
        Void: identity,
        #TODO: other AST types here
    } | _external_typeof_fn_map



def typeof(ast: AST, scope: Scope, params:bool=False) -> TypeExpr:
    """Basically like evaluate, but just returns the type information. Doesn't actually evaluate the AST"""
    typeof_fn_map = get_typeof_fn_map()

    ast_type = type(ast)
    if ast_type in typeof_fn_map:
        return typeof_fn_map[ast_type](ast, scope, params)

    pdb.set_trace()
    raise NotImplementedError(f'typeof not implemented for {ast_type}')


# def promote/coerce() -> Type: #or make_compatible
# promotion_table = {...}
# type_tree = {...}

# def typecheck(ast: AST, scope: Scope) -> bool:
#     """Check if the given AST is well-formed from a type perspective"""
#     match ast:
#         case Call(): return typecheck_call(ast, scope)
#         case Index(): return typecheck_index(ast, scope)
#         case Mul(): return typecheck_binary_dispatch(ast, scope)
#         case _: raise NotImplementedError(f'typecheck not implemented for {type(ast)}')

# def infer_types(ast: AST, scope: Scope) -> AST:


def typeof_identifier(ast: Identifier, scope: Scope, params:bool=False) -> TypeExpr:
    var = scope.get(ast.name)
    if var is None:
        raise KeyError(f'variable "{ast.name}" not found in scope')
    if var.type is not untyped:
        return var.type

    return typeof(var.value, scope, params)



# abstract base type to register new callable types
class CallableBase(AST): ...
_callable_types = (PrototypeBuiltin, FunctionLiteral, CallableBase)
# def register_callable(cls: type[AST]):
#     _callable_types.append(cls)

def typeof_call(ast: Call, scope: Scope, params:bool=False) -> TypeExpr:
    pdb.set_trace()
    ...

# def simple_typecheck_resolve_ast(ast: AST, scope: Scope) -> AST:
#     """Resolve the AST to a type. This is a simple version that doesn't do any complex type checking"""
#     if isinstance(ast, Identifier):
#         var = scope.get(ast.name)
#         return var.value
#     if isinstance(ast, AtHandle):
#         return ast.operand
#     if isinstance(ast, Group):
#         pdb.set_trace()
#         ...
#     if isinstance(ast, Access):
#         pdb.set_trace()
#         ...

#     # no resolving possible
#     return ast

def typecheck_call(ast: Call, scope: Scope) -> bool:
    #For now, just the simplest check. is f callable. ignore rest of type checking
    f_type = typeof(ast.f, scope)
    if not isinstance(f_type, Type):
        pdb.set_trace()
        ...
        #TBD how to handle the different TypeExpr's

    if isinstance(f_type, Type) and issubclass(f_type.t, _callable_types):
        return True

    return False
    pdb.set_trace()
    # f = ast.f
    # f = simple_typecheck_resolve_ast(f, scope) # resolve to a value
    # if isinstance(f, Identifier):
    #     var = scope.get(f.name)
    #     f = var.value
    # if isinstance(f, AtHandle):
    #     f = f.operand

    if isinstance(f, tuple(_callable_types)):
        #TODO: longer term, want to check that the expected args match the given args
        return True

    # if isinstance(f, Group):
    #     pdb.set_trace()
    #     # get the type of the group items... handling void, and if multiple, then answer is False...
    #     ...
    # if isinstance(f, Access):
    #     pdb.set_trace()
    #     ...
    #TODO: replace all this with full typechecking...
    # t = typeof(f, scope)
    # if t in _callable_types: return True
    # return False
    return False


# Abstract base types to register new indexable/indexer types
class IndexableBase(AST): ...
class IndexerBase(AST): ...

_indexable_types = (Array, Range, IndexableBase)
_indexer_types = (Array, Range, IndexerBase)
# def register_indexable(cls: type[AST]):
#     _indexable_types.append(cls)
# def register_indexer(cls: type[AST]):
#     _indexer_types.append(cls)

def typeof_index(ast: Index, scope: Scope, params:bool=False) -> TypeExpr:
    pdb.set_trace()
    ...

def typecheck_index(ast: Index, scope: Scope) -> bool:
    # left = simple_typecheck_resolve_ast(ast.left, scope)
    # right = simple_typecheck_resolve_ast(ast.right, scope)
    # if isinstance(left, _indexable_types) and isinstance(right, _indexer_types):
    #     return True


    #for now super simple checks on left and right
    left_type = typeof(ast.left, scope)
    right_type = typeof(ast.right, scope)
    if not isinstance(left_type, Type) or not isinstance(right_type, Type):
        pdb.set_trace()
        #TODO: more complex cases involving type expressions...
        raise NotImplementedError('typecheck_index not implemented for non-Type left side')

    if issubclass(left_type.t, _indexable_types) and issubclass(right_type.t, _indexer_types):
        return True

    return False



# abstract base type to register new multipliable types
class MultipliableBase(AST): ...
_multipliable_types = (Int, Array, Range, MultipliableBase) #TODO: add more types
# def register_multipliable(cls: type[AST]):
#     _multipliable_types.append(cls)
def typecheck_multiply(ast: Mul, scope: Scope) -> bool:
    # left = simple_typecheck_resolve_ast(ast.left, scope)
    # right = simple_typecheck_resolve_ast(ast.right, scope)
    # if isinstance(left, _multipliable_types) and isinstance(right, _multipliable_types):
    #     return True

    #TODO: full type checking to check if values are multipliable
    # pdb.set_trace()
    left_type = typeof(ast.left, scope)
    right_type = typeof(ast.right, scope)
    if not isinstance(left_type, Type) or not isinstance(right_type, Type):
        pdb.set_trace()
        raise NotImplementedError('typecheck_multiply not implemented for non-Type left side')

    if issubclass(left_type.t, _multipliable_types) and issubclass(right_type.t, _multipliable_types):
        return True

    return False



def typeof_group(ast: Group, scope: Scope, params:bool=False) -> TypeExpr:
    expressed: list[TypeExpr] = []
    for expr in ast.items:
        res = typeof(expr, scope, params)
        if res is not void:
            expressed.append(res)
    if len(expressed) == 0:
        return Type(Void)
    if len(expressed) == 1:
        return expressed[0]
    raise NotImplementedError(f'Block with multiple expressions not yet supported. {ast=}, {expressed=}')


def typeof_block(ast: Block, scope: Scope, params:bool=False) -> TypeExpr:
    scope = Scope(scope)
    return typeof_group(Group(ast.items), scope, params)






def typeof_at_handle(ast: AtHandle, scope: Scope, params:bool=False) -> TypeExpr:
    if not isinstance(ast.operand, Identifier):
        raise NotImplementedError('typeof_at_handle only implemented for Identifiers')
    return typeof_identifier(ast.operand, scope, params)


def typeof_express(ast: Express, scope: Scope, params:bool=False) -> TypeExpr:
    var = scope.get(ast.id.name)

    # if we were told what the type is, return that (as it should be the main source of truth)
    if isinstance(var.type, Type) and var.type is not untyped:
        return var.type

    return typeof(var.value, scope, params)


def typeof_binary_dispatch(ast: BinOp, scope: Scope, params:bool=False) -> TypeExpr:
    pdb.set_trace()
    raise NotImplementedError('typeof_binary_dispatch not implemented')

def typecheck_binary_dispatch(ast: BinOp, scope: Scope) -> bool:
    op = type(ast)
    if op not in binary_dispatch_table:
        return False
    left_type = typeof(ast.left, scope)
    right_type = typeof(ast.right, scope)
    if not isinstance(left_type, Type) or not isinstance(right_type, Type):
        pdb.set_trace()
        raise NotImplementedError('typecheck_binary_dispatch not implemented for non-Type left side')

    if (left_type.t, right_type.t) not in binary_dispatch_table[op]:
        return False

    return True

def typeof_array(ast: Array, scope: Scope, params:bool=False) -> TypeExpr:
    if not params:
        return Type(Array)
    pdb.set_trace()
    ...
    raise NotImplementedError('typeof_array not implemented when params=True')






class ObjectBase(AST): ...

def typeof_access(ast: Access, scope: Scope, params:bool=False) -> TypeExpr:
    left = typeof(ast.left, scope, params)

    # happy path: left was an object
    if isinstance(left, Type) and issubclass(left.t, ObjectBase) and left.parameters is not None:
        parameters = left.parameters
        assert len(parameters.items) == 1, f'expected only one parameter for object access. {parameters=}'
        scope, = parameters.items
        assert isinstance(scope, Scope), f'expected parameter to be a scope. {parameters.items[0]=}'
        if isinstance(ast.right, Identifier):
            handle, id = False, ast.right
        elif isinstance(ast.right, AtHandle):
            handle, id = True, ast.right.operand
            assert isinstance(id, Identifier), f'expected id to be an Identifier. {id=}. Other types not yet supported'
        elif isinstance(ast.right, Access):
            raise ValueError('Right hand side should not be access. Access should be left associative')
        else:
            raise NotImplementedError(f'Access right-hand-side not implemented for {type(ast.right)}')

        if handle:
            return typeof_identifier(id, scope, params)
        return typeof_express(Express(id), scope, params)



    pdb.set_trace()
    raise NotImplementedError(f'typeof_access not implemented for {type(ast)}')





# TODO: for now, just a super simple dispatch table
binary_dispatch_table = {
    Mul: {
        (Int, Int): Int,
        # (Int, Float): Float,
        # (Float, Float): Float,
    }
}




# UnaryDispatchKey =  tuple[type[UnaryPrefixOp]|type[UnaryPostfixOp], type[SimpleValue[T]]]
# unary_dispatch_table: dict[UnaryDispatchKey[T], TypingCallable[[T], AST]] = {
#     (Not, Int): lambda l: Int(~l),
#     (Not, Bool): lambda l: Bool(not l),
#     (UnaryPos, Int): lambda l: Int(l),
#     (UnaryNeg, Int): lambda l: Int(-l),
#     (UnaryMul, Int): lambda l: Int(l),
#     (UnaryDiv, Int): lambda l: Int(1/l),
# }

# BinaryDispatchKey = tuple[type[BinOp], type[SimpleValue[T]], type[SimpleValue[U]]]
# # These are all symmetric meaning you can swap the operand types and the same function will be used (but the arguments should not be swapped)
# binary_dispatch_table: dict[BinaryDispatchKey[T, U], TypingCallable[[T, U], AST]|TypingCallable[[U, T], AST]] = {
#     (And, Int, Int): lambda l, r: Int(l & r),
#     (And, Bool, Bool): lambda l, r: Bool(l and r),
#     (Or, Int, Int): lambda l, r: Int(l | r),
#     (Or, Bool, Bool): lambda l, r: Bool(l or r),
#     (Xor, Int, Int): lambda l, r: Int(l ^ r),
#     (Xor, Bool, Bool): lambda l, r: Bool(l != r),
#     (Nand, Int, Int): lambda l, r: Int(~(l & r)),
#     (Nand, Bool, Bool): lambda l, r: Bool(not (l and r)),
#     (Nor, Int, Int): lambda l, r: Int(~(l | r)),
#     (Nor, Bool, Bool): lambda l, r: Bool(not (l or r)),
#     (Add, Int, Int): lambda l, r: Int(l + r),
#     (Add, Int, Float): lambda l, r: Float(l + r),
#     (Add, Float, Float): lambda l, r: Float(l + r),
#     (Sub, Int, Int): lambda l, r: Int(l - r),
#     (Sub, Int, Float): lambda l, r: Float(l - r),
#     (Sub, Float, Float): lambda l, r: Float(l - r),
#     (Mul, Int, Int): lambda l, r: Int(l * r),
#     (Mul, Int, Float): lambda l, r: Float(l * r),
#     (Mul, Float, Float): lambda l, r: Float(l * r),
#     (Div, Int, Int): int_int_div,
#     (Div, Int, Float): float_float_div,
#     (Div, Float, Float): float_float_div,
#     (Mod, Int, Int): lambda l, r: Int(l % r),
#     (Mod, Int, Float): lambda l, r: Float(l % r),
#     (Mod, Float, Float): lambda l, r: Float(l % r),
#     (Pow, Int, Int): lambda l, r: Int(l ** r),
#     (Pow, Int, Float): lambda l, r: Float(l ** r),
#     (Pow, Float, Float): lambda l, r: Float(l ** r),
#     (Less, Int, Int): lambda l, r: Bool(l < r),
#     (Less, Int, Float): lambda l, r: Bool(l < r),
#     (Less, Float, Float): lambda l, r: Bool(l < r),
#     (LessEqual, Int, Int): lambda l, r: Bool(l <= r),
#     (LessEqual, Int, Float): lambda l, r: Bool(l <= r),
#     (LessEqual, Float, Float): lambda l, r: Bool(l <= r),
#     (Greater, Int, Int): lambda l, r: Bool(l > r),
#     (Greater, Int, Float): lambda l, r: Bool(l > r),
#     (Greater, Float, Float): lambda l, r: Bool(l > r),
#     (GreaterEqual, Int, Int): lambda l, r: Bool(l >= r),
#     (GreaterEqual, Int, Float): lambda l, r: Bool(l >= r),
#     (GreaterEqual, Float, Float): lambda l, r: Bool(l >= r),
#     (Equal, Int, Int): lambda l, r: Bool(l == r),
#     (Equal, Float, Float): lambda l, r: Bool(l == r),
#     (Equal, Bool, Bool): lambda l, r: Bool(l == r),
#     (Equal, String, String): lambda l, r: Bool(l == r),
#     # (NotEqual, Int, Int): lambda l, r: Bool(l != r),

# }

# unsymmetric_binary_dispatch_table: dict[BinaryDispatchKey[T, U], ] = {
#     #e.g. (Mul, String, Int): lambda l, r: String(l * r), # if we follow python's behavior
# }

# # dispatch table for more complicated values that can't be automatically unpacked by the dispatch table
# # TODO: actually ideally just have a single table
# CustomBinaryDispatchKey = tuple[type[BinOp], type[T], type[U]]
# custom_binary_dispatch_table: dict[CustomBinaryDispatchKey[T, U], TypingCallable[[T, U], AST]] = {
#     (Add, Array, Array): lambda l, r: Array(l.items + r.items), #TODO: this will be removed in favor of spread. array add will probably be vector add
#     # (BroadcastOp, Array, Array): broadcast_array_op,
#     # (BroadcastOp, NpArray, NpArray): broadcast_array_op,
#     # (BroadcastOp, Int, Array): broadcast_array_op,
#     # (BroadcastOp, Float, Array): broadcast_array_op,

# }17:Tb8b,from pathlib import Path
from argparse import ArgumentParser, REMAINDER
from .backend import backend_names, get_backend, python_interpreter, python_repl, qbe_compiler, get_version
from .utils import Options

import pdb



def main():
    arg_parser = ArgumentParser(description='Dewy Compiler')

    # positional argument for the file to compile
    arg_parser.add_argument('file', nargs='?', help='.dewy file to run. If not provided, will enter REPL mode using the python backend')

    # mutually exclusive flags for specifying the backend to use
    group = arg_parser.add_mutually_exclusive_group()
    group.add_argument('-i', action='store_true', help='(DEFAULT) Run in interpreter mode with the python backend')
    group.add_argument('-c', action='store_true', help='Run in compiler mode with the QBE backend (not implemented yet)')
    group.add_argument('--backend', type=str, help=f'Specify a backend compiler/interpreter by name to use. Backends will include: {backend_names} (however currently only python is available).')

    arg_parser.add_argument('-v', '--version', action='version', version=f'Dewy {get_version()}', help='Print version information and exit')
    arg_parser.add_argument('-p', '--disable-rich-print', action='store_true', help='Disable using rich for printing stack traces')
    arg_parser.add_argument('args', nargs=REMAINDER, help='Arguments after the file are passed directly to program')
    arg_parser.add_argument('--verbose', action='store_true', help='Print verbose output')
    arg_parser.add_argument('--tokens', action='store_true', help='Print tokens for the input expression')


    args = arg_parser.parse_args()

    # if file is not provided, ensure that -c and --backend are not provided
    if not args.file and (args.c or args.backend):
        arg_parser.error('Cannot enter REPL mode when -c or --backend is provided')

    # use rich for pretty traceback printing
    #TODO: maybe add a util or something for trying to import rich and replacing print in all files
    if not args.disable_rich_print:
        try:
            from rich import traceback
            traceback.install(show_locals=True)
        except:
            print('rich unavailable for import. using built-in printing')

    options = Options(args.tokens, args.verbose)

    # if no file is provided, enter REPL mode
    if args.file is None:
        python_repl(args.args, options)
        return
    

    if args.backend:
        # use user specified backend
        backend = get_backend(args.backend)
    elif args.c:
        # default compiler is qbe
        backend = qbe_compiler
    elif args.i:
        # default interpreter is python
        backend = python_interpreter
    else:
        # default with no args is currently python #qbe
        backend = python_interpreter # qbe_compiler

    # run with the selected backend
    backend(Path(args.file), args.args, options)




if __name__ == '__main__':
    main()
18:T949c,from typing import Generator, Sequence, cast, Callable as TypingCallable
from enum import Enum, auto
from dataclasses import dataclass
from itertools import groupby, chain as iterchain

from .syntax import (
    AST,
    Access,
    Declare,
    PointsTo, BidirPointsTo,
    Type,
    ListOfASTs, PrototypeTuple, Block, BareRange, DotDotDot, CollectInto, SpreadOutFrom, Array, Group, Range, ObjectLiteral, Dict, BidirDict, TypeParam,
    Void, Undefined, void, undefined, untyped,
    String, IString,
    Flowable, Flow, If, Loop, Default,
    PrototypeFunctionLiteral, PrototypeBuiltin, Call,
    Index,
    PrototypeIdentifier, Identifier, TypedIdentifier, ReturnTyped, UnpackTarget, Assign,
    Int, Bool,
    Range, IterIn,
    BinOp,
    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,
    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv, AtHandle,
    Backticks, CycleLeft, CycleRight, Suppress,
    BroadcastOp,
    DeclarationType,
    DeclareGeneric, Parameterize,
)
from .tokenizer import (
    Token,
    Block_t,
    Operator_t,
    ShiftOperator_t,
    Juxtapose_t,
    Comma_t,
    String_t,
    Escape_t,
    TypeParam_t,
    Undefined_t,
    Identifier_t,
    Integer_t,
    Boolean_t,
    BasedNumber_t,
    RawString_t,
    DotDot_t, DotDotDot_t, Backticks_t,
    Keyword_t,
)
from .postok import (
    RangeJuxtapose_t,
    EllipsisJuxtapose_t,
    TypeParamJuxtapose_t,
    BackticksJuxtapose_t,
    get_next_chain,
    Chain,
    is_op, is_binop, is_unary_prefix_op, is_unary_postfix_op,
    Flow_t,
    Declare_t,
    OpChain_t,
    BroadcastOp_t,
    CombinedAssignmentOp_t,
)
from .utils import (
    bool_to_bool,
    based_number_to_int,
)

import pdb





def top_level_parse(tokens: list[Token]) -> AST:
    """Main entrypoint to kick off parsing a sequence of tokens"""

    ast = parse(tokens)
    if isinstance(ast, ListOfASTs):
        ast = Group(ast.asts)

    return ast

def parse_generator(tokens: list[Token]) -> Generator[AST, None, None]:
    """
    Parse all tokens into a sequence of ASTs
    """

    while len(tokens) > 0:
        chain, tokens = get_next_chain(tokens)
        yield parse_chain(chain)


def parse(tokens: list[Token]) -> AST:
    items = [*parse_generator(tokens)]

    # depending on how many expressions were parsed, return an AST or container
    if len(items) == 0:
        ast = void # literally nothing was parsed
    elif len(items) == 1:
        ast = items[0]
    else:
        ast = ListOfASTs(items)

    return ast

@dataclass
class qint:
    """
    quantum int for dealing with precedences that are multiple values at the same time
    qint's can only be strictly greater or strictly less than other values. Otherwise it's ambiguous
    In the case of ambiguous precedences, the symbol table is needed for helping resolve the ambiguity
    """
    values: set[int]

    def __post_init__(self):
        assert len(self.values) > 1, f'qint must have more than one value. Got {self.values}'

    def __gt__(self, other: 'int|qint') -> bool:
        if isinstance(other, int):
            return all(v > other for v in self.values)
        return all(v > other for v in self.values)

    def __lt__(self, other: 'int|qint') -> bool:
        if isinstance(other, int):
            return all(v < other for v in self.values)
        return all(v < other for v in self.values)

    def __ge__(self, other: 'int|qint') -> bool: return self.__gt__(other)
    def __le__(self, other: 'int|qint') -> bool: return self.__lt__(other)
    def __eq__(self, other: object) -> bool: return False


# class QAST(AST):
#     """
#     Quantum AST for dealing with ambiguous precedence
#     Simplest usage will just look see which expression passes typechecking
#     More complex versions can include something (lambdas?) to determine which case should be selected
#     """
#     asts: list[AST]
#     # predicates: list[TypingCallable] | None = None  # uncomment if we actually use this

#     def __post_init__(self):
#         assert len(self.asts) > 1, f'QAST must have more than one value. Got {self.asts}'

#     def __str__(self):
#         return f'QAST([{", ".join(str(i) for i in self.asts)}])'

class QJux(AST):
    """
    Quantum Juxtapose for dealing with the three operators vanilla juxtapose could be:
    - call e.g. a(b)
    - index e.g. a[b]
    - multiply e.g. a * b
    """
    call: Call|None   # syntactically we might know if left is not callable
    index: Index|None # syntactically we definitely know if it's not index if right is not Array or Range
    mul: Mul

    def __str__(self):
        call = 'call | ' if self.call is not None else ''
        index = 'index | ' if self.index is not None else ''
        return f'QJux(({call}{index}multiply): {self.mul.left}{self.mul.right})'
        # call = f'{self.call}, ' if self.call is not None else ''
        # index = f'{self.index}, ' if self.index is not None else ''
        # return f'QJux({call}{index}{self.mul})'

######### Operator Precedence Table #########
# TODO: class for compund operators, e.g. += -= .+= .-= not=? not>? etc.
# TODO: how to handle unary operators in the table? perhaps make PrefixOperator_t/PostfixOperator_t classes?
# TODO: add specification of associativity for each row
class Associativity(Enum):
    left = auto()  # left-to-right
    right = auto()  # right-to-left
    unary = auto()  # out-to-in
    # prefix = auto()
    # postfix = auto()
    none = auto()
    fail = auto()


"""
[HIGHEST PRECEDENCE]
    (prefix) @
    . <jux call> <jux index access>
    <jux ellipsis>                      //e.g. [...args]
    <jux type param>                    //e.g. <T>(x:T):>T
    (prefix) ` (postfix) `
    (prefix) not
    ^                                   //right-associative
    <jux mul>
    / * %
    + -
    << >> <<< >>> <<! !>>
    ,                                   //tuple maker
    <jux range>                         //e.g. [first,second..last]
    in
    =? >? <? >=? <=? not=? <=> is? isnt? @?
    (postfix) ?
    and nand &
    xor xnor                            //following C's precedence: and > xor > or
    or nor |
    as transmute
    :                                   //e.g. let x:int
    :>                                  //e.g. let x:():>int => 42
    =>
    |>                                  //function pipe operators
    <|
    -> <->                              //dict pointers
    = .= <op>= .<op>=  (e.g. += .+=)    //right-associative (but technically causes a type error since assignments can't be chained)
    else
    (postfix) ;
    <seq> (i.e. space)
[LOWEST PRECEDENCE]

[Notes]
.. for ranges is not an operator, it is an expression. it uses juxtapose to bind to left/right arguments (or empty), and type-checks left and right
if-else-loop chain expr is more like a single unit, so it doesn't really have a precedence. but they act like they have the lowest precedence since the expressions they capture will be full chains only broken by space/seq
the unary versions of + - * / % have the same precedence as their binary versions
"""
operator_groups: list[tuple[Associativity, Sequence[Operator_t]]] = list(reversed([
    (Associativity.unary, [Operator_t('@')]),
    (Associativity.left, [Operator_t('.'), Juxtapose_t(None)]),  # jux-call, jux-index
    (Associativity.none, [TypeParamJuxtapose_t(None)]),
    (Associativity.none, [EllipsisJuxtapose_t(None)]),  # jux-ellipsis
    (Associativity.none, [BackticksJuxtapose_t(None)]),  # jux-backticks
    (Associativity.unary, [Operator_t('not'), Operator_t('~')]),
    (Associativity.right,  [Operator_t('^')]),
    (Associativity.left, [Juxtapose_t(None)]),  # jux-multiply
    (Associativity.left, [Operator_t('*'), Operator_t('/'), Operator_t('%')]),
    (Associativity.left, [Operator_t('+'), Operator_t('-')]),
    (Associativity.left, [*map(ShiftOperator_t, ['<<', '>>', '<<<', '>>>', '<<!', '!>>'])]),
    (Associativity.none,  [Comma_t(',')]),
    (Associativity.left, [RangeJuxtapose_t(None)]),  # jux-range
    (Associativity.none, [Operator_t('in')]),
    (Associativity.left, [Operator_t('=?'), Operator_t('>?'), Operator_t('<?'), Operator_t('>=?'), Operator_t('<=?')]),
    (Associativity.unary, [Operator_t('?')]),
    (Associativity.left, [Operator_t('and'), Operator_t('nand'), Operator_t('&')]),
    (Associativity.left, [Operator_t('xor'), Operator_t('xnor')]),
    (Associativity.left, [Operator_t('or'), Operator_t('nor'), Operator_t('|')]),
    (Associativity.none,  [Operator_t('as'), Operator_t('transmute')]),
    (Associativity.none, [Operator_t(':')]),
    (Associativity.right, [Operator_t(':>')]),
    (Associativity.right,  [Operator_t('=>')]),  # () => () => () => 42
    (Associativity.right, [Operator_t('|>')]),
    (Associativity.left, [Operator_t('<|')]),
    (Associativity.fail,  [Operator_t('->'), Operator_t('<->')]),
    (Associativity.fail,  [Operator_t('=')]),
    (Associativity.none,  [Operator_t('else')]),
]))
precedence_table: dict[Operator_t, int | qint] = {}
associativity_table: dict[int, Associativity] = {}
for i, (assoc, group) in enumerate(operator_groups):

    # mark precedence level i as the specified associativity
    associativity_table[i] = assoc

    # insert all ops in the row into the precedence table at precedence level i
    for op in group:
        if op not in precedence_table:
            precedence_table[op] = i
            continue

        val = precedence_table[op]
        if isinstance(val, int):
            precedence_table[op] = qint({val, i})
        else:
            precedence_table[op] = qint(val.values | {i})


def operator_precedence(op: Operator_t|OpChain_t|BroadcastOp_t|CombinedAssignmentOp_t) -> int | qint:

    # for complex operators, extract the actual operator that determines precedence
    if isinstance(op, CombinedAssignmentOp_t):
        op = op.assign # combined assignment has same precedence as regular assignment
    if isinstance(op, BroadcastOp_t):
        op = op.op # precedence should be based on the operator attached to the . operator
    if isinstance(op, OpChain_t):
        op = op.ops[0] # opchain precedence is determined by the first operator in the chain

    try:
        return precedence_table[op]
    except:
        raise ValueError(f"ERROR: expected operator, got {op=} which failed to return a value from the operator precedence table") from None


def operator_associativity(op: Operator_t | int) -> Associativity|set[Associativity]:
    if not isinstance(op, int):
        i = operator_precedence(op)
        # assert isinstance(i, int), f'Cannot determine associativity of operator ({op}) with multiple precedence levels ({i})'
    else:
        i = op
    try:
        if isinstance(i, int):
            return associativity_table[i]
        return {associativity_table[v] for v in i.values}
    except:
        raise ValueError(f"Error: failed to determine associativity for operator {op}") from None



def parse_chain(chain: Chain[Token]) -> AST:
    assert isinstance(chain, Chain), f"ERROR: parse chain must be called on Chain[Token], got {type(chain)}"

    if len(chain) == 0:
        return void
    if len(chain) == 1:
        return parse_single(chain[0])

    try:
        left, op, right = split_by_lowest_precedence(chain)
    except AmbiguousPrecedenceError as e:
        pdb.set_trace()
        ... #TODO: handle ambiguous precedence error by making a QAST
        return build_quantum_expr(e.ops, e.ranks, e.assocs, e.tokens)

    left, right = parse_chain(left), parse_chain(right)

    assert not (left is void and right is void), f"Internal Error: both left and right returned void during parse chain, implying both left and right side of operator were empty, i.e. chain was invalid: {chain}"

    # 3 cases are prefix expr, postfix expr, or binary expr
    if left is void:
        return build_unary_prefix_expr(op, right)
    if right is void:
        return build_unary_postfix_expr(left, op)
    return build_bin_expr(left, op, right)


class AmbiguousPrecedenceError(ValueError):
    def __init__(self, ops: list[Operator_t], ranks: list[int], assocs: list[Associativity], tokens: Chain[Token]):
        self.ops = ops
        self.ranks = ranks
        self.assocs = assocs
        self.tokens = tokens
        super().__init__(f"Ambiguous precedence for operators {ops=} with ranks {ranks=} in token stream {tokens=}")


def split_by_lowest_precedence(tokens: Chain[Token]) -> tuple[Chain[Token], Token, Chain[Token]]:
    """
    return the integer index/indices of the lowest precedence operator(s) in the given list of tokens
    """
    assert isinstance(tokens, Chain), f"ERROR: `split_by_lowset_precedence()` may only be called on explicitly known Chain[Token], got {type(tokens)}"

    # collect all operators, their indices, and their associativity from the list of tokens
    idxs, ops = zip(*[(i, token) for i, token in enumerate(tokens) if is_op(token)])
    idxs, ops = cast(list[int], idxs), cast(list[Token], ops)
    assocs = [operator_associativity(op) for op in ops]

    # simple cases of none or one operator
    if len(ops) == 0:
        raise ValueError("INTERNAL ERROR: Attempted to split chain with no operators which shouldn't happen")
    if len(ops) == 1:
        i, = idxs
        op, = ops
        return Chain(tokens[:i]), op, Chain(tokens[i+1:])

    # when more than one op present, find the lowest precedence one

    # case of all unary operators has different splitting logic
    if all(assoc == Associativity.unary for assoc in assocs):
        return unary_split_by_lowest_precedence(tokens, ops, idxs)

    # filter out any unary operators
    assocs, idxs, ops = zip(*[(a, i, op) for a, i, op in zip(assocs, idxs, ops) if a is not Associativity.unary])
    assocs, idxs, ops = cast(list[Associativity], assocs), cast(list[int], idxs), cast(list[Token], ops)

    # continue handling binary operators as before
    ranks = [operator_precedence(op) for op in ops]
    min_rank = min(ranks)
    min_idx = ranks.index(min_rank)

    # verify that the min is strictly less than or equal to all other ranks
    if not all(min_rank <= r for r in ranks[:min_idx] + ranks[min_idx+1:]):
        raise AmbiguousPrecedenceError(ops, ranks, assocs, tokens)

    # find operators with precedence equal to the current minimum
    op_idxs = [i for i, r in zip(idxs, ranks) if r == min_rank or r is min_rank]

    if len(op_idxs) == 1:
        i, = op_idxs
        return Chain(tokens[:i]), tokens[i], Chain(tokens[i+1:])

    # handling when multiple ops have the same precedence, select based on associativity rules
    if isinstance(min_rank, qint):
        assocs_set = set(assocs) #{operator_associativity(i) for i in min_rank.values}
        if len(assocs_set) > 1:
            raise NotImplementedError(f'TODO: need to type check to deal with multiple/ambiguous operator associativities: {assocs_set}')
        assoc, = assocs_set
    else:
        assoc = operator_associativity(min_rank)

    match assoc:
        case Associativity.left: i = op_idxs[-1]
        case Associativity.right: i = op_idxs[0]
        case Associativity.unary: raise ValueError(f'INTERNAL ERROR: there should not be any unary operators in the list of operators at this point')
        case Associativity.none: i = op_idxs[-1]  # default to left. handled later in parsing
        case Associativity.fail: raise ValueError(f'Cannot handle multiple given operators in chain {tokens}, as lowest precedence operator is marked as un-associable.')

    return Chain(tokens[:i]), tokens[i], Chain(tokens[i+1:])


def unary_split_by_lowest_precedence(tokens: Chain[Token], ops: list[Token], idxs:list[int]) -> tuple[Chain[Token], Token, Chain[Token]]:
    """
    split the list of tokens by the lowest precedence unary operator
    """
    # unary split looks at the left=leftmost prefix operator and the right=rightmost postfix operator
    # if left is None, then it's right. if right is None, then it's left
    # otherwise, it's determined by which has the lower precedence
    # if both have the same precedence (shouldn't generally happen), probably just do left to right

    #TODO: this might actually fail for the jux operators because to determine if they are prefix or postfix requires looking at the left and right token...
    pdb.set_trace()


    # get the leftmost and rightmost operators
    left_op = ops[0] if is_unary_prefix_op(ops[0]) else None
    left_idx = idxs[0]
    if left_op is not None:
        assert left_idx == 0, f'INTERNAL ERROR: expected left operator to be at the start of the list of tokens, got {left_idx=}, {tokens=}'
    
    right_op = ops[-1] if is_unary_postfix_op(ops[-1]) else None
    right_idx = idxs[-1]
    if right_op is not None:
        assert right_idx == len(tokens) - 1, f'INTERNAL ERROR: expected right operator to be at the end of the list of tokens, got {right_idx=}, {tokens=}'

    if left_op is None and right_op is None:
        raise ValueError(f'INTERNAL ERROR: no unary operators found in list of operators {ops=}')

    # determine which operator is the lowest precedence
    if left_op is None:
        i = right_idx
    elif right_op is None:
        i = left_idx
    else:
        # use precedence to determine lower precedence op
        left_rank = operator_precedence(left_op)
        right_rank = operator_precedence(right_op)
        i = left_idx if left_rank <= right_rank else right_idx

    return Chain(tokens[:i]), tokens[i], Chain(tokens[i+1:])


def parse_single(token: Token) -> AST:
    """Parse a single token into an AST"""
    match token:
        case Undefined_t(): return undefined
        case Identifier_t(): return PrototypeIdentifier(token.src)
        case Integer_t(): return Int(int(token.src))
        case Boolean_t(): return Bool(bool_to_bool(token.src))
        case BasedNumber_t(): return Int(based_number_to_int(token.src))
        case RawString_t(): return String(token.to_str())
        case DotDot_t(): return BareRange(void, void)
        case DotDotDot_t(): return DotDotDot()
        case Backticks_t(src=src): return Backticks(src)
        case String_t(): return parse_string(token)
        case Block_t(): return parse_block(token)
        case TypeParam_t(): return parse_type_param(token)
        case Flow_t(): return parse_flow(token)
        case Declare_t(): return parse_declare(token)

        case _:
            # TODO handle other types...
            pdb.set_trace()
            ...

    pdb.set_trace()
    raise NotImplementedError()
    ...


def build_bin_expr(left: AST, op: Token, right: AST) -> AST:
    """create a unary prefix expression AST from the op and right AST"""

    match op:
        #TODO: replace vanilla juxtapose with prototype?
        # when split_by_lowest_precedence is ambiguous we will create a QAST which has all possible ASTs, and disambiguation will happen at runtime/compiletime
        # then we will replace Juxtapose_t here with JuxtaposeCall_t | JuxtaposeIndex_t | JuxtaposeMul_t
        case Juxtapose_t(): return build_quantum_juxtapose(left, right) #return QAST([Call(left, right), Index(left, right), Mul(left, right)])

        case Operator_t(op='|>'): return Call(right, left)
        case Operator_t(op='<|'): return Call(left, right)
        case Operator_t(op='='): return Assign(left, right)
        case Operator_t(op='=>'): return PrototypeFunctionLiteral(left, right)
        case Operator_t(op='->'): return PointsTo(left, right)
        case Operator_t(op='<->'): return BidirPointsTo(left, right)
        case Operator_t(op='.'): return Access(left, right)

        # a bunch of simple cases:
        case ShiftOperator_t(op='<<'):  return LeftShift(left, right)
        case ShiftOperator_t(op='>>'):  return RightShift(left, right)
        case ShiftOperator_t(op='<<<'): return LeftRotate(left, right)
        case ShiftOperator_t(op='>>>'): return RightRotate(left, right)
        case ShiftOperator_t(op='<<!'): return LeftRotateCarry(left, right)
        case ShiftOperator_t(op='!>>'): return RightRotateCarry(left, right)
        case Operator_t(op='+'): return Add(left, right)
        case Operator_t(op='-'): return Sub(left, right)
        case Operator_t(op='*'): return Mul(left, right)
        case Operator_t(op='/'): return Div(left, right)
        case Operator_t(op=''): return IDiv(left, right)
        case Operator_t(op='%'): return Mod(left, right)
        case Operator_t(op='^'): return Pow(left, right)

        # comparison operators
        case Operator_t(op='=?'): return Equal(left, right)
        case Operator_t(op='>?'): return Greater(left, right)
        case Operator_t(op='<?'): return Less(left, right)
        case Operator_t(op='>=?'): return GreaterEqual(left, right)
        case Operator_t(op='<=?'): return LessEqual(left, right)
        case Operator_t(op='in?'): return MemberIn(left, right)
        # case Operator_t(op='is?'): return Is(left, right)
        # case Operator_t(op='isnt?'): return Isnt(left, right)
        # case Operator_t(op='<=>'): return ThreewayCompare(left, right)

        # Logical Operators. TODO: outtype=Bool is not flexible enough...
        case Operator_t(op='and'|'&'): return And(left, right)
        case Operator_t(op='or'|'|'): return Or(left, right)
        case Operator_t(op='nand'): return Nand(left, right)
        case Operator_t(op='nor'): return Nor(left, right)
        case Operator_t(op='xor'): return Xor(left, right)
        case Operator_t(op='xnor'): return Xnor(left, right)

        # Misc Operators
        case Operator_t(op=':'):
            if isinstance(left, PrototypeIdentifier): return TypedIdentifier(Identifier(left.name), right)
            #TBD if there are other things that can have type annotations beyond identifiers
            raise ValueError(f'ERROR: can only apply a type to an identifier. Got {left=}, {right=}')
        case Operator_t(op=':>'): return ReturnTyped(left, right)

        case TypeParamJuxtapose_t():
            if isinstance(left, TypeParam):
                return DeclareGeneric(left, right)
            if isinstance(right, TypeParam):
                return Parameterize(left, right)
            raise ValueError(f"INTERNAL ERROR: TypeParamJuxtapose must be attached to a type param. {left=}, {right=}")

        case EllipsisJuxtapose_t():
            if isinstance(left, DotDotDot):
                return CollectInto(right)
            if isinstance(right, DotDotDot):
                return SpreadOutFrom(left)
            raise ValueError(f'INTERNAL ERROR: EllipsisJuxtapose must be attached to an ellipsis token. {left=}, {right=}')

        case RangeJuxtapose_t():
            if isinstance(right, BareRange):
                assert right.left is void, f"ERROR: can't attach expression to range, range already has values. Got {left=}, {right=}"
                right.left = left
                return right

            if isinstance(left, BareRange):
                assert left.right is void, f"ERROR: can't attach expression to range, range already has values. Got {left=}, {right=}"
                left.right = right
                return left

            raise ValueError(f'INTERNAL ERROR: Range Juxtapose must be next to a range. Got {left=}, {right=}')

        case BackticksJuxtapose_t():
            assert isinstance(left, Backticks) or isinstance(right, Backticks), f'INTERNAL ERROR: BackticksJuxtapose must be attached to a backticks token. Got {left=}, {right=}'
            if isinstance(left, Backticks):
                return CycleLeft(right, len(left.backticks))
            return CycleRight(left, len(right.backticks))

        case Comma_t():
            # TODO: combine left or right tuples into a single tuple
            if isinstance(left, PrototypeTuple) and isinstance(right, PrototypeTuple):
                return PrototypeTuple([*left.items, *right.items])
            elif isinstance(left, PrototypeTuple):
                return PrototypeTuple([*left.items, right])
            elif isinstance(right, PrototypeTuple):
                return PrototypeTuple([left, *right.items])
            else:
                return PrototypeTuple([left, right])

        case Operator_t(op='else'):
            if isinstance(left, Flow) and isinstance(right, Flow):
                # merge left+right as single flow
                return Flow([*left.branches, *right.branches])
            elif isinstance(left, Flow):
                # append right to left
                assert not isinstance(left.branches[-1], Default), f"ERROR: can't merge default branch into middle of flow. Got: {left=}, {right=}"
                if isinstance(right, Flowable):
                    return Flow([*left.branches, right])
                return Flow([*left.branches, Default(right)])

            elif isinstance(right, Flow):
                # prepend left to right
                assert isinstance(left, Flowable), f"ERROR: can only prepend Flowables to left of a Flow. Got: {left=}, {right=}"
                return Flow([left, *right.branches])
            else:
                # create a new flow out of the left and right
                assert isinstance(left, Flowable), f"ERROR: can only create a Flow from Flowables. Got: {left=}, {right=}"
                if isinstance(right, Flowable):
                    return Flow([left, right])
                return Flow([left, Default(right)])

        case Operator_t(op='in'):
            return IterIn(left, right)
        
        case OpChain_t(ops=list() as ops) if len(ops) > 1: #TODO: shouldn't be possible to make an OpChain with 1 or 0 ops
            for unary_op in reversed(ops[1:]):
                right = build_unary_prefix_expr(unary_op, right)
            return build_bin_expr(left, ops[0], right)

        case CombinedAssignmentOp_t(op=op):
            return Assign(left, build_bin_expr(left, op, right))

        case BroadcastOp_t(op=op):
            expr = build_bin_expr(left, op, right)
            assert isinstance(expr, BinOp), f'INTERNAL ERROR: expected BinOp, got {expr=}'
            return BroadcastOp(expr)

        case _:
            pdb.set_trace()
            raise NotImplementedError(f'Parsing of operator {op} has not been implemented yet')

_non_callables = (Int, Bool, String, IString, Array, Range, Dict, BidirDict, ObjectLiteral, Void, DotDotDot, BareRange, Backticks)
def build_quantum_juxtapose(left: AST, right: AST) -> QJux | Mul:
    call = Call(left, right) if not isinstance(left, _non_callables) else None
    index = Index(left, right) if isinstance(right, (Array, Range)) else None
    mul = Mul(left, right)

    # default to just mul if the other options definitely don't work
    if call is None and index is None:
        return mul

    return QJux(call=call, index=index, mul=mul)


def build_unary_prefix_expr(op: Token, right: AST) -> AST:
    """create a unary prefix expression AST from the op and right AST"""
    match op:
        # normal prefix operators
        case Operator_t(op='+'): return UnaryPos(right)
        case Operator_t(op='-'): return UnaryNeg(right)
        case Operator_t(op='*'): return UnaryMul(right)
        case Operator_t(op='/'): return UnaryDiv(right)
        case Operator_t(op='not'|'~'): return Not(right)  # TODO: don't want to hardcode Bool here!
        case Operator_t(op='@'): return AtHandle(right)

        # binary operators that appear to be unary because the left can be void
        # => called as unary prefix op means left was ()/void
        case Operator_t(op='=>'): return PrototypeFunctionLiteral(void, right)

        case OpChain_t(ops=list() as ops):
            for unary_op in reversed(ops):
                right = build_unary_prefix_expr(unary_op, right)
            return right

        case _:
            raise ValueError(f"INTERNAL ERROR: {op=} is not a known unary prefix operator")


def build_unary_postfix_expr(left: AST, op: Token) -> AST:
    """create a unary postfix expression AST from the left AST and op token"""
    match op:
        # normal postfix operators
        case Operator_t(op='!'): raise NotImplementedError(f"TODO: postfix op: {op=}")  # return Fact(left)

        # binary operators that appear to be unary because the right can be void
        # anything juxtaposed with void is treated as a zero-arg call()
        case Juxtapose_t():
            return Call(left)

        case _:
            raise NotImplementedError(f"TODO: {op=}")

def parse_string(token: String_t) -> String | IString:
    """Convert a string token to an AST"""

    if len(token.body) == 1 and isinstance(token.body[0], str):
        return String(token.body[0])

    # else handle interpolation strings
    parts = []
    for chunk in token.body:
        if isinstance(chunk, str):
            parts.append(chunk)
        elif isinstance(chunk, Escape_t):
            parts.append(chunk.to_str())
        else:
            ast = parse(chunk.body)
            if isinstance(ast, Block):
                parts.append(ast)
            elif isinstance(ast, ListOfASTs):
                pdb.set_trace()
                # not sure if this should ever come up, might be a parse bug
                # or might just need to convert to a block...
            else:
                parts.append(Block([ast]))

    # combine any adjacent Strings into a single string (e.g. if there were escapes)
    parts = iterchain(*((''.join(g),) if issubclass(t, str) else (*g,) for t, g in groupby(parts, type)))
    # convert any free strings to ASTs
    parts = [p if not isinstance(p, str) else String(p) for p in parts]

    # cast because pyright complains
    parts = cast(list[AST], parts)
    return IString(parts)


def as_dict_inners(items:list[AST]) -> list[PointsTo] | None:
    """Determine if the inner items indicate the container is a Dict (i.e. all items are points-to)"""
    if all(isinstance(i, PointsTo) for i in items):
        return cast(list[PointsTo], items)
    return None

def as_bidir_dict_inners(items:list[AST]) -> list[BidirPointsTo] | None:
    """Determine if the inner items indicate the container is a BidirDict (i.e. all items are bidir-points-to)"""
    if all(isinstance(i, BidirPointsTo) for i in items):
        return cast(list[BidirPointsTo], items)
    return None

def as_array_inners(items:list[AST]) -> list[AST] | None:
    """Determine if the inner items indicate the container is an Array (i.e. no points-to, assigns, or declarations)"""
    invalid_types = (Declare, Assign, PointsTo, BidirPointsTo)
    if any(isinstance(i, invalid_types) for i in items):
        return None
    return items

def as_object_inners(items:list[AST]) -> list[AST] | None:
    """determine if the inner items indicate the container is an Object (i.e. no points-to, and should contain at least one assign or declaration)"""
    invalid_types = (PointsTo, BidirPointsTo)
    expected_types = (Assign, Declare)
    if any(isinstance(i, invalid_types) for i in items):
        return None
    if not any(isinstance(i, expected_types) for i in items):
        return None
    return items

def parse_block(block: Block_t) -> AST:
    """Convert a block token to an AST"""

    # parse the inside of the block
    inner = parse(block.body)

    delims = block.left + block.right
    match delims, inner:
        case '()' | '{}' | '[]', Void():
            return inner
        case '()', ListOfASTs():
            return Group(inner.asts)
        case '{}', ListOfASTs():
            return Block(inner.asts)
        case '[]', ListOfASTs():
            if (asts:=as_dict_inners(inner.asts)) is not None:
                return Dict(asts)
            elif (asts:=as_bidir_dict_inners(inner.asts)) is not None:
                return BidirDict(asts)
            elif (asts:=as_array_inners(inner.asts)) is not None:
                return Array(asts)
            elif (asts:=as_object_inners(inner.asts)) is not None:
                return ObjectLiteral(inner.asts)
            # elif (asts:=as_array_generator_inners(inner.asts)) is not None:
            #     return ArrayGenerator(asts)
            # elif (asts:=as_dict_generator_inners(inner.asts)) is not None:
            #     return DictGenerator(asts)
            # elif (asts:=as_bidict_generator_inners(inner.asts)) is not None:
            #     return BidirDictGenerator(asts)
            #error cases
            if any(isinstance(i, PointsTo) for i in inner.asts) and not all(isinstance(i, PointsTo) for i in inner.asts):
                raise ValueError(f"ERROR: cannot mix PointsTo with other types in a dict: {inner=}")
            #TBD other known cases
            #otherwise there is an issue with the parser
            raise ValueError(f"INTERNAL ERROR: could not determine container type for {inner=}. Should have been suitably disambiguated by parser...")
        case '()' | '[]' | '(]' | '[)', BareRange():
            return Range(inner.left, inner.right, delims)

        # catch all cases for any type of AST inside a block or range
        case '()', _:
            return Group([inner])
        case '{}', _:
            return Block([inner])
        case '[]', PointsTo():
            return Dict([inner])
        case '[]', BidirPointsTo():
            return BidirDict([inner])
        case '[]', Assign() | Declare():
            return ObjectLiteral([inner])
        case '[]', _:
            # TODO: handle if this should be an object or dictionary instead of an array
            return Array([inner])
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'block parse not implemented for {block.left+block.right}, {type(inner)}')



def parse_type_param(param: TypeParam_t) -> TypeParam:
    items = parse(param.body)
    if isinstance(items, ListOfASTs):
        return TypeParam(items.asts)
    return TypeParam([items])


def parse_flow(flow: Flow_t) -> Flowable:

    # special case for closing else clause in a flow chain. Treat as `<if> <true> <clause>`
    if flow.keyword is None:
        return Default(parse_chain(flow.clause))

    assert flow.condition is not None, f"ERROR: flow condition must be present for {flow=}"
    cond = parse_chain(flow.condition)
    clause = parse_chain(flow.clause)

    match flow.keyword:
        case Keyword_t(src='if'): return If(cond, clause)
        case Keyword_t(src='loop'): return Loop(cond, clause)
        case _:
            pdb.set_trace()
            ...
            raise NotImplementedError('TODO: other flow keywords, namely lazy')
    pdb.set_trace()
    ...


def parse_declare(declare: Declare_t) -> Declare:
    expr = parse_chain(declare.expr)
    assert isinstance(expr, (PrototypeIdentifier, Identifier, TypedIdentifier, ReturnTyped, UnpackTarget, Assign)), f'ERROR: expected identifier, typed-identifier, or unpack target for declare expression, got {expr=}'
    match declare:
        case Declare_t(keyword=Keyword_t(src='let')): return Declare(DeclarationType.LET, expr)
        case Declare_t(keyword=Keyword_t(src='const')): return Declare(DeclarationType.CONST, expr)
        # case Declare_t(keyword=Keyword_t(src='local_const')): return Declare(DeclarationType.LOCAL_CONST, expr)
        # case Declare_t(keyword=Keyword_t(src='fixed_type')): return Declare(DeclarationType.FIXED_TYPE, expr)
        case _:
            raise ValueError(f"ERROR: unknown declare keyword {declare.keyword=}. Expected one of {DeclarationType.__members__}. {declare=}")
    pdb.set_trace()
    raise NotImplementedError






################################ Docs Markdown Helpers ################################
opname_map = {
    '@': 'reference',
    '.': 'access',
    '^': 'power',
    '*': 'multiply',
    '/': 'divide',
    '%': 'modulus',
    '+': 'add',
    '-': 'subtract',
    '<<': 'left shift',
    '>>': 'right shift',
    '>>>': 'rotate left no carry',
    '<<<': 'rotate right no carry',
    '<<!': 'rotate left with carry',
    '!>>': 'rotate right with carry',
    '>?': 'greater than',
    '<?': 'less than',
    '>=?': 'greater than or equal',
    '<=?': 'less than or equal',
    '=?': 'equal',
    'and': 'and',
    'nand': 'nand',
    '&': 'and',
    'xor': 'xor',
    'xnor': 'xnor',
    'or': 'or',
    'nor': 'nor',
    '|': 'or',
    '=>': 'function arrow',
    '=': 'bind',
    'else': 'flow alternate',
    ';': 'semicolon',
    'in': 'in',
    'as': 'as',
    'transmute': 'transmute',
    '|>': 'pipe',
    '<|': 'reverse pipe',
    '->': 'right pointer',
    '<->': 'bidir pointer',
    '<-': 'left pointer',
    ':': 'type annotation',

    Comma_t(None): 'comma',
    Juxtapose_t(None): 'unknown juxtapose',
    EllipsisJuxtapose_t(None): 'ellipsis juxtapose',
    RangeJuxtapose_t(None): 'range juxtapose',
    TypeParamJuxtapose_t(None): 'type param juxtapose',
}


def get_precedence_table_markdown() -> str:
    """return a string that is the markdown table for the docs containing all the operators"""
    header = '| Precedence | Operator | Name | Associativity |\n| --- | --- | --- | --- |'

    def get_ops_str(ops: list[Operator_t | ShiftOperator_t | Juxtapose_t | Comma_t]) -> str:
        return '<br>'.join(f'`{op.op if isinstance(op, (Operator_t, ShiftOperator_t)) else op.__class__.__name__[:-2].lower()}`' for op in ops)

    def get_opnames_str(ops: list[Operator_t | ShiftOperator_t | Juxtapose_t | Comma_t]) -> str:
        return '<br>'.join(f'{opname_map.get(op.op, None) if isinstance(op, (Operator_t, ShiftOperator_t)) else op.__class__.__name__[:-2].lower()}' for op in ops)

    def get_row_str(row: tuple[Associativity, list[Operator_t | ShiftOperator_t | Juxtapose_t | Comma_t]]) -> str:
        assoc, group = row
        return f'{get_ops_str(group)} | {get_opnames_str(group)} | {assoc.name}'

    rows = [
        f'| {i} | {get_row_str(row)} |'
        for i, row in reversed([*enumerate(operator_groups)])
    ]

    return header + '\n' + '\n'.join(rows)
19:T5b2f,from .tokenizer import (
    tokenize, tprint, full_traverse_tokens,
    unary_prefix_operators,
    unary_postfix_operators,
    binary_operators,
    opchain_starters,
    Token,
    Keyword_t, Undefined_t, Void_t, End_t, New_t,
    WhiteSpace_t, Escape_t,
    Identifier_t, Hashtag_t,
    Block_t, TypeParam_t,
    RawString_t, String_t,
    Integer_t, BasedNumber_t, Boolean_t,
    DotDot_t, DotDotDot_t, Backticks_t,
    Juxtapose_t, Operator_t, ShiftOperator_t, Comma_t,
)

from typing import Generator, overload, cast
from abc import ABC, abstractmethod


import pdb


# A chain is just a list of tokens that is known to be directly parsable as an expression without any other syntax
# i.e. it is the result of calls to `get_next_chain()`
# all other syntax is wrapped up into compound tokens
# it should literally just be a sequence of atoms and operators
from typing import TypeVar
T = TypeVar('T', bound=Token)
class Chain(list[T]):
    """class for explicitly annotating that a token list is a single chain"""

# class Chain[T](list[T]):
#     """class for explicitly annotating that a token list is a single chain"""


############### NEW TOKENS CREATED BY POST-TOKENIZATION PROCESS ###############

class Flow_t(Token):
    @overload
    def __init__(self, keyword: None, condition: None, clause: Chain[Token]): ...  # closing else
    @overload
    def __init__(self, keyword: Keyword_t, condition: Chain[Token], clause: Chain[Token]): ...  # if, loop, lazy

    def __init__(self, keyword: Keyword_t | None, condition: Chain[Token] | None, clause: Chain[Token]):
        if keyword is None and condition is not None:
            raise ValueError("closing else should have no condition. `keyword` and `condition` should both be None")
        self.keyword = keyword
        self.condition = condition
        self.clause = clause

    def __repr__(self) -> str:
        return f"<Flow_t: {self.keyword}: {self.condition} {self.clause}>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        if self.condition is not None:
            yield self.condition
        yield self.clause


# class Do_t(Token):...
# class Return_t(Token):...
# class Express_t(Token):...


class Declare_t(Token):
    def __init__(self, keyword: Keyword_t, expr: Chain[Token]):
        self.keyword = keyword
        self.expr = expr

    def __repr__(self) -> str:
        return f"<Declare_t: {self.keyword} {self.expr}>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield [self.keyword] #appraently flow doesn't yield the keyword. tbd if it matters...
        yield self.expr


class RangeJuxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __repr__(self) -> str:
        return "<RangeJuxtapose_t>"

    def __hash__(self) -> int:
        return hash(RangeJuxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, RangeJuxtapose_t)


class EllipsisJuxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __repr__(self) -> str:
        return "<EllipsisJuxtapose_t>"

    def __hash__(self) -> int:
        return hash(EllipsisJuxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, EllipsisJuxtapose_t)


class BackticksJuxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __repr__(self) -> str:
        return "<BackticksJuxtapose_t>"

    def __hash__(self) -> int:
        return hash(BackticksJuxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, BackticksJuxtapose_t)


class TypeParamJuxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __repr__(self) -> str:
        return "<TypeParamJuxtapose_t>"

    def __hash__(self) -> int:
        return hash(TypeParamJuxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, TypeParamJuxtapose_t)

class OpChain_t(Token):
    def __init__(self, ops:list[Operator_t]):
        assert len(ops) > 1, f"OpChain_t must have at least 2 operators. Got {len(ops)} operators"
        self.ops = ops

    def __repr__(self) -> str:
        return f"<OpChain_t: {''.join(op.op for op in self.ops)}>"

    def __hash__(self) -> int:
        return hash((OpChain_t, tuple(self.ops)))

    def __eq__(self, other) -> bool:
        return isinstance(other, OpChain_t) and self.ops == other.ops

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield cast(list[Token], self.ops)

class BroadcastOp_t(Token):
    def __init__(self, dot:Operator_t, op:Operator_t|OpChain_t):
        assert isinstance(dot, Operator_t) and dot.op == '.', f"VectorizedOp_t must have a '.' operator. Got {dot}"
        self.dot = dot
        self.op = op

    def __repr__(self) -> str:
        return f"<VectorizedOp_t: {self.dot}, {self.op}>"

    def __hash__(self) -> int:
        return hash((BroadcastOp_t, self.dot, self.op))

    def __eq__(self, other) -> bool:
        return isinstance(other, BroadcastOp_t) and self.dot == other.dot and self.op == other.op

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield [self.dot]
        yield [self.op]


class CombinedAssignmentOp_t(Token):
    def __init__(self, op:Operator_t|OpChain_t|BroadcastOp_t, assign:Operator_t):
        assert isinstance(assign, Operator_t) and assign.op == '=', f"CombinedAssignmentOp_t must have an '=' operator. Got {assign}"
        self.op = op
        self.assign = assign

    def __repr__(self) -> str:
        return f"<CombinedAssignmentOp_t: {self.op}, {self.assign}>"

    def __hash__(self) -> int:
        return hash((CombinedAssignmentOp_t, self.op, self.assign))

    def __eq__(self, other) -> bool:
        return isinstance(other, CombinedAssignmentOp_t) and self.op == other.op and self.assign == other.assign

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield [self.op]
        yield [self.assign]


atom_tokens = (
    Identifier_t,
    Integer_t,
    Boolean_t,
    BasedNumber_t,
    RawString_t,
    String_t,
    Block_t,
    TypeParam_t,
    Hashtag_t,
    DotDot_t,
    DotDotDot_t,
    Backticks_t,
    Flow_t,
    Undefined_t,
)

# atoms that can be juxtaposed (so juxtaposes next to them shouldn't be removed)
jux_atoms = (
    DotDot_t,
    DotDotDot_t,
    Backticks_t,
)

non_jux_ops = (
    Operator_t,
    ShiftOperator_t,
    Comma_t
)



class ShouldBreakTracker(ABC):
    @abstractmethod
    def op_breaks_chain(self, token: Token) -> bool: ...

    @abstractmethod
    def view(self, tokens: list[Token]) -> None: ...


class ShouldBreakFlowTracker(ShouldBreakTracker):
    def __init__(self):
        self.flows_seen = 0

    def op_breaks_chain(self, token: Token) -> bool:
        # should only be operators
        if isinstance(token, Operator_t) and token.op == 'else':
            if self.flows_seen == 0:
                return True
            self.flows_seen -= 1

        return False

    def view(self, tokens: list[Token]) -> None:
        # view each token without any ability to do anything
        # keep track of how many flows we've seen
        for token in tokens:
            if isinstance(token, Flow_t) and token.keyword is not None:
                self.flows_seen += 1
            if isinstance(token, Operator_t) and token.op == 'else':
                raise ValueError("should not be seeing else here")
            if isinstance(token, Keyword_t) and token.src in ('if', 'loop', 'lazy'):
                raise ValueError("should not be seeing if/loop/lazy here. Everything should be bundled up into a flow")


def invert_whitespace(tokens: list[Token]) -> None:
    """
    removes all whitespace tokens, and insert juxtapose tokens between adjacent pairs (i.e. not separated by whitespace)

    Args:
        tokens (list[Token]): list of tokens to modify. This is modified in place.
    """

    # juxtapose singleton token so we aren't wasting memory
    jux = Juxtapose_t(None)

    i = 0
    while i < len(tokens):
        # delete whitespace if it comes up
        if isinstance(tokens[i], WhiteSpace_t):
            tokens.pop(i)
            continue

        # recursively handle inverting whitespace for blocks
        if isinstance(tokens[i], (Block_t, TypeParam_t)):
            invert_whitespace(tokens[i].body)
        elif isinstance(tokens[i], String_t):
            for child in tokens[i].body:
                if isinstance(child, Block_t):
                    invert_whitespace(child.body)

        # insert juxtapose if no whitespace between tokens
        if i + 1 < len(tokens) and not isinstance(tokens[i + 1], WhiteSpace_t):
            tokens.insert(i + 1, jux)
            i += 1
        i += 1

    # finally, remove juxtapose tokens next to operators that are not whitespace sensitive
    i = 1
    while i < len(tokens) - 1:
        left, middle, right = tokens[i-1:i+2]
        #TODO: somewhere around here, need to fix how @ isn't juxtaposable but should be on the left depending on lots of stuff...
        if isinstance(middle, Juxtapose_t) \
        and (isinstance(left, non_jux_ops) or isinstance(right, non_jux_ops))\
        and not isinstance(left, jux_atoms) and not isinstance(right, jux_atoms):
            tokens.pop(i)
            continue
        i += 1


def _get_next_prefixes(tokens: list[Token]) -> tuple[list[Token], list[Token]]:
    prefixes = []
    while len(tokens) > 0 and is_unary_prefix_op(tokens[0]):
        prefixes.append(tokens.pop(0))

    return prefixes, tokens


def _get_next_postfixes(tokens: list[Token]) -> tuple[list[Token], list[Token]]:
    postfixes = []
    while len(tokens) > 0 and is_unary_postfix_op(tokens[0], exclude_semicolon=True):
        postfixes.append(tokens.pop(0))

    return postfixes, tokens


def _get_next_atom(tokens: list[Token]) -> tuple[Token, list[Token]]:
    if len(tokens) == 0:
        raise ValueError(f"ERROR: expected atom, got {tokens=}")

    # TODO: this is going to be unnecessary as expressions will have been bundled up into single tokens
    if isinstance(tokens[0], Keyword_t):
        return _get_next_keyword_expr(tokens)

    if isinstance(tokens[0], atom_tokens):
        return tokens[0], tokens[1:]

    raise ValueError(f"ERROR: expected atom, got {tokens[0]=}")


def _get_next_chunk(tokens: list[Token]) -> tuple[list[Token], list[Token]]:
    chunk = []
    t, tokens = _get_next_prefixes(tokens)
    chunk.extend(t)

    t, tokens = _get_next_atom(tokens)
    if t is None:
        raise ValueError(f"ERROR: expected atom, got {tokens[0]=}")
    chunk.append(t)

    t, tokens = _get_next_postfixes(tokens)
    chunk.extend(t)

    return chunk, tokens


def is_unary_prefix_op(token: Token) -> bool:
    """
    Determines if a token could be a unary prefix operator.
    Note that this is not mutually exclusive with being a postfix operator or a binary operator.
    """
    return isinstance(token, Operator_t) and token.op in unary_prefix_operators \
        or isinstance(token, OpChain_t) and token.ops[0].op in unary_prefix_operators


def is_unary_postfix_op(token: Token, exclude_semicolon: bool = False) -> bool:
    """
    Determines if a token could be a unary postfix operator.
    Optionally can exclude semicolon from the set of operators.
    Note that this is not mutually exclusive with being a prefix operator or a binary operator.
    """
    if exclude_semicolon:
        return isinstance(token, Operator_t) and token.op in unary_postfix_operators - {';'}
    return isinstance(token, Operator_t) and token.op in unary_postfix_operators


def is_binop(token: Token) -> bool:
    """
    Determines if a token could be a binary operator.
    Note that this is not mutually exclusive with being a prefix operator or a postfix operator.
    """
    return isinstance(token, Operator_t) and token.op in binary_operators or isinstance(token, (ShiftOperator_t, Comma_t, Juxtapose_t, RangeJuxtapose_t, EllipsisJuxtapose_t, BackticksJuxtapose_t, TypeParamJuxtapose_t, OpChain_t, BroadcastOp_t, CombinedAssignmentOp_t))


def is_op(token: Token) -> bool:
    return is_binop(token) or is_unary_prefix_op(token) or is_unary_postfix_op(token)


def is_opchain_starter(token: Token) -> bool:
    return isinstance(token, Operator_t) and token.op in opchain_starters


def _get_next_keyword_expr(tokens: list[Token]) -> tuple[Token, list[Token]]:
    """package up the next keyword expression into a single token"""
    if len(tokens) == 0:
        raise ValueError(f"ERROR: expected keyword expression, got {tokens=}")
    t, tokens = tokens[0], tokens[1:]

    if not isinstance(t, Keyword_t):
        raise ValueError(f"ERROR: expected keyword expression, got {t=}")

    match t:
        case Keyword_t(src='if' | 'loop' | 'lazy'):
            cond, tokens = get_next_chain(tokens)
            clause, tokens = get_next_chain(tokens, tracker=ShouldBreakFlowTracker())
            return Flow_t(t, cond, clause), tokens
        case Keyword_t(src='closing_else'):
            clause, tokens = get_next_chain(tokens, tracker=ShouldBreakFlowTracker())
            return Flow_t(None, None, clause), tokens
        case Keyword_t(src='do'):
            clause, tokens = get_next_chain(tokens)
            # assert next token is a do_keyward
            # depending on the keyward, get a condition, or condition+clause
            pdb.set_trace()
            ...
        case Keyword_t(src='return'):
            # TBD how to do this one...
            pdb.set_trace()
            ...
        case Keyword_t(src='express'):
            pdb.set_trace()
            ...
        case Keyword_t(src='let' | 'const' | 'local_const' | 'fixed_type'):
            expr, tokens = get_next_chain(tokens)
            return Declare_t(t, expr), tokens


    raise NotImplementedError("TODO: handle keyword based expressions")
    # return #chain?
    # yield #chain
    # (break | continue) #hashtag? //note the hashtag should be an entire chain if present
    # (let | const) #chain


def get_next_chain(tokens: list[Token], *, tracker: ShouldBreakTracker = None, op_blacklist: set[Token] = None) -> tuple[Chain[Token], list[Token]]:
    """
    grab the next single expression chain of tokens from the given list of tokens

    Also wraps up keyword-based expressions (if loop etc.) into a single token

    A chain is represented by the following grammar:
        #chunk = #prefix_op* #atom_expr (#postfix_op - ';')*
        #chain = #chunk (#binary_op #chunk)* ';'?

    Args:
        tokens (list[Token]): list of tokens to grab the next chain from
        tracker (ShouldBreakTracker, optional): tracker for complex analysis to determine if an operator should break the chain. Defaults to None.
        op_blacklist (set[Token], optional): simpler handler for operators that should break the chain. Defaults to None.

    Returns:
        next, rest (list[Token], list[Token]): the next chain of tokens, and the remaining tokens
    """

    if op_blacklist is None:
        op_blacklist = set()

    chain = []

    # grab the first chunk and let the tracker view it
    chunk, tokens = _get_next_chunk(tokens)
    chain.extend(chunk)
    if tracker is not None:
        tracker.view(chunk)

    while len(tokens) > 0 and is_binop(tokens[0]) and (tracker is None or not tracker.op_breaks_chain(tokens[0])) and tokens[0] not in op_blacklist:
        # get the operator, and continuing chunk, then let the tracker view it
        chain.append(tokens.pop(0))
        chunk, tokens = _get_next_chunk(tokens)
        chain.extend(chunk)
        if tracker is not None:
            tracker.view(chunk)

    # if there's a semicolon, it ends the chain
    if len(tokens) > 0 and isinstance(tokens[0], Operator_t) and tokens[0].op == ';':
        chain.append(tokens.pop(0))

    return Chain(chain), tokens


def narrow_juxtapose(tokens: list[Token]) -> None:
    """
    range juxtapose:
    convert [<token>, <jux>, <..>] into [<token>, <range_jux>, <..>]
    convert [<..>, <jux>, <token>] into [<..>, <range_jux>, <token>]
    if .. doesn't connect to anything on the left or right, connect it to undefined

    ellipsis juxtapose:
    convert [<...>, <jux>, <token>] into [<...>, <ellipsis_jux>, <token>]

    type param juxtapose:
    convert [<token>, <jux>, <type_param>] into [<token>, <type_param_jux>, <type_param>]
    convert [<type_param>, <jux>, <token>] into [<type_param>, <type_param_jux>, <token>]
    """
    range_jux = RangeJuxtapose_t(None)
    ellipsis_jux = EllipsisJuxtapose_t(None)
    backticks_jux = BackticksJuxtapose_t(None)
    type_param_jux = TypeParamJuxtapose_t(None)
    undefined = Undefined_t(None)
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        left_is_jux = i > 0 and isinstance(stream[i-1], Juxtapose_t)
        right_is_jux = i + 1 < len(stream) and isinstance(stream[i+1], Juxtapose_t)

        # handle range jux
        if isinstance(token, DotDot_t):
            if i + 1 < len(stream):
                if isinstance(stream[i+1], Juxtapose_t):
                    stream[i+1] = range_jux
                else:
                    stream[i+1:i+1] = [range_jux, undefined]
            if i > 0:
                if isinstance(stream[i-1], Juxtapose_t):
                    stream[i-1] = range_jux
                else:
                    stream[i:i] = [undefined, range_jux]
                    gen.send(i+3)

        # handle ellipsis jux
        elif isinstance(token, DotDotDot_t):
            # ellipsis can be optionally juxtaposed, but when it is juxtaposed, it may only be juxtaposed on one side
            if left_is_jux and right_is_jux:
                raise ValueError(f"ERROR: ellipsis operator {token} must be juxtaposed on either zero or one side. Got ...{stream[i-2:i+3]}...")
            if left_is_jux:
                stream[i-1] = ellipsis_jux
            if right_is_jux:
                stream[i+1] = ellipsis_jux

        # handle type param jux
        elif isinstance(token, TypeParam_t):
            if left_is_jux:
                stream[i-1] = type_param_jux
            elif right_is_jux:
                stream[i+1] = type_param_jux

        # handle backticks jux
        elif isinstance(token, Backticks_t):
            # only left or right can be juxtaposed, but not both, and not neither
            if (left_is_jux and right_is_jux) or (not left_is_jux and not right_is_jux):
                raise ValueError(f"ERROR: backticks operator {token} must be juxtaposed on a exactly one side. Got ...{stream[i-2:i+3]}...")

            if left_is_jux:
                stream[i-1] = backticks_jux
            elif right_is_jux:
                stream[i+1] = backticks_jux



def convert_bare_else(tokens: list[Token]) -> None:
    """
    convert any instances of `else` without a flow keyword after, and convert to `else` `if` `true`
    """
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if isinstance(token, Operator_t) and token.op == 'else':
            if i+1 < len(stream) and isinstance(stream[i+1], Keyword_t) and stream[i+1].src in ('if', 'loop', 'lazy'):
                continue
            # stream[i+1:i+1] = [Keyword_t('if'), Boolean_t('true')]
            stream.insert(i+1, Keyword_t('closing_else'))


def bundle_conditionals(tokens: list[Token]) -> None:
    """
    Convert sequences of tokens that represent conditionals (if, loop, etc.) into a single expression token
    """
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if isinstance(token, Keyword_t) and token.src in ('if', 'loop', 'lazy'):
            flow_chain, tokens = get_next_chain(stream[i:])
            stream[i] = flow_chain[0]
            stream[i+1:] = [*flow_chain[1:], *tokens]


def make_chain_operators(tokens: list[Token]) -> None:
    """Convert consecutive operator tokens into a single opchain token"""
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if is_opchain_starter(token):
            j = 1
            while i+j < len(stream) and is_unary_prefix_op(stream[i+j]):
                j += 1
            if j > 1:
                # convert the prefix operators into a single token
                stream[i:i+j] = [OpChain_t([stream[i+k] for k in range(j)])]
                gen.send(i+j)
                continue

def make_broadcast_operators(tokens: list[Token]) -> None:
    """Convert any . operator next to a binary operator or opchain into a broadcast operator"""
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if isinstance(token, Operator_t) and token.op == '.':
            if len(stream) > i+1 and is_binop(stream[i+1]) or isinstance(stream[i+1], OpChain_t):
                stream[i:i+2] = [BroadcastOp_t(token, stream[i+1])]
                gen.send(i+2)

def make_combined_assignment_operators(tokens: list[Token]) -> None:
    """Convert any combined assignment operators into a single token"""
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if is_binop(token) or isinstance(token, OpChain_t) or isinstance(token, BroadcastOp_t):
            if i+1 < len(stream) and isinstance(stream[i+1], Operator_t) and stream[i+1].op == '=':
                stream[i:i+2] = [CombinedAssignmentOp_t(token, stream[i+1])]
                gen.send(i+2)

def post_process(tokens: list[Token]) -> None:
    """post process the tokens to make them ready for parsing"""

    # remove whitespace, and insert juxtapose tokens
    invert_whitespace(tokens)

    if len(tokens) == 0:
        return

    # find any instances of <else> without a flow keyword after, and convert to <else> <if> <true>
    convert_bare_else(tokens)

    # bundle up conditionals into single token expressions
    bundle_conditionals(tokens)

    # combine operator chains into a single operator token
    make_chain_operators(tokens)

    # convert any . operator next to a binary operator or opchain (e.g. .+ .^/-) into a broadcast operator
    make_broadcast_operators(tokens)

    # convert any combined assignment operators (e.g. += -= etc.) into a single token
    make_combined_assignment_operators(tokens)

    # convert juxtapose tokens to more specific types if possible
    narrow_juxtapose(tokens)


    # make the actual list of chains

    # based on types, replace jux with jux_mul or jux_call
    # TODO: actually this probably would need to be done during parsing, since we can't get a type for a complex/compound expression...


def test():
    with open('../../../examples/hello.dewy') as f:
        src = f.read()

    tokens = tokenize(src)

    # chainer process
    post_process(tokens)

    pdb.set_trace()
    ...


def test2():
    """gauntlet of multiple tests from example file"""
    with open('../../../examples/syntax3.dewyl') as f:
        lines = f.readlines()

    # filter out empty lines
    lines = [l for line in lines if (l := line.strip())]

    for line in lines:
        tokens = tokenize(line)

        # chainer process
        post_process(tokens)

        # other stuff? pass to the parser? etc.

    pdb.set_trace()
    ...


def test_hello():
    line = "printl'Hello, World!'"

    tokens = tokenize(line)
    post_process(tokens)

    pdb.set_trace()
    ...


if __name__ == '__main__':
    # test()
    # test2()
    test_hello()
1a:T2dcb,"""after the main parsing, post parse to handle any remaining prototype asts within the main ast"""

from .syntax import (
    AST,
    Access,
    Declare,
    PointsTo, BidirPointsTo,
    Type,
    ListOfASTs, PrototypeTuple, Block, BareRange, Ellipsis, DotDotDot, CollectInto, SpreadOutFrom, Array, Group, Range, ObjectLiteral, Dict, BidirDict, TypeParam,
    Void, Undefined, void, undefined, untyped,
    String, IString,
    Flowable, Flow, If, Loop, Default,
    PrototypeFunctionLiteral, PrototypeBuiltin, Call,
    Index,
    PrototypeIdentifier, Express, Identifier, TypedIdentifier, ReturnTyped, UnpackTarget, Assign,
    Int, Bool,
    Range, IterIn,
    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,
    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv, AtHandle,
    CycleLeft, CycleRight, Suppress,
    BroadcastOp,
    DeclarationType,
    DeclareGeneric, Parameterize,
)
from .parser import QJux

from typing import Callable as TypingCallable
from dataclasses import field
import pdb


class Signature(AST):
    pkwargs: list[AST] = field(default_factory=list)
    pargs:   list[AST] = field(default_factory=list)
    kwargs:  list[AST] = field(default_factory=list)
    #TODO: probably keep track of spread args i.e. "spargs"

    def _is_delimited(self) -> bool:
        n_args = len(self.pkwargs) + len(self.pargs) + len(self.kwargs)
        if n_args > 1 or n_args == 0:
            return False
        if any(isinstance(i, Assign) for i in self.pkwargs):
            return False
        if len(self.pargs) > 0 or len(self.kwargs) > 0:
            return False
        return True

    def __str__(self):
        pkwargs = ' '.join(str(i) for i in self.pkwargs)
        pargs   = ' '.join(str(i) for i in self.pargs)
        kwargs  = ' '.join(str(i) for i in self.kwargs)

        if pargs:
            pargs = f' #pos_only {pargs}'
        if kwargs:
            kwargs = f' #kw_only {kwargs}'

        s = f'{pkwargs}{pargs}{kwargs}'.strip()

        if self._is_delimited():
            return s
        return f'({s})'


# basically just convert all the different types of args to a normalized format (i.e. group)
class FunctionLiteral(AST):
    args: Signature
    body: AST

    def __str__(self):
        return f'{self.args} => {self.body}'



def post_parse(ast: AST) -> AST:

    # any conversions should probably run simplest to most complex
    ast = convert_prototype_tuples(ast)
    ast = convert_bare_ranges(ast)
    ast = convert_bare_ellipses(ast)
    ast = convert_prototype_function_literals(ast)
    ast = convert_prototype_identifiers(ast)

    # at the end of the post parse process
    if not ast.is_settled():
        raise ValueError(f'INTERNAL ERROR: Parse was incomplete. AST still has prototypes\n{ast!r}')

    return ast


def convert_prototype_identifiers(ast: AST) -> AST:
    """Convert all PrototypeIdentifiers to either Identifier or Express, depending on the context"""
    ast = Group([ast])
    for i in (gen := ast.__full_traversal_iter__()):

        # skip ASTs that don't have any Prototypes
        if i.is_settled():
            continue

        # if we ever get to a bare identifier, treat it like an express
        if isinstance(i, PrototypeIdentifier):
            gen.send(Express(Identifier(i.name)))
            continue

        match i:
            case Call(f=PrototypeIdentifier(name=name), args=args):
                gen.send(Call(Identifier(name), args))
            case Call(args=None) | Call(f=AtHandle()) | Call(f=Group()): ...
            # case Call(args=args): ... #TODO: handling when args is not none... generally will be a list of identifiers that need to be converted directly to Identifier
            case Call():
                pdb.set_trace()
                ...
            case AtHandle(operand=PrototypeIdentifier(name=name)):
                gen.send(AtHandle(Identifier(name)))
            case AtHandle():
                pdb.set_trace()
                ...          
            case Assign(left=PrototypeIdentifier(name=name), right=right):
                gen.send(Assign(Identifier(name), right))
            case Assign(left=Array() as arr, right=right):
                target = convert_prototype_to_unpack_target(arr)
                gen.send(Assign(target, right))
            case Assign():
                pdb.set_trace()
                ...
            case IterIn(left=PrototypeIdentifier(name=name), right=right):
                gen.send(IterIn(Identifier(name), right))
            case IterIn(left=Array() as arr, right=right):
                target = convert_prototype_to_unpack_target(arr)
                gen.send(IterIn(target, right))
            case IterIn():
                pdb.set_trace()
                ...
            case UnpackTarget(): #TODO: may not need this one
                pdb.set_trace()
                ...
            case Declare(decltype=decltype, target=PrototypeIdentifier(name=name)):
                gen.send(Declare(decltype, Identifier(name)))
            case Declare(decltype=decltype, target=Array() as arr):
                pdb.set_trace()
                ...
            case Declare(decltype=decltype, target=Group() as group):
                pdb.set_trace()
                ...
            case Declare(): ... # all other declare cases are handled as normal
            case Index(): ... # I think all index cases are handled as normal
            case Access(left=left, right=PrototypeIdentifier(name=name)):
                gen.send(Access(left, Identifier(name)))
            case Access(left=left, right=AtHandle(operand=PrototypeIdentifier(name=name))):
                gen.send(Access(left, AtHandle(Identifier(name))))
            case Access():
                pdb.set_trace()
                ...

            # cases that themselves don't get adjusted but may contain nested children that need to be converted
            case IString() | Group() | Block() | PrototypeTuple() | Array() | ObjectLiteral() | Dict() | BidirDict() | FunctionLiteral() | Signature() | Range() | Loop() | If() | Flow() | Default() \
                | PointsTo() | BidirPointsTo() | Equal() | Less() | LessEqual() | Greater() | GreaterEqual() | LeftShift() | RightShift() | LeftRotate() | RightRotate() | LeftRotateCarry() | RightRotateCarry() | Add() | Sub() | Mul() | Div() | IDiv() | Mod() | Pow() | And() | Or() | Xor() | Nand() | Nor() | Xnor() | MemberIn() \
                | BroadcastOp() | SpreadOutFrom() | QJux() \
                | Not() | UnaryPos() | UnaryNeg() | UnaryMul() | UnaryDiv() | CycleLeft() | CycleRight() \
                | TypedIdentifier():
                ...
            #TBD cases: Type() | ListOfASTs() | BareRange() | Ellipsis() | Spread() | TypeParam() | Flowable() | Flow() | PrototypeBuiltin() | Builtin() | Express() | ReturnTyped() | SequenceUnpackTarget() | ObjectUnpackTarget() | DeclarationType() | DeclareGeneric() | Parameterize():
            case _:  # all others are traversed as normal
                raise ValueError(f'Unhandled case {type(i)}')
            #     pdb.set_trace()
            #     ...

    return ast.items[0]

#TODO: maybe have one of these for Array, Object, Dict, BidirDict depending on what is to be unpacked
#      hard though because also requires the type of whatever is being unpacked
#      because array unpack and object unpack can look the same syntactically
def convert_prototype_to_unpack_target(ast: Array) -> UnpackTarget:
    """Convert an Array of PrototypeIdentifiers or other ASTs to an UnpackTarget"""
    for i in (gen := ast.__full_traversal_iter__()):
        if i.is_settled():
            continue

        match i:
            case PrototypeIdentifier(name=name):
                gen.send(Identifier(name))
            case Assign(left=PrototypeIdentifier(name=name), right=right):
                gen.send(Assign(Identifier(name), right))
            case Array() as arr:
                gen.send(convert_prototype_to_unpack_target(arr))
            case CollectInto(): ...
            case TypedIdentifier(): ...
            case _:
                raise NotImplementedError(f'Unhandled case {type(i)} in convert_prototype_to_unpack_target')

    return UnpackTarget(ast.items)


def convert_prototype_tuples(ast: AST) -> AST:
    """For now, literally just turn all tuples into arrays"""
    ast = Group([ast])
    for i in (gen := ast.__full_traversal_iter__()):
        if isinstance(i, PrototypeTuple):
            gen.send(Array(i.items))
    return ast.items[0]

def convert_bare_ranges(ast: AST) -> AST:
    """Convert all BareRanges to Ranges with inclusive bounds"""
    ast = Group([ast])
    for i in (gen := ast.__full_traversal_iter__()):
        if isinstance(i, BareRange):
            gen.send(Range(i.left, i.right, '[]'))
    return ast.items[0]


def convert_bare_ellipses(ast: AST) -> AST:
    """Convert all remaining DotDotDots (that were not juxtaposed) to Ellipsis"""
    ast = Group([ast])
    for i in (gen := ast.__full_traversal_iter__()):
        if isinstance(i, DotDotDot):
            gen.send(Ellipsis())
    return ast.items[0]


def convert_prototype_function_literals(ast: AST) -> AST:
    ast = Group([ast])
    for i in (gen := ast.__full_traversal_iter__()):
        if isinstance(i, PrototypeFunctionLiteral):
            args = normalize_function_args(i.args)
            gen.send(FunctionLiteral(args, i.body))
    return ast.items[0]


def normalize_function_arg(arg: AST) -> tuple[list[AST], list[AST], list[AST]]:
    pkwarg, parg, kwarg = [], [], []
    match arg:
        case Void(): ...
        case PrototypeIdentifier(name=name):
            pkwarg.append(Identifier(name))
        case Identifier() | TypedIdentifier() | Assign():
            pkwarg.append(arg)
        case Array() as arr:
            pdb.set_trace()
            parg.append(convert_prototype_to_unpack_target(arr))
        # case Spread() | UnpackTarget():
        #     pdb.set_trace()
        #     ...
        # case Dict() | BidirDict():
        #     pdb.set_trace()
        #     ...
        #     #copilot suggested these could be kwargs, though I suspect it won't work (i.e. how is default vs no default handled? name -> void)
        #     #think about though. could use identifiers directly instead of strings

        case _:
            raise NotImplementedError(f'normalize_signature not implemented yet for {arg=}')

    return pkwarg, parg, kwarg


# def array_items_to_unpack_target(items: list[AST]) -> UnpackTarget:
#     """Convert an Array of ASTs to an UnpackTarget"""
#     unpack_items = []
#     for i in items:
#         match i:
#             case Identifier() | PrototypeIdentifier() | TypedIdentifier() | Assign() | Spread() | UnpackTarget():
#                 unpack_items.append(i)
#             case Array(items):
#                 unpack_items.append(array_items_to_unpack_target(items))
#             case _:
#                 raise NotImplementedError(f'array_items_to_unpack_target not implemented yet for {i=}')
#     return UnpackTarget(unpack_items)


def normalize_function_args(signature: AST) -> Signature:
    """Convert all the different function arg syntax options to a normalized format (group)"""
    if not isinstance(signature, Group):
        return Signature(*normalize_function_arg(signature))

    pkwargs, pargs, kwargs = [], [], []
    for i in signature.items:
        pkw, p, kw = normalize_function_arg(i)
        pkwargs.extend(pkw)
        pargs.extend(p)
        kwargs.extend(kw)
    return Signature(pkwargs, pargs, kwargs)


1b:T53ad,from abc import ABC, abstractmethod, ABCMeta
from typing import get_args, get_origin, Generator, Any, Literal, Union, dataclass_transform, Callable as TypingCallable
from types import UnionType
from dataclasses import dataclass, field, fields
from enum import Enum, auto
# from fractions import Fraction

from .tokenizer import Operator_t, escape_whitespace  # TODO: move into utils

import pdb


@dataclass_transform()
class AST(ABC):
    def __init_subclass__(cls: type['AST'], **kwargs):
        """
        - automatically applies the dataclass decorator with repr=False to AST subclasses
        """
        super().__init_subclass__(**kwargs)

        # Apply the dataclass decorator with repr=False to the subclass
        dataclass(repr=False)(cls)

    # TODO: add property to all ASTs for function complete/locked/etc. meaning it and all children are settled
    @abstractmethod
    def __str__(self) -> str:
        """Return a string representation of the AST in a canonical dewy code format"""

    def __repr__(self) -> str:
        """
        Returns a string representation of the AST tree with correct indentation for each sub-component

        e.g.
        SomeAST(prop0=..., prop1=...)
         child0=SomeSubAST(...)
         child1=SomeOtherAST(...)
            a=ThisAST(...)
            b=ThatAST(...)
         child2=AST2(...)
             something=ThisLastAST(...)

        Where all non-ast attributes of a node are printed on the same line as the node itself
        and all children are recursively indented a level and printed on their own line
        """
        return '\n'.join(self._gentree())

    def _gentree(self, prefix: str = '') -> Generator[str, None, None]:
        """
        a recursive generator helper function for __repr__

        Args:
            prefix: str - the string to prepend to each child line (root line already has prefix)
            name: str - the name of the current node in the tree
            # draw_branches: bool - whether each item should be drawn with branches or only use whitespace

        Returns:
            str: the string representation of the AST tree
        """
        # prefix components:
        space = '    '
        branch = '   '
        # pointers:
        tee = ' '
        last = ' '

        attrs_str = ', '.join(f'{k}={v}' for k, v in self.__iter_members__() if not isinstance(v, AST))
        yield f'{self.__class__.__name__}({attrs_str})'
        children = tuple((k, v) for k, v in self.__iter_members__() if isinstance(v, AST))
        pointers = [tee] * (len(children) - 1) + [last]
        for (k, v), pointer in zip(children, pointers):
            extension = branch if pointer == tee else space
            gen = v._gentree(f'{prefix}{extension}')
            name = f'{k}=' if k else ''
            yield f'{prefix}{pointer}{name}{next(gen)}'     # first line gets name and pointer
            yield from gen                                  # rest of lines already have a prefix

    def __iter_members__(self) -> Generator[tuple[str, Any], 'AST', None]:
        """
        A method for getting all properties on the AST instance (including child ASTs, and non-AST properties).
        Returns a generator of tuples of the form (property_name, property_value)
        Allows replacing the current AST with a new one during iteration via .send()
        NOTE: Does not recurse into child ASTs
        """
        for key, value in self.__dict__.items():
            # any direct children are ASTs
            if isinstance(value, AST):
                replacement = yield key, value
                if replacement is not None:
                    setattr(self, key, replacement)
                    yield

            # any direct children are containers of ASTs
            elif is_ast_container(self.__class__.__annotations__.get(key)):
                if value is None:
                    continue

                if isinstance(value, list):
                    for i, item in enumerate(value):
                        replacement = yield '', item
                        if replacement is not None:
                            value[i] = replacement
                            yield
                # elif isinstance(value, some_other_container_type): ...
                else:
                    raise NotImplementedError(f'__iter_members__ over {type(value)} (from member "{key}") of {self} is not yet implemented')

            # properties that are not ASTs
            else:
                if key.startswith('_'): # skip private properties
                    continue
                _ = yield key, value
                assert _ is None, f'ILLEGAL: attempted to replace non-AST value "{key}" during __iter_members__ for ast {self}'

    def __iter__(self) -> Generator['AST', None, None]:
        """DEPRECATED: Use __iter_asts__ instead"""
        raise DeprecationWarning(f'__iter__ is deprecated. Use __iter_asts__ instead')

    def __iter_asts__(self) -> Generator['AST', None, None]:
        """Return a generator of the direct children ASTs of the AST"""
        for _, child in self.__iter_members__():
            if isinstance(child, AST):
                yield child


    def __full_traversal_iter__(self) -> Generator['AST', 'AST', None]:
        """
        Recursive in-order traversal of all child ASTs of the current AST instance
        Has ability to replace the current AST with a new one during iteration via .send()
        """
        for _, child in (gen := self.__iter_members__()):
            if isinstance(child, AST):
                replacement = yield child
                if replacement is not None:
                    gen.send(replacement)
                    yield
                    child = replacement # allow traversal over all children of the replacement
                yield from child.__full_traversal_iter__()


    def is_settled(self) -> bool:
        """Return True if the neither the AST, nor any of its descendants, are prototypes"""
        for child in self.__iter_asts__():
            if not child.is_settled():
                return False
        return True


def is_ast_container(type_hint: type | None) -> bool:
    """
    Determine if the type hint is a container of ASTs.
    e.g. list[AST], set[SomeSubclassOfAST|OtherSubclassOfAST], etc.

    Args:
        type_hint: type | None - the type hint to check. If None, returns False

    Returns:
        bool: True if any of the contained types are subclasses of AST, False otherwise
    """
    if type_hint is None:
        return False

    # class constructors are not containers
    if get_origin(type_hint) is type:
        return False

    # python callables are not containers regardless of if they take in or return ASTs
    if get_origin(type_hint) == get_origin(TypingCallable):
        return False


    # Iterate over all contained types
    args = get_args(type_hint)
    for arg in args:
        # Handle Union types (e.g., Union[B, C] or B | C)
        if get_origin(arg) is Union:
            if any(issubclass(sub_arg, AST) for sub_arg in get_args(arg) if isinstance(sub_arg, type)):
                return True
        elif isinstance(arg, UnionType):
            if any(issubclass(sub_arg, AST) for sub_arg in arg.__args__ if isinstance(sub_arg, type)):
                return True
        # Check if the argument itself is a subclass of the base class
        elif isinstance(arg, type) and issubclass(arg, AST):
            return True

    # no AST subclasses found
    return False


class PrototypeAST(AST, ABC):
    """Used to represent AST nodes that are not complete, and must be removed before the whole AST is evaluated"""

    def is_settled(self) -> bool:
        """By definition, prototypes are not settled"""
        return False


class Delimited(ABC):
    """used to track which ASTs are printed with their own delimiter so they can be juxtaposed without extra parentheses"""

class TypeParam(AST, Delimited):
    items: list[AST]

    def __str__(self):
        return f'<{" ".join(map(str, self.items))}>'
class Type(AST):
    t: type[AST]
    parameters: TypeParam | None = None

    def __str__(self) -> str:
        if self.parameters:
            return f'{self.t.__name__}{self.parameters}'
        return self.t.__name__




# TODO: turn into a singleton...
# untyped type for when a declaration doesn't specify a type
class Untyped(AST):
    def __str__(self) -> str:
        return 'untyped'
untyped = Type(Untyped)


class Undefined(AST):
    """undefined singleton"""
    def __new__(cls):
        if not hasattr(cls, 'instance'):
            cls.instance = super(Undefined, cls).__new__(cls)
        return cls.instance

    def __str__(self) -> str:
        return 'undefined'


# undefined shorthand, for convenience
undefined = Undefined()


class Void(AST):
    """void singleton"""
    def __new__(cls):
        if not hasattr(cls, 'instance'):
            cls.instance = super(Void, cls).__new__(cls)
        return cls.instance

    def __str__(self) -> str:
        return 'void'


# void shorthand, for convenience
void = Void()


# assign is just a binop?
# perhaps bring this one back since it's syntax that distinguishes it, not type checking
# class Assign(AST):
#     # TODO: allow bind to take in an unpack structure
#     target: Declare | Identifier | UnpackTarget
#     value: AST

#     def __str__(self):
#         return f'{self.target} = {self.value}'


class ListOfASTs(PrototypeAST):
    """Intermediate step for holding a list of ASTs that are probably captured by a container"""
    asts: list[AST]

    def __str__(self):
        return f'{", ".join(map(str, self.asts))}'


class PrototypeTuple(PrototypeAST):
    """
    A comma separated list of expressions (not wrapped in parentheses) e.g. 1, 2, 3
    There is no special in-memory representation of a tuple, it is literally just a const list
    """
    items: list[AST]

    def __str__(self):
        return f'{", ".join(map(str, self.items))}'


class Group(AST, Delimited):
    items: list[AST]

    def __str__(self):
        return f'({" ".join(map(str, self.items))})'


class Block(AST, Delimited):
    items: list[AST]

    def __str__(self):
        return f'{{{" ".join(map(str, self.items))}}}'


# class Number(AST):
#     val: int | float | Fraction

class Bool(AST):
    val: bool

    def __str__(self) -> str:
        return str(self.val).lower()


class Int(AST):
    val: int

    def __str__(self) -> str:
        return str(self.val)


class String(AST, Delimited):
    val: str

    def __str__(self) -> str:
        return f'"{escape_whitespace(self.val)}"'


class IString(AST, Delimited):
    parts: list[AST]

    def __str__(self):
        s = ''
        for part in self.parts:
            if isinstance(part, String):
                s += part.val
            else:
                s += f'{part}'
        return f'"{s}"'


class Flowable(AST, ABC):
    ...
    # def was_entered(self) -> bool:
    #     """Determine if the flowable branch was entered. Should reset before performing calls to flow and checking this."""
    #     raise NotImplementedError(f'flowables must implement `was_entered()`. No implementation found for {self.__class__}')

    # def reset_was_entered(self) -> None:
    #     """reset the state of was_entered, in preparation for executing branches in a flow"""
    #     raise NotImplementedError(f'flowables must implement `reset_was_entered()`. No implementation found for {self.__class__}')


class Flow(AST):
    branches: list[Flowable]

    def __str__(self):
        return ' else '.join(map(str, self.branches))


class If(Flowable):
    condition: AST
    body: AST

    def __str__(self):
        return f'if {self.condition} {self.body}'


class Loop(Flowable):
    condition: AST
    body: AST

    def __str__(self):
        return f'loop {self.condition} {self.body}'


class Default(Flowable):
    body: AST

    def __str__(self):
        return f'{self.body}'


class PrototypeFunctionLiteral(PrototypeAST):
    args: AST
    body: AST

    def __str__(self):
        if isinstance(self.args, Delimited):
            return f'{self.args} => {self.body}'
        return f'({self.args}) => {self.body}'


class PrototypeBuiltin(PrototypeAST):
    args: Group
    return_type: AST

    def __str__(self):
        return f'({self.args}):> {self.return_type} => ...'


class Call(AST):
    f: AST
    args: None | AST = None

    def __str__(self):
        if self.args is None:
            return f'{self.f}()'
        if isinstance(self.args, Delimited):
            return f'{self.f}{self.args}'
        return f'{self.f}({self.args})'

from typing import cast
class BinOp(AST, ABC):
    left: AST
    right: AST

    def __post_init__(self):
        self._space = cast(bool, getattr(self, '_space', True))
        self._op = cast(str, getattr(self, '_op', None))
        assert isinstance(self._op, str), f'BinOp subclass "{self.__class__.__name__}" must define an `_op` attribute'

    def __str__(self) -> str:
        if self._space:
            return f'{self.left} {self._op} {self.right}'
        return f'{self.left}{self._op}{self.right}'

class Assign(BinOp):
    _op = '='
class PointsTo(BinOp):
    _op = '->'
class BidirPointsTo(BinOp):
    _op = '<->'
class Access(BinOp):
    _op = '.'
    _space = False

class Index(BinOp):
    _op = ''
    _space = False

class Equal(BinOp):
    _op = '=?'

# covered by OpChain([Not, Equal])
# class NotEqual(BinOp):
#     _op = 'not=?'

class Less(BinOp):
    _op = '<?'

class LessEqual(BinOp):
    _op = '<=?'

class Greater(BinOp):
    _op = '>?'

class GreaterEqual(BinOp):
    _op = '>=?'

class  LeftShift(BinOp):
    _op = '<<'

class  RightShift(BinOp):
    _op = '>>'

class LeftRotate(BinOp):
    _op = '<<<'

class RightRotate(BinOp):
    _op = '>>>'

class LeftRotateCarry(BinOp):
    _op = '<<!'

class RightRotateCarry(BinOp):
    _op = '!>>'

class Add(BinOp):
    _op = '+'

class Sub(BinOp):
    _op = '-'

class Mul(BinOp):
    _op = '*'

class Div(BinOp):
    _op = '/'

class IDiv(BinOp):
    _op = ''

class Mod(BinOp):
    _op = '%'

class Pow(BinOp):
    _op = '^'

class And(BinOp):
    _op = 'and'

class Or(BinOp):
    _op = 'or'

class Xor(BinOp):
    _op = 'xor'

class Nand(BinOp):
    _op = 'nand'

class Nor(BinOp):
    _op = 'nor'

class Xnor(BinOp):
    _op = 'xnor'

class IterIn(BinOp):
    _op = 'in'

class MemberIn(BinOp):
    _op = 'in?'

class UnaryPrefixOp(AST, ABC):
    operand: AST

    def __post_init__(self):
        self._space = cast(bool, getattr(self, '_space', False))
        self._op = cast(str, getattr(self, '_op', None))
        assert isinstance(self._op, str), f'UnaryPrefixOp subclass "{self.__class__.__name__}" must define an `_op` attribute'

    def __str__(self) -> str:
        if self._space:
            return f'{self._op} {self.operand}'
        return f'{self._op}{self.operand}'

class Not(UnaryPrefixOp):
    _op = 'not'
    _space = True

class UnaryNeg(UnaryPrefixOp):
    _op = '-'

class UnaryPos(UnaryPrefixOp):
    _op = '+'

class UnaryMul(UnaryPrefixOp):
    _op = '*'

class UnaryDiv(UnaryPrefixOp):
    _op = '/'



class AtHandle(UnaryPrefixOp):
    _op = '@'
    def __str__(self):
        if isinstance(self.operand, (Delimited, Identifier)):
            return f'@{self.operand}'
        return f'@({self.operand})'


class UnaryPostfixOp(AST, ABC):
    operand: AST

    def __post_init__(self):
        self._op = cast(str, getattr(self, '_op', None))
        assert isinstance(self._op, str), f'UnaryPostfixOp subclass "{self.__class__.__name__}" must define an `_op` attribute'

    def __str__(self) -> str:
        return f'{self.operand}{self._op}'

class Suppress(UnaryPostfixOp):
    _op = ';'


class BroadcastOp(AST):
    op: BinOp

    def __str__(self):
        return f'{self.op.left} .{self.op._op} {self.op.right}'

# TBD if need. For now, parser just does `left = left op right` for `left op= right`
#     needed if we wanted to instead actually do slightly different things when doing an update assign
#     ony case I can think of is from overloading:
#     ```
#     myfunc = () => 'first version'
#     // three possible ways overloading would work...
#     myfunc  |= (a:int b:int) => 'second version'  // myfunc  = myfunc  | (a:int b:int) => 'second version' // fails because myfunc needs to be @myfunc on the right side
#     @myfunc |= (a:int b:int) => 'second version'  // @myfunc = @myfunc | (a:int b:int) => 'second version'
#     myfunc  |= (a:int b:int) => 'second version'  // myfunc  = @myfunc | (a:int b:int) => 'second version'
#     ```
# Also note that vectorized ops might be `op: BinOp | CombinedAssign` if we implement this
# class CombinedAssign(AST):
#     op: BinOp

#     def __str__(self) -> str:
#         return f'{self.op.left} {self.op._op}= {self.op.right}'


class BareRange(PrototypeAST):
    left: AST
    right: AST

    def __str__(self) -> str:
        return f'{self.left}..{self.right}'


class Ellipsis(AST):
    def __str__(self) -> str:
        return '...'


class Backticks(PrototypeAST):
    backticks: str
    def __str__(self) -> str:
        return f'{self.backticks}'


class CycleLeft(AST):
    operand: AST
    num_steps: int

    def __str__(self):
        return f'{"`"*self.num_steps}{self.operand}'


class CycleRight(AST):
    operand: AST
    num_steps: int

    def __str__(self):
        return f'{self.operand}{"`"*self.num_steps}'


class DotDotDot(PrototypeAST):
    def __str__(self) -> str:
        return f'...'

class CollectInto(AST):
    right: AST

    def __str__(self):
        return f'...{self.right}'

class SpreadOutFrom(AST):
    left: AST

    def __str__(self):
        return f'{self.left}...'



class Range(AST):
    left: AST
    right: AST
    brackets: Literal['[]', '[)', '(]', '()']

    def __str__(self) -> str:
        return f'{self.brackets[0]}{self.left}..{self.right}{self.brackets[1]}'


class Array(AST, Delimited):
    items: list[AST] # list[T] where T is not PointsTo or BidirPointsTo. Might have Declare or Assign. Must have at least 1 expression!

    def __str__(self):
        return f'[{" ".join(map(str, self.items))}]'


class Dict(AST, Delimited):
    items: list[PointsTo]

    def __str__(self):
        return f'[{" ".join(map(str, self.items))}]'


class BidirDict(AST, Delimited):
    items: list[BidirPointsTo]

    def __str__(self):
        return f'[{" ".join(map(str, self.items))}]'


class ObjectLiteral(AST, Delimited):
    items: list[AST] # list[Declare|Assign|AST] has to have at least 1 declare or assignment, and no expressions!

    def __str__(self):
        return f'[{" ".join(map(str, self.items))}]'




class DeclareGeneric(AST):
    left: TypeParam
    right: AST

    def __str__(self):
        return f'{self.left}{self.right}'


class Parameterize(AST):
    left: AST
    right: TypeParam

    def __str__(self):
        return f'{self.left}{self.right}'


class PrototypeIdentifier(PrototypeAST):
    name: str
    def __str__(self) -> str:
        return f'{self.name}'

class Identifier(AST):
    name: str
    def __str__(self) -> str:
        return f'{self.name}'


class Express(AST):
    id: Identifier

    def __str__(self) -> str:
        return f'{self.id}'

class TypedIdentifier(AST):
    id: Identifier
    type: AST

    def __str__(self) -> str:
        return f'{self.id}:{self.type}'


class ReturnTyped(BinOp):
    _op = ':>'

class UnpackTarget(AST):
    target: 'list[Identifier | TypedIdentifier | UnpackTarget | Assign | CollectInto]'
    def __str__(self) -> str:
        return f'[{" ".join(map(str, self.target))}]'

class DeclarationType(Enum):
    LET = auto()
    CONST = auto()
    # LOCAL_CONST = auto()
    # FIXED_TYPE = auto()

    # default for binding without declaring
    # DEFAULT = LET


class Declare(AST):
    decltype: DeclarationType
    target: Identifier | TypedIdentifier | ReturnTyped | UnpackTarget | Assign

    def __str__(self):
        return f'{self.decltype.name.lower()} {self.target}'



if __name__ == '__main__':
    # DEBUG testing tree string printing
    class _Add(AST):
        l: AST
        r: AST

        def __str__(self) -> str:
            return f'{self.l} + {self.r}'

    class _Mul(AST):
        l: AST
        r: AST

        def __str__(self) -> str:
            return f'{self.l} * {self.r}'

    class _List(AST):
        items: list[AST]

        def __str__(self) -> str:
            return f'[{", ".join(map(str, self.items))}]'

    class _Int(AST):
        value: int

        def __str__(self) -> str:
            return str(self.value)

    # big long test ast
    test = _Add(
        _Add(
            _Int(1),
            _List([_Int(2), _Int(3), _Int(4), _Int(5)])
        ),
        _Mul(
            _Int(2),
            _Add(
                _Mul(
                    _Int(3),
                    _Int(4)
                ),
                _Mul(
                    _Int(5),
                    _Int(6)
                )
            )
        )
    )

    print(repr(test))
    print(str(test))
    # class Broken(AST):
    #     num: int
    #     def __str__(self) -> str:
    #         return f'{self.num}'
    #     def __iter__(self) -> Generator['AST', None, None]:
    #         yield Int(self.num)
1c:T9c1d,from abc import ABC
import inspect
from typing import Callable, Type, Generator
from types import UnionType
from functools import lru_cache
from .utils import CoordString

import pdb


"""
[tasks]
- clean up eat_block
    - general cleanup
    - break out eat_ matching into smaller functions?
    - figure out tie breaking process:
        1. prefer @full_eat over @peek_eat ---> TODO: implement
        2. prefer longest matches
        3. prefer higher precedence
        4. error
- make all tokens keep the source they come from (for error reporting/keeping track of row/col of the token)
"""


class Token(ABC):
    def __repr__(self) -> str:
        """default repr for tokens is just the class name"""
        return f"<{self.__class__.__name__}>"

    def __hash__(self) -> int:
        raise NotImplementedError(f'hash is not implemented for token type {type(self)}')

    def __eq__(self, __value: object) -> bool:
        raise NotImplementedError(f'equals is not implemented for token type {type(self)}')

    def __iter__(self) -> Generator['list[Token]', None, None]:
        """
        Iter is used by full_traverse_tokens for iterating over any contained tokens.
        e.g. Block_t.body TypeParam_t.body, String_t.body (interpolation blocks only), etc.
        """
        raise NotImplementedError(f'iter is not implemented for token type {type(self)}')


class WhiteSpace_t(Token):
    def __init__(self, _): ...


class Operator_t(Token):
    def __init__(self, op: str):
        self.op = op

    def __repr__(self) -> str:
        return f"<Operator_t: `{self.op}`>"

    def __hash__(self) -> int:
        return hash((Operator_t, self.op))

    def __eq__(self, other) -> bool:
        return isinstance(other, Operator_t) and self.op == other.op


class Juxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __repr__(self) -> str:
        return f"<Juxtapose_t>"

    def __hash__(self) -> int:
        return hash(Juxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, Juxtapose_t)


class Keyword_t(Token):
    def __init__(self, src: str):
        self.src = src.lower()

    def __repr__(self) -> str:
        return f"<Keyword_t: {self.src}>"

    def __hash__(self) -> int:
        return hash((Keyword_t, self.src))

    def __eq__(self, other) -> bool:
        return isinstance(other, Keyword_t) and self.src == other.src


class Identifier_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Identifier_t: {self.src}>"


class Hashtag_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Hashtag_t: {self.src}>"


class Block_t(Token):
    def __init__(self, body: list[Token], left: str, right: str):
        self.body = body
        self.left = left
        self.right = right

    def __repr__(self) -> str:
        body_str = ', '.join(repr(token) for token in self.body)
        return f"<Block_t: {self.left}{body_str}{self.right}>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield self.body


class TypeParam_t(Token):
    def __init__(self, body: list[Token]):
        self.body = body

    def __repr__(self) -> str:
        body_str = ', '.join(repr(token) for token in self.body)
        return f"<TypeParam_t: `<{body_str}>`>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield self.body


class Escape_t(Token):
    escape_map = {
        '\\n': '\n', '\\r': '\r', '\\t': '\t', '\\b': '\b', '\\f': '\f', '\\v': '\v', '\\a': '\a', '\\0': '\0', '\\\\': '\\'
    }

    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Escape_t: {self.src}>"

    def to_str(self) -> str:
        """Convert the escape sequence to the character it represents"""

        # unicode escape (may be several characters long)
        if self.src.startswith('\\U') or self.src.startswith('\\u'):
            return chr(int(self.src[2:], 16))
        assert len(self.src) == 2 and self.src[0] == '\\', "internal error. Ill-posed escape sequence"

        # known escape sequence
        if self.src in self.escape_map:
            esc = self.escape_map[self.src]
            # construct a CoordString at the position of the original escape
            return CoordString.from_existing(esc, self.src[:len(esc)].row_col_map)

        # unknown escape sequence (i.e. just replicate the character)
        return self.src[1]


class RawString_t(Token):
    def __init__(self, body: str):
        self.body = body

    def __repr__(self) -> str:
        return f"<RawString_t: {self.body}>"

    def to_str(self) -> str:
        body = self.body
        if body.startswith('r"""') or body.startswith("r'''"):
            body = body[4:-3]
        elif body.startswith('r"') or body.startswith("r'"):
            body = body[2:-1]
        else:
            raise ValueError(f"Internal Error: unrecognized delimiters on raw string: {repr(self)}")
        return body


class String_t(Token):
    def __init__(self, body: list[str | Escape_t | Block_t]):
        self.body = body

    def __repr__(self) -> str:
        return f"<String_t: {self.body}>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        for token in self.body:
            if isinstance(token, Block_t):
                yield token.body

# class Number_t(Token, ABC):...


class Integer_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Integer_t: {self.src}>"


class BasedNumber_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<BasedNumber_t: {self.src}>"


class Undefined_t(Token):
    def __init__(self, _): ...

    def __hash__(self) -> int:
        return hash(Undefined_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, Undefined_t)

    def __repr__(self) -> str:
        return "<Undefined_t>"


class Void_t(Token):
    def __init__(self, _): ...

    def __hash__(self) -> int:
        return hash(Void_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, Void_t)

    def __repr__(self) -> str:
        return "<Void_t>"


class End_t(Token):
    def __init__(self, _): ...

    def __hash__(self) -> int:
        return hash(End_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, End_t)

    def __repr__(self) -> str:
        return "<End_t>"


class New_t(Token):
    def __init__(self, _): ...

    def __hash__(self) -> int:
        return hash(New_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, New_t)

    def __repr__(self) -> str:
        return "<New_t>"


class Boolean_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Boolean_t: {self.src}>"


class ShiftOperator_t(Operator_t):
    def __init__(self, op: str):
        self.op = op

    def __repr__(self) -> str:
        return f"<ShiftOperator_t: `{self.op}`>"

    def __hash__(self) -> int:
        return hash((ShiftOperator_t, self.op))

    def __eq__(self, other) -> bool:
        return isinstance(other, ShiftOperator_t) and self.op == other.op


class Comma_t(Operator_t):
    def __init__(self, op: str):
        self.op = op

    def __hash__(self) -> int:
        return hash(Comma_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, Comma_t)


class DotDot_t(Token):
    def __init__(self, src: str):
        self.src = src


class DotDotDot_t(Token):
    def __init__(self, src: str):
        self.src = src

class Backticks_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Backticks_t: {self.src}>"


# identify token classes that should take precedence over others when tokenizing
# each row is a list of token types that are confusable in their precedence order. e.g. [Keyword, Unit, Identifier] means Keyword > Unit > Identifier
# only confusable token classes need to be included in the table
precedence_table = [
    [Keyword_t, Undefined_t, Void_t, End_t, New_t, Boolean_t, Operator_t, DotDot_t, Backticks_t, Identifier_t],
]
precedence = {cls: len(row)-i for row in precedence_table for i, cls in enumerate(row)}

# mark which tokens cannot be repeated in a list of tokens. E.g. whitespace should always be merged into a single token
idempotent_tokens = {
    WhiteSpace_t
}

# paired delimiters for blocks, ranges, groups, etc.
pair_opening_delims = '{(['
pair_closing_delims = '})]'

# which closing delimiters are allowed for each opening delimiter
valid_delim_closers = {
    '{': '}',
    '(': ')]',
    '[': '])',
    # '<': '>'
}

# list of all operators sorted from longest to shortest
# TODO: make @ and ... into expressions (perhaps with lower precedence calling than regular calls?)
unary_prefix_operators = {'+', '-', '*', '/', 'not', '~', '@'}#, '...'}
unary_postfix_operators = {'?', ';'}
binary_operators = {
    '+', '-', '*', '/', '%', '^',
    '=?', '>?', '<?', '>=?', '<=?', 'in?', 'is?', 'isnt?', '<=>',
    '|', '&',
    'and', 'or', 'nand', 'nor', 'xor', 'xnor', '??',
    'else',
    '=', ':=', 'as', 'in', 'transmute',
    '@?',
    '|>', '<|', '=>',
    '->', '<->', #'<-', #reverse arrow is dumb
    '.', ':', ':>'
}
opchain_starters = {'+', '-', '*', '/', '%', '^'}
operators = sorted(
    [*(unary_prefix_operators | unary_postfix_operators | binary_operators)],
    key=len,
    reverse=True
)
# TODO: may need to separate |> from regular operators since it may confuse type param
shift_operators = sorted(['<<', '>>', '<<<', '>>>', '<<!', '!>>'], key=len, reverse=True)
keywords = ['loop', 'lazy', 'do', 'if', 'match', 'return', 'yield', 'break', 'continue',
            'async', 'await', 'import', 'from', 'let', 'const', 'local_const', 'fixed_type']
# TODO: what about language values, e.g. void, undefined, end, units, etc.? probably define at compile time, rather than in the compiler

# note that the prefix is case insensitive, so call .lower() when matching the prefix
# numbers may have _ as a separator (if _ is not in the set of digits)
number_bases = {
    '0b': {*'01'},  # binary
    '0t': {*'012'},  # ternary
    '0q': {*'0123'},  # quaternary
    '0s': {*'012345'},  # seximal
    '0o': {*'01234567'},  # octal
    '0d': {*'0123456789'},  # decimal
    '0z': {*'0123456789xeXE'},  # dozenal
    '0x': {*'0123456789abcdefABCDEF'},  # hexadecimal
    '0u': {*'0123456789abcdefghijklmnopqrstuvABCDEFGHIJKLMNOPQRSTUV'},  # base 32 (duotrigesimal)
    '0r': {*'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'},  # base 36 (hexatrigesimal)
    '0y': {*'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!$'},  # base 64 (tetrasexagesimal)
}


# units = #actually units should probably not be specific tokens, but recognized identifiers since the user can make their own units


def peek_eat(cls: Type[Token], whitelist: list[Type[Token]] | None = None, blacklist: list[Type[Token]] | None = None):
    """
    Decorator for functions that eat tokens, but only return how many characters would make up the token.
    Makes function return include constructor for token class that it tries to eat, in tupled with return.

    whitelist and blacklist can be used to specify parent token contexts that may or may not consume this type as a child
    """
    assert issubclass(cls, Token), f"cls must be a subclass of Token, but got {cls}"
    if whitelist is not None and blacklist is not None:
        raise ValueError("cannot specify both whitelist and blacklist")

    def decorator(eat_func: Callable[[str], int | None]):
        def wrapper(src: str) -> tuple[int | None, Type[Token]]:
            return eat_func(src), cls
        wrapper._is_peek_eat_decorator = True  # make it easy to check if a function has this decorator
        wrapper._eat_func = eat_func
        wrapper._token_cls = cls
        wrapper._whitelist = whitelist
        wrapper._blacklist = blacklist
        return wrapper
    return decorator

# TODO: full eat probably won't need to take the class as an argument, since the function will know how to construct the token itself


def full_eat(whitelist: list[Type[Token]] | None = None, blacklist: list[Type[Token]] | None = None):
    def decorator(eat_func: Callable[[str], tuple[int, Token] | None]):
        """
        Decorator for functions that eat tokens, and return the token itself if successful.
        TBD what this actually does...for now, largely keep unmodified, but attach the metadata to the wrapped function
        """
        # pull cls it from the return type of eat_func (which should be a Union[tuple[int, Token], None])
        cls = inspect.signature(eat_func).return_annotation.__args__[0].__args__[1]
        assert issubclass(cls, Token), f"cls must be a subclass of Token, but got {cls}"
        if whitelist is not None and blacklist is not None:
            raise ValueError("cannot specify both whitelist and blacklist")

        def wrapper(*args, **kwargs):
            return eat_func(*args, **kwargs), cls
        wrapper._is_full_eat_decorator = True  # make it easy to check if a function has this decorator
        wrapper._eat_func = eat_func
        wrapper._token_cls = cls
        wrapper._whitelist = whitelist
        wrapper._blacklist = blacklist

        return wrapper
    return decorator


def get_peek_eat_funcs_with_name() -> tuple[tuple[str, Callable], ...]:
    return tuple((name, func) for name, func in globals().items() if callable(func) and getattr(func, '_is_peek_eat_decorator', False))


def get_full_eat_funcs_with_name() -> tuple[tuple[str, Callable], ...]:
    return tuple((name, func) for name, func in globals().items() if callable(func) and getattr(func, '_is_full_eat_decorator', False))


def get_eat_funcs() -> tuple[Callable, ...]:
    return tuple(func for name, func in get_peek_eat_funcs_with_name() + get_full_eat_funcs_with_name())


@lru_cache()
def get_contextual_eat_funcs(context: Type[Token]) -> tuple[Callable, ...]:
    """Get all the eat functions that are valid in the given context"""
    return tuple(func for func in get_eat_funcs() if (func._whitelist is None or context in func._whitelist) and (func._blacklist is None or context not in func._blacklist))


@lru_cache()
def get_func_precedences(funcs: tuple[Callable]) -> tuple[int, ...]:
    assert isinstance(funcs, tuple)
    return tuple(precedence.get(func._token_cls, 0) for func in funcs)


@peek_eat(WhiteSpace_t)
def eat_line_comment(src: str) -> int | None:
    """eat a line comment, return the number of characters eaten"""
    if src.startswith('//'):
        try:
            return src.index('\n') + 1
        except ValueError:
            return len(src)
    return None


@peek_eat(WhiteSpace_t)
def eat_block_comment(src: str) -> int | None:
    """
    Eat a block comment, return the number of characters eaten
    Block comments are of the form /{ ... }/ and can be nested.
    """
    if not src.startswith("/{"):
        return None

    nesting_level = 0
    i = 0

    while i < len(src):
        if src[i:].startswith('/{'):
            nesting_level += 1
            i += 2
        elif src[i:].startswith('}/'):
            nesting_level -= 1
            i += 2

            if nesting_level == 0:
                return i
        else:
            i += 1

    raise ValueError("unterminated block comment")
    # return None


@peek_eat(WhiteSpace_t)
def eat_whitespace(src: str) -> int | None:
    """Eat whitespace, return the number of characters eaten"""
    i = 0
    while i < len(src) and src[i].isspace():
        i += 1
    return i if i > 0 else None


@peek_eat(Keyword_t)
def eat_keyword(src: str) -> int | None:
    """
    Eat a reserved keyword, return the number of characters eaten

    #keyword = {in} | {as} | {loop} | {lazy} | {if} | {and} | {or} | {xor} | {nand} | {nor} | {xnor} | {not};# | {true} | {false};

    noting that keywords are case insensitive
    """

    max_len = max(len(keyword) for keyword in keywords)

    lower_src = src[:max_len].lower()
    for keyword in keywords:
        if lower_src.startswith(keyword):
            # TBD if we need to check that the next character is not an identifier character
            return len(keyword)

    return None


# TODO: expand the list of valid identifier characters
digits = set('0123456789')
alpha = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz')
greek = set('')
misc = set('_?!$&')

start_characters = (alpha | greek | misc) - {'?'}
continue_characters = (alpha | digits | greek | misc)


@peek_eat(Identifier_t)
def eat_identifier(src: str) -> int | None:
    """
    Eat an identifier, return the number of characters eaten

    Identifiers:
    - may not start with a number or a question mark
    - may not end with a question mark
    - may use (TODO enumerate the full chars list somewhere. for now copying from python)

    """
    if not src[0] in start_characters:
        return None

    i = 1
    while i < len(src) and src[i] in continue_characters:
        i += 1

    # while last character is ?, remove it
    while i > 1 and src[i-1] == '?':
        i -= 1

    return i


@peek_eat(Hashtag_t)
def eat_hashtag(src: str) -> int | None:
    """
    Eat a hashtag, return the number of characters eaten

    hashtags are special identifiers that start with #
    """

    if src.startswith('#'):
        i, _ = eat_identifier(src[1:])
        if i is not None:
            return i + 1

    return None


@peek_eat(Escape_t, whitelist=[String_t])
def eat_escape(src: str) -> int | None:
    r"""
    Eat an escape sequence, return the number of characters eaten
    Escape sequences must be either a known escape sequence:
    - \n newline
    - \r carriage return
    - \t tab
    - \b backspace
    - \f form feed
    - \v vertical tab
    - \a alert
    - \0 null
    - \u##..# or \U##..# for an arbitrary unicode character. May have any number of hex digits

    or a \ followed by an unknown character. In this case, the escape converts to just the unknown character
    This is how to insert characters that are otherwise illegal inside a string, e.g.
    - \' converts to just a single quote '
    - \{ converts to just a single open brace {
    - \\ converts to just a single backslash \
    - \m converts to just a single character m
    - etc.
    """
    if not src.startswith('\\'):
        return None

    if len(src) == 1:
        raise ValueError("unterminated escape sequence")

    if src[1] in 'uU':
        i = 2
        while i < len(src) and src[i].isxdigit():
            i += 1
        if i == 2:
            raise ValueError("invalid unicode escape sequence")
        return i

    # if src[1] in 'nrtbfva0':
    #     return 2

    # all other escape sequences (known or unknown) are just a single character
    return 2


@full_eat()
def eat_string(src: str) -> tuple[int, String_t] | None:
    r"""
    strings are delimited with either single (') or double quotes (")
    the character portion of a string may contain any character except the delimiter, \, or {.
    strings may be multiline
    strings may contain escape sequences of the form \s where s is either a known escape sequence or a single character
    strings may interpolation blocks which open with { and close with }

    Tokenizing of escape sequences and interpolation blocks is handled as sub-tokenization task via eat_block and eat_escape

    returns the number of characters eaten and an instance of the String token, containing the list of tokens/string chunks/escape sequences
    """

    # determine the starting delimiter, or exit if there is none
    if src.startswith('"""') or src.startswith("'''"):
        delim = src[:3]
        i = 3
    elif src.startswith('"') or src.startswith("'"):
        delim = src[0]
        i = 1
    else:
        return None

    # keep track of chunks, and the start index of the current chunk
    chunk_start = i
    body = []

    # add character sequences, escapes, and block sections until the end of the string
    while i < len(src) and not src[i:].startswith(delim):

        # regular characters
        if src[i] not in '\\{':
            i += 1
            continue

        # add the previous chunk before handling the escape/interpolation block
        if i > chunk_start:
            body.append(src[chunk_start:i])

        if src[i] == '\\':
            res, _ = eat_escape(src[i:])
            if res is None:
                raise ValueError("invalid escape sequence")
            body.append(Escape_t(src[i:i+res]))
            i += res

        else:  # src[i] == '{':
            assert src[i] == '{', "internal error"
            res, _ = eat_block(src[i:])
            if res is None:
                raise ValueError("invalid block")
            n_eaten, block = res
            body.append(block)
            i += n_eaten

        # update the chunk start
        chunk_start = i

    if i == len(src):
        raise ValueError("unterminated string")

    # add the final chunk
    if i > chunk_start:
        body.append(src[chunk_start:i])

    return i + len(delim), String_t(body)


@peek_eat(RawString_t)
def eat_raw_string(src: str) -> int | None:
    """
    raw strings start with `r`, followed by a delimiter, one of ' " ''' or \"""
    raw strings may contain any character except the delimiter.
    Escapes and interpolations are ignored.
    The string ends at the first instance of the delimiter
    """
    if not src.startswith('r'):
        return None
    i = 1

    if src[i:].startswith('"""') or src[i:].startswith("'''"):
        delim = src[i:i+3]
        i += 3
    elif src[i:].startswith('"') or src[i:].startswith("'"):
        delim = src[i]
        i += 1
    else:
        return None

    while i < len(src) and not src[i:].startswith(delim):
        i += 1

    if i == len(src):
        raise ValueError("unterminated raw string")

    return i + len(delim)


@peek_eat(Integer_t)
def eat_integer(src: str) -> int | None:
    """
    eat an integer, return the number of characters eaten
    integers are of the form [0-9]+
    """
    if not src[0].isdigit():
        return None

    i = 1
    while i < len(src) and src[i].isdigit() or src[i] == '_':
        i += 1
    return i


@peek_eat(BasedNumber_t)
def eat_based_number(src: str) -> int | None:
    """
    eat a based number, return the number of characters eaten

    based numbers have a (case-insensitive) prefix (0p) identifying the base, and (case-sensitive) allowed digits
    """
    try:
        digits = number_bases[src[:2].lower()]
    except KeyError:
        return None

    i = 2
    while i < len(src) and src[i] in digits or src[i] == '_':
        i += 1

    return i if i > 2 else None


@peek_eat(Undefined_t)
def eat_undefined(src: str) -> int | None:
    """
    eat the undefined token, return the number of characters eaten
    """
    sample = src[:9].lower()
    if sample.startswith('undefined'):
        return 9
    return None


@peek_eat(Void_t)
def eat_void(src: str) -> int | None:
    """
    eat the void token, return the number of characters eaten
    """
    sample = src[:4].lower()
    if sample.startswith('void'):
        return 4
    return None


@peek_eat(End_t)
def eat_end(src: str) -> int | None:
    """
    eat the end token, return the number of characters eaten
    """
    sample = src[:3].lower()
    if sample.startswith('end'):
        return 3
    return None

@peek_eat(New_t)
def eat_new(src: str) -> int | None:
    """
    eat the new token, return the number of characters eaten
    """
    sample = src[:3].lower()
    if sample.startswith('new'):
        return 3
    return None


@peek_eat(Boolean_t)
def eat_boolean(src: str) -> int | None:
    """
    eat a boolean, return the number of characters eaten

    booleans are either true or false (case-insensitive)
    """
    sample = src[:5].lower()
    if sample.startswith('true'):
        return 4
    elif sample.startswith('false'):
        return 5

    return None


@peek_eat(Operator_t)
def eat_operator(src: str) -> int | None:
    """
    eat a unary or binary operator, return the number of characters eaten

    picks the longest matching operator

    see `operators` for full list of operators
    """
    for op in operators:
        if src.startswith(op):
            return len(op)
    return None


@peek_eat(ShiftOperator_t, blacklist=[TypeParam_t])
def eat_shift_operator(src: str) -> int | None:
    """
    eat a shift operator, return the number of characters eaten

    picks the longest matching operator.
    Shift operators are not allowed in type parameters, e.g. `>>` is not recognized in `Foo<Bar<Baz<T>>, U>`

    see `shift_operators` for full list of operators
    """
    for op in shift_operators:
        if src.startswith(op):
            return len(op)
    return None


@peek_eat(Comma_t)
def eat_comma(src: str) -> int | None:
    """
    eat a comma, return the number of characters eaten
    """
    return 1 if src.startswith(',') else None


@peek_eat(DotDot_t)
def eat_dotdot(src: str) -> int | None:
    """
    eat a dotdot, return the number of characters eaten
    """
    return 2 if src.startswith('..') else None


@peek_eat(DotDotDot_t)
def eat_dotdotdot(src: str) -> int | None:
    """
    eat a dotdotdot, return the number of characters eaten
    """
    return 3 if src.startswith('...') else None


@peek_eat(Backticks_t)
def eat_cycle(src: str) -> int | None:
    """
    eat one or more cycle operators, return the number of characters eaten
    """
    i = 0
    while i < len(src) and src[i] == '`':
        i += 1
    return i if i > 0 else None

class EatTracker:
    i: int
    tokens: list[Token]


@full_eat()
def eat_type_param(src: str) -> tuple[int, TypeParam_t] | None:
    """
    eat a type parameter, return the number of characters eaten and an instance of the TypeParam token

    type parameters are of the form <...> where ... is a sequence of tokens.
    Type parameters may not start with `<<` or contain any shift operators (`<<`, `<<<`, `>>`, `>>>`)
    Internally encountered shift operators are considered to be delimiters for the type parameter
    """
    if not src.startswith('<') or src.startswith('<<'):
        return None

    i = 1
    body: list[Token] = []

    while i < len(src) and src[i] != '>':

        funcs = get_contextual_eat_funcs(TypeParam_t)
        precedences = get_func_precedences(funcs)
        res = get_best_match(src[i:], funcs, precedences)

        if res is None:
            return None
        n_eaten, token = res

        if isinstance(token, Token):
            # add the already-eaten token to the list of tokens
            body.append(token)
        else:
            # add a new instance of the token to the list of tokens (handling idempotent token cases)
            token_cls = token
            if not body or token_cls not in idempotent_tokens or not isinstance(body[-1], token_cls):
                body.append(token_cls(src[i:i+n_eaten]))

        # increment the index
        i += n_eaten

    if i == len(src):
        return None

    return i + 1, TypeParam_t(body)


@full_eat()
def eat_block(src: str, tracker: EatTracker | None = None) -> tuple[int, Block_t] | None:
    """
    Eat a block, return the number of characters eaten and an instance of the Block token

    blocks are { ... } or ( ... ) and may contain sequences of any other tokens including other blocks

    if return_partial is True, then returns (i, body) in the case where the eat process fails, instead of None
    """

    if not src or src[0] not in pair_opening_delims:
        return None

    # save the opening delimiter
    left = src[0]

    i = 1
    body: list[Token] = []

    if tracker:
        tracker.i = i
        tracker.tokens = body

    while i < len(src) and src[i] not in pair_closing_delims:
        # run all root eat functions
        # if multiple, resolve for best match (TBD... current is longest match + precedence)
        # if no match, return None

        # TODO: probably break this inner part into a function that eats the next token, given a list of eat functions
        # could also think about ways to specify other multi-match resolutions, other than longest match + precedence...
        # run all the eat functions on the current src
        funcs = get_contextual_eat_funcs(Block_t)
        precedences = get_func_precedences(funcs)
        res = get_best_match(src[i:], funcs, precedences)

        # if we didn't match anything, return None
        if res is None:
            return None

        n_eaten, token = res

        if isinstance(token, Token):
            # add the already-eaten token to the list of tokens
            body.append(token)
        else:
            # add a new instance of the token to the list of tokens (handling idempotent token cases)
            token_cls = token
            if not body or token_cls not in idempotent_tokens or not isinstance(body[-1], token_cls):
                body.append(token_cls(src[i:i+n_eaten]))

        # increment the index
        i += n_eaten
        if tracker:
            tracker.i = i

    if i == len(src):
        if tracker:  # only return an exception for the top level block. nested blocks can return None
            raise ValueError("unterminated block")
        return None

    # closing delim (doesn't need to match opening delim)
    right = src[i]
    assert left in pair_opening_delims and right in pair_closing_delims, f"invalid block delimiters: {left} {right}"

    # include closing delimiter in character count
    i += 1
    if tracker:
        tracker.i = i

    return i, Block_t(body, left=left, right=right)


def get_best_match(src: str, eat_funcs: list, precedences: list[int]) -> tuple[int, Type[Token] | Token] | None:
    # TODO: handle selecting between full_eat and peek_eat functions that were successful...
    #      general, just need to clarify the selection order precedence

    # may return none if no match
    # may return (i, token_cls) if peek match
    # may return (i, token) if full match

    matches = [eat_func(src) for eat_func in eat_funcs]

    # find the longest token that matched. if multiple tied for longest, use the one with the highest precedence.
    # raise an error if multiple tokens tied, and they have the same precedence
    def key(x):
        (res, _cls), precedence = x
        if res is None:
            return 0, precedence
        if isinstance(res, tuple):
            res, _token = res  # full_eat functions return a tuple of (num_chars_eaten, token)
        return res, precedence

    matches = [*zip(matches, precedences)]
    best = max(matches, key=key)
    ties = [match for match in matches if key(match) == key(best)]
    if len(ties) > 1:
        raise ValueError(f"multiple tokens matches tied {[match[0][1].__name__ for match in ties]}: {repr(src)}\nPlease disambiguate by providing precedence levels for these tokens.")

    (res, token_cls), _ = best

    # force the type annotations
    res: tuple[int, Token] | int | None
    token_cls: type[Token]

    if res is None:
        return None

    if isinstance(res, int):
        return res, token_cls

    if isinstance(res, tuple):
        return res

    raise ValueError(f"Internal Error: invalid return type from eat function: {res}")


def tokenize(src: str) -> list[Token]:

    # insert src into a block
    src = f'{{\n{src}\n}}'

    # convert string to a coordinate string (for keeping track of row/col numbers)
    src = CoordString(src, anchor=(-1, 0))

    # eat tokens for a block
    tracker = EatTracker()
    try:
        res, _cls = eat_block(src, tracker=tracker)
    except Exception as e:
        raise ValueError(f"failed to tokenize: ```{escape_whitespace(src[tracker.i:])}```.\nCurrent tokens: {tracker.tokens}") from e

    # check if the process failed
    if res is None:
        raise ValueError(f"failed to tokenize: ```{escape_whitespace(src[tracker.i:])}```.\nCurrent tokens: {tracker.tokens}")

    (i, block) = res
    tokens = block.body

    # ensure that all blocks have valid open/close pairs
    validate_block_braces(tokens)

    return tokens


def full_traverse_tokens(tokens: list[Token]) -> Generator[tuple[int, Token, list[Token]], int, None]:
    """
    Walk all tokens recursively, allowing for modification of the tokens list as it is traversed.

    So long as modifications do not occur before the current token, this will safely iterate over all tokens.
    This will not yield string or escape chunks in strings, but will yield interpolated blocks.

    While traversing, the user can overwrite the current index by calling .send(new_index).

    e.g.
    ```python
    gen = full_traverse_tokens(tokens)
    for i, token, stream in gen:
        #do something with current token
        #...

        #maybe overwrite the current index
        if should_overwrite:
            gen.send(new_index)
    ```

    Do not call .send() twice in a row without calling next() in between. This will cause unexpected behavior.

    Args:
        tokens: the list of tokens to traverse

    Yields:
        i: the index of the current token in the current token list
        token: the current token
        stream: the current token list
    """

    i = 0

    while i < len(tokens):
        """
        1. get next token
        2. send current to user
        3. increment index (or overwrite it)
        4. recurse into blocks
        """

        # get the current token
        token = tokens[i]

        # send the current index to the user. possibly receive a new index to continue from
        overwrite_i = yield i, token, tokens

        # only calls to next() will continue execution. calls to .send do nothing wait
        if overwrite_i is not None:
            assert (yield) is None, ".send() may only be called once per iteration."
            i = overwrite_i
        else:
            i += 1

        # for tokens that have defined __iter__ methods, yield their contents
        try:
            for children in token:
                yield from full_traverse_tokens(children)
        except NotImplementedError:
            pass


def traverse_tokens(tokens: list[Token]) -> Generator[Token, None, None]:
    """
    Convenience function over full_traverse_tokens. Walk all tokens recursively

    Does not allow for modification of the tokens list as it is traversed.
    To modify during traversal, use `full_traverse_tokens` instead.

    Args:
        tokens: the list of tokens to traverse

    Yields:
        token: the current token
    """
    for _, token, _ in full_traverse_tokens(tokens):
        yield token


def validate_block_braces(tokens: list[Token]) -> None:
    """
    Checks that all blocks have valid open/close pairs.

    For example, ranges may have differing open/close pairs, e.g. [0..10), (0..10], etc.
    But regular blocks must have matching open/close pairs, e.g. { ... }, ( ... ), [ ... ]
    Performs some validation, without knowing if the block is a range or a block.
    So more validation is needed when the actual block type is known.

    Raises:
        AssertionError: if a block is found with an invalid open/close pair
    """
    for token in traverse_tokens(tokens):
        if isinstance(token, Block_t):
            assert token.left in valid_delim_closers, f'INTERNAL ERROR: left block opening token is not a valid token. Expected one of {[*valid_delim_closers.keys()]}. Got \'{token.left}\''
            assert token.right in valid_delim_closers[token.left], f'ERROR: mismatched opening and closing braces. For opening brace \'{token.left}\', expected one of \'{valid_delim_closers[token.left]}\''


def validate_functions():

    # Validate the @peek_eat function signatures
    peek_eat_functions = get_peek_eat_funcs_with_name()
    for name, wrapper_func in peek_eat_functions:
        func = wrapper_func._eat_func
        signature = inspect.signature(func)
        param_types = [param.annotation for param in signature.parameters.values()
                       if param.default is inspect.Parameter.empty]
        return_type = signature.return_annotation

        # Check if the function has the correct signature
        if len(param_types) != 1 or param_types[0] != str or return_type != int | None:
            pdb.set_trace()
            raise ValueError(f"{func.__name__} has an invalid signature: `{signature}`. Expected `(src: str) -> int | None`")

    # Validate the @full_eat function signatures
    full_eat_functions = get_full_eat_funcs_with_name()
    for name, wrapper_func in full_eat_functions:
        func = wrapper_func._eat_func
        signature = inspect.signature(func)
        param_types = [param.annotation for param in signature.parameters.values()
                       if param.default is inspect.Parameter.empty]
        return_type = signature.return_annotation

        # Check if the function has the correct signature
        error_message = f"{func.__name__} has an invalid signature: `{signature}`. Expected `(src: str) -> tuple[int, Token] | None`"
        if not (isinstance(return_type, UnionType) and len(return_type.__args__) == 2 and type(None) in return_type.__args__):
            raise ValueError(error_message)
        A, B = return_type.__args__
        if B is not type(None):
            B, A = A, B
        if not (isinstance(A, type(tuple)) and len(A.__args__) == 2 and A.__args__[0] is int and issubclass(A.__args__[1], Token)):
            raise ValueError(error_message)
        if len(param_types) != 1 or param_types[0] != str:
            pdb.set_trace()
            raise ValueError(error_message)

    # check for any functions that start with eat_ but are not decorated with @eat
    peek_eat_func_names = {name for name, _ in peek_eat_functions}
    full_eat_func_names = {name for name, _ in full_eat_functions}
    for name, func in globals().items():
        if name.startswith("eat_") and callable(func) and name not in peek_eat_func_names and name not in full_eat_func_names:
            raise ValueError(f"`{name}()` function is not decorated with @peek_eat or @full_eat")


def escape_whitespace(s: str):
    """convert a string to one where all non-space whitespace is escaped"""
    escape_map = {
        '\t': '\\t',
        '\r': '\\r',
        '\f': '\\f',
        '\v': '\\v',
        '\n': '\\n',
    }
    return ''.join(escape_map.get(c, c) for c in s)


def tprint(token: Token, level=0):
    """
    print a token with a certain indentation level.

    If tokens contain nested tokens, they will be printed recursively with an increased indentation level
    """
    print(f'{"    "*level}', end='')
    if isinstance(token, Block_t):
        print(f'<Block {token.left}{token.right}>')
        for t in token.body:
            tprint(t, level=level+1)
    elif isinstance(token, String_t):
        print(f'<String>')
        for t in token.body:
            tprint(t, level=level+1)
    elif isinstance(token, TypeParam_t):
        print(f'<TypeParam>')
        for t in token.body:
            tprint(t, level=level+1)
    else:
        print(token)


def test():
    import sys
    """simple test dewy program"""

    try:
        path = sys.argv[1]
    except IndexError:
        raise ValueError("Usage: `python tokenizer.py path/to/file.dewy>`")

    with open(path) as f:
        src = f.read()

    tokens = tokenize(src)
    print(f'matched tokens:')
    tprint(Block_t(left='{', right='}', body=tokens))
    # for t in tokens:
    #     tprint(t, level=1)


if __name__ == "__main__":
    validate_functions()
    test()
1d:T22f4,from typing import TypeVar, Generic, Callable
import pdb



from dataclasses import dataclass
@dataclass
class Options:
    tokens: bool
    verbose: bool
    #TODO: other command line options


def wrap_coords(method: Callable):
    def wrapped_method(self, *args, **kwargs):
        result = method(self, *args, **kwargs)
        if isinstance(result, str) and len(result) == len(self):
            custom_str = CoordString(result)
            custom_str.row_col_map = self.row_col_map
            return custom_str
        else:
            raise ValueError("coord_string_method must return a string of the same length as the original string")
        return result

    return wrapped_method


def fail_coords(method: Callable):
    def wrapped_method(self, *args, **kwargs):
        raise ValueError(f"coord_string_method {method} cannot be called on a CoordString, as it will not return a CoordString")
    return wrapped_method


class CoordString(str):
    """
    Drop-in replacement for str that keeps track of the coordinates of each character in the string

    Identical to normal strings, but attaches the `row_col(i:int) -> tuple[int, int]` method
    which returns the (row, column) of the character at index i

    Args:
        anchor (tuple[int,int], optional): The row and column of the top left of the string. Defaults to (0, 0).
    """
    def __new__(cls, *args, anchor: tuple[int, int] = (0, 0), **kwargs):
        self = super().__new__(cls, *args, **kwargs)
        row, col = anchor
        self.row_col_map = self._generate_row_col_map(row, col)

        return self

    # TODO: make init so that class recognized .row_col_map as property on instances
    # def __init__(self, s:str, row_col_map:list[tuple[int,int]]):

    def _generate_row_col_map(self, row=0, col=0) -> list[tuple[int, int]]:
        row_col_map = []
        for c in self:
            if c == '\n':
                row_col_map.append((row, col))
                row += 1
                col = 0
            else:
                row_col_map.append((row, col))
                col += 1
        return row_col_map

    def __getitem__(self, key):
        if isinstance(key, slice):
            sliced_str = super().__getitem__(key)
            sliced_row_col_map = self.row_col_map[key]
            custom_str = CoordString(sliced_str)
            custom_str.row_col_map = sliced_row_col_map
            return custom_str
        return super().__getitem__(key)

    def loc(self, index):
        return self.row_col_map[index]

    @staticmethod
    def from_existing(new_str: str, old_coords: list[tuple[int, int]]) -> 'CoordString':
        new_coord_str = CoordString(new_str)
        new_coord_str.row_col_map = old_coords
        return new_coord_str

    # wrappers for string methods that should return CoordStrings
    def lstrip(self, *args, **kwargs):
        result = super().lstrip(*args, **kwargs)
        custom_str = CoordString(result)
        custom_str.row_col_map = self.row_col_map[len(self)-len(result):]
        return custom_str

    def rstrip(self, *args, **kwargs):
        result = super().rstrip(*args, **kwargs)
        custom_str = CoordString(result)
        custom_str.row_col_map = self.row_col_map[:len(result)]
        return custom_str

    def strip(self, *args, **kwargs):
        return self.lstrip(*args, **kwargs).rstrip(*args, **kwargs)

    @wrap_coords
    def capitalize(self): return super().capitalize()

    @wrap_coords
    def casefold(self): return super().casefold()

    @wrap_coords
    def lower(self): return super().lower()

    @wrap_coords
    def upper(self): return super().upper()

    @wrap_coords
    def swapcase(self): return super().swapcase()

    @wrap_coords
    def title(self): return super().title()

    @wrap_coords
    def translate(self, table): return super().translate(table)

    @wrap_coords
    def replace(self, old, new, count=-1): return super().replace(old, new, count)

    @fail_coords
    def center(self, *args, **kwargs): ...

    @fail_coords
    def expandtabs(self, *args, **kwargs): ...

    @fail_coords
    def ljust(self, *args, **kwargs): ...

    @fail_coords
    def zfill(self, *args, **kwargs): ...


# TODO: maybe make adding this string with other regular str illegal


int_parsable_base_prefixes = {
    '0b': 2,  '0B': 2,
    '0t': 3,  '0T': 3,
    '0q': 4,  '0Q': 4,
    '0s': 6,  '0S': 6,
    '0o': 8,  '0O': 8,
    '0d': 10, '0D': 10,
    # '0z':12, #uses different digits Z/z and X/x (instead of A/a and B/b expected by int())
    '0x': 16, '0X': 16,
    '0u': 32, '0U': 32,
    '0r': 36, '0R': 36,
    # '0y':64, #more than int's max parsable base (36)
}


def based_number_to_int(src: str) -> int:
    """
    convert a number in a given base to an int
    """
    prefix, digits = src[:2], src[2:]
    if prefix in int_parsable_base_prefixes:
        return int(digits, int_parsable_base_prefixes[prefix])
    elif prefix == '0z':
        raise NotImplementedError(f"base {prefix} is not supported")
    elif prefix == '0y':
        raise NotImplementedError(f"base {prefix} is not supported")
    else:
        raise ValueError(f"INTERNAL ERROR: base {prefix} is not a valid base")


def bool_to_bool(src: str) -> bool:
    """
    convert a (case-insensitive) bool literal to a bool
    """
    try:
        return bool(['false', 'true'].index(src.lower()))
    except ValueError:
        raise ValueError(f"INTERNAL ERROR: bool {src} is not a valid bool") from None


class CaselessStr(str):
    def __init__(self, s: str):
        super().__init__()

    def __eq__(self, other: str):
        return self.casefold() == other.casefold()

    def __hash__(self):
        return hash(self.casefold())

    def __repr__(self):
        return f"(caseless)'{self.casefold()}'"

    def __str__(self):
        return self.casefold()


T = TypeVar('T')
U = TypeVar('U')


class CaseSelectiveDict(Generic[T]):
    """A dictionary where keys may be either case sensitive or case insensitive"""

    def __init__(self, **kwargs):
        self.caseless: dict[CaselessStr, any] = {}
        self.caseful: dict[str, any] = {}
        for k, v in kwargs.items():
            self.__setitem__(k, v)  # Use setitem to handle initial assignments

    def __getitem__(self, key: str | CaselessStr) -> T:
        try:
            return self.caseless[CaselessStr(key)]
        except KeyError:
            pass
        try:
            return self.caseful[key]
        except KeyError:
            pass

        raise KeyError(f"Key {key} not found") from None

    def __setitem__(self, key: str | CaselessStr, value: T):
        if not isinstance(key, (str, CaselessStr)):
            raise TypeError(f"Key must be of type str or CaselessStr, not '{type(key)}'")

        if CaselessStr(key) in self.caseless:
            if not isinstance(key, CaselessStr):
                key = CaselessStr(key)
            self.caseless[key] = value
            return

        if isinstance(key, CaselessStr):
            # need to check if the new key was in the caseful keys. This is an ERROR
            caseful_keys = [*self.caseful.keys()]
            folded_caseful_keys = [k.casefold() for k in caseful_keys]
            if key in folded_caseful_keys:
                existing_key = caseful_keys[folded_caseful_keys.index(key)]
                raise KeyError(f"Cannot insert key {repr(key)}. Caseful version {repr(existing_key)} already exists")
            self.caseless[key] = value
            return

        self.caseful[key] = value

    def __delitem__(self, key: str | CaselessStr):
        key = str(key)
        try:
            del self.caseless[CaselessStr(key)]
            return
        except KeyError:
            pass
        try:
            del self.caseful[key]
            return
        except KeyError:
            pass

        raise KeyError(f"Key {key} not found") from None

    def __contains__(self, key: str | CaselessStr):
        try:
            return CaselessStr(key) in self.caseless or key in self.caseful
        except TypeError:
            return False

    def has_key(self, key: str | CaselessStr):
        return key in self

    def get(self, key: str | CaselessStr, default: U = None) -> T | U:
        try:
            return self[key]
        except KeyError:
            return default

    def __len__(self):
        return len(self.caseless) + len(self.caseful)

    def __iter__(self):
        return iter(self.keys())

    def items(self):
        return {**self.caseless, **self.caseful}.items()

    def keys(self) -> list[str | CaselessStr]:
        return [*self.caseless.keys(), *self.caseful.keys()]

    def values(self) -> list[T]:
        return [*self.caseless.values(), *self.caseful.values()]

    def __repr__(self):
        return f"CaseSelectiveDict({self.caseless}, {self.caseful})"

    def __str__(self):
        items = {**self.caseless, **self.caseful}
        return f'{items}'
1e:T512,"""
Collection of all the Dewy Language backends
"""

from .python import python_interpreter, python_repl
from .qbe import qbe_compiler
from .llvm import llvm_compiler
from .c import c_compiler
from .x86_64 import x86_64_compiler
from .arm import arm_compiler
from .riscv import riscv_compiler
from .shell import shell_compiler
from typing import Protocol
from pathlib import Path
from ..utils import Options

class Backend(Protocol):
    def __call__(self, path: Path, args: list[str], options: Options) -> None:
        ...


backend_map: dict[str, Backend] = {
    'python': python_interpreter,
    'qbe': qbe_compiler,
    'llvm': llvm_compiler,
    'c': c_compiler,
    'x86_64': x86_64_compiler,
    'arm': arm_compiler,
    'riscv': riscv_compiler,
    'sh': shell_compiler,
    'zsh': shell_compiler,
    'bash': shell_compiler,
    'fish': shell_compiler,
    'posix': shell_compiler,
    'powershell': shell_compiler,
}
backend_names = [*backend_map.keys()]


def get_backend(name: str) -> Backend:
    try:
        return backend_map[name.lower()]
    except:
        raise ValueError(f'Unknown backend "{name}"') from None


def get_version() -> str:
    """Return the semantic version of the language"""
    return (Path(__file__).parent.parent.parent / 'VERSION').read_text().strip()
1f:Tbb2b,from ..tokenizer import tokenize
from ..postok import post_process
from ..dtypes import (
    Scope as DTypesScope,
    typecheck_call, typecheck_index, typecheck_multiply,
    register_typeof, short_circuit,
    CallableBase, IndexableBase, IndexerBase, MultipliableBase, ObjectBase,
)
from ..parser import top_level_parse, QJux
from ..syntax import (
    AST,
    Type, TypeParam,
    PointsTo, BidirPointsTo,
    ListOfASTs, PrototypeTuple, Block, Array, Group, Range, ObjectLiteral, Dict, BidirDict, UnpackTarget,
    TypedIdentifier,
    Void, void, Undefined, undefined, untyped,
    String, IString,
    Flowable, Flow, If, Loop, Default,
    Identifier, Express, Declare,
    PrototypeBuiltin, Call, Access, Index,
    Assign,
    Int, Bool,
    Range, IterIn,
    BinOp,
    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,
    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    UnaryPrefixOp, UnaryPostfixOp,
    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv, AtHandle,
    CycleLeft, CycleRight, Suppress,
    BroadcastOp,
    CollectInto, SpreadOutFrom,
)

from ..postparse import post_parse, FunctionLiteral, Signature, normalize_function_args
from ..utils import Options

from dataclasses import dataclass, field
from pathlib import Path
from typing import Protocol, cast, Callable as TypingCallable, Any, Generic
from functools import cache
from collections import defaultdict
from types import SimpleNamespace


import pdb



def python_interpreter(path: Path, args:list[str], options: Options) -> None:
    # get the source code and tokenize
    src = path.read_text()
    tokens = tokenize(src)
    post_process(tokens)

    # parse tokens into AST
    ast = top_level_parse(tokens)
    ast = post_parse(ast)

    # debug printing
    if options.verbose:
        print_ast(ast)
        print(repr(ast))

    # run the program
    res = top_level_evaluate(ast)
    if res is not void:
        print(res)

def python_repl(args: list[str], options: Options):
    try:
        from easyrepl import REPL
    except ImportError:
        print('easyrepl is required for REPL mode. Install with `pip install easyrepl`')
        return

    # Set up scope to share between REPL calls
    scope = Scope.default()
    insert_builtins(scope)

    # get the source code and tokenize
    for src in REPL(history_file='~/.dewy/repl_history'):
        # Check for custom commands
        match src:
            case 'exit' | 'quit':
                return
            case 'help':
                print('Commands:')
                print('  exit|quit: exit the REPL')
                print('  help: display this help message')
                continue

        try:
            tokens = tokenize(src)
            post_process(tokens)
            if options.tokens:
                print(tokens)

            # parse tokens into AST
            ast = top_level_parse(tokens)
            ast = post_parse(ast)

            # debug printing
            if options.verbose:
                print_ast(ast)
                print(repr(ast))

            # run the program (sharing the same scope)
            res = evaluate(ast, scope)
            if res is not void:
                print(res)
        except Exception as e:
            print(f'Error: {e}')

    print() # newline after exiting REPL with ctrl+d

def print_ast(ast: AST):
    """little helper function to print out the equivalent source code of an AST"""
    print('```dewy')
    if isinstance(ast, (Block, Group)):
        for i in ast.__iter_asts__(): print(i)
    else:
        print(ast)
    print('```')


def top_level_evaluate(ast:AST) -> AST:
    scope = Scope.default()
    insert_builtins(scope)
    return evaluate(ast, scope)


############################ Runtime helper classes ############################

class MetaNamespace(SimpleNamespace):
    """A simple namespace for storing AST meta attributes for use at runtime"""
    def __getattribute__(self, key: str) -> Any | None:
        """Get the attribute associated with the key, or None if it doesn't exist"""
        try:
            return super().__getattribute__(key)
        except AttributeError:
            return None

    def __setattr__(self, key: str, value: Any) -> None:
        """Set the attribute associated with the key"""
        super().__setattr__(key, value)


class MetaNamespaceDict(defaultdict):
    """A defaultdict that preprocesses AST keys to use the classname + memory address as the key"""
    def __init__(self):
        super().__init__(MetaNamespace)

    # add preprocessing to both __getitem__ and __setitem__ to handle AST keys
    # apparently __setitem__ always calls __getitem__ so we only need to override __getitem__
    def __getitem__(self, item: AST) -> Any | None:
        key = f'::{item.__class__.__name__}@{hex(id(item))}'
        return super().__getitem__(key)

@dataclass
class Scope(DTypesScope):
    """An extension of the Scope used during parsing to support runtime"""
    meta: dict[AST, MetaNamespace] = field(default_factory=MetaNamespaceDict)

    def __repr__(self):
        return f'<Scope@{hex(id(self))}>'


class Iter(AST):
    item: AST
    i: int

    def __str__(self):
        return f'Iter({self.item}, i={self.i})'

class BuiltinArgsPreprocessor(Protocol):
    def __call__(self, args: list[AST], kwargs: dict[str, AST], scope: Scope) -> tuple[list[Any], dict[str, Any]]: ...

class Builtin(CallableBase):
    signature: Signature
    preprocessor: BuiltinArgsPreprocessor
    action: TypingCallable[..., AST]
    return_type: AST

    def __str__(self):
        return f'{self.signature}: {self.return_type} => {self.action}'

    def from_prototype(proto: PrototypeBuiltin, preprocessor: BuiltinArgsPreprocessor, action: TypingCallable[..., AST]) -> 'Builtin':
        return Builtin(
            signature=normalize_function_args(proto.args),
            preprocessor=preprocessor,
            action=action,
            return_type=proto.return_type,
        )

# hacky for now. longer term, want full signature type checking for functions!
register_typeof(Builtin, short_circuit(Builtin))

class Closure(CallableBase):
    fn: FunctionLiteral
    scope: Scope

    def __str__(self):
        return f'{self.fn} with <Scope@{hex(id(self.scope))}>'
        # scope_lines = []
        # for name, var in self.scope.vars.items():
        #     line = f'{var.decltype.name.lower()} {name}'
        #     if var.type is not untyped:
        #         line += f': {var.type}'
        #     if var.value is self:
        #         line += ' = <self>'
        #     elif var.value is not void:
        #         line += f' = {var.value}'
        #     scope_lines.append(line)
        # scope_contents = ', '.join(scope_lines)
        # return f'{self.fn} with scope=[{scope_contents}]'

# register_callable(Builtin)
# register_callable(Closure)

register_typeof(Closure, short_circuit(Closure))

class Object(ObjectBase):
    scope: Scope

    def __str__(self):
        chunks = []
        for name, var in self.scope.vars.items():
            chunk = f'{var.decltype.name.lower()} {name}'
            if var.type is not untyped:
                chunk += f': {var.type}'
            #TODO: need to handle any recursion loops...
            elif var.value is not void:
                chunk += f' = {var.value}'
            chunks.append(chunk)
        if len(chunks) < 5:
            return f'[{" ".join(chunks)}]'
        newline = '\n'
        #TODO: py3.12 remove {newline} and replace with direct \n
        return f'[{newline}    {f"{newline}    ".join(chunks)}{newline}]'


def typeof_object(obj: Object, scope: Scope, params:bool=False) -> Type:
    return Type(Object, TypeParam([obj.scope]))
register_typeof(Object, typeof_object)

class Float(MultipliableBase):
    val: float

    def __str__(self):
        return f'{self.val}'

############################ Evaluation functions ############################

#DEBUG supporting py3.11
from typing import TypeVar
T = TypeVar('T', bound=AST)
U = TypeVar('U', bound=AST)
class EvalFunc(Protocol):
    def __call__(self, ast: T, scope: Scope) -> AST: ...

def no_op(ast: T, scope: Scope) -> T:
    """For ASTs that just return themselves when evaluated"""
    return ast

# #py3.12 version
# class EvalFunc[T](Protocol):
#     def __call__(self, ast: T, scope: Scope) -> AST: ...
#
# def no_op[T](ast: T, scope: Scope) -> T:
#     """For ASTs that just return themselves when evaluated"""
#     return ast

def cannot_evaluate(ast: AST, scope: Scope) -> AST:
    raise ValueError(f'INTERNAL ERROR: evaluation of type {type(ast)} is not possible')


@cache
def get_eval_fn_map() -> dict[type[AST], EvalFunc]:
    return {
        Declare: evaluate_declare,
        QJux: evaluate_qjux,
        Call: evaluate_call,
        Block: evaluate_block,
        Group: evaluate_group,
        Array: evaluate_array,
        Dict: evaluate_dict,
        PointsTo: evaluate_points_to,
        BidirDict: evaluate_bidir_dict,
        BidirPointsTo: evaluate_bidir_points_to,
        ObjectLiteral: evaluate_object_literal,
        Object: no_op,
        Access: evaluate_access,
        Index: evaluate_index,
        Assign: evaluate_assign,
        IterIn: evaluate_iter_in,
        FunctionLiteral: evaluate_function_literal,
        Closure: evaluate_closure,
        Builtin: evaluate_builtin,
        String: no_op,
        IString: evaluate_istring,
        Identifier: cannot_evaluate,
        Express: evaluate_express,
        Int: no_op,
        Float: no_op,
        Bool: no_op,
        Range: no_op,
        Flow: evaluate_flow,
        Default: evaluate_default,
        If: evaluate_if,
        Loop: evaluate_loop,
        UnaryPos: evaluate_unary_dispatch,
        UnaryNeg: evaluate_unary_dispatch,
        UnaryMul: evaluate_unary_dispatch,
        UnaryDiv: evaluate_unary_dispatch,
        Not: evaluate_unary_dispatch,
        Greater: evaluate_binary_dispatch,
        GreaterEqual: evaluate_binary_dispatch,
        Less: evaluate_binary_dispatch,
        LessEqual: evaluate_binary_dispatch,
        Equal: evaluate_binary_dispatch,
        And: evaluate_binary_dispatch,
        Or: evaluate_binary_dispatch,
        Xor: evaluate_binary_dispatch,
        Nand: evaluate_binary_dispatch,
        Nor: evaluate_binary_dispatch,
        Xnor: evaluate_binary_dispatch,
        Add: evaluate_binary_dispatch,
        Sub: evaluate_binary_dispatch,
        Mul: evaluate_binary_dispatch,
        Div: evaluate_binary_dispatch,
        Mod: evaluate_binary_dispatch,
        Pow: evaluate_binary_dispatch,
        LeftShift: evaluate_binary_dispatch,
        RightShift: evaluate_binary_dispatch,
        LeftRotate: evaluate_binary_dispatch,
        RightRotate: evaluate_binary_dispatch,
        LeftRotateCarry: evaluate_binary_dispatch,
        RightRotateCarry: evaluate_binary_dispatch,
        CycleLeft: evaluate_binary_dispatch,
        AtHandle: evaluate_at_handle,
        Undefined: no_op,
        Void: no_op,
        #TODO: other AST types here
    }

def evaluate(ast:AST, scope:Scope) -> AST:
    eval_fn_map = get_eval_fn_map()

    ast_type = type(ast)
    if ast_type in eval_fn_map:
        return eval_fn_map[ast_type](ast, scope)

    raise NotImplementedError(f'evaluation not implemented for {ast_type}')


def suspend(ast:AST, scope:Scope) -> Closure:
    """Wrap an AST in a Closure to be evaluated later"""
    return Closure(fn=FunctionLiteral(args=Signature(), body=ast), scope=scope)


def evaluate_declare(ast: Declare, scope: Scope):
    match ast.target:
        case Identifier(name):
            value = void
            type = untyped
        case TypedIdentifier(id=Identifier(name), type=type):
            value = void
        case Assign(left=Identifier(name), right=right):
            value = evaluate(right, scope)
            type = untyped
        case Assign(left=TypedIdentifier(id=Identifier(name), type=type), right=right):
            value = evaluate(right, scope)
        case Assign(left=UnpackTarget() as target, right=right):
            right = evaluate(right, scope)
            #TODO: need to declare each value being unpacked. something with this effect (but also makes new declarations for each):
            # unpack_assign(target, right, scope)
            pdb.set_trace()
            ...

        case _:
            raise NotImplementedError(f'Declare not implemented yet for {ast.target=}')

    scope.declare(name, value, type, ast.decltype)
    return void

def evaluate_qjux(ast: QJux, scope: Scope) -> AST:
    if ast.call is not None and typecheck_call(ast.call, scope):
        return evaluate_call(ast.call, scope)
    if ast.index is not None and typecheck_index(ast.index, scope):
        return evaluate_index(ast.index, scope)
    if typecheck_multiply(ast.mul, scope):
        return evaluate_binary_dispatch(ast.mul, scope)

    raise ValueError(f'Typechecking failed to match a valid evaluation for QJux. {ast=}')

# def evaluate_qast(ast: QAST, scope: Scope):
#     # use type checking to determine which branch of the QAST to evaluate
#     candidates: list[AST] = [a for a in ast.asts if typecheck(a, scope)]
#     if len(candidates) == 0:
#         raise ValueError(f'No valid candidates for QAST. {ast=}')
#     if len(candidates) > 1:
#         raise ValueError(f'Multiple valid candidates for QAST. {ast=}, {candidates=}')
#     selected, = candidates
#     return evaluate(selected, scope)


def evaluate_call(call: Call, scope: Scope) -> AST:
    f = call.f

    # get the expression of the group
    if isinstance(f, Group):
        f = evaluate(f, scope)

    # get the value pointed to by the identifier
    if isinstance(f, Identifier):
        f = scope.get(f.name).value

    # if this is a handle, do a partial evaluation rather than a call
    if isinstance(f, AtHandle):
        return apply_partial_eval(f.operand, call.args, scope)

    # AST being called must be TypingCallable
    assert isinstance(f, (Builtin, Closure)), f'expected Function or Builtin, got {f}'

    # save the args of the call as metadata for the function AST
    call_args, call_kwargs = collect_calling_args(call.args, scope)
    scope.meta[f].call_args = call_args, call_kwargs

    # run the function and return the result
    if isinstance(f, Builtin):
        return evaluate_builtin(f, scope)
    if isinstance(f, Closure):
        return evaluate_closure(f, scope)

    pdb.set_trace()
    raise NotImplementedError(f'Function evaluation not implemented yet')

#TODO: longer term this might also return a list/dict of spread args passed into the function
def collect_calling_args(args: AST | None, scope: Scope) -> tuple[list[AST], dict[str, AST]]:
    """
    Collect the arguments that a function is being called with
    e.g. `let fn = (a b c) => a + b + c; fn(1 c=2 3)`
    then the calling args are [1, 3] and {c: 2}

    Args:
        args: the arguments being passed to the function. If None, then treat as a no-arg call
        scope: the scope in which the function is being called

    Returns:
        a tuple of the positional arguments and keyword arguments
    """
    match args:
        case None | Void(): return [], {}
        case Identifier(name): return [scope.get(name).value], {}
        case Assign(left=Identifier(name)|TypedIdentifier(id=Identifier(name)), right=right): return [], {name: right}
        # case Assign(left=UnpackTarget() as target, right=right): raise NotImplementedError('UnpackTarget not implemented yet')
        case Assign(): raise NotImplementedError('Assign not implemented yet') #called recursively if a calling arg was an keyword arg rather than positional
        case CollectInto(right=right):
            pdb.set_trace()
            ... #right should be iterable, so extend with the values it expresses
                #whether to add to args or kwargs depends on each type from right
            val = evaluate(right, scope)
            match val:
                case Array(items): ... #return [collect_calling_args(i, scope) for i in items]
        case Group(items):
            call_args, call_kwargs = [], {}
            for i in items:
                a, kw = collect_calling_args(i, scope)
                call_args.extend(a)
                call_kwargs.update(kw)
            return call_args, call_kwargs

        #TODO: eventually it should just be anything that is left over is positional args rather than specifying them all out
        case Int() | String() | IString() | Range() | Call() | Access() | Index() | Express() | QJux() | UnaryPrefixOp() | UnaryPostfixOp() | BinOp() | BroadcastOp():
            return [args], {}
        # case Call(): return [args], {}
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'collect_args not implemented yet for {args}')


    raise NotImplementedError(f'collect_args not implemented yet for {args}')


def get_arg_name(arg: AST) -> str:
    """little helper function to get the name of an argument"""
    match arg:
        case Identifier(name): return name
        case TypedIdentifier(id=Identifier(name)): return name
        case Assign(left=Identifier(name)): return name
        case Assign(left=TypedIdentifier(id=Identifier(name))): return name
        case _:
            raise NotImplementedError(f'get_arg_name not implemented for {arg=}')

# TODO: this should maybe take in 2 scopes, one for the closure scope, and one for the callings scope... or closure scope args should already be evaluated...
# TODO: resolve args should handle the full gamut of possible types of arguments in a function
#       position or keyword arguments (with or without defaults)
#       position only arguments (with or without defaults)
#       keyword only arguments (with or without defaults)
# Note: the function signature will stay the same since calling a function or builtin just amounts to setting args and kwargs
# TODO: probably expand to include a container for unpack targets
def resolve_calling_args(signature: Signature, args: list[AST], kwargs: dict[str, AST], caller_scope: Scope, closure_scope: Scope = Scope()) -> tuple[dict[str, AST], dict[str, AST]]:
    """
    Resolve the final list of arguments the function actually receives
    Properly handles when the signature includes defaults, keyword args, positional args, partial evaluation, etc.

    e.g. if we have:

    ```dewy
    let fn = (a, b, c) => a + b + c
    fn = @fn(c=3)
    fn(1, a=2)
    ```

    then we would resolve to args={b: 1} kwargs={a: 2, c: 3}

    This is mainly for interfacing with python functions which want *args, **kwargs
    """
    # for now, just assume all args are position or keyword args
    # partial eval converts that particular arg to keyword only
    sig_pkwargs, sig_pargs, sig_kwargs = signature.pkwargs, signature.pargs, signature.kwargs
    dewy_args, dewy_kwargs = {}, {}

    # evaluate all the args and kwargs
    args = [evaluate(arg, caller_scope) for arg in args]
    kwargs = {name: evaluate(arg, caller_scope) for name, arg in kwargs.items()}


    # first pull out the calling keyword arguments
    dewy_kwargs.update(kwargs)
    sig_pkwargs = [*filter(lambda item: get_arg_name(item) not in kwargs, sig_pkwargs)]
    sig_kwargs = [*filter(lambda item: get_arg_name(item) not in kwargs, sig_kwargs)]

    #remaining kwargs are added to the dewy_kwargs
    for arg in sig_kwargs:
        assert isinstance(arg, Assign), f'INTERNAL ERROR: {arg=} is not an Assign'
        name = get_arg_name(arg)
        dewy_kwargs[name] = evaluate(arg.right, closure_scope)

    if len(sig_pargs) + len(sig_pkwargs) < len(args):
        raise ValueError(f'Too many positional arguments for function. {signature=}, {args=}, {kwargs=}')

    # next, pair up the positional arguments
    for spec, arg in zip(sig_pargs + sig_pkwargs, args):
        name = get_arg_name(spec)
        dewy_args[name] = arg

    # all remaining arguments must have a value provided by the signature
    for arg in (sig_pargs + sig_pkwargs)[len(args):]:
        #TODO: handle unpacking/spread
        name = get_arg_name(arg)
        if not isinstance(arg, Assign):
            pdb.set_trace()
            raise ValueError(f'Non-Assign arguments remaining unpaired in signature. {signature=}, {args=}, {kwargs=}')
        dewy_kwargs[name] = evaluate(arg.right, closure_scope)

    return dewy_args, dewy_kwargs



def update_signature(signature: Signature, args: list[AST], scope: Scope) -> Signature:
    """Given values to partially apply to a function, update the call signature to reflect the new values"""
    call_args, call_kwargs = collect_calling_args(args, scope)
    sig_pkwargs, sig_pargs, sig_kwargs = signature.pkwargs.copy(), signature.pargs.copy(), signature.kwargs.copy()
    for item in sig_kwargs:
        name = get_arg_name(item)
        if name in call_kwargs:
            sig_kwargs = [*filter(lambda i: get_arg_name(i) != name, sig_kwargs)]
            right = suspend(call_kwargs[name], scope) #((lambda ast, scope: lambda: evaluate(ast, scope))(call_kwargs[name], scope))
            sig_kwargs.append(Assign(left=Identifier(name), right=right))
    for item in sig_pkwargs:
        name = get_arg_name(item)
        if name in call_kwargs:
            sig_pkwargs = [*filter(lambda i: get_arg_name(i) != name, sig_pkwargs)]
            right = suspend(call_kwargs[name], scope) #Suspense((lambda ast, scope: lambda: evaluate(ast, scope))(call_kwargs[name], scope))
            # any pkwargs become kwargs when a value is given by keyword or position
            sig_kwargs.append(Assign(left=Identifier(name), right=right))

    # update the positional arguments
    for item in call_args:
        if len(sig_pargs) > 0:
            parg, sig_pargs = sig_pargs[0], sig_pargs[1:]
            name = get_arg_name(parg)
        elif len(sig_pkwargs) > 0:
            parg, sig_pkwargs = sig_pkwargs[0], sig_pkwargs[1:]
            name = get_arg_name(parg)
        else:
            raise ValueError(f'Too many positional arguments for function. {signature=}, {args=}')
        right = suspend(item, scope) #Suspense((lambda ast, scope: lambda: evaluate(ast, scope))(item, scope))
        sig_kwargs.append(Assign(left=Identifier(name), right=right))

    return Signature(pkwargs=sig_pkwargs, pargs=sig_pargs, kwargs=sig_kwargs)

def apply_partial_eval(f: AST, args: list[AST], scope: Scope) -> AST:
    match f:
        # # this case shouldn't really be possible since you have to wrap a function literal in parenthesis to @ it, turning it into a Closure
        # case FunctionLiteral(args=signature, body=body):
        #     new_signature = update_signature(signature, args, scope)
        #     return FunctionLiteral(args=new_signature, body=body)

        case Closure(fn=FunctionLiteral(args=signature, body=body), scope=closure_scope):
            new_signature = update_signature(signature, args, scope)
            return Closure(fn=FunctionLiteral(args=new_signature, body=body), scope=closure_scope)

        case Builtin(signature=signature, preprocessor=preprocessor, action=action, return_type=return_type):
            new_signature = update_signature(signature, args, scope)
            return Builtin(signature=new_signature, preprocessor=preprocessor, action=action, return_type=return_type)

        case Identifier(name):
            f = scope.get(name).value
            return apply_partial_eval(f, args, scope)
        case _:
            raise NotImplementedError(f'Partial evaluation not implemented yet for {f=}')


def evaluate_group(ast: Group, scope: Scope):

    expressed: list[AST] = []
    for expr in ast.items:
        res = evaluate(expr, scope)
        if res is not void:
            expressed.append(res)
    if len(expressed) == 0:
        return void
    if len(expressed) == 1:
        return expressed[0]
    raise NotImplementedError(f'Block with multiple expressions not yet supported. {ast=}, {expressed=}')


def evaluate_block(ast: Block, scope: Scope):
    scope = Scope(scope)
    return evaluate_group(Group(ast.items), scope)

def evaluate_array(ast: Array, scope: Scope) -> Array:
    return Array([evaluate(i, scope) for i in ast.items])

def evaluate_dict(ast: Dict, scope: Scope) -> Dict:
    return Dict([evaluate(kv, scope) for kv in ast.items])

def evaluate_points_to(ast: PointsTo, scope: Scope) -> PointsTo:
    return PointsTo(evaluate(ast.left, scope), evaluate(ast.right, scope))

def evaluate_bidir_dict(ast: BidirDict, scope: Scope) -> BidirDict:
    return BidirDict([evaluate(kv, scope) for kv in ast.items])

def evaluate_bidir_points_to(ast: BidirPointsTo, scope: Scope) -> BidirPointsTo:
    return BidirPointsTo(evaluate(ast.left, scope), evaluate(ast.right, scope))

def evaluate_object_literal(ast: ObjectLiteral, scope: Scope) -> Object:
    obj_scope = Scope(scope)
    evaluate(Group(ast.items), obj_scope)
    return Object(scope=obj_scope)

def evaluate_access(ast: Access, scope: Scope) -> AST:
    left = evaluate(ast.left, scope)
    right = ast.right
    match right:
        case Identifier():
            return evaluate_id_access(left, right, scope, evaluate_right=True)
        case AtHandle(operand=Identifier() as id):
            return evaluate_id_access(left, id, scope, evaluate_right=False)
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'evaluate_access not implemented yet for {right=}. {left=}')

def evaluate_index(ast: Index, scope: Scope) -> AST:
    left = evaluate(ast.left, scope)
    right = evaluate(ast.right, scope)
    match left, right:
        case Array(items), Array(items=[Int(i)]):
            return items[i]
        case _:
            pdb.set_trace()
    pdb.set_trace()
    ...

def evaluate_id_access(left: AST, right: Identifier, scope: Scope, evaluate_right=True) -> AST:
    match left:
        case Object(scope):
            access = scope.get(right.name, search_parents=False).value
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'evaluate_id_access not implemented yet for {left=}, {right=}')
        
    if evaluate_right:
        return evaluate(access, scope)
    return access

# def evaluate_at_handle_access(left: AST, right: AtHandle, scope: Scope) -> AST:
#     pdb.set_trace()
#     raise NotImplementedError(f'evaluate_at_handle_access not implemented yet for {left=}, {right=}')

#TODO: other access types (which are probably more like vectorized ops)
#      vectorized_call. perhaps this is just the catch all case for anything not an identifier? to use an identifier as an arg, just wrap in parens
#      vectorized_index? is that a coherent concept? honestly probably just let regular multidimensional indexing handle that case

def evaluate_assign(ast: Assign, scope: Scope):
    match ast:
        case Assign(left=Identifier(name), right=right):
            right = evaluate(right, scope)
            scope.assign(name, right)
            return void
        case Assign(left=UnpackTarget() as target, right=right):
            right = evaluate(right, scope)
            unpack_assign(target, right, scope)
            return void
    pdb.set_trace()
    raise NotImplementedError('Assign not implemented yet')


def evaluate_iter_in(ast: IterIn, scope: Scope):

    # helper function for progressing the iterator
    def step_iter_in(iter_props: tuple[TypingCallable, Iter], scope: Scope) -> AST:
        binder, iterable = iter_props
        cond, val = iter_next(iterable).items
        binder(val)
        return cond

    # if the iterator properties are already in the scope, use them
    if (res := scope.meta[ast].props) is not None:
        return step_iter_in(cast(tuple[TypingCallable, Iter], res), scope)

    # otherwise initialize since this is the first time we're hitting this IterIn
    match ast:
        case IterIn(left=Identifier(name), right=right):
            right = evaluate(right, scope)
            props = lambda x: scope.assign(name, x), Iter(item=right, i=0)
            scope.meta[ast].props = props
            return step_iter_in(props, scope)
        case IterIn(left=UnpackTarget() as target, right=right):
            right = evaluate(right, scope)
            props = lambda x: unpack_assign(target, x, scope), Iter(item=right, i=0)
            scope.meta[ast].props = props
            return step_iter_in(props, scope)

    pdb.set_trace()
    raise NotImplementedError('IterIn not implemented yet')


#TODO: this is really only for array unpacking. need to handle object unpacking as well...
#      need to check what type value
def unpack_assign(target: UnpackTarget, value: AST, scope: Scope):

    # current inefficient hack to unpack strings
    if isinstance(value, String):
        value = Array([String(c) for c in value.val])

    # current types supporting unpacking
    if not isinstance(value, (Array, Dict, PointsTo, BidirDict, BidirPointsTo, Undefined)):
        raise NotImplementedError(f'unpack_assign() is not yet implemented for {value=}')

    # determine how many targets will be assigned, and if spread is present
    num_targets = len(target.target)
    num_spread = sum(isinstance(t, CollectInto) for t in target.target)
    if num_spread > 1: raise RuntimeError(f'Only one collect-into is allowed in unpacking. {target=}, {value=}')

    # undefined unpacks as many undefineds as there are non-spread targets
    if isinstance(value, Undefined):
        value = Array([undefined for _ in range(num_targets - num_spread)])

    # verify if enough values to unpack, and set up generator (using built in iteration over ASTs children)
    num_values = len([*value.__iter_asts__()])
    spread_size = num_values - num_targets + 1  # if a spread is present, how many elements it will take
    if num_targets - num_spread > num_values: raise RuntimeError(f'Not enough values to unpack. {num_targets=}, {target=}, {value=}')
    gen = value.__iter_asts__()

    for left in target.target:
        match left:
            case Identifier(name):
                scope.assign(name, next(gen))
            # #TODO: object member renamed unpack. need to get the member of the object and assign it to the new name
            # case Assign(left=Identifier(name), right=right):
            #     scope.assign(name, right)
            case UnpackTarget():
                unpack_assign(left, next(gen), scope)
            case CollectInto(right=Identifier(name)):
                scope.assign(name, Array([next(gen) for _ in range(spread_size)]))
            case CollectInto(right=UnpackTarget() as left):
                unpack_assign(left, Array([next(gen) for _ in range(spread_size)]), scope)
            case _:
                pdb.set_trace()
                raise NotImplementedError(f'unpack_assign not implemented for {left=} and right={next(gen)}')

    # if there are any remaining values, raise an error
    if (remaining := [*gen]):
        raise RuntimeError(f'Too many values to unpack. {num_targets=}, {target=}, {value=}, {remaining=}')

# TODO: probably break this up into one function per type of iterable
def iter_next(iter: Iter):
    match iter.item:
        case Array(items):
            if iter.i >= len(items):
                cond, val = Bool(False), undefined
            else:
                cond, val = Bool(True), items[iter.i]
            iter.i += 1
            return Array([cond, val])
        case Dict(items):
            if iter.i >= len(items):
                cond, val = Bool(False), undefined
            else:
                cond, val = Bool(True), items[iter.i]
            iter.i += 1
            return Array([cond, val])
        case Range(left=Int(val=l), right=Void()|Undefined(), brackets=brackets):
            offset = int(brackets[0] == '(') # handle if first value is exclusive
            cond, val = Bool(True), Int(l + iter.i + offset)
            iter.i += 1
            return Array([cond, val])
        case Range(left=Int(val=l), right=Int(val=r), brackets=brackets):
            offset = int(brackets[0] == '(') # handle if first value is exclusive
            end_offset = int(brackets[1] == ']')
            i = l + iter.i + offset
            if i > r + end_offset - 1:
                cond, val = Bool(False), undefined
            else:
                cond, val = Bool(True), Int(i)
            iter.i += 1
            return Array([cond, val])
        case Range(left=Array(items=[Int(val=r0), Int(val=r1)]), right=Void()|Undefined(), brackets=brackets):
            offset = int(brackets[0] == '(') # handle if first value is exclusive
            step = r1 - r0
            cond, val = Bool(True), Int(r0 + (iter.i + offset) * step)
            iter.i += 1
            return Array([cond, val])
        case Range(left=Array(items=[Int(val=r0), Int(val=r1)]), right=Int(val=r2), brackets=brackets):
            offset = int(brackets[0] == '(') # handle if first value is exclusive
            end_offset = int(brackets[1] == ']')
            step = r1 - r0
            i = r0 + (iter.i + offset) * step
            if i > r2 + end_offset - 1:
                cond, val = Bool(False), undefined
            else:
                cond, val = Bool(True), Int(i)
            iter.i += 1
            return Array([cond, val])
        #TODO: other range cases...
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'iter_next not implemented yet for {iter.item=}')



def evaluate_function_literal(ast: FunctionLiteral, scope: Scope):
    return Closure(fn=ast, scope=scope)

def evaluate_closure(ast: Closure, scope: Scope):
    closure_scope = Scope(ast.scope)
    caller_scope = Scope(scope)
    args, kwargs = scope.meta[ast].call_args or ([], {})
    signature = ast.fn.args

    # attach all the args to scope so body can access them
    dewy_args, dewy_kwargs = resolve_calling_args(signature, args, kwargs, caller_scope, closure_scope)
    for name, value in (dewy_args | dewy_kwargs).items():
        closure_scope.assign(name, value)

    return evaluate(ast.fn.body, closure_scope)


def evaluate_builtin(ast: Builtin, scope: Scope):
    caller_scope = Scope(scope)
    call_args, call_kwargs = scope.meta[ast].call_args or ([], {})
    dewy_args, dewy_kwargs = resolve_calling_args(ast.signature, call_args, call_kwargs, caller_scope)
    py_args, py_kwargs = ast.preprocessor([*dewy_args.values()], dewy_kwargs, caller_scope)
    return ast.action(*py_args, **py_kwargs)


def evaluate_istring(ast: IString, scope: Scope) -> String:
    parts = (py_stringify(i, scope) for i in ast.parts)
    return String(''.join(parts))


def evaluate_express(ast: Express, scope: Scope):
    val = scope.get(ast.id.name).value
    return evaluate(val, scope)

def evaluate_flow(ast: Flow, scope: Scope):
    for branch in ast.branches:
        #TODO: slightly hacky way to get the child scope created by the branch (so we can check if it was entered)
        child_scope = None
        def save_child_scope(scope: Scope):
            nonlocal child_scope
            child_scope = scope

        match branch:
            case Default(): res = evaluate_default(branch, scope, save_child_scope)
            case If(): res = evaluate_if(branch, scope, save_child_scope)
            case Loop(): res = evaluate_loop(branch, scope, save_child_scope)
            case _:
                pdb.set_trace()
                raise NotImplementedError(f'evaluate_flow not implemented for flow type {branch=}')

        # if the branch was entered, return its result
        assert child_scope.meta[branch].was_entered is not None, f'INTERNAL ERROR: {branch=} .was_entered was not set'
        if child_scope.meta[branch].was_entered:
            return res

    # if no branches were entered, return void
    return void

def evaluate_default(ast: Default, scope: Scope, save_child_scope:TypingCallable[[Scope], None]=lambda _: None):
    scope = Scope(scope)
    save_child_scope(scope)
    scope.meta[ast].was_entered = True
    return evaluate(ast.body, scope)

#TODO: this needs improvements!
#Issue URL: https://github.com/david-andrew/dewy-lang/issues/2
def evaluate_if(ast: If, scope: Scope, save_child_scope:TypingCallable[[Scope], None]=lambda _: None):
    scope = Scope(scope)
    save_child_scope(scope)
    scope.meta[ast].was_entered = False
    if cast(Bool, evaluate(ast.condition, scope)).val:
        scope.meta[ast].was_entered = True
        return evaluate(ast.body, scope)

    # is this correct if the If isn't entered?
    return void

#TODO: this needs improvements!
def evaluate_loop(ast: Loop, scope: Scope, save_child_scope:TypingCallable[[Scope], None]=lambda _: None):
    scope = Scope(scope)
    save_child_scope(scope)
    scope.meta[ast].was_entered = False
    while cast(Bool, evaluate(ast.condition, scope)).val:
        scope.meta[ast].was_entered = True
        evaluate(ast.body, scope)

    # for now loops can't return anything
    return void
    # ast.body
    # ast.condition
    # pdb.set_trace()



def int_int_div(l: int, r: int) -> Int | Float | Undefined:
    if r == 0:
        return undefined
    res = l / r
    if res.is_integer():
        return Int(int(res))
    else:
        return Float(res)

def float_float_div(l: int|float, r: int|float) -> Float | Undefined:
    if r == 0:
        return undefined
    return Float(l / r)

class SimpleValue(Protocol, Generic[T]):
    val: T


# @dataclass
# class Dispatch:
#     fn_table: dict[tuple[Any, ...], TypingCallable[..., AST]]
#     type_symmetric: bool = True
#     # etc.
# DispatchTable = dict[type[UnaryPrefixOp|UnaryPostfixOp|BinOp], Dispatch]
# PromotionTable = dict[tuple[type[T], type[U]], TypingCallable[[T|U], T|U]]
# promote_rule(Int, Float64) == Float64
# promote_type(Int, Float64)  # Float64

UnaryDispatchKey =  tuple[type[UnaryPrefixOp]|type[UnaryPostfixOp], type[SimpleValue[T]]]
unary_dispatch_table: dict[UnaryDispatchKey[T], TypingCallable[[T], AST]] = {
    (Not, Int): lambda l: Int(~l),
    (Not, Bool): lambda l: Bool(not l),
    (UnaryPos, Int): lambda l: Int(l),
    (UnaryNeg, Int): lambda l: Int(-l),
    (UnaryMul, Int): lambda l: Int(l),
    (UnaryDiv, Int): lambda l: Int(1/l),
}

BinaryDispatchKey = tuple[type[BinOp], type[SimpleValue[T]], type[SimpleValue[U]]]
# These are all symmetric meaning you can swap the operand types and the same function will be used (but the arguments should not be swapped)
binary_dispatch_table: dict[BinaryDispatchKey[T, U], TypingCallable[[T, U], AST]|TypingCallable[[U, T], AST]] = {
    (And, Int, Int): lambda l, r: Int(l & r),
    (And, Bool, Bool): lambda l, r: Bool(l and r),
    (Or, Int, Int): lambda l, r: Int(l | r),
    (Or, Bool, Bool): lambda l, r: Bool(l or r),
    (Xor, Int, Int): lambda l, r: Int(l ^ r),
    (Xor, Bool, Bool): lambda l, r: Bool(l != r),
    (Nand, Int, Int): lambda l, r: Int(~(l & r)),
    (Nand, Bool, Bool): lambda l, r: Bool(not (l and r)),
    (Nor, Int, Int): lambda l, r: Int(~(l | r)),
    (Nor, Bool, Bool): lambda l, r: Bool(not (l or r)),
    (Add, Int, Int): lambda l, r: Int(l + r),
    (Add, Int, Float): lambda l, r: Float(l + r),
    (Add, Float, Float): lambda l, r: Float(l + r),
    (Sub, Int, Int): lambda l, r: Int(l - r),
    (Sub, Int, Float): lambda l, r: Float(l - r),
    (Sub, Float, Float): lambda l, r: Float(l - r),
    (Mul, Int, Int): lambda l, r: Int(l * r),
    (Mul, Int, Float): lambda l, r: Float(l * r),
    (Mul, Float, Float): lambda l, r: Float(l * r),
    (Div, Int, Int): int_int_div,
    (Div, Int, Float): float_float_div,
    (Div, Float, Float): float_float_div,
    (Mod, Int, Int): lambda l, r: Int(l % r),
    (Mod, Int, Float): lambda l, r: Float(l % r),
    (Mod, Float, Float): lambda l, r: Float(l % r),
    (Pow, Int, Int): lambda l, r: Int(l ** r),
    (Pow, Int, Float): lambda l, r: Float(l ** r),
    (Pow, Float, Float): lambda l, r: Float(l ** r),
    (Less, Int, Int): lambda l, r: Bool(l < r),
    (Less, Int, Float): lambda l, r: Bool(l < r),
    (Less, Float, Float): lambda l, r: Bool(l < r),
    (LessEqual, Int, Int): lambda l, r: Bool(l <= r),
    (LessEqual, Int, Float): lambda l, r: Bool(l <= r),
    (LessEqual, Float, Float): lambda l, r: Bool(l <= r),
    (Greater, Int, Int): lambda l, r: Bool(l > r),
    (Greater, Int, Float): lambda l, r: Bool(l > r),
    (Greater, Float, Float): lambda l, r: Bool(l > r),
    (GreaterEqual, Int, Int): lambda l, r: Bool(l >= r),
    (GreaterEqual, Int, Float): lambda l, r: Bool(l >= r),
    (GreaterEqual, Float, Float): lambda l, r: Bool(l >= r),
    (Equal, Int, Int): lambda l, r: Bool(l == r),
    (Equal, Float, Float): lambda l, r: Bool(l == r),
    (Equal, Bool, Bool): lambda l, r: Bool(l == r),
    (Equal, String, String): lambda l, r: Bool(l == r),
    # (NotEqual, Int, Int): lambda l, r: Bool(l != r),
    (LeftShift, Int, Int): lambda l, r: Int(l << r),
    (RightShift, Int, Int): lambda l, r: Int(l >> r),

}

unsymmetric_binary_dispatch_table: dict[BinaryDispatchKey[T, U], ] = {
    #e.g. (Mul, String, Int): lambda l, r: String(l * r), # if we follow python's behavior
}

# dispatch table for more complicated values that can't be automatically unpacked by the dispatch table
# TODO: actually ideally just have a single table
CustomBinaryDispatchKey = tuple[type[BinOp], type[T], type[U]]
custom_binary_dispatch_table: dict[CustomBinaryDispatchKey[T, U], TypingCallable[[T, U], AST]] = {
    (Add, Array, Array): lambda l, r: Array(l.items + r.items), #TODO: this will be removed in favor of spread. array add will probably be vector add
    # (BroadcastOp, Array, Array): broadcast_array_op,
    # (BroadcastOp, NpArray, NpArray): broadcast_array_op,
    # (BroadcastOp, Int, Array): broadcast_array_op,
    # (BroadcastOp, Float, Array): broadcast_array_op,

}

#TODO: handling short circuiting for logical operators. perhaps have them in a separate dispatch table

def evaluate_binary_dispatch(op: BinOp, scope: Scope):
    # evaluate the operands
    left = evaluate(op.left, scope)
    right = evaluate(op.right, scope)
    
    # if either operand is undefined, the result is undefined
    if isinstance(left, Undefined) or isinstance(right, Undefined):
        return undefined
    
    # dispatch to the appropriate function
    key = (type(op), type(left), type(right))
    if key in binary_dispatch_table:
        left, right = cast(SimpleValue[T], left), cast(SimpleValue[U], right)
        return binary_dispatch_table[key](left.val, right.val)
    if key in custom_binary_dispatch_table:
        return custom_binary_dispatch_table[key](left, right)

    # if the key wasn't found, try the reverse key (by swapping the types of the operands)
    reverse_key = (type(op), type(right), type(left))
    if reverse_key in binary_dispatch_table:
        left, right = cast(SimpleValue[U], left), cast(SimpleValue[T], right)
        return binary_dispatch_table[reverse_key](left.val, right.val)
    if reverse_key in custom_binary_dispatch_table:
        return custom_binary_dispatch_table[reverse_key](right, left)

    raise NotImplementedError(f'Binary dispatch not implemented for {key=}')

def evaluate_unary_dispatch(op: UnaryPrefixOp|UnaryPostfixOp, scope: Scope):
    # evaluate the operand
    operand = evaluate(op.operand, scope)

    # if the operand is undefined, the result is undefined
    if isinstance(operand, Undefined):
        return undefined
    
    # dispatch to the appropriate function
    key = (type(op), type(operand))
    if key in unary_dispatch_table:
        operand = cast(SimpleValue[T], operand)
        return unary_dispatch_table[key](operand.val)
    
    raise NotImplementedError(f'Unary dispatch not implemented for {key=}')


def evaluate_at_handle(ast: AtHandle, scope: Scope):
    match ast.operand:
        case Identifier(name):
            return scope.get(name).value
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'AtHandle not implemented for {ast.operand=}')

############################ Builtin functions and helpers ############################

# all references to python functions go through this interface to allow for easy swapping
from functools import partial
class BuiltinFuncs:
    printl=print
    print=partial(print, end='')
    readl=input

#TODO: consider adding a flag repr vs str, where initially str is used, but children get repr. 
# as is, stringifying should put quotes around strings that are children of other objects 
# but top level printed strings should not show their quotes
def py_stringify(ast: AST, scope: Scope, top_level:bool=False) -> str:    
    # don't evaluate. already evaluated by resolve_calling_args
    ast = evaluate(ast, scope) if not isinstance(ast, (Builtin, Closure)) else ast
    match ast:
        # types that require special handling (i.e. because they have children that need to be stringified)
        case String(val): return val# if top_level else f'"{val}"'
        case Array(items): return f"[{' '.join(py_stringify(i, scope) for i in items)}]"
        case Dict(items): return f"[{' '.join(py_stringify(kv, scope) for kv in items)}]"
        case PointsTo(left, right): return f'{py_stringify(left, scope)}->{py_stringify(right, scope)}'
        case BidirDict(items): return f"[{' '.join(py_stringify(kv, scope) for kv in items)}]"
        case BidirPointsTo(left, right): return f'{py_stringify(left, scope)}<->{py_stringify(right, scope)}'
        case Range(left, right, brackets): return f'{brackets[0]}{py_stringify_range_operands(left, scope)}..{py_stringify_range_operands(right, scope)}{brackets[1]}'
        case Closure(fn): return f'{fn}'
        case FunctionLiteral() as fn: return f'{fn}'
        case Builtin() as fn: return f'{fn}'
        case Object() as obj: return f'{obj}'
        # case AtHandle() as at: return py_stringify(evaluate(at, scope), scope)

        # can use the built-in __str__ method for these types
        case Int() | Float() | Bool() | Undefined(): return str(ast)

        # TBD what other types need to be handled
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'stringify not implemented for {type(ast)}')
    pdb.set_trace()


    raise NotImplementedError('stringify not implemented yet')

def py_stringify_range_operands(ast: AST, scope: Scope) -> str:
    """helper function to stringify range operands which may be a single value or a tuple (represented as an array)"""
    if isinstance(ast, Array):
        return f"{','.join(py_stringify(i, scope) for i in ast.items)}"
    return py_stringify(ast, scope)

def preprocess_py_print_args(args: list[AST], kwargs: dict[str, AST], scope: Scope) -> tuple[list[Any], dict[str, Any]]:
    py_args = [py_stringify(i, scope, top_level=True) for i in args]
    py_kwargs = {k: py_stringify(v, scope) for k, v in kwargs.items()}
    return py_args, py_kwargs

def py_printl(s:str) -> Void:
    BuiltinFuncs.printl(s)
    return void

def py_print(s:str) -> Void:
    BuiltinFuncs.print(s)
    return void

def py_readl() -> String:
    return String(BuiltinFuncs.readl())

def insert_builtins(scope: Scope):
    """replace prototype builtin stubs with actual implementations"""
    if 'printl' in scope.vars:
        assert isinstance((proto:=scope.vars['printl'].value), PrototypeBuiltin)
        scope.vars['printl'].value = Builtin.from_prototype(proto, preprocess_py_print_args, py_printl)
    if 'print' in scope.vars:
        assert isinstance((proto:=scope.vars['print'].value), PrototypeBuiltin)
        scope.vars['print'].value = Builtin.from_prototype(proto, preprocess_py_print_args, py_print)
    if 'readl' in scope.vars:
        assert isinstance((proto:=scope.vars['readl'].value), PrototypeBuiltin)
        scope.vars['readl'].value = Builtin.from_prototype(proto, lambda *a: ([],{}), py_readl)
20:T2c76,from ...tokenizer import tokenize
from ...postok import post_process
from ...dtypes import (
    Scope as DTypesScope,
    typecheck_call, typecheck_index, typecheck_multiply,
    register_typeof, short_circuit,
    CallableBase, IndexableBase, IndexerBase, MultipliableBase, ObjectBase,
)
from ...parser import top_level_parse, QJux
from ...syntax import (
    AST,
    Type, TypeParam,
    PointsTo, BidirPointsTo,
    ListOfASTs, PrototypeTuple, Block, Array, Group, Range, ObjectLiteral, Dict, BidirDict, UnpackTarget,
    TypedIdentifier,
    Void, void, Undefined, undefined, untyped,
    String, IString,
    Flowable, Flow, If, Loop, Default,
    Identifier, Express, Declare,
    PrototypeBuiltin, Call, Access, Index,
    Assign,
    Int, Bool,
    Range, IterIn,
    BinOp,
    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,
    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    UnaryPrefixOp, UnaryPostfixOp,
    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv, AtHandle,
    CycleLeft, CycleRight, Suppress,
    BroadcastOp,
    CollectInto, SpreadOutFrom,
)

from ...postparse import post_parse, FunctionLiteral, Signature, normalize_function_args
from ...utils import Options

from dataclasses import dataclass, field
from pathlib import Path
from typing import Protocol, cast, Callable as TypingCallable, Any, Generic, Literal
from functools import cache
from collections import defaultdict
from types import SimpleNamespace
from itertools import count


import pdb


from functools import cache
from pathlib import Path
from ...utils import Options


# TODO: not quite sure what to do about scope. for now, just steal from
from ..python import Scope, Closure

# command to compile a .qbe file to an executable
# qbe <file>.ssa | gcc -x assembler -static -o hello 

def qbe_compiler(path: Path, args: list[str], options: Options) -> None:
    # get the source code and tokenize
    src = path.read_text()
    tokens = tokenize(src)
    post_process(tokens)

    # parse tokens into AST
    ast = top_level_parse(tokens)
    ast = post_parse(ast)

    # debug printing
    if options.verbose:
        print(repr(ast))

    # generate the program qbe
    qbe = top_level_compile(ast)
    ssa = str(qbe)
    #DEBUG
    print(ssa)

    # TODO:
    # write the ssa to a file
    # compile the ssa with qbe
    # construct the full executable
    # run the executable



def top_level_compile(ast: AST) -> 'QbeModule':
    scope = Scope.default()
    qbe = QbeModule()
    compile(ast, scope, qbe)
    return qbe

@dataclass
class CAST:
    ast: AST
    meta: SimpleNamespace


# TODO: include user defined struct types...
QbeType = Literal['w', 'l', 's', 'd', 'b', 'h'] | str

@dataclass
class QbeBlock:
    label: str
    lines: list[str]

    def __str__(self) -> str:
        return '\n    '.join([self.label, *self.lines])

@dataclass
class QbeArg:
    name: str
    type: QbeType

    def __str__(self) -> str:
        return f'{self.type} {self.name}'

@dataclass
class QbeFunction:
    name: str
    export: bool
    args: list[QbeArg]
    ret: QbeType | None
    blocks: list[QbeBlock]

    def __str__(self) -> str:
        export = 'export ' if self.export else ''
        args = ', '.join(map(str, self.args))
        ret = f'{self.ret} ' if self.ret else ''
        blocks = '\n'.join(map(str, self.blocks))
        return f'{export}function {ret}{self.name}({args}) {{\n{blocks}\n}}'

@dataclass
class QbeModule:
    functions: list[QbeFunction] = field(default_factory=list)
    global_data: list[str] = field(default_factory=list)
    global_counter = count(0)

    # TODO: function for getting identifiers, or next counter


    def __str__(self) -> str:
        functions = '\n\n'.join(map(str, self.functions))
        global_data = '\n'.join(self.global_data)
        return f'{global_data}\n\n{functions}'


from typing import Protocol, TypeVar
T = TypeVar('T', bound=AST)
U = TypeVar('U', bound=AST)
class CompileFunc(Protocol):
    #TODO: what is the correct return type here?
    #      - str for the name of the expression as stored in the scope if needs to be looked up?
    #         - could be good because we make us of scopes, and help keep things organized
    #         - what gets stored in the scope though? and how does the scope name map to the name in the QBE module?
    #      - CAST which includes the AST, and any qbe identifiers for it?
    def __call__(self, ast: T, scope: Scope, qbe: QbeModule) -> CAST: ...



@cache
def get_compile_fn_map() -> dict[type[AST], CompileFunc]:
    return {
        # Declare: compile_declare,
        QJux: compile_qjux,
        Call: compile_call,
        # Block: compile_block,
        # Group: compile_group,
        # Array: compile_array,
        # Dict: compile_dict,
        # PointsTo: compile_points_to,
        # BidirDict: compile_bidir_dict,
        # BidirPointsTo: compile_bidir_points_to,
        # ObjectLiteral: compile_object_literal,
        # Object: no_op,
        # Access: compile_access,
        # Index: compile_index,
        # Assign: compile_assign,
        # IterIn: compile_iter_in,
        # FunctionLiteral: compile_function_literal,
        # Closure: compile_closure,
        # Builtin: compile_builtin,
        String: compile_string,
        # IString: compile_istring,
        # Identifier: cannot_evaluate,
        # Express: compile_express,
        # Int: no_op,
        # Float: no_op,
        # Bool: no_op,
        # Range: no_op,
        # Flow: compile_flow,
        # Default: compile_default,
        # If: compile_if,
        # Loop: compile_loop,
        # UnaryPos: compile_unary_dispatch,
        # UnaryNeg: compile_unary_dispatch,
        # UnaryMul: compile_unary_dispatch,
        # UnaryDiv: compile_unary_dispatch,
        # Not: compile_unary_dispatch,
        # Greater: compile_binary_dispatch,
        # GreaterEqual: compile_binary_dispatch,
        # Less: compile_binary_dispatch,
        # LessEqual: compile_binary_dispatch,
        # Equal: compile_binary_dispatch,
        # And: compile_binary_dispatch,
        # Or: compile_binary_dispatch,
        # Xor: compile_binary_dispatch,
        # Nand: compile_binary_dispatch,
        # Nor: compile_binary_dispatch,
        # Xnor: compile_binary_dispatch,
        # Add: compile_binary_dispatch,
        # Sub: compile_binary_dispatch,
        # Mul: compile_binary_dispatch,
        # Div: compile_binary_dispatch,
        # Mod: compile_binary_dispatch,
        # Pow: compile_binary_dispatch,
        # AtHandle: compile_at_handle,
        # Undefined: no_op,
        # Void: no_op,
        # #TODO: other AST types here
    }



def compile(ast:AST, scope:Scope, qbe: QbeModule):
    compile_fn_map = get_compile_fn_map()

    ast_type = type(ast)
    if ast_type in compile_fn_map:
        return compile_fn_map[ast_type](ast, scope, qbe)

    raise NotImplementedError(f'AST type {ast_type} not implemented yet')

def compile_qjux(ast: QJux, scope: Scope, qbe: QbeModule):
    if ast.call is not None and typecheck_call(ast.call, scope):
        return compile_call(ast.call, scope, qbe)
    if ast.index is not None and typecheck_index(ast.index, scope):
        return compile_index(ast.index, scope, qbe)
    if typecheck_multiply(ast.mul, scope):
        return compile_binary_dispatch(ast.mul, scope, qbe)

    raise ValueError(f'Typechecking failed to match a valid evaluation for QJux. {ast=}')



def compile_string(ast: String, scope: Scope, qbe: QbeModule):
    pdb.set_trace()
    ...

def compile_call(call: Call, scope: Scope, qbe: QbeModule):
    f = call.f

    # get the expression of the group
    if isinstance(f, Group):
        pdb.set_trace()
        f = evaluate(f, scope)

    # get the value pointed to by the identifier
    if isinstance(f, Identifier):
        f = scope.get(f.name).value

    # if this is a handle, do a partial evaluation rather than a call
    if isinstance(f, AtHandle):
        pdb.set_trace()
        return apply_partial_eval(f.operand, call.args, scope)

    # AST being called must be TypingCallable
    assert isinstance(f, (PrototypeBuiltin, Closure)), f'expected Function or Builtin, got {f}'

    # save the args of the call as metadata for the function AST
    call_args, call_kwargs = collect_calling_args(call.args, scope)
    scope.meta[f].call_args = call_args, call_kwargs

    # run the function and return the result
    if isinstance(f, PrototypeBuiltin):
        return compile_call_pyaction(f, scope, qbe)
    if isinstance(f, Closure):
        return compile_call_closure(f, scope, qbe)

    pdb.set_trace()
    raise NotImplementedError(f'Function evaluation not implemented yet')





#TODO: longer term this might also return a list/dict of spread args passed into the function
def collect_calling_args(args: AST | None, scope: Scope) -> tuple[list[AST], dict[str, AST]]:
    """
    Collect the arguments that a function is being called with
    e.g. `let fn = (a b c) => a + b + c; fn(1 c=2 3)`
    then the calling args are [1, 3] and {c: 2}

    Args:
        args: the arguments being passed to the function. If None, then treat as a no-arg call
        scope: the scope in which the function is being called

    Returns:
        a tuple of the positional arguments and keyword arguments
    """
    match args:
        case None | Void(): return [], {}
        case Identifier(name): return [scope.get(name).value], {}
        case Assign(left=Identifier(name)|TypedIdentifier(id=Identifier(name)), right=right): return [], {name: right}
        # case Assign(left=UnpackTarget() as target, right=right): raise NotImplementedError('UnpackTarget not implemented yet')
        case Assign(): raise NotImplementedError('Assign not implemented yet') #called recursively if a calling arg was an keyword arg rather than positional
        case CollectInto(right=right):
            pdb.set_trace()
            ... #right should be iterable, so extend with the values it expresses
                #whether to add to args or kwargs depends on each type from right
            val = evaluate(right, scope)
            match val:
                case Array(items): ... #return [collect_calling_args(i, scope) for i in items]
        case Group(items):
            call_args, call_kwargs = [], {}
            for i in items:
                a, kw = collect_calling_args(i, scope)
                call_args.extend(a)
                call_kwargs.update(kw)
            return call_args, call_kwargs

        #TODO: eventually it should just be anything that is left over is positional args rather than specifying them all out
        case Int() | String() | IString() | Range() | Call() | Access() | Index() | Express() | QJux() | UnaryPrefixOp() | UnaryPostfixOp() | BinOp() | BroadcastOp():
            return [args], {}
        # case Call(): return [args], {}
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'collect_args not implemented yet for {args}')


    raise NotImplementedError(f'collect_args not implemented yet for {args}')










def compile_call_pyaction(f: PrototypeBuiltin, scope: Scope, qbe: QbeModule):
    pdb.set_trace()
    ...


def compile_call_closure(f: Closure, scope: Scope, qbe: QbeModule):
    pdb.set_trace()
    ...21:T8e5,//TODO: uncomment when types are supported
/{
// simple xorshift+ generator
state:uint64 = 123456789
rand = ():uint64 => {
    state xor= state >> 21
    state xor= state << 35
    state xor= state >> 4
    
    return state * 2_685821_657736_338717 //TODO: divide by uint64.MAX (18_446744_073709_551615)
}
half = 9_223372_036854_775807
a = rand <? half
b = rand <? half
c = rand <? half
d = rand <? half
e = rand <? half
f = rand <? half
g = rand <? half
h = rand <? half
i = rand <? half
j = rand <? half
k = rand <? half
l = rand <? half
m = rand <? half
n = rand <? half
o = rand <? half
p = rand <? half
q = rand <? half
r = rand <? half
s = rand <? half
t = rand <? half
u = rand <? half
v = rand <? half
w = rand <? half
x = rand <? half
y = rand <? half
z = rand <? half
}/

//manually specify bools
a = false
b = true
c = true
d = false
e = true
f = false
g = true
h = false
i = true
j = false
k = true
l = false
m = true
n = false
o = true
p = false
q = true
r = false
s = true
t = false
u = true
v = false
w = true
x = false
y = true
z = false



if a
    printl'a'
else if b
    if c
        if d
            if e
                printl'bcde'
            else if f
                if g
                    if h
                        printl'bcdfgh'
                    else if i
                        printl'bcdfgi'
                    else if j
                        printl'bcdfgj'
                    else
                        printl'bcdfg[]'
                else if k
                    printl'bcdfk'
                else if l
                    if m
                        printl'bcdflm'
                    else
                        printl'bcdfl[]'
                else
                    printl'bcdf[]'
            else if n
                printl'bcdn'
            else
                printl'bcd[]'
        else if o
            printl'bco'
        else if p
            if q
                printl'bcpq'
            else
                printl'bcp[]'
        else
            printl'bc[]'
    else
        printl'b[]'
else if r
    printl'r'
else if s
    if t
        printl'st'
    else if u
        printl'su'
if v
    if w
        printl'vw'
    else if x
        printl'x'
if y
    printl'y'
else if z
    printl'z'
else
    printl'[]'
22:T626,// simplest case
x = 10
plus_5 = () => x + 5
printl(plus_5())

// taking in args
x = 12
my_closure = y => x + y
printl(my_closure(-5))


// returning a closure from a child scope
a = 13
my_closure = {
    b = 10
    fn = c => a + b + c // uses `b` from this scope and `a` from parent scope
    @fn
}
printl(my_closure(5))



// print the handle to the closure itself
printl(@my_closure)
printl(@printl)


// some pathological cases
my_print = {
    x = 5
    s = 'string with internal reference to x={x}'
    @printl(s)
}
my_print


my_print = {
    my_str = {
        x = 5
        s = 'string with internal reference to x={x}'
        s
    }
    fn = @printl(my_str)
    @fn
}
my_print



// Very deeply nested closures combined with partial evaluation
X = 'xpple'
Y = 'yanana'
fn = {
    Z = 'zeach'
    fn = {
        A = () => '@Apricot'
        fn = {
            B = () => '@Blueberry'
            fn = {
                fn = (x y z a b c d) => {
                    printl'x="{x}"\ny="{y}"\nz="{z}"\na="{a}"\nb="{b}"\nc="{c}"\nd="{d}"'
                }
                @fn
            }
            printl'fn={@fn}'
            @fn(b=B) // note just B should evaluate the string on calling fn
        }
        printl'fn={@fn}'
        @fn(a=@A)  // note that @A should evaluate the string inside of the IString
    }
    printl'fn={@fn}'
    @fn(z=Z d='manually assigning D' c='unused C')
}
printl'fn={@fn}'
fn = @fn(y=Y x=X)
printl'fn={@fn}'
fn = @fn(c='C')
printl'fn={@fn}'
printl(@fn)
fn
printl()
printl(@fn(c='a different C'))
fn(c='an even more different C')23:T57f,//examples of each of the possible function signatures

// all args start out as positional or keyword (regardless of if they have defaults)
// partial application with positional args turns them into keyword only args
// partial application with keyword args turns them into positional args
// TBD unpack types probably need to be positional?
// TBD spread types probably need to be positional? 
//     or can we spread each of the possible types? I feel like spread just collects up everything not already specified in the signature

//////////////// Positional Arguments ////////////////
// no arguments
f0 = () => 0 // can call with `f0` or `f0()`

// 1 positional argument
f1 = x => x + 1     // can call with `f1(5)`
f1b = (x) => x + 1  // can call with `f1b(5)`

// 2 positional arguments
f2 = (x y) => x + y // can call with `f2(5 6)`

// 3 positional arguments
f3 = (x y z) => x + y + z // can call with `f3(5 6 7)`

printl'Positional Arguments'
printl(f0)
printl(f1(5))
printl(f2(5 6))
printl(f3(5 6 7))


//////////////// Optional Arguments ////////////////
// 1 positional and 1 optional keyword-only argument
f2b = (x y=2) => x + y // can call with `f2b(5)` or `f2b(5 y=6)`

// 1 optional keyword-only argument
f1c = (x=2) => x + 1 // can call with 'f1c' or `f1c()` or `f1c(x=5)`

printl'Optional Arguments'
printl(f2b(5))
printl(f2b(5 y=6))
printl(f1c)
printl(f1c(x=3))


//TODO: more examples24:Tbe6,/{ 
    Dewy Docs mdbook preprocessor:
    find code blocks labelled 'dewy' or 'dewy, editable' and convert them to iframes
}/

#main = () => {
    // mdbook calls preprocessor twice, first with args ["supports", <renderer>], 
    // and then if the first call exited with 0, the actual preprocessor run occurs
    // ignore the second argument (meaning support for all renderers)
    if sys.argv.length >? 1 and sys.argv[1] =? "supports"
        sys.exit(0)

    // get and parse the json input from stdin
    // TODO: need to make a json parser
    context, book = parse_json(read())

    loop section in book['sections']
    {
        section['Chapter']['content'] |>= process_markdown

        loop subitem in section['Chapter']['sub_items']
            subitem['Chapter']['content'] |>= process_markdown

        print(dump_json(book))
    }
}


counter = iter[0..]
process_markdown = (input_markdown) => {
    lines = input_markdown.split('\n')

    // lines starting with ```dewy
    starts = [
        loop i in [0..] and line in lines 
            if line[..6] =? '```dewy' 
                i
    ]

    // early return if no dewy code blocks
    if starts.length =? 0
        return input_markdown

    // ```dewy lines that are followed by ', editable' (whitespace invariant)
    editables = [
        loop i in starts
        {
            remainder = lines[i][7..].strip
            remainder[0] = ',' and remainder[1..].strip =? 'editable'
        }
    ]

    // match closing ``` lines
    stops = [
        loop i in starts
            loop line in lines[i..]
                if line =? '```'
                {
                    j
                    break
                }
    ]

    if starts.length not=? stops.length
        printl'Error: mismatched dewy code block starts and ends'
        sys.exit(1)

    return [
        loop 
            start in starts 
            and stop in stops 
            and editable in editables
            and prev_stop in [0 ...(stops.+1)]
        {
            //push the previous non-code block content
            if start >? prev_stop
                lines[prev_stop..start).join('\n')

            i = next(counter) // to give each iframe a unique id
            page = if editable 'demo_only' else 'src_only'
            code = lines(start..stop).join('\n')
            encoded_code = url_quote(code)
            
            // push the iframe replacement
            f'
                <iframe
                    src="https://david-andrew.github.io/iframes/dewy/{page}?src={encoded_code}&id=DewyIframe{i}"
                    style="width: 100%; border-radius: 0.5rem;"
                    id="DewyIframe{i}"
                    frameBorder="0"
                ></iframe>
            '
        }

        // push the last non-code block content
        if stops[-1]+1 <? lines.length
            lines[stops[-1]+1..].join('\n')
 
    ].join('\n')
}


//TODO: implement these functions
let parse_json = () => {}
let dump_json = () => {}
let url_quote = () => {}
let f = () => {}25:Tc05,// examples of syntax used in dewy
// line comments
/{ block/multiline comments }/

// typed declaration
apple: uint64
banana: map<int string>
peach: array<int length=N>  //array of ints with length N...
let pear: set<range>  // let indicates that this is definitely a new declaration, even if the identifier already exists



// unpack assignment examples
A = 1..10
B = [loop a in A -a]
loop [a b] in [A B] {}

// object with nested objects to unpack
my_obj = [
    apple = [1 2 3 4 [
        ultimate_answer = 42
    ]]
    banana = 10
    peach = [
        purple = 23
        blue = 12
        orange = 'orange'
    ]
]

// nested unpack assignment. tbd if the top level is `[unpack, params, etc] = obj`, or `obj as [unpack, params, etc]`
[
    [
        a1
        a2 
        a3 
        a4 
        [answer = ultimate_answer] = a5
    ] = apple
    renamed_banana = banana
    [purple blue orange] = peach
] = my_obj
// unpacked variables are:
//   a1 = 1, a2 = 2, a3 = 3, a4 = 4
//   answer = 42
//   renamed_banana = 10
//   purple = 23, blue = 12, orange = 'orange'

// unpacking dictionaries probably treats them as just the list of key -> value pairs
// unpacking sets, probably just treats the elements like a normal array
// unpacking ranges treats them as a normal array

// `...` can be used in unpack to coalesce extra elements for list-like containers
// there may only be 1 `...` in an unpack (otherwise it would be ambiguous which elements to collect)
// the variable receiving the `...` will be of the same type as the original object being unpacked
my_arr = [1 2 3 4 5 6 7 8 9]
[a1 a2 a3 ...my_arr a8 a9] = my_arr  //a1 = 1, a2 = 2, a3 = 3, my_arr = [4, 5, 6, 7], a8 = 8, a9 = 9

my_dict = ['apple' -> 1 'banana' -> 2 'peach' -> 3 'pie' -> 4]
[d1 ...dict_left] = my_dict //d1 = ('apple' -> 1), dict_left = ['banana' -> 2 'peach' -> 3 'pie' -> 4]

// random range note: for step sizes other than +1, use range_iter constructor e.g. range_iter(start to stop, step=5)
my_range = 1..inf
loop my_range.length >? 0 ( [i ...my_range] = my_range )
// first iteration: i = 1, my_range = 2..inf
// second iteration: i = 2, my_range = 3..inf
// third iteration: i = 3, my_range = 4..inf
// ...
// for forever

//[...my_range i] = my_range //will probably set my_range = 1 to inf, i = inf

// unpacking too many values, or named values that don't exist just sets them to undefined



// assignment expressions (i.e. python's walrus operator from https://www.python.org/dev/peps/pep-0572/)
// Handle a matched regex
if (match = pattern.search(data); match) not =? undefined
{
    // Do something with match
}

// A loop that can't be trivially rewritten using 2-arg iter()
loop (chunk = file.read(8192) chunk.length >? 0)
{
   process(chunk)
}

// Reuse a value that's expensive to compute
[y=f(x) y y**2 y**3]

// Share a subexpression between a comprehension filter clause and its output
filtered_data = [for x in data if (y=f(x) y) not =? undefined y]
//though you could also just write this like so
filtered_data = [for x in data {y=f(x) if y not =? undefined y}]
26:T1510,///////////////////// STRING INTERPOLATION /////////////////////

/{Todo: probably break each section for different syntaxes into different files?}/

//silly example with keyword vs identifier
loop i i

r'this is a raw string \'  expr  'a separate string later'

// simple blocks
{   }
( /{comment inside}/ )
{ 2+2 }
( 2+2 )


//string interpolation
my_string = '2 + 2 = {2+2}'

//complex interpolation
s = "first 10 primes are: {
    primes = [2]
    loop i in [3, 5..) and primes.length <? 10
        if i .% primes |> product not =? 0 
            primes.push(i)
    primes
}"


//alternative prime generator + getting first 10 primes
#ctx
primes = [
    2
    lazy i in [3, 5..)
        if i .% #ctx.primes .=? 0 |> @reduce(, (prev, v) => prev and v)
            i
][..10)

//TBD if there is a parallel way to do this where the i .% primes dispatches each operation, and fails immediately on any returning false
#label
primes = [
    2
    lazy i in [3, 5..)
        if not parallel_or(p => i % p =? 0, #label.primes)
            i
][..10)
//parallel or is like goroutines with cancel once any is true...should have it be more flexible, e.g. able to use any of the boolean keywords that can short circuit
//actually probably don't want to need to specify that it's parallel. Instead if there's an operation over a vector, it gets parallelized if possible.

//nested interpolation
s2 = 'this is an outer string, and {'this is an interior string with "{my_string}" in it'}'





const add = (a:int b:int): int => { /{return sum of a and b}/ }
let div = (a:real b:real): real? => { /{return a / b}/ }

// function type with named default argument
my_func = (s:string kwarg:bool=false): void => {}

//you probably can do the verbose version as well (probably useful for when you're just defining the interface without the implementation)
my_func: (s:string kwarg:bool=false): void



// example annotations for function types
() => ()
() => void
() => bool
int => bool
a: int => bool
(int int) => int
<T>(T T) => T
<T>(a:T b:T) => T

// object type
[a:int? b:string]

//? (optional) is sugar for |void
[a:int|void b:string]

// operators juxtaposed to identifiers
aorb
a or b
a+b


//based number literals
0b1010_0011_0101_0110_1001_1010_1100_1111
0B0101_1111_1010_1110_0011_0101_1001_1100

0t012_221_012_221_012_221_012_221
0t211_001_211_001_211_001_211_001

0q331_231_223_131_331_231_223_131
0Q123_321_123_321_123_321_123_321

0s123_450_123_450_123_450_123_450
0S543_210_543_210_543_210_543_210

0o123_456_701_234_567_012_345_670
0O012_345_670_123_456_701_234_567

0d123_456_789_012_345_678_901_234
0D987_654_321_098_765_432_109_876

0z123_456_789_xe0_123_456_789_xe0
0ZEX9_876_543_210_987_654_321_09E

0x1234_5678_9abc_def0_1234_5678_9abc_def0
0XFEDC_BA98_7654_3210_fedc_ba98_7654_3210

0u0123456789abcdefghijklmnopqrstuv0123456
0UVUTSRQPONMLKJIHGFEDCBA9876543210vutsrq

0r0123456789abcdefghijklmnopqrstuvwxyz012345
0RZYXWVUTSRQPONMLKJIHGFEDCBA9876543210zyxwv

0y0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!$
0Y$!ZYXWVUTSRQPONMLKJIHGFEDCBA9876543210zyxwvutsrqponmlkjihgfedcba

//Units TODO




[a b c] = [1 2 3]                                       //a=1, b=2, c=3
[a [b c]] = [1 [2 3]]                                   //a=1, b=2, c=3
[a [b c] d] = [1 [2 3] 4]                               //a=1, b=2, c=3, d=4
[a ...b] = [1 2 3 4]                                    //a=1, b=[2 3 4]
[a ...b c] = [1 2 3 4 5]                                //a=1, b=[2 3 4], c=5
[a ...b [c [...d e f]]] = [1 2 3 4 [5 [6 7 8 9 10]]]    //a=1, b=[2 3 4], c=5, d=[6 7 8], e=9, f=10



// silly things that are technically valid
x = loop i in [0..10] i //x=10

y = [
    if something 
        x
    else loop i in something_else
        y
    else if z
        z
    else loop i in last_thing
        w
    else
        ()
]


apple & banana
apple&banana
apple | banana
apple|banana





///////////// String prefixes ////////////////
p = s:string => [
    //process s based on / and \ separators
    //store result in this object
    route:array<string> = ...
    filename:string? = ...
    extension:string? = ...
]

p"this/is/a/file/path.ext"

//other prefixes
re"[^i*&2@]"                            // regex literal
t'my_token'                             //token literal. probably my version of enums
r'this is a raw string'                 //raw string. technically handled during tokenizing, there is no r function
(dewy)r'''printl("Hello, World!")'''    //dewy source code literal. uses raw string so that we don't have to worry about {}.

ipa"t vkavit dus aidam tam knggatsinskw akwaum aplavit maia t vidit dus kwd st bnum" //international phonetic alphabet literal
(apl)r"life  {1  . 3 4 = +/ + 1 0 1 . 1 0 1  }"  //apl expression literal
apl<|r"life  {1  . 3 4 = +/ + 1 0 1 . 1 0 1  }"  //same as above

'''this is a regular string with triple quotes'''
"""this is a regular string with triple quotes"""

///////////// object prefixes ////////////////
//doubly linked list
dll[1 2 3 4 6 5 3 6 3 2]
@linked_list(double=true)[1 2 3 4 6 5 3 6 3 2]

//set literal syntax
set[4 3 6 4 6 4 2 2 4 5]



// silly example for generating a list of ones
ones = n => {l = [...[1..n]] l.=1 l}
ones(10) // [1 1 1 1 1 1 1 1 1 1]
//alternate
ones = n => [loop i in 1..n 1]11:[["$","$13",null,{"fallback":null,"children":["$","$L14",null,{"children":["$","$L15",null,{"dewy_interpreter_source":[{"name":"src/__init__.py","code":""},{"name":"src/dtypes.py","code":"$16"},{"name":"src/frontend.py","code":"$17"},{"name":"src/parser.py","code":"$18"},{"name":"src/postok.py","code":"$19"},{"name":"src/postparse.py","code":"$1a"},{"name":"src/syntax.py","code":"$1b"},{"name":"src/tokenizer.py","code":"$1c"},{"name":"src/utils.py","code":"$1d"},{"name":"src/backend/__init__.py","code":"$1e"},{"name":"src/backend/arm.py","code":"from pathlib import Path\nfrom ..utils import Options\n\ndef arm_compiler(path: Path, args: list[str], options: Options) -> None:\n    raise NotImplementedError('ARM backend is not yet supported')\n"},{"name":"src/backend/c.py","code":"from pathlib import Path\nfrom ..utils import Options\n\ndef c_compiler(path: Path, args: list[str], options: Options) -> None:\n    raise NotImplementedError('C backend is not yet supported')\n"},{"name":"src/backend/llvm.py","code":"from pathlib import Path\nfrom ..utils import Options\n\ndef llvm_compiler(path: Path, args: list[str], options: Options) -> None:\n    raise NotImplementedError('LLVM backend is not yet supported')\n"},{"name":"src/backend/python.py","code":"$1f"},{"name":"src/backend/qbe/__init__.py","code":"from .qbe import qbe_compiler"},{"name":"src/backend/qbe/qbe.py","code":"$20"},{"name":"src/backend/riscv.py","code":"from pathlib import Path\nfrom ..utils import Options\n\ndef riscv_compiler(path: Path, args: list[str], options: Options) -> None:\n    raise NotImplementedError('RISC-V backend is not yet supported')\n"},{"name":"src/backend/shell.py","code":"from pathlib import Path\nfrom ..utils import Options\n\ndef shell_compiler(path: Path, args: list[str], options: Options) -> None:\n    \"\"\"this would target sh/powershell/etc. all simultaneously\"\"\"\n    # TODO: find the explanation of how this works\n    # https://en.wikipedia.org/wiki/Polyglot_(computing)\n    raise NotImplementedError('Shell backend is not yet supported')\n"},{"name":"src/backend/x86_64.py","code":"from pathlib import Path\nfrom ..utils import Options\n\ndef x86_64_compiler(path: Path, args: list[str], options: Options) -> None:\n    raise NotImplementedError('x86_64 backend is not yet supported')\n"}],"dewy_examples":{"good_examples":[{"name":"hello.dewy","code":"printl'Hello, World!'"},{"name":"hello_func.dewy","code":"main = () => printl'Hello, World!'\nmain"},{"name":"hello_name.dewy","code":"print\"What's your name? \"\nname = readl\nprintl'Hello {name}!'"},{"name":"hello_loop.dewy","code":"print\"What's your name? \"\nname = readl\ni = 0\nloop i <? 10 {\n    printl'Hello {name}!'\n    i = i + 1\n}"},{"name":"anonymous_func.dewy","code":"(() => printl'Hello from an anonymous function!')()"},{"name":"if_else.dewy","code":"print\"What's your name? \"\nname = readl\nif name =? 'Alice' printl'Hello Alice!'\nelse printl'Hello stranger!'"},{"name":"if_else_if.dewy","code":"print\"What's your name? \"\nname = readl\nif name =? 'Alice' printl'Hello Alice!'\nelse if name =? 'Bob' printl'Hello Bob!'\nelse printl'Hello stranger!'"},{"name":"dangling_else.dewy","code":"// if a then if b then s else s2\n// See: https://en.wikipedia.org/wiki/Dangling_else\n\na = false\nb = true\n\nif a\n    if b\n        printl's'\n    else\n        printl's2'\nelse\n    printl's3'"},{"name":"if_tree.dewy","code":"$21"},{"name":"loop_in_iter.dewy","code":"loop i in 0,2..20\n    printl(i)"},{"name":"loop_and_iters.dewy","code":"loop i in 0.. and j in (2..20)\n    printl'{i} and {j}'"},{"name":"enumerate_list.dewy","code":"// example enumerating a list\nfruits = ['apple' 'banana' 'peach' 'pear' 'pineapple']\nloop i in 0.. and fruit in fruits\n    printl'{i}: {fruit}'"},{"name":"loop_or_iters.dewy","code":"loop i in (0..20] or j in [0,2..10) \n    printl'{i} or {j}'"},{"name":"nested_loop.dewy","code":"loop i in 0,2..10\n    loop j in 0,2..10\n        printl'{i},{j}'"},{"name":"block_printing.dewy","code":"loop i in 0,2..5 {\n    loop j in 0,2..5 {\n        loop k in 0,2..5 {\n            loop l in 0,2..5 {\n                loop m in 0,2..5 {\n                    printl'{i},{j},{k},{l},{m}'\n                }\n            }\n        }\n    }\n}"},{"name":"row_vs_col.dewy","code":"unit = [1 2 3]\nrow = [1,2,3]\ncol = [[1] [2] [3]] // tbd if better way to make this, but generally not necessary since 1d arrays are treated as column vectors\nmat = [1,2,3 4,5,6 7,8,9]\ntensor = [(1,2,3),(4,5,6),(7,8,9) (10,11,12),(13,14,15),(16,17,18) (19,20,21),(22,23,24),(25,26,27)]\n\n// currently not supported\nmat2 = [\n    1 2 3\n    4 5 6\n    7 8 9\n]\n\n\nprintl(unit)\nprintl(row)\nprintl(col)\nprintl(mat)\nprintl(tensor)\nprintl(mat2)"},{"name":"objects.dewy","code":"obj = [\n    let a = 5\n    let b = 10\n    let fn = () => a + b\n    let fn2 = x => (a + b) * x\n]\n\nprintl(obj)\nprintl(obj.a)\nprintl(obj.b)\nprintl(obj.fn)\n//printl(obj.fn2(5)) // parse issue with . and call() currently causing ambiguity\n\n\nsomething_global = 42\n\nobj2 = [\n    let A = [\n        let x = 5\n        let y = 10\n        let fn = () => x + y\n    ]\n    let B = (X Y) => [\n        let x = X\n        let y = Y\n        let fn = () => x + y\n    ]\n    let C = A.fn + (B(5 10)).fn\n]\n\n\nprintl(obj2)\nprintl(obj2.A)\nprintl(obj2.A.x)\nprintl(obj2.A.y)\nprintl(obj2.A.fn)\nprintl(obj2.@B)\n//TODO: fix parse issue causing needing extra parenthesis/@ for disambiguation here\nprintl((obj2.@B)(5 10))\nprintl(((obj2.@B)(3 4)).x)\nprintl(((obj2.@B)(6 7)).y)\nprintl(((obj2.@B)(8 9)).fn)\nprintl(obj2.C)\n\n// can't access members not in the direct object scope\n//printl(obj.something_global)\n//printl(obj2.something_global)"},{"name":"unpack_array.dewy","code":"s = ['Hello' ['World' '!'] 5 10]\nprintl's={s}'\n\na, b, c, d = s\nprintl'a={a} b={b} c={c} d={d}'\n\na, ...b = s\nprintl'a={a} b={b}'\n\n...a, b = s\nprintl'a={a} b={b}'\n\na, [b c], ...d = s\nprintl'a={a} b={b} c={c} d={d}'\n\na, ...b, c, d, e = s\nprintl'a={a} b={b} c={c} d={d} e={e}'\n\n//error tests\n//a, b, c, d, e = s         //error: not enough values to unpack\n//a, b = s                  //error: too many values to unpack\n//a, ...b, c, d, e, f = s   //error: too many values to unpack\n"},{"name":"unpack_dict.dewy","code":"d1 = ['a' -> 1 'b' -> 2 'c' -> 3]\n\na, b, c = d1\nprintl'a={a} b={b} c={c}'\n\na, ...b = d1\nprintl'a={a} b={b}'\n\n...a, b = d1\nprintl'a={a} b={b}'\n\na, [b c], ...d = d1\nprintl'a={a} b={b} c={c} d={d}'\n\n\n\nd2 = ['a' <-> 1 'b' <-> 2 'c' <-> 3 'd' <-> ['e' -> 4 'f' -> 5]]\n\na, b, c, d = d2\nprintl'a={a} b={b} c={c} d={d}'\n\na, ...b, c, d, e = d2\nprintl'a={a} b={b} c={c} d={d} e={e}'\n\n[ka va], [kb vb], [kc vc], [kd [ke vf]] = d2\nprintl'ka={ka} va={va} kb={kb} vb={vb} kc={kc} vc={vc} kd={kd} ke={ke} vf={vf}'"},{"name":"functions.dewy","code":"let fn = (x y) => x + 5 + y\nprintl(fn(8 1))"},{"name":"partial_functions.dewy","code":"let add = (a b) => a + b\nlet add5 = @add(5)\nlet thirteen = @add5(8)\nlet add7 = @add(a=7)\nlet add10 = @add(b=10)\n\nprintl(add(3 5))\nprintl(add5(2))\nprintl(thirteen)\nprintl(add7(3))\nprintl(add10(3))\n\nlet fortytwo = @add10(32)\nlet fortythree = @add10(0 b=43)\nlet fortyfour = @add10(a=34)\nlet fortyfive = @add5(40)\nlet fortysix = @add5(a=6 40)\n\nprintl(fortytwo)\nprintl(fortythree)\nprintl(fortyfour)\nprintl(fortyfive)\nprintl(fortysix)"},{"name":"closure.dewy","code":"$22"},{"name":"function_signatures.dewy","code":"$23"},{"name":"opchains.dewy","code":"// in dewy you can combine arithmetic operators into an opchain\nx = 2\ny = 3\nz = 4\nprintl(+x)          // 0+x\nprintl(/x)          // 1/x\nprintl(/-x)         // 1/-x\nprintl(-/x)         // 0-1/x\nprintl(-x^2)        // 0-x^2\nprintl(/x^2)        // 1/x^2\nprintl(/x-2)        // 1/x-2\nprintl(-x/2)        // 0-x/2\nprintl(25-/2)       // 25-1/2\nprintl(25+-/2)      // 25+0-1/2\nprintl(2^/-+*32)    // 2^(1/(0-(0+(1*32))))\n\nprintl(x*+3)\nprintl(y/-4)\nprintl(z^/2)\n\nprintl(10^/-2)\n\nprintl(100^/2)\nprintl(5+-1)"},{"name":"fizzbuzz-1.dewy","code":"// fizbuzz that works with the current version of dewy\nmultiples = [3 5]\nwords = ['Fizz' 'Buzz']\nloop i in [0..100)\n{\n    printed_words = false\n    loop multiple in multiples and word in words\n    {\n        if i % multiple =? 0 \n        { \n            print(word)\n            printed_words = true\n        }\n    }\n    if not printed_words print(i)\n    printl()\n}"},{"name":"fizzbuzz0.dewy","code":"taps = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nloop i in [0..100)\n{\n    printed_words = false\n    loop [tap string] in taps \n    {\n        if i % tap =? 0 \n        { \n            print(string)\n            printed_words = true\n        }\n    }\n    if not printed_words print(i)\n    printl()\n}"},{"name":"random.dewy","code":"// simple xorshift+ generator\n// could be simplified when types are supported (i.e. `state: uint64` `rand = ():> uint64`, etc.)\n// for now have extra `and` operations to simulate truncation \n\nUINT64_MAX = 0xFFFF_FFFF_FFFF_FFFF\nstate = 123456789\nrand = () => {\n    state xor= state >> 21\n    state xor= state << 35\n    state xor= state >> 4\n    state and= UINT64_MAX\n    \n    state * 2_685821_657736_338717 and UINT64_MAX\n}\n\n\nsum = 0\nloop i in 1..1000 {\n    r = rand / UINT64_MAX\n    r |> printl\n    sum += r\n}\n\nprintl'avg = {sum / 1000}'"},{"name":"primes.dewy","code":"/{simple program for generating prime numbers}/\nprintl(2)\nprimes = [2]\nloop candidate in 3,5..100 {\n    no_factors = true\n    loop p in primes and p*p <? candidate+1 {\n        if candidate % p =? 0 {\n            no_factors = false\n            // break //not supported yet..\n        }\n    }\n    if no_factors {\n        printl(candidate)\n        primes = primes + [candidate]  //TODO: this behavior actually will probably be removed in favor of the below. add between arrays will be like vector addition\n        //primes = [primes... candidate]\n        //primes.push(i) // not supported yet.. ambiguous parse since jux has qint precedence, while . equals the higher precedence\n    }\n}"}],"bad_examples":[{"name":"tensors.dewy","code":"\ntensor1 = [(1,2,3),(4,5,6),(7,8,9) (10,11,12),(13,14,15),(16,17,18) (19,20,21),(22,23,24),(25,26,27)]\n\n\ntensor2 = [\n     1  2  3\n     4  5  6\n     7  8  9\n\n    10 11 12\n    13 14 15\n    16 17 18\n\n    19 20 21\n    22 23 24\n    25 26 27\n]\n\ntensor3 = [\n     1  2  3\n     4  5  6\n     7  8  9\n\n    10 11 12\n    13 14 15\n    16 17 18\n\n    19 20 21\n    22 23 24\n    25 26 27\n\n\n    28 29 30\n    31 32 33\n    34 35 36\n\n    37 38 39\n    40 41 42\n    43 44 45\n\n    46 47 48\n    49 50 51\n    52 53 54\n\n\n    55 56 57\n    58 59 60\n    61 62 63\n\n    64 65 66\n    67 68 69\n    70 71 72\n\n    73 74 75\n    76 77 78\n    79 80 81\n]\n\n// multiline expression means non-whitespace sensitive for outer array, but inner array is whitespace sensitive\ntensor4 = [\n    loop i in 0..3 9i .+\n    [\n        1 2 3\n        4 5 6\n        7 8 9\n    ]\n]\n\n\nprintl(tensor1)\nprintl(tensor2)\nprintl(tensor3)\nprintl(tensor4)"},{"name":"arrays.dewy","code":"arr = [0 1 2 3 4 5 6 7 8 9]\nb = 4 x = 10\narr[2] |> printl\narr[b] |> printl\narr[[2 3 4]] |> printl\narr(2..5) |> printl"},{"name":"unpack_object.dewy","code":"o1 = [a='Hello' b=['World' '!'] c=5 d=10]\n\na, b, c, d = o1\nprintl'a={a} b={b} c={c} d={d}'\n\nb, c = o1\nprintl'b={b} c={c}'\n\na, ...rest = o1\nprintl'a={a} rest={rest}'\n\nd, c, ...rest, b = o1\nprintl'd={d} c={c} rest={rest} b={b}'\n\n[a [b1 b2]=b] = o1\nprintl'a={a} b1={b1} b2={b2}'"},{"name":"declare.dewy","code":"//some examples of declarations\nlet x\nlet y = 10\nlet z: int = 100 + 1000\nlet w: SomeType<a=10 b=20> = 15\nlet v = (a:int b:int):> int => a + b + 10\n\nconst a\nconst b = '{1000/10}'\nconst c: int = { 10000 }\n\nlocal_const \nlocal_const  = 100(100)\nlocal_const : int = 1_000_000\n\nfixed_type A\nfixed_type B = 0x1000\nfixed_type C: int = 0b1000\n"},{"name":"loop_iter_manual.dewy","code":"it = [0,2..10].iter\n[cond i] = it.next\nloop cond {\n    printl(i)\n    [cond i] = it.next\n}"},{"name":"range_iter_test.dewy","code":"r = 0,2..20\nit = r.iter\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next) //last iteration. should return [true, 20]\nprintl(it.next) //should return [false, undefined]\nprintl(it.next)\nprintl(it.next)"},{"name":"ops.dewy","code":"// regular opchains\n2^/-3 |> printl\n\n//// broadcast ops\n[1 2 3] .* 4 |> printl\n4 ./ [1 2 3] |> printl\n[1 2 3] .^ [4 5 6] |> printl\n[[1 2 3]] .^ [[4] [5] [6]] |> printl\n\n\n// assignment ops\na = 2\na ^= 3\na |> printl\n\n// broadcast opchains\n[1 2 3] .^/-2 |> printl\n2 .^/-[1 2 3] |> printl\n\n// assignment opchains\na = 2\na ^/-= 3\na |> printl\n\n// broadcast assignment\nb = [1 2 3]\nb .^= 4\nb |> printl\nc = 4\nc ./= [1 2 3]\nc |> printl\n\n// broadcast assignment opchains\nd = [1 2 3]\nd .^/-= 2\nd |> printl\n"},{"name":"shebang.dewy","code":"#!//home/david/dev/dewy-lang/dewy\n\n//shows how shebangs can be syntactically valid as a hashtag `#!` followed by a comment\nprintl'this code was invoked as an executable script with a shebang line!'"},{"name":"fizzbuzz1.dewy","code":"taps = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nrange = [0..100)\n\n//indexing at [new ..] and [.. new] adds singleton dimensions wherever there is new\nword_bools = range[new ..] .% taps.keys[.. new] .=? 0\n\n// ` means transpose, which behaves like python's zip()\nwords_grid = [taps.values word_bools]`.map(\n    [word bools] => bools.map(b => if b word else '')\n)\n\nraw_lines = word_grid`.map(line_words => line_words.join(''))\n\nlines = [raw_lines range]`.map(\n    (raw_line i) => if raw_line.length =? 0 '{i}' else raw_line\n)\n\nlines.join'\\n' |> printl\n"},{"name":"primes2.dewy","code":"// generating primes with more advanced features\n#ctx\nprimes = [\n    2\n    loop i in [3, 5..)\n        if i .% #ctx.primes .=? 0 |> @any |> @not\n            i\n][..10)"},{"name":"mdbook_preprocessor.dewy","code":"$24"},{"name":"random.dewy","code":"// simple xorshift+ generator\n// could be simplified when types are supported (i.e. `state: uint64` `rand = ():> uint64`, etc.)\n// for now have extra `and` operations to simulate truncation \n\nUINT64_MAX = 0xFFFF_FFFF_FFFF_FFFF\nstate = 123456789\nrand = () => {\n    state xor= state >> 21\n    state xor= state << 35\n    state xor= state >> 4\n    state and= UINT64_MAX\n    \n    state * 2_685821_657736_338717 and UINT64_MAX\n}\n\n\nsum = 0\nloop i in 1..1000 {\n    r = rand / UINT64_MAX\n    r |> printl\n    sum += r\n}\n\nprintl'avg = {sum / 1000}'"},{"name":"fast_inverse_sqrt.dewy","code":"\n\n// fast inverse square-root. see: https://en.wikipedia.org/wiki/Fast_inverse_square_root#Overview_of_the_code\nfast_isqrt = (x:f32) => {\n    let y:f32, i:u32\n    \n    i = 0.5x transmute u32      // evil floating point bit level hacking\n    i = 0x5f3759df - (i >> 1)   // what the fuck?\n    y = i transmute f32\n    y *= 1.5 - (0.5x)y^2        // 1st iteration of newton's method\n    //y *= 1.5 - (0.5x)y^2      // 2nd iteration (optional)\n\n    return y\n}\n\n\n//TODO: use autodiff to calculate the derivative automatically?\n//isqrt = (x:number) => 1/x^0.5\n//diff(isqrt)"},{"name":"rule110.dewy","code":"// proof that dewy is turing complete\n// rule 110 would grow the vector from the front, so instead we reverse everything for efficiency\n// for now use parenthesis where precedence filter needed. eventually should be able to remove with precedence filter\n\nprogress = world:vector<bit> => {\n    update:bit = 0\n    loop i in 0..world.length\n    {\n        if i >? 0 world[i-1] = update //TODO: #notfirst handled by compiler unrolling the loop into prelude, interludes, and postlude\n        update = 0b01110110 << (world[i-1..i+1] .?? 0 .<< [2 1 0])\n    }\n    world.push(update)\n}\n\nworld: vector<bit> = [1]\nloop true\n{\n    printl(world)\n    progress(world)\n}"},{"name":"dewy_syntax_examples.dewy","code":"$25"},{"name":"syntax.dewy","code":"$26"},{"name":"tokenizer.dewy","code":"//demo of manual dewy tokenizer written in dewy\n\n// (template) instance of the token class\nTokenBase = [\n    name = 'Token'\n    __repr__ = () => '<{name}>'\n]\nToken = type(TokenBase)\n\n\n// class constructor for Keyword token type\nKeyword = src:string => [\n    ...TokenBase\n    name = 'Keyword'\n    __repr__ = () => '<{name}: {src}>'\n]\n\n\neat_fn_type = src:string :> int?\n\n\n/{\n    Eat a reserved keyword, return the number of characters eaten\n\n    #keyword = {in} | {as} | {loop} | {lazy} | {if} | {and} | {or} | {xor} | {nand} | {nor} | {xnor} | {not}; \n\n    noting that keywords are case insensitive\n}/\neat_keyword = src:string :> int? => {\n    keywords = ['in' 'as' 'loop' 'lazy' 'if' 'and' 'or' 'xor' 'nand' 'nor' 'xnor' 'not']\n    max_len = [loop k in keywords k.length].max\n    lower_src = src[..max_len].lowercase\n    loop k in keywords\n        if lower_src.startswith(k)\n            return k.length\n    return undefined\n}\n"}]}}]}]}],["$","h3",null,{"className":"text-2xl mt-6 mb-2 font-quadon","children":"About"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"Dewy is a programming language I have been developing off and on since 2016. It is a general purpose language, designed with engineering applications in mind. Think the functionality and ease of use of matlab or python combined with the speed of a compiled language like C or Rust, but with its own unique flare."}],["$","p",null,{"className":"text-xl font-gentona text-justify mb-2","children":"Some key planned features include:"}],["$","ul",null,{"className":"list-disc mb-6 pl-10 text-xl font-gentona","children":[["$","li",null,{"children":[["$","strong",null,{"children":"Functional and Imperative"}]," - Dewy is an imperative language with strong support for functional programming. This allows for a very flexible programming style, where you can use the best tool for the job."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Expression based syntax"}]," - Dewy uses an expression based syntax, meaning that everything is an expression. This allows for a very simple yet powerful syntax, where common language features often are just a free consequence of the syntax"]}],["$","li",null,{"children":[["$","strong",null,{"children":"Garbage-collector-free memory management"}]," - Dewy uses a unique memory management system, allowing for fast and efficient memory management without the need for a garbage collector."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Strong type system"}]," - Dewy has a powerful static type system with inference, reminiscent of those in Typescript and Julia."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Built in unit system"}]," - Dewy has a built in unit system, allowing you to easily work with units and convert between them. This is especially useful for engineering applications."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Strong math support"}]," - Dewy has strong support many math features, including complex numbers, quaternions, vectors, matrices, and more. This is especially useful for engineering applications."]}]]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"An example of the common FizzBuzz program implemented in Dewy might look like this:"}],["$","$L27",null,{"src":"multiples = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nloop i in [0..100)\n{\n    printed_words = false\n    loop [multiple word] in multiples \n    {\n        if i % multiple =? 0 \n        { \n            print(multiple)\n            printed_words = true\n        }\n    }\n    if not printed_words print(i)\n    printl()\n}"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"Or a more functional style implementation might look like this:"}],["$","$L27",null,{"src":"multiples = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nrange = [0..100)\n\n//indexing at [new ..] and [.. new] adds singleton dimensions wherever there is new\nword_bools = range[new ..] .% multiples.keys[.. new] .=? 0\n\n// ` means transpose, which behaves like python's zip()\nword_grid = [multiples.values word_bools]`.map(\n    [word bools] => bools.map(b => if b word else '')\n)\n\nraw_lines = word_grid`.map(line_words => line_words.join(''))\n\nlines = [raw_lines range]`.map(\n    (raw_line i) => if raw_line.length =? 0 '{i}' else raw_line\n)\n\nlines.join'\\n' |> printl"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"For clarity, the variables at each step look like so:"}],["$","$L27",null,{"src":"word_bools = [[true false false true false false true false ...]\n             [true false false false false true false false ...]]\n\nword_grid = [['Fizz' '' '' 'Fizz' '' '' 'Fizz' '' '' 'Fizz' '' '' ...]\n            ['Buzz' '' '' '' '' 'Buzz' '' '' '' '' 'Buzz' '' '' ...]]\n\nraw_lines = ['FizzBuzz' '' '' 'Fizz' '' 'Buzz' 'Fizz' '' '' 'Fizz' 'Buzz' '' ...]\n\nlines = ['FizzBuzz' '1' '2' 'Fizz' '4' 'Buzz' 'Fizz' '7' '8' 'Fizz' 'Buzz' '11' ...]"}],["$","h3",null,{"className":"text-2xl mt-6 mb-2 font-quadon","children":"Current Status"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"Currently I'm working through a simple interpreter for the language (powering the demo above). I've got a tokenizer, a basic interpreter backend, and handling of a few basic types of syntaxes via the parser. But much of the syntax is still unimplemented, hence the long list of \"Broken Examples\". Thus the current focus is finishing parser support for the rest of the syntax features."}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["Previously I had been doing a lot of development on bleeding edge"," ",["$","$L28",null,{"href":"/projects/dewy_old","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"parser generators"}],", but that ended up being too big of a time sink for not much visible progress. Instead, for the time being, I ended up just hand rolling a parser in python, which has led to actually runnable code! I'll definitely revisit parser generators in the future when the language is further along."]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["After the parser is complete, the next steps will be working on compiling to different backends. Initially I was planning to target"," ",["$","$L28",null,{"href":"https://en.wikipedia.org/wiki/LLVM#Intermediate_representation","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"LLVM"}]," as the primary backend, however I recently discovered ",["$","$L28",null,{"href":"https://c9x.me/compile/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"QBE"}],", which is a much lighter alternative that supposedly gets 70% of the performance of LLVM for only 10% of the code. Longer term I'm interested in supporting a wider range of backend targets, like C, a universal"," ",["$","$L28",null,{"href":"https://en.wikipedia.org/wiki/Polyglot_(computing)","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"polyglot"}]," targeting many scripting languages simultaneously (sh, bash, windows cmd, powershell, javascript, python, etc.), and eventually LLVM too. At some point I'll start building out the standard library, and bootstrapping the compiler to be able to compile itselfat which point we might be ready for a version 0 release!"]}],["$","h3",null,{"className":"text-2xl mt-6 mb-2 font-quadon","children":"About the Demo"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["The demo above was actually pretty complex to put together. The current interpreter is written in python, and this website is statically hosted, which meant the demo required some way to statically run python code without a server. For this, I used ",["$","$L28",null,{"href":"https://pyodide.org/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Pyodide"}],", which is basically ",["$","$L28",null,{"href":"https://github.com/python/cpython","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"CPython"}]," compiled to"," ",["$","$L28",null,{"href":"https://webassembly.org/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"WebAssembly"}]," via"," ",["$","$L28",null,{"href":"https://emscripten.org/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Emscripten."}]]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["Pyodide itself isn't too difficult to use, except for the fact that it doesn't have good support for asynchronous standard inputit really wants to halt the entire UI while you type input into a stock browser popup prompt. To get around this, I found a"," ",["$","$L28",null,{"href":"https://www.npmjs.com/package/sync-message","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"handy library"}]," where you run pyodide in a web worker, and then any time it wants to read input, the worker makes a synchronous XHR request to a service worker, blocking the pyodide web worker until the service worker receives a response from the main thread with the input, which the service worker can then pass back to the pyodide worker. Suffice it to say, I don't think I ever want to deal with service workers again."]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["Now that python is handled, the next aspect is getting the Dewy interpreter itself to run. For this, I fetch (at website build time) the source code directly from"," ",["$","$L28",null,{"href":"https://github.com/david-andrew/dewy-lang/tree/master/src/compiler","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"github"}],". I then abuse the python import lib to allow loading \"modules\" directly from strings, and then pass all of the dewy source in as string modules. Then I have a little wrapper function for the entry point which receives a dewy source code string, and runs the program. The entry point can then be called from the browser via a javascript wrapper function."]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["The final piece of the puzzle is the text entry, and terminal emulator. For text input, I'm using the ",["$","$L28",null,{"href":"https://codemirror.net/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Code Mirror Library"}]," with a custom syntax highlighter. For the terminal, I use the ",["$","$L28",null,{"href":"https://xtermjs.org/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"xterm.js"}]," library. I then hooked up stdin and stdout from pyodide to interact with the terminal, and voila! A Dewy interpreter running in the browser."]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"There are definitely some rough edges, and the parser only supports a small handful of features, but it runs! It's probably the easiest way to try out the language, and I'm looking forward to getting all of the broken example programs working!"}],["$","h3",null,{"className":"text-2xl mt-6 mb-2 font-quadon","children":"Links"}],["$","div",null,{"className":"flex flex-col space-y-3","children":[["$","div",null,{"className":"flex flex-row text-sm items-center","children":[["$","$L29",null,{"src":{"src":"/_next/static/media/github.8d24eda3.svg","height":484,"width":496,"blurWidth":0,"blurHeight":0},"alt":"github icon","className":"inline-block w-8 h-8 mr-2  pointer-events-none select-none","draggable":false}],["$","span",null,{"className":"align-middle","children":["$","$L28",null,{"href":"https://github.com/david-andrew/dewy-lang","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Github Repo"}]}]]}],["$","div",null,{"className":"flex flex-row text-sm items-center","children":[["$","$L29",null,{"src":{"src":"/_next/static/media/docs.71257e0f.svg","height":512,"width":384,"blurWidth":0,"blurHeight":0},"alt":"docs icon","className":"inline-block w-8 h-8 mr-2  pointer-events-none select-none","draggable":false}],["$","span",null,{"className":"align-middle","children":["$","$L28",null,{"href":"https://david-andrew.github.io/dewy-lang/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Language Docs"}]}]]}]]}]]
