1:HL["/_next/static/media/a34f9d1faa5f3315-s.p.woff2",{"as":"font","type":"font/woff2"}]
2:HL["/_next/static/css/91c4e337ea558be1.css",{"as":"style"}]
0:["4CDu21Pcj81s8XPY8ygmJ",[[["",{"children":["projects",{"children":["dewy",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],"$L3",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/91c4e337ea558be1.css","precedence":"next"}]],"$L4"]]]]
5:HL["/_next/static/css/85fa6dafca566008.css",{"as":"style"}]
3:[null,"$L6",null]
4:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"David Samson"}],["$","meta","2",{"name":"description","content":"Generated by create next app"}],["$","meta","3",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","link","4",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","meta","5",{"name":"next-size-adjust"}]]
7:I{"id":27250,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","5570:static/chunks/app/projects/layout-d40e5a49d71535d2.js"],"name":"Navbar","async":false}
8:I{"id":19885,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","5570:static/chunks/app/projects/layout-d40e5a49d71535d2.js"],"name":"GithubTimestampsProvider","async":false}
9:I{"id":19885,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","5570:static/chunks/app/projects/layout-d40e5a49d71535d2.js"],"name":"ProjectsContextProvider","async":false}
a:I{"id":47767,"chunks":["2272:static/chunks/webpack-2a17145d963940e1.js","2971:static/chunks/fd9d1056-bc91c3b3fa0b78fd.js","596:static/chunks/596-fc24fa4abd14b1c2.js"],"name":"default","async":false}
b:I{"id":57920,"chunks":["2272:static/chunks/webpack-2a17145d963940e1.js","2971:static/chunks/fd9d1056-bc91c3b3fa0b78fd.js","596:static/chunks/596-fc24fa4abd14b1c2.js"],"name":"default","async":false}
d:I{"id":49488,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","8861:static/chunks/8861-ed8850a5f8092c40.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","3185:static/chunks/app/layout-d7bf00f983e66157.js"],"name":"ColorPicker","async":false}
6:["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","script",null,{"src":"/pyodideCommsService.js","async":true}]}],["$","body",null,{"className":"__className_36bd41 overflow-hidden bg-slate-900","children":["$","div",null,{"className":"w-screen h-screen bg-black overflow-hidden","children":[["$","$L7",null,{}],["$","div",null,{"style":{"height":"calc(100vh - var(--navbar-height))"},"className":"overflow-x-hidden","children":["$","$L8",null,{"children":["$","$L9",null,{"children":["$","$La",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":["$","div",null,{"className":"fixed w-screen h-screen bg-black flex justify-center items-center","children":["$","div",null,{"role":"status","children":[["$","svg",null,{"aria-hidden":"true","className":"w-32 h-32 mr-2 text-gray-200 animate-spin dark:text-gray-600 fill-accent","viewBox":"0 0 100 101","fill":"none","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M100 50.5908C100 78.2051 77.6142 100.591 50 100.591C22.3858 100.591 0 78.2051 0 50.5908C0 22.9766 22.3858 0.59082 50 0.59082C77.6142 0.59082 100 22.9766 100 50.5908ZM9.08144 50.5908C9.08144 73.1895 27.4013 91.5094 50 91.5094C72.5987 91.5094 90.9186 73.1895 90.9186 50.5908C90.9186 27.9921 72.5987 9.67226 50 9.67226C27.4013 9.67226 9.08144 27.9921 9.08144 50.5908Z","fill":"currentColor"}],["$","path",null,{"d":"M93.9676 39.0409C96.393 38.4038 97.8624 35.9116 97.0079 33.5539C95.2932 28.8227 92.871 24.3692 89.8167 20.348C85.8452 15.1192 80.8826 10.7238 75.2124 7.41289C69.5422 4.10194 63.2754 1.94025 56.7698 1.05124C51.7666 0.367541 46.6976 0.446843 41.7345 1.27873C39.2613 1.69328 37.813 4.19778 38.4501 6.62326C39.0873 9.04874 41.5694 10.4717 44.0505 10.1071C47.8511 9.54855 51.7191 9.52689 55.5402 10.0491C60.8642 10.7766 65.9928 12.5457 70.6331 15.2552C75.2735 17.9648 79.3347 21.5619 82.5849 25.841C84.9175 28.9121 86.7997 32.2913 88.1811 35.8758C89.083 38.2158 91.5421 39.6781 93.9676 39.0409Z","fill":"currentFill"}]]}],["$","span",null,{"className":"sr-only","children":"Loading..."}]]}]}],"loadingStyles":[],"hasLoading":true,"error":"$undefined","errorStyles":"$undefined","template":["$","$Lb",null,{}],"templateStyles":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"childProp":{"current":[null,"$Lc",null],"segment":"projects"},"styles":[]}]}]}]}],["$","$Ld",null,{}]]}]}]]}]
e:I{"id":19885,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","5570:static/chunks/app/projects/layout-d40e5a49d71535d2.js"],"name":"GithubTimestampsFetcher","async":false}
f:I{"id":52160,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","5570:static/chunks/app/projects/layout-d40e5a49d71535d2.js"],"name":"Heading","async":false}
12:I{"id":27250,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","4809:static/chunks/4809-4864fcdd4c57b3e2.js","5570:static/chunks/app/projects/layout-d40e5a49d71535d2.js"],"name":"NavbarDummy","async":false}
c:[["$","$Le",null,{"projects":[{"title":"Blob Opera Performances","imgSrc":{"src":"/_next/static/media/blob_opera_nox.e6f3aa2a.png","height":414,"width":512,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAJ1BMVEU4EgowEQlEIRV0Vzx9ZEo+JhlXOyBCT1RTMx0+LyVEUlxuTS5AS1HBZ0aTAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALklEQVR4nC3IuREAIAwDMMcOeYD95+XgKNQIhpahHYwUawuGoQuM6az0OwvPnwMUSwDO9qMceAAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":6},"summary":"Virtual choir performances leveraging the blob opera as a front end for voice synthesis","lastUpdated":"February 2021","tags":["Python","Blob Opera","choir","music","synthesis"],"route":"blob_opera"},{"title":"Boat Simulator","imgSrc":{"src":"/_next/static/media/boat_simulator.7cc391fd.jpg","height":1280,"width":2048,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAb/xAAdEAABAgcAAAAAAAAAAAAAAAAAAQQCAxESFSFT/8QAFAEBAAAAAAAAAAAAAAAAAAAABf/EABYRAAMAAAAAAAAAAAAAAAAAAAABEf/aAAwDAQACEQMRAD8AkFks7K49tuLmABWIOrP/2Q==","blurWidth":8,"blurHeight":5},"summary":"Spring 2017 HopHacks submission","lastUpdated":"March 2017","tags":["Unity","C#","3D game"],"route":"boat_simulator"},{"title":"Bueller Board","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Fall 2015 HopHacks submission: Midi keyboard that used user provided audio samples, a.k.a. the 'goat keyboard'","lastUpdated":"September 2015","tags":["midi","music"],"route":"bueller_board"},{"title":"Composer","imgSrc":{"src":"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":8},"summary":"React based composing software that acts as a front-end for LilyPond","lastUpdated":"January 2021","tags":["React","TypeScript","SMuFL","LilyPond","music","composition"],"route":"composer"},{"title":"Choir Compositions","imgSrc":{"src":"/_next/static/media/music_staff.3145785a.png","height":1616,"width":2745,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAHlBMVEUGBgZISEhSUlIeHh5paWk+Pj4vLy9cXFx7e3ufn5/bdycMAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAJ0lEQVR4nCXBtwEAIAwDMLe0/x9mQIKFjwN3jZBwxGowpFZGm7cpPAkxAH3f0FE4AAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":5},"summary":"","lastUpdated":"May 2015","tags":["music","choral","composition"],"route":"compositions"},{"title":"Dewy Programming Language","github":"dewy","imgSrc":{"src":"/_next/static/media/dewy_dandelion.e2efa7ee.jpg","height":1600,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z","blurWidth":8,"blurHeight":8},"summary":"An engineering focused programming language I am developing","tags":["Python","compilers","parsers","LLVM","Programming Languages"],"route":"dewy"},{"title":"Generalized Parsing","lastUpdated":"2022-02-06","imgSrc":{"src":"/_next/static/media/dewy_dandelion.e2efa7ee.jpg","height":1600,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z","blurWidth":8,"blurHeight":8},"summary":"Previous work on the Dewy Programming Language, namely a custom SRNGLR parser written entirely in C","tags":["C","compilers","parsers","SRNGLR","LLVM"],"route":"dewy_old"},{"title":"UR5 Draw Robot","imgSrc":{"src":"/_next/static/media/mona_lisa_contour.9bc5cbc9.jpg","height":1016,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAeEAACAgEFAQAAAAAAAAAAAAABAgAEAwUHEiFRsf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAExAv/aAAwDAQACEQMRAD8AtbT3HvV9WGZVPB8RHXqsT8iIgoVqn//Z","blurWidth":8,"blurHeight":5},"summary":"UR5 robot arm project","lastUpdated":"December 2017","tags":["Matlab","UR5 robot","ROS"],"route":"drawbot"},{"title":"EasyREPL","imgSrc":{"src":"/_next/static/media/pypi_logo.f6536a80.svg","height":58,"width":66,"blurWidth":0,"blurHeight":0},"summary":"Python package for easily creating Read-Eval-Print Loops (REPLs)","github":"easyrepl","tags":["Python","PyPI","REPL"],"route":"easyrepl"},{"title":"Hacking Harmony or The Demon Chipmunk Choir","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"Ensemble","summary":"2019 Peabody Hackathon Submission. Choral music synthesis via autotuned google text-to-speech, AKA the demon chipmunk choir","tags":["Google text-to-speech API","matlab","python"],"route":"ensemble_peabody"},{"title":"Escort Mission 2020","imgSrc":{"src":"/_next/static/media/escort_mission_lamb.4c525bc4.png","height":128,"width":128,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAHlBMVEUA/gAAugD/+//h9+EAdAAirCKG/YZf2l9ri2uA/4BNpwkIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAK0lEQVR4nE2KuREAMAyDbPnff+HcSU1ooMDsI1P2XWdURDEOOMYAo6dbj3gNJgBhtLya0gAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":8},"github":"escort_mission_2020","summary":"Submission for the 2020 GMTK Game Jam","tags":["Godot","GDScript","2D game"],"route":"escort_mission"},{"title":"Foxing Animatronic","imgSrc":{"src":"/_next/static/media/foxing_animatronic.91d20002.png","height":1252,"width":1540,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAJ1BMVEUGFwsJIxEILhUiJQkdSRwWIQoUORYULA8RRSArMAYZEwYyVzM6OglI+AgfAAAACXBIWXMAAAsTAAALEwEAmpwYAAAANElEQVR4nBXKuREAIAwEsT2/GOi/XgbFoqs9I6DvdY8CW7O9DBSzvQ3OKFOCE5Vp+mdJBg8iegD5GB/ChgAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":7},"summary":"Manually actuated animatronic robot featured in the Foxing music video 'Slapstick'","lastUpdated":"June 2018","tags":["Solidworks","mechanical design","animatronic","Foxing","music"],"route":"foxing_animatronic"},{"title":"Mechatronics Robots","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Robots from mechatronics","lastUpdated":"May 2019","tags":["Arduino","C++","SolidWorks","mechanical design"],"route":"mechatronics"},{"title":"Mehve (Working Title)","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"mehve","summary":"3D adventure game inspire by \"Nausicaa of the Valley of the Wind\"","tags":["Godot","GDScript","3D game"],"route":"mehve"},{"title":"Musical DL","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"MusicalDL","summary":"Using deep learning to generate choral music in the style of JS Bach","tags":["Python","Pytorch","AI/ML","choral","music","generation"],"route":"musical_dl"},{"title":"NAND 2 Tetris","imgSrc":{"src":"/_next/static/media/nand2tetris.660feb3d.png","height":346,"width":396,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAPFBMVEUAAABvKgIhHhw+KR0uJAFlRBU2GAYJBAENDw/TVQF/ZRBuWwrEoxytlRBiVBJJHQC7SwG5YCWSOwDqziPUksrRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAN0lEQVR4nCWKWw7AIAyA0Fbbqnvo7n/XZZMfEgKAscllu870J7tj2Tk6NSK151BcLilNwd3z974dVQEdrZJh0wAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":7},"github":"nand2tetris","summary":"A 16-bit computer built from scratch, starting with NAND gates","tags":["Computer Architecture","HDL","Assembly","Hack","Jack","Compilers","Operating Systems","Virtual Machines"],"route":"nand2tetris"},{"title":"PDF Chatter","imgSrc":{"src":"/_next/static/media/pypi_logo.f6536a80.svg","height":58,"width":66,"blurWidth":0,"blurHeight":0},"summary":"LLM powered Q&A over extracted PDF text","github":"pdf-chatter","tags":["Python","Optical Character Recognition (OCR)","Large Language Models (LLMs)","Nougat-OCR","GPT-4"],"route":"pdf_chatter"},{"title":"pOngBot","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Autonomous beer pong playing robot","lastUpdated":"June 2020","tags":["Arduino","C++","computer vision","Viola-Jones","mechanical design"],"route":"pongbot"},{"title":"PRS19: Fret Press Robot","imgSrc":{"src":"/_next/static/media/prs2019_preview.aed05a2a.png","height":2093,"width":3061,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAPFBMVEVJPTmhmIiCa1SBfXdBQD1IOzB3bGA6My5dUkyXlpNPRTyio6ChpadSTUm0q5e+taCun2xwYl2qrq5mXU/I9hK/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAL0lEQVR4nAXBhwEAIAjAsKoo4B7//2rCXFZLjoGwh4poIj5TJRfOsAok3G/v3toHGpkBOY6KaOIAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":5},"github":"PRS_robot","summary":"Automatic guitar fret press robot. Mechanical Engineering Master's Design captsone project","tags":["C++","Arduino","mechanical design"],"route":"prs19"},{"title":"Rewind","imgSrc":{"src":"/_next/static/media/rewind_title.a1e43a09.png","height":540,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAElBMVEUqHjguITs4J0JmWW1PRFo/MUqO6aYhAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAH0lEQVR4nGNggAMmJkYmRhCDhZmFmZUBxGRkgIhAAQADKgAhbD4/MgAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":5},"summary":"2018 Video Game Desgn (EN.601.355) capstone project","lastUpdated":"May 2018","tags":["Unity","C#","2D game"],"route":"rewind"},{"title":"RoboJay","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"lastUpdated":"May 2018","summary":"A balancing robot designed to give campus tours to incoming JHU freshmen","tags":["robotics","feedback control","navigation","BeagleBone","ROS"],"route":"robojay"},{"title":"High Power Rocketry","imgSrc":{"src":"/_next/static/media/rebel_scum.34e7823c.jpg","height":1365,"width":2048,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAaEAACAgMAAAAAAAAAAAAAAAAAARESIjFB/8QAFQEBAQAAAAAAAAAAAAAAAAAAAgP/xAAXEQADAQAAAAAAAAAAAAAAAAAAAhNR/9oADAMBAAIRAxEAPwC6o4w5OwATo+immH//2Q==","blurWidth":8,"blurHeight":5},"summary":"Level 1 & 2 High Powered Rocket built with the Johns Hopkins Rocketry Club, and Spaceport America Cup 2018","lastUpdated":"January 2018","tags":["High Power Rocketry","Arduino","C++","mechanical design","Tripoli"],"route":"rocketry"},{"title":"Silver Void","imgSrc":{"src":"/_next/static/media/silver_void_cover_art.cc703812.png","height":1439,"width":1823,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAIVBMVEUCAgEWGhAMDQgiFiEnJyYwMC4aCRZHJygZDgQ9EDxLU0cSomUyAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALUlEQVR4nC3KyQkAMBDDQHntzdV/wSEQ/QaEYkeAutPhJ0NBFdv0kWDM57euCwrsAGo10HJLAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":6},"github":"SilverVoid","summary":"Submission for Acerola Jam 0","tags":["Godot","GDScript","3D Game","Space Simulator","Bullet Hell"],"route":"silver_void"},{"title":"so voice!","imgSrc":{"src":"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":8},"lastUpdated":"December 2022","summary":"Choral music synthesis with deep learning (Continuation of Musical DL)","tags":["Python","Pytorch","AI/ML","choral","music","synthesis"],"route":"so_voice"},{"title":"Terminal Ray Tracer","imgSrc":{"src":"/_next/static/media/terminal_ray_tracer.00db7e4a.png","height":2043,"width":2881,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAbFBMVEWSamvNrLWWP0yUKVeYinmmnKTsEheTcnnmh4+Xfo6pV2GMMDasvsa5aXiPRVFVs2uOWF9WnGxXtVObh4ypREzbYmnsPkXIba/y/P10bHmfbcouPtDAMDdJbZOtmERQbkXfwcqTb3g+yNdOcdnEQLLCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAOklEQVR4nAXBBQKAIAAAsSMl7MZG//9HN/Igu6YtYDT+7qnhSK++tI+IcH5BViXZqn0Tk8M8Ti3rbH9KhwLIHy1s0QAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":6},"github":"TerminalRayTracer","summary":"A dependency-free ray tracer written in C that runs directly in a linux terminal","tags":["C","ray tracing","CLI","linux"],"route":"terminal_ray_tracer"},{"title":"Cloud Timelapse","imgSrc":{"src":"/_next/static/media/timelapse.b57dd258.jpg","height":3468,"width":4624,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAGAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAcEAACAgIDAAAAAAAAAAAAAAAAAQIFAxETYdH/xAAVAQEBAAAAAAAAAAAAAAAAAAADBP/EABYRAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAwDAQACEQMRAD8AlG4r56fBlT6ivQAUqwYX/9k=","blurWidth":8,"blurHeight":6},"github":"timelapse","summary":"A simple python project for taking timelapses of clouds from a webcam","tags":["Python","OpenCV","Raspberry Pi","timelapse","clouds"],"route":"timelapse"},{"title":"uSkipSpoilers","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"uSkipSpoilers","summary":"A small chrome extension for blocking spoilers in YouTube videos","tags":["React","TypeScript","Chrome","Extension"],"route":"uskipspoilers"},{"title":"This Website","github":"david-andrew.github.io","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"externalLink":"https://github.com/david-andrew/david-andrew.github.io","summary":"This website, written in react/typescript","tags":["Next.js","React","TypeScript","Tailwind CSS","WebAssembly"],"route":"website"},{"title":"WSE18: Machine Shop Biometric Interlock","imgSrc":{"src":"/_next/static/media/wse18.7405315e.png","height":2054,"width":2456,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAUVBMVEU8OTSVjYNlX1nIwbpLQjaqnpBOSUOJgHU8MCsyKSXo4dUfHh1iU0R8aWMUDAdoYlull4ialI2BZkzBuKyNdleyrKaQosO9iGCNY0PTj19hbYD9ZLzaAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAO0lEQVR4nAXBhQHAMAwDMBdCZRj/f+gkiLgdPDJcpJS6MbxKOJEBa1pH5whYpXl9DfEoc2A90MJC671/OekCOef5C1cAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":7},"summary":"Biometric security interlock system. Mechanical Engineering Senior Design capstone project","lastUpdated":"May 2018","tags":["Raspberry Pi","Python","C++","Qt","interlock","fingerprint","biometric"],"route":"wse18"},{"title":"Ziggy V (Working Title)","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Concept for a Real-Time-Strategy crossed with First-Person-Shooter","lastUpdated":"January 2021","tags":["Godot","GDScript","FPS x RTS","3D game"],"route":"ziggy_v"}]}],["$","div",null,{"className":"mx-auto px-4 sm:px-6 lg:px-8 max-w-[1190px]","children":[["$","$Lf",null,{"projects":[{"title":"Blob Opera Performances","imgSrc":{"src":"/_next/static/media/blob_opera_nox.e6f3aa2a.png","height":414,"width":512,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAJ1BMVEU4EgowEQlEIRV0Vzx9ZEo+JhlXOyBCT1RTMx0+LyVEUlxuTS5AS1HBZ0aTAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALklEQVR4nC3IuREAIAwDMMcOeYD95+XgKNQIhpahHYwUawuGoQuM6az0OwvPnwMUSwDO9qMceAAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":6},"summary":"Virtual choir performances leveraging the blob opera as a front end for voice synthesis","lastUpdated":"February 2021","tags":["Python","Blob Opera","choir","music","synthesis"],"route":"blob_opera"},{"title":"Boat Simulator","imgSrc":{"src":"/_next/static/media/boat_simulator.7cc391fd.jpg","height":1280,"width":2048,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAb/xAAdEAABAgcAAAAAAAAAAAAAAAAAAQQCAxESFSFT/8QAFAEBAAAAAAAAAAAAAAAAAAAABf/EABYRAAMAAAAAAAAAAAAAAAAAAAABEf/aAAwDAQACEQMRAD8AkFks7K49tuLmABWIOrP/2Q==","blurWidth":8,"blurHeight":5},"summary":"Spring 2017 HopHacks submission","lastUpdated":"March 2017","tags":["Unity","C#","3D game"],"route":"boat_simulator"},{"title":"Bueller Board","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Fall 2015 HopHacks submission: Midi keyboard that used user provided audio samples, a.k.a. the 'goat keyboard'","lastUpdated":"September 2015","tags":["midi","music"],"route":"bueller_board"},{"title":"Composer","imgSrc":{"src":"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":8},"summary":"React based composing software that acts as a front-end for LilyPond","lastUpdated":"January 2021","tags":["React","TypeScript","SMuFL","LilyPond","music","composition"],"route":"composer"},{"title":"Choir Compositions","imgSrc":{"src":"/_next/static/media/music_staff.3145785a.png","height":1616,"width":2745,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAHlBMVEUGBgZISEhSUlIeHh5paWk+Pj4vLy9cXFx7e3ufn5/bdycMAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAJ0lEQVR4nCXBtwEAIAwDMLe0/x9mQIKFjwN3jZBwxGowpFZGm7cpPAkxAH3f0FE4AAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":5},"summary":"","lastUpdated":"May 2015","tags":["music","choral","composition"],"route":"compositions"},{"title":"Dewy Programming Language","github":"dewy","imgSrc":{"src":"/_next/static/media/dewy_dandelion.e2efa7ee.jpg","height":1600,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z","blurWidth":8,"blurHeight":8},"summary":"An engineering focused programming language I am developing","tags":["Python","compilers","parsers","LLVM","Programming Languages"],"route":"dewy"},{"title":"Generalized Parsing","lastUpdated":"2022-02-06","imgSrc":{"src":"/_next/static/media/dewy_dandelion.e2efa7ee.jpg","height":1600,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAIAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAgEAABAgUFAAAAAAAAAAAAAAABAAIDBQYSMREiMkFR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwX/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oADAMBAAIRAxEAPwCAyFJ3U08ExHTO/e5wIPLHltuO9UREukVun//Z","blurWidth":8,"blurHeight":8},"summary":"Previous work on the Dewy Programming Language, namely a custom SRNGLR parser written entirely in C","tags":["C","compilers","parsers","SRNGLR","LLVM"],"route":"dewy_old"},{"title":"UR5 Draw Robot","imgSrc":{"src":"/_next/static/media/mona_lisa_contour.9bc5cbc9.jpg","height":1016,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAX/xAAeEAACAgEFAQAAAAAAAAAAAAABAgAEAwUHEiFRsf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAExAv/aAAwDAQACEQMRAD8AtbT3HvV9WGZVPB8RHXqsT8iIgoVqn//Z","blurWidth":8,"blurHeight":5},"summary":"UR5 robot arm project","lastUpdated":"December 2017","tags":["Matlab","UR5 robot","ROS"],"route":"drawbot"},{"title":"EasyREPL","imgSrc":{"src":"/_next/static/media/pypi_logo.f6536a80.svg","height":58,"width":66,"blurWidth":0,"blurHeight":0},"summary":"Python package for easily creating Read-Eval-Print Loops (REPLs)","github":"easyrepl","tags":["Python","PyPI","REPL"],"route":"easyrepl"},{"title":"Hacking Harmony or The Demon Chipmunk Choir","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"Ensemble","summary":"2019 Peabody Hackathon Submission. Choral music synthesis via autotuned google text-to-speech, AKA the demon chipmunk choir","tags":["Google text-to-speech API","matlab","python"],"route":"ensemble_peabody"},{"title":"Escort Mission 2020","imgSrc":{"src":"/_next/static/media/escort_mission_lamb.4c525bc4.png","height":128,"width":128,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAHlBMVEUA/gAAugD/+//h9+EAdAAirCKG/YZf2l9ri2uA/4BNpwkIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAK0lEQVR4nE2KuREAMAyDbPnff+HcSU1ooMDsI1P2XWdURDEOOMYAo6dbj3gNJgBhtLya0gAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":8},"github":"escort_mission_2020","summary":"Submission for the 2020 GMTK Game Jam","tags":["Godot","GDScript","2D game"],"route":"escort_mission"},{"title":"Foxing Animatronic","imgSrc":{"src":"/_next/static/media/foxing_animatronic.91d20002.png","height":1252,"width":1540,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAJ1BMVEUGFwsJIxEILhUiJQkdSRwWIQoUORYULA8RRSArMAYZEwYyVzM6OglI+AgfAAAACXBIWXMAAAsTAAALEwEAmpwYAAAANElEQVR4nBXKuREAIAwEsT2/GOi/XgbFoqs9I6DvdY8CW7O9DBSzvQ3OKFOCE5Vp+mdJBg8iegD5GB/ChgAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":7},"summary":"Manually actuated animatronic robot featured in the Foxing music video 'Slapstick'","lastUpdated":"June 2018","tags":["Solidworks","mechanical design","animatronic","Foxing","music"],"route":"foxing_animatronic"},{"title":"Mechatronics Robots","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Robots from mechatronics","lastUpdated":"May 2019","tags":["Arduino","C++","SolidWorks","mechanical design"],"route":"mechatronics"},{"title":"Mehve (Working Title)","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"mehve","summary":"3D adventure game inspire by \"Nausicaa of the Valley of the Wind\"","tags":["Godot","GDScript","3D game"],"route":"mehve"},{"title":"Musical DL","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"MusicalDL","summary":"Using deep learning to generate choral music in the style of JS Bach","tags":["Python","Pytorch","AI/ML","choral","music","generation"],"route":"musical_dl"},{"title":"NAND 2 Tetris","imgSrc":{"src":"/_next/static/media/nand2tetris.660feb3d.png","height":346,"width":396,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAPFBMVEUAAABvKgIhHhw+KR0uJAFlRBU2GAYJBAENDw/TVQF/ZRBuWwrEoxytlRBiVBJJHQC7SwG5YCWSOwDqziPUksrRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAN0lEQVR4nCWKWw7AIAyA0Fbbqnvo7n/XZZMfEgKAscllu870J7tj2Tk6NSK151BcLilNwd3z974dVQEdrZJh0wAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":7},"github":"nand2tetris","summary":"A 16-bit computer built from scratch, starting with NAND gates","tags":["Computer Architecture","HDL","Assembly","Hack","Jack","Compilers","Operating Systems","Virtual Machines"],"route":"nand2tetris"},{"title":"PDF Chatter","imgSrc":{"src":"/_next/static/media/pypi_logo.f6536a80.svg","height":58,"width":66,"blurWidth":0,"blurHeight":0},"summary":"LLM powered Q&A over extracted PDF text","github":"pdf-chatter","tags":["Python","Optical Character Recognition (OCR)","Large Language Models (LLMs)","Nougat-OCR","GPT-4"],"route":"pdf_chatter"},{"title":"pOngBot","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Autonomous beer pong playing robot","lastUpdated":"June 2020","tags":["Arduino","C++","computer vision","Viola-Jones","mechanical design"],"route":"pongbot"},{"title":"PRS19: Fret Press Robot","imgSrc":{"src":"/_next/static/media/prs2019_preview.aed05a2a.png","height":2093,"width":3061,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAPFBMVEVJPTmhmIiCa1SBfXdBQD1IOzB3bGA6My5dUkyXlpNPRTyio6ChpadSTUm0q5e+taCun2xwYl2qrq5mXU/I9hK/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAL0lEQVR4nAXBhwEAIAjAsKoo4B7//2rCXFZLjoGwh4poIj5TJRfOsAok3G/v3toHGpkBOY6KaOIAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":5},"github":"PRS_robot","summary":"Automatic guitar fret press robot. Mechanical Engineering Master's Design captsone project","tags":["C++","Arduino","mechanical design"],"route":"prs19"},{"title":"Rewind","imgSrc":{"src":"/_next/static/media/rewind_title.a1e43a09.png","height":540,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAElBMVEUqHjguITs4J0JmWW1PRFo/MUqO6aYhAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAH0lEQVR4nGNggAMmJkYmRhCDhZmFmZUBxGRkgIhAAQADKgAhbD4/MgAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":5},"summary":"2018 Video Game Desgn (EN.601.355) capstone project","lastUpdated":"May 2018","tags":["Unity","C#","2D game"],"route":"rewind"},{"title":"RoboJay","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"lastUpdated":"May 2018","summary":"A balancing robot designed to give campus tours to incoming JHU freshmen","tags":["robotics","feedback control","navigation","BeagleBone","ROS"],"route":"robojay"},{"title":"High Power Rocketry","imgSrc":{"src":"/_next/static/media/rebel_scum.34e7823c.jpg","height":1365,"width":2048,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAaEAACAgMAAAAAAAAAAAAAAAAAARESIjFB/8QAFQEBAQAAAAAAAAAAAAAAAAAAAgP/xAAXEQADAQAAAAAAAAAAAAAAAAAAAhNR/9oADAMBAAIRAxEAPwC6o4w5OwATo+immH//2Q==","blurWidth":8,"blurHeight":5},"summary":"Level 1 & 2 High Powered Rocket built with the Johns Hopkins Rocketry Club, and Spaceport America Cup 2018","lastUpdated":"January 2018","tags":["High Power Rocketry","Arduino","C++","mechanical design","Tripoli"],"route":"rocketry"},{"title":"Silver Void","imgSrc":{"src":"/_next/static/media/silver_void_cover_art.cc703812.png","height":1439,"width":1823,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAIVBMVEUCAgEWGhAMDQgiFiEnJyYwMC4aCRZHJygZDgQ9EDxLU0cSomUyAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALUlEQVR4nC3KyQkAMBDDQHntzdV/wSEQ/QaEYkeAutPhJ0NBFdv0kWDM57euCwrsAGo10HJLAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":6},"github":"SilverVoid","summary":"Submission for Acerola Jam 0","tags":["Godot","GDScript","3D Game","Space Simulator","Bullet Hell"],"route":"silver_void"},{"title":"so voice!","imgSrc":{"src":"/_next/static/media/YeArlingtonMusickeLabLogo.cf893b61.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAFVBMVEUEBARFRUVQUFA3NzckJCQXFxdfX192C4FUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALElEQVR4nE2LtxEAMAyE+CDtP7JPnakoAH60CgwNFQr2ibdytUhjT3xt8q8PDVwAX2ljebUAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":8},"lastUpdated":"December 2022","summary":"Choral music synthesis with deep learning (Continuation of Musical DL)","tags":["Python","Pytorch","AI/ML","choral","music","synthesis"],"route":"so_voice"},{"title":"Terminal Ray Tracer","imgSrc":{"src":"/_next/static/media/terminal_ray_tracer.00db7e4a.png","height":2043,"width":2881,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAMAAADJ2y/JAAAAbFBMVEWSamvNrLWWP0yUKVeYinmmnKTsEheTcnnmh4+Xfo6pV2GMMDasvsa5aXiPRVFVs2uOWF9WnGxXtVObh4ypREzbYmnsPkXIba/y/P10bHmfbcouPtDAMDdJbZOtmERQbkXfwcqTb3g+yNdOcdnEQLLCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAOklEQVR4nAXBBQKAIAAAsSMl7MZG//9HN/Igu6YtYDT+7qnhSK++tI+IcH5BViXZqn0Tk8M8Ti3rbH9KhwLIHy1s0QAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":6},"github":"TerminalRayTracer","summary":"A dependency-free ray tracer written in C that runs directly in a linux terminal","tags":["C","ray tracing","CLI","linux"],"route":"terminal_ray_tracer"},{"title":"Cloud Timelapse","imgSrc":{"src":"/_next/static/media/timelapse.b57dd258.jpg","height":3468,"width":4624,"blurDataURL":"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAGAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAcEAACAgIDAAAAAAAAAAAAAAAAAQIFAxETYdH/xAAVAQEBAAAAAAAAAAAAAAAAAAADBP/EABYRAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAwDAQACEQMRAD8AlG4r56fBlT6ivQAUqwYX/9k=","blurWidth":8,"blurHeight":6},"github":"timelapse","summary":"A simple python project for taking timelapses of clouds from a webcam","tags":["Python","OpenCV","Raspberry Pi","timelapse","clouds"],"route":"timelapse"},{"title":"uSkipSpoilers","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"github":"uSkipSpoilers","summary":"A small chrome extension for blocking spoilers in YouTube videos","tags":["React","TypeScript","Chrome","Extension"],"route":"uskipspoilers"},{"title":"This Website","github":"david-andrew.github.io","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"externalLink":"https://github.com/david-andrew/david-andrew.github.io","summary":"This website, written in react/typescript","tags":["Next.js","React","TypeScript","Tailwind CSS","WebAssembly"],"route":"website"},{"title":"WSE18: Machine Shop Biometric Interlock","imgSrc":{"src":"/_next/static/media/wse18.7405315e.png","height":2054,"width":2456,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAMAAAACh/xsAAAAUVBMVEU8OTSVjYNlX1nIwbpLQjaqnpBOSUOJgHU8MCsyKSXo4dUfHh1iU0R8aWMUDAdoYlull4ialI2BZkzBuKyNdleyrKaQosO9iGCNY0PTj19hbYD9ZLzaAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAO0lEQVR4nAXBhQHAMAwDMBdCZRj/f+gkiLgdPDJcpJS6MbxKOJEBa1pH5whYpXl9DfEoc2A90MJC671/OekCOef5C1cAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":7},"summary":"Biometric security interlock system. Mechanical Engineering Senior Design capstone project","lastUpdated":"May 2018","tags":["Raspberry Pi","Python","C++","Qt","interlock","fingerprint","biometric"],"route":"wse18"},{"title":"Ziggy V (Working Title)","imgSrc":{"src":"/_next/static/media/logo.43586adb.png","height":960,"width":960,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAG1BMVEUHBwcwMDAAAABMaXEAAAAAAAAgICAFBQUmJiYApgMbAAAACHRSTlP9/C8AtOqwLojbm7gAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAxSURBVHicPYtbCgAwCMNSH9P7n3joYNCPQFK8AsIcS4A0DkhwkHq30AOjetSP3738AhZmAKbLVdBfAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":8},"summary":"Concept for a Real-Time-Strategy crossed with First-Person-Shooter","lastUpdated":"January 2021","tags":["Godot","GDScript","FPS x RTS","3D game"],"route":"ziggy_v"}]}],["$","$La",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$Lb",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$","$La",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children","dewy","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$Lb",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$L10","$L11",null],"segment":"__PAGE__"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/85fa6dafca566008.css","precedence":"next"}]]}],"segment":"dewy"},"styles":[]}],["$","$L12",null,{}]]}]]
10:null
13:"$Sreact.suspense"
14:I{"id":33699,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","4769:static/chunks/4769-afbb25c509513e18.js","9659:static/chunks/9659-468d9b7f9560cf55.js","6413:static/chunks/6413-9fe1335479bb3ecd.js","6896:static/chunks/6896-b4076f39777e66ab.js","3215:static/chunks/3215-a5826776a257bdd1.js","1134:static/chunks/app/projects/dewy/page-8f79a01bf60b6ee9.js"],"name":"NoSSR","async":false}
15:I{"id":43215,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","4769:static/chunks/4769-afbb25c509513e18.js","9659:static/chunks/9659-468d9b7f9560cf55.js","6413:static/chunks/6413-9fe1335479bb3ecd.js","6896:static/chunks/6896-b4076f39777e66ab.js","3215:static/chunks/3215-a5826776a257bdd1.js","1134:static/chunks/app/projects/dewy/page-8f79a01bf60b6ee9.js"],"name":"","async":false}
24:I{"id":53123,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","4769:static/chunks/4769-afbb25c509513e18.js","9659:static/chunks/9659-468d9b7f9560cf55.js","6413:static/chunks/6413-9fe1335479bb3ecd.js","6896:static/chunks/6896-b4076f39777e66ab.js","3215:static/chunks/3215-a5826776a257bdd1.js","1134:static/chunks/app/projects/dewy/page-8f79a01bf60b6ee9.js"],"name":"DewyCodeBlock","async":false}
25:I{"id":46685,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","4769:static/chunks/4769-afbb25c509513e18.js","9659:static/chunks/9659-468d9b7f9560cf55.js","6413:static/chunks/6413-9fe1335479bb3ecd.js","6896:static/chunks/6896-b4076f39777e66ab.js","3215:static/chunks/3215-a5826776a257bdd1.js","1134:static/chunks/app/projects/dewy/page-8f79a01bf60b6ee9.js"],"name":"","async":false}
26:I{"id":63222,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","3222:static/chunks/3222-d51cb51b116d5c1d.js","4769:static/chunks/4769-afbb25c509513e18.js","9659:static/chunks/9659-468d9b7f9560cf55.js","6413:static/chunks/6413-9fe1335479bb3ecd.js","6896:static/chunks/6896-b4076f39777e66ab.js","3215:static/chunks/3215-a5826776a257bdd1.js","1134:static/chunks/app/projects/dewy/page-8f79a01bf60b6ee9.js"],"name":"Image","async":false}
16:T863,from pathlib import Path
from argparse import ArgumentParser, REMAINDER
from .backend import backend_names, get_backend, python_interpreter, qbe_compiler, get_version

import pdb



def main():
    arg_parser = ArgumentParser(description='Dewy Compiler')

    # positional argument for the file to compile
    arg_parser.add_argument('file', help='.dewy file to run')

    # mutually exclusive flags for specifying the backend to use
    group = arg_parser.add_mutually_exclusive_group()
    group.add_argument('-i', action='store_true', help='(DEFAULT) Run in interpreter mode with the python backend')
    group.add_argument('-c', action='store_true', help='Run in compiler mode with the llvm backend (not implemented yet)')
    group.add_argument('--backend', type=str, help=f'Specify a backend compiler/interpreter by name to use. Backends will include: {backend_names} (however currently only python is available).')

    arg_parser.add_argument('-v', '--version', action='version', version=f'Dewy {get_version()}', help='Print version information and exit')
    arg_parser.add_argument('-p', '--disable-rich-print', action='store_true', help='Disable using rich for printing stack traces')
    arg_parser.add_argument('args', nargs=REMAINDER, help='Arguments after the file are passed directly to program')

    args = arg_parser.parse_args()

    # use rich for pretty traceback printing
    #TODO: maybe add a util or something for trying to import rich and replacing print in all files
    if not args.disable_rich_print:
        try:
            from rich import traceback
            traceback.install(show_locals=True)
        except:
            print('rich unavailable for import. using built-in printing')

    # default interpreter is python. default compiler is qbe. default with no args is python.
    if args.backend:
        backend = get_backend(args.backend)
    elif args.c:
        backend = qbe_compiler
    elif args.i:
        backend = python_interpreter
    else:
        backend = python_interpreter

    # run with the selected backend
    backend(Path(args.file), args.args)


if __name__ == '__main__':
    main()
17:T885b,from typing import Generator, Sequence, cast, overload, Literal, Type as PyType
from enum import Enum, auto
from dataclasses import dataclass, field
from itertools import groupby, chain as iterchain

from .syntax import (
    AST,
    Access,
    Declare,
    PointsTo, BidirPointsTo,
    Type,
    ListOfASTs, Tuple, Block, BareRange, Ellipsis, Spread, Array, Group, Range, Object, Dict, BidirDict, TypeParam,
    Void, Undefined, void, undefined, untyped,
    String, IString,
    Flowable, Flow, If, Loop, Default,
    FunctionLiteral, PrototypePyAction, PyAction, Call,
    Index,
    PrototypeIdentifier, Identifier, TypedIdentifier, TypedGroup, UnpackTarget, Assign,
    Int, Bool,
    Range, IterIn,
    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,
    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv,
    DeclarationType,
    DeclareGeneric, Parameterize,
)
from .tokenizer import (
    Token,
    Block_t,
    Operator_t,
    ShiftOperator_t,
    Juxtapose_t,
    Comma_t,
    String_t,
    Escape_t,
    TypeParam_t,
    Undefined_t,
    Identifier_t,
    Integer_t,
    Boolean_t,
    BasedNumber_t,
    RawString_t,
    DotDot_t, DotDotDot_t,
    Keyword_t,
)
from .postok import (
    RangeJuxtapose_t,
    EllipsisJuxtapose_t,
    TypeParamJuxtapose_t,
    get_next_chain,
    Chain,
    is_op,
    Flow_t,
    Declare_t,
)
from .utils import (
    bool_to_bool,
    based_number_to_int,
)

import pdb



# Scope class only used during parsing to keep track of callables
@dataclass
class Scope:
    @dataclass
    class _var():
        # name:str #name is stored in the dict key
        decltype: DeclarationType
        type: Type
        value: AST

    parent: 'Scope | None' = None
    # callables: dict[str, AST | None] = field(default_factory=dict) #TODO: maybe replace str->AST with str->signature (where signature might be constructed based on the func structure)
    vars: 'dict[str, Scope._var]' = field(default_factory=dict)

    @overload
    def get(self, name:str, throw:Literal[True]=True) -> 'Scope._var': ...
    @overload
    def get(self, name:str, throw:Literal[False]) -> 'Scope._var|None': ...
    def get(self, name:str, throw:bool=True) -> 'Scope._var|None':
        for s in self:
            if name in s.vars:
                return s.vars[name]

        if throw:
            raise KeyError(f'variable {name} not found in scope')
        return None

    def assign(self, name:str, value:AST):
        assert len(DeclarationType.__members__) == 2, f'expected only 2 declaration types: let, const. found {DeclarationType.__members__}'

        # var is already declared in current scope
        if name in self.vars:
            var = self.vars[name]
            assert var.decltype != DeclarationType.CONST, f"Attempted to assign to constant variable: {name=}{var=}. {value=}"
            var.value = value
            return

        var = self.get(name, throw=False)

        # var is not declared in any scope
        if var is None:
            self.let(name, value, untyped)
            return

        # var was declared in a parent scope
        if var.decltype == DeclarationType.LET:
            var.value = value
            return

        raise ValueError(f'Attempted to assign to constant variable: {name=}{var=}. {value=}')

    def declare(self, name:str, value:AST, type:Type, decltype:DeclarationType):
        if name in self.vars:
            var = self.vars[name]
            assert var.decltype != DeclarationType.CONST, f"Attempted to {decltype.name.lower()} declare a value that is const in this current scope. {name=}{var=}. {value=}"

        self.vars[name] = Scope._var(decltype, type, value)

    def let(self, name:str, value:AST, type:Type):
        self.declare(name, value, type, DeclarationType.LET)

    def const(self, name:str, value:AST, type:Type):
        self.declare(name, value, type, DeclarationType.CONST)


    #TODO: this should be expanded/more comprihensive/etc.
    def is_callable(self, name: str) -> bool:
        var = self.get(name)
        return var.type.name == 'callable'


    def __iter__(self) -> Generator['Scope', None, None]:
        """return an iterator that walks up each successive parent scope. Starts with self"""
        s = self
        while s is not None:
            yield s
            s = s.parent

    #TODO: these should actually be defined in python.py. There should maybe only be stubs here..
    @classmethod
    def default(cls: PyType['Scope']) -> 'Scope':
        return cls(vars={
            'printl': Scope._var(
                DeclarationType.CONST,
                Type('callable'),
                PrototypePyAction(
                    TypedIdentifier(Identifier('x'), Type('string')),
                    Type('void')
                )
            ),
            'print': Scope._var(
                DeclarationType.CONST,
                Type('callable'),
                PrototypePyAction(
                    TypedIdentifier(Identifier('x'), Type('string')),
                    Type('void')
                )
            ),
            'readl': Scope._var(
                DeclarationType.CONST,
                Type('callable'),
                PrototypePyAction(
                    void,
                    Type('string')
                )
            )
        })




def top_level_parse(tokens: list[Token]) -> AST:
    """Main entrypoint to kick off parsing a sequence of tokens"""

    scope = Scope.default()
    ast = parse(tokens, scope)
    if isinstance(ast, ListOfASTs):
        ast = Group(ast.asts)

    # post processing on the parsed AST
    # express_identifiers(ast)
    # tuples_to_arrays(ast)
    # ensure_no_prototypes(ast) #ensure all settled...
    # ensure_no_unwrapped_ranges(ast)
    # set_ast_scopes(ast, scope)

    return ast

def parse_generator(tokens: list[Token], scope: Scope) -> Generator[AST, None, None]:
    """
    Parse all tokens into a sequence of ASTs
    """

    while len(tokens) > 0:
        chain, tokens = get_next_chain(tokens)
        yield parse_chain(chain, scope)


def parse(tokens: list[Token], scope: Scope) -> AST:
    items = [*parse_generator(tokens, scope)]

    # depending on how many expressions were parsed, return an AST or container
    if len(items) == 0:
        ast = void # literally nothing was parsed
    elif len(items) == 1:
        ast = items[0]
    else:
        ast = ListOfASTs(items)

    return ast

@dataclass
class qint:
    """
    quantum int for dealing with precedences that are multiple values at the same time
    qint's can only be strictly greater or strictly less than other values. Otherwise it's ambiguous
    In the case of ambiguous precedences, the symbol table is needed for helping resolve the ambiguity
    """
    values: set[int]

    def __post_init__(self):
        assert len(self.values) > 1, f'qint must have more than one value. Got {self.values}'

    def __gt__(self, other: 'int|qint') -> bool:
        if isinstance(other, int):
            return all(v > other for v in self.values)
        return all(v > other for v in self.values)

    def __lt__(self, other: 'int|qint') -> bool:
        if isinstance(other, int):
            return all(v < other for v in self.values)
        return all(v < other for v in self.values)

    def __ge__(self, other: 'int|qint') -> bool: return self.__gt__(other)
    def __le__(self, other: 'int|qint') -> bool: return self.__lt__(other)
    def __eq__(self, other: object) -> bool: return False


######### Operator Precedence Table #########
# TODO: class for compund operators, e.g. += -= .+= .-= not=? not>? etc.
# TODO: how to handle unary operators in the table? perhaps make PrefixOperator_t/PostfixOperator_t classes?
# TODO: add specification of associativity for each row
class Associativity(Enum):
    left = auto()  # left-to-right
    right = auto()  # right-to-left
    prefix = auto()
    postfix = auto()
    none = auto()
    fail = auto()


"""
[HIGHEST PRECEDENCE]
    (prefix) @
    . <jux call> <jux index access>
    <jux ellipsis>                      //e.g. [...args]
    :                                   //e.g. let x:int
    (prefix) not
    (postfix) ? `
    ^                                   //right-associative
    <jux mul>
    / * %
    + -
    << >> <<< >>> <<! !>>
    in
    =? >? <? >=? <=? not=? <=> is? isnt? @?
    and nand &
    xor xnor                            //following C's precedence: and > xor > or
    or nor |
    ,                                   //tuple maker
    <jux range>                         //e.g. [first,second..last]
    =>
    = .= <op>= .<op>=  (e.g. += .+=)    //right-associative (but technically causes a type error since assignments can't be chained)
    else
    (postfix) ;
    <seq> (i.e. space)
[LOWEST PRECEDENCE]

TODO:
- add operators: as transmute |> <| -> <-> <- :

[Notes]
.. for ranges is not an operator, it is an expression. it uses juxtapose to bind to left/right arguments (or empty), and type-checks left and right
if-else-loop chain expr is more like a single unit, so it doesn't really have a precedence. but they act like they have the lowest precedence since the expressions they capture will be full chains only broken by space/seq
the unary versions of + - * / % have the same precedence as their binary versions
"""
operator_groups: list[tuple[Associativity, Sequence[Operator_t]]] = list(reversed([
    (Associativity.prefix, [Operator_t('@')]),
    (Associativity.left, [Operator_t('.'), Juxtapose_t(None)]),  # jux-call, jux-index
    (Associativity.right, [EllipsisJuxtapose_t(None)]),  # jux-ellipsis
    (Associativity.none, [TypeParamJuxtapose_t(None)]),
    (Associativity.none, [Operator_t(':')]),
    (Associativity.prefix, [Operator_t('not')]),
    (Associativity.right,  [Operator_t('^')]),
    (Associativity.left, [Juxtapose_t(None)]),  # jux-multiply
    (Associativity.left, [Operator_t('*'), Operator_t('/'), Operator_t('%')]),
    (Associativity.left, [Operator_t('+'), Operator_t('-')]),
    (Associativity.left, [*map(ShiftOperator_t, ['<<', '>>', '<<<', '>>>', '<<!', '!>>'])]),
    (Associativity.none,  [Comma_t(',')]),
    (Associativity.left, [RangeJuxtapose_t(None)]),  # jux-range
    (Associativity.none, [Operator_t('in')]),
    (Associativity.left, [Operator_t('=?'), Operator_t('>?'), Operator_t('<?'), Operator_t('>=?'), Operator_t('<=?')]),
    (Associativity.left, [Operator_t('and'), Operator_t('nand'), Operator_t('&')]),
    (Associativity.left, [Operator_t('xor'), Operator_t('xnor')]),
    (Associativity.left, [Operator_t('or'), Operator_t('nor'), Operator_t('|')]),
    (Associativity.right,  [Operator_t('=>')]),  # () => () => () => 42
    (Associativity.fail,  [Operator_t('=')]),
    (Associativity.none,  [Operator_t('else')]),
]))
precedence_table: dict[Operator_t, int | qint] = {}
associativity_table: dict[int, Associativity] = {}
for i, (assoc, group) in enumerate(operator_groups):

    # mark precedence level i as the specified associativity
    associativity_table[i] = assoc

    # insert all ops in the row into the precedence table at precedence level i
    for op in group:
        if op not in precedence_table:
            precedence_table[op] = i
            continue

        val = precedence_table[op]
        if isinstance(val, int):
            precedence_table[op] = qint({val, i})
        else:
            precedence_table[op] = qint(val.values | {i})


def operator_precedence(op: Operator_t) -> int | qint:

    # TODO: handling compound operators like .+, +=, .+=, etc.
    # if isinstance(op, CompoundOperator_t):
    #     op = op.base

    try:
        return precedence_table[op]
    except:
        raise ValueError(f"ERROR: expected operator, got {op=}") from None


def operator_associativity(op: Operator_t | int) -> Associativity:
    if not isinstance(op, int):
        i = operator_precedence(op)
        assert isinstance(i, int), f'Cannot determine associativity of operator ({op}) with multiple precedence levels ({i})'
    else:
        i = op
    try:
        return associativity_table[i]
    except:
        raise ValueError(f"Error: failed to determine associativity for operator {op}") from None


def is_callable(ast:AST, scope: Scope):
    match ast:
        # ASTs the have to be evaluated to determine the type
        case PrototypeIdentifier(name):
            return scope.is_callable(name)
        #TODO: any other types that need to be evaluated to determine if callable

        # known callable ASTs
        case PyAction() | FunctionLiteral():
            return True

        # known non-callables
        case Int() | String() | Bool(): #TODO: rest of them..
            return False

        case _:
            raise ValueError(f"ERROR: unhandled case to check if is_callable: {ast=}")

    pdb.set_trace()

def is_indexable(ast:AST, scope: Scope):
    pdb.set_trace()
    raise NotImplementedError


def parse_chain(chain: Chain[Token], scope: Scope) -> AST:
    assert isinstance(chain, Chain), f"ERROR: parse chain must be called on Chain[Token], got {type(chain)}"

    if len(chain) == 0:
        return void
    if len(chain) == 1:
        return parse_single(chain[0], scope)

    left, op, right = split_by_lowest_precedence(chain, scope)
    left, right = parse_chain(left, scope), parse_chain(right, scope)

    assert not (left is void and right is void), f"Internal Error: both left and right returned void during parse chain, implying both left and right side of operator were empty, i.e. chain was invalid: {chain}"

    # 3 cases are prefix expr, postfix expr, or binary expr
    if left is void:
        return build_unary_prefix_expr(op, right, scope)
    if right is void:
        return build_unary_postfix_expr(left, op, scope)
    return build_bin_expr(left, op, right, scope)




def split_by_lowest_precedence(tokens: Chain[Token], scope: Scope) -> tuple[Chain[Token], Token, Chain[Token]]:
    """
    return the integer index/indices of the lowest precedence operator(s) in the given list of tokens
    """
    assert isinstance(
        tokens, Chain), f"ERROR: `split_by_lowset_precedence()` may only be called on explicitly known Chain[Token], got {type(tokens)}"

    # collect all operators and their indices in the list of tokens
    idxs, ops = zip(*[(i, token) for i, token in enumerate(tokens) if is_op(token)])

    if len(ops) == 0:
        pdb.set_trace()
        # TODO: how to handle this case?
        # return Chain(), None, Chain()
        raise ValueError()
    if len(ops) == 1:
        i, = idxs
        op, = ops
        return Chain(tokens[:i]), op, Chain(tokens[i+1:])

    # when more than one op present, find the lowest precedence one
    ranks = [operator_precedence(op) for op in ops]
    min_rank = min(ranks)

    # verify that the min is strictly less than or equal to all other ranks
    if not all(min_rank <= r for r in ranks):
        # TODO: probably enumerate out all permutations of the ambiguous operators and return all of them as a list of lists of indices
        # make use of scope/chain typeof to disambiguate if need be
        raise NotImplementedError(f"TODO: ambiguous precedence for {ops=} with {ranks=}, in token stream {tokens=}")

    # find operators with precedence equal to the current minimum
    op_idxs = [i for i, r in zip(idxs, ranks) if r == min_rank]

    if len(op_idxs) == 1:
        i, = op_idxs
        return Chain(tokens[:i]), tokens[i], Chain(tokens[i+1:])

    # handling when multiple ops have the same precedence, select based on associativity rules
    if isinstance(min_rank, qint):
        assocs = {operator_associativity(i) for i in min_rank.values}
        if len(assocs) > 1:
            raise NotImplementedError(
                f'TODO: need to type check to deal with multiple/ambiguous operator associativities: {assocs}')
        assoc, = assocs
    else:
        assoc = operator_associativity(min_rank)

    match assoc:
        case Associativity.left: i = op_idxs[-1]
        case Associativity.right: i = op_idxs[0]
        case Associativity.prefix: i = op_idxs[0]
        case Associativity.postfix: i = op_idxs[-1]
        case Associativity.none: i = op_idxs[-1]  # default to left. handled later in parsing
        case Associativity.fail: raise ValueError(f'Cannot handle multiple given operators in chain {tokens}, as lowest precedence operator is marked as un-associable.')

    return Chain(tokens[:i]), tokens[i], Chain(tokens[i+1:])





def parse_single(token: Token, scope: Scope) -> AST:
    """Parse a single token into an AST"""
    match token:
        case Undefined_t(): return undefined
        case Identifier_t(): return PrototypeIdentifier(token.src)
        case Integer_t(): return Int(int(token.src))
        case Boolean_t(): return Bool(bool_to_bool(token.src))
        case BasedNumber_t(): return Int(based_number_to_int(token.src))
        case RawString_t(): return String(token.to_str())
        case DotDot_t(): return BareRange(void, void)
        case DotDotDot_t(): return Ellipsis()
        case String_t(): return parse_string(token, scope)
        case Block_t(): return parse_block(token, scope)
        case TypeParam_t(): return parse_type_param(token, scope)
        case Flow_t(): return parse_flow(token, scope)
        case Declare_t(): return parse_declare(token, scope)

        case _:
            # TODO handle other types...
            pdb.set_trace()
            ...

    pdb.set_trace()
    raise NotImplementedError()
    ...


def build_bin_expr(left: AST, op: Token, right: AST, scope: Scope) -> AST:
    """create a unary prefix expression AST from the op and right AST"""

    match op:
        case Juxtapose_t():
            if is_callable(left, scope):
                return Call(left, right)
            elif isinstance(right, (Range, Array)) and is_indexable(left, scope):
                return Index(left, right)
            else:
                return Mul(left, right)

        case Operator_t(op='='): return Assign(left, right)
        case Operator_t(op='=>'): return FunctionLiteral(left, right)
        case Operator_t(op='->'): return PointsTo(left, right)
        # case Operator_t(op='<-'): return PointsTo(right, left) #TBD if we just remove this one...
        case Operator_t(op='<->'): return BidirPointsTo(left, right)
        case Operator_t(op='.'): return Access(left, right)

        # a bunch of simple cases:
        case ShiftOperator_t(op='<<'):  return LeftShift(left, right)
        case ShiftOperator_t(op='>>'):  return RightShift(left, right)
        case ShiftOperator_t(op='<<<'): return LeftRotate(left, right)
        case ShiftOperator_t(op='>>>'): return RightRotate(left, right)
        case ShiftOperator_t(op='<<!'): return LeftRotateCarry(left, right)
        case ShiftOperator_t(op='!>>'): return RightRotateCarry(left, right)
        case Operator_t(op='+'): return Add(left, right)
        case Operator_t(op='-'): return Sub(left, right)
        case Operator_t(op='*'): return Mul(left, right)
        case Operator_t(op='/'): return Div(left, right)
        case Operator_t(op='÷'): return IDiv(left, right)
        case Operator_t(op='%'): return Mod(left, right)
        case Operator_t(op='^'): return Pow(left, right)

        # comparison operators
        case Operator_t(op='=?'): return Equal(left, right)
        case Operator_t(op='>?'): return Greater(left, right)
        case Operator_t(op='<?'): return Less(left, right)
        case Operator_t(op='>=?'): return GreaterEqual(left, right)
        case Operator_t(op='<=?'): return LessEqual(left, right)
        case Operator_t(op='in?'): return MemberIn(left, right)
        # case Operator_t(op='is?'): return Is(left, right)
        # case Operator_t(op='isnt?'): return Isnt(left, right)
        # case Operator_t(op='<=>'): return ThreewayCompare(left, right)

        # Logical Operators. TODO: outtype=Bool is not flexible enough...
        case Operator_t(op='and'): return And(left, right)
        case Operator_t(op='or'): return Or(left, right)
        case Operator_t(op='nand'): return Nand(left, right)
        case Operator_t(op='nor'): return Nor(left, right)
        case Operator_t(op='xor'): return Xor(left, right)
        case Operator_t(op='xnor'): return Xnor(left, right)

        # Misc Operators
        case Operator_t(op=':'):
            if isinstance(left, PrototypeIdentifier): return TypedIdentifier(Identifier(left.name), right)
            if isinstance(left, Group): return TypedGroup(left, right)
            raise ValueError(f'ERROR: can only apply a type to an identifier or a (group). Got {left=}, {right=}')

        case TypeParamJuxtapose_t():
            if isinstance(left, TypeParam):
                return DeclareGeneric(left, right)
            if isinstance(right, TypeParam):
                return Parameterize(left, right)
            raise ValueError(f"INTERNAL ERROR: TypeParamJuxtapose must be attached to a type param. {left=}, {right=}")

        case EllipsisJuxtapose_t():
            assert isinstance(left, Ellipsis), f'INTERNAL ERROR: EllipsisJuxtapose was attached to a non ellipsis token. {left=}, {right=}'
            return Spread(right)

        case RangeJuxtapose_t():
            if isinstance(right, BareRange):
                assert right.left is void, f"ERROR: can't attach expression to range, range already has values. Got {left=}, {right=}"
                right.left = left
                return right

            if isinstance(left, BareRange):
                assert left.right is void, f"ERROR: can't attach expression to range, range already has values. Got {left=}, {right=}"
                left.right = right
                return left

            raise ValueError(f'INTERNAL ERROR: Range Juxtapose must be next to a range. Got {left=}, {right=}')

        case Comma_t():
            # TODO: combine left or right tuples into a single tuple
            if isinstance(left, Tuple) and isinstance(right, Tuple):
                return Tuple([*left.items, *right.items])
            elif isinstance(left, Tuple):
                return Tuple([*left.items, right])
            elif isinstance(right, Tuple):
                return Tuple([left, *right.items])
            else:
                return Tuple([left, right])

        case Operator_t(op='else'):
            if isinstance(left, Flow) and isinstance(right, Flow):
                # merge left+right as single flow
                return Flow([*left.branches, *right.branches])
            elif isinstance(left, Flow):
                # append right to left
                assert not isinstance(left.branches[-1], Default), f"ERROR: can't merge default branch into middle of flow. Got: {left=}, {right=}"
                if isinstance(right, Flowable):
                    return Flow([*left.branches, right])
                return Flow([*left.branches, Default(right)])

            elif isinstance(right, Flow):
                # prepend left to right
                assert isinstance(left, Flowable), f"ERROR: can only prepend Flowables to left of a Flow. Got: {left=}, {right=}"
                return Flow([left, *right.branches])
            else:
                # create a new flow out of the left and right
                assert isinstance(left, Flowable), f"ERROR: can only create a Flow from Flowables. Got: {left=}, {right=}"
                if isinstance(right, Flowable):
                    return Flow([left, right])
                return Flow([left, Default(right)])

        case Operator_t(op='in'):
            return IterIn(left, right)

        case _:
            pdb.set_trace()
            raise NotImplementedError(f'Parsing of operator {op} has not been implemented yet')


def build_unary_prefix_expr(op: Token, right: AST, scope: Scope) -> AST:
    """create a unary prefix expression AST from the op and right AST"""
    match op:
        # normal prefix operators
        case Operator_t(op='+'): return UnaryPos(right)
        case Operator_t(op='-'): return UnaryNeg(right)
        case Operator_t(op='*'): return UnaryMul(right)
        case Operator_t(op='/'): return UnaryDiv(right)
        case Operator_t(op='not'): return Not(right)  # TODO: don't want to hardcode Bool here!
        case Operator_t(op='@'): raise NotImplementedError(f"TODO: prefix op: {op=}")
        case Operator_t(op='...'): raise NotImplementedError(f"TODO: prefix op: {op=}")

        # binary operators that appear to be unary because the left can be void
        # => called as unary prefix op means left was ()/void
        case Operator_t(op='=>'): return FunctionLiteral(void, right)

        case _:
            raise ValueError(f"INTERNAL ERROR: {op=} is not a known unary prefix operator")


def build_unary_postfix_expr(left: AST, op: Token, scope: Scope) -> AST:
    """create a unary postfix expression AST from the left AST and op token"""
    match op:
        # normal postfix operators
        case Operator_t(op='!'): raise NotImplementedError(f"TODO: postfix op: {op=}")  # return Fact(left)

        # binary operators that appear to be unary because the right can be void
        # anything juxtaposed with void is treated as a zero-arg call()
        case Juxtapose_t():
            return Call(left)

        case _:
            raise NotImplementedError(f"TODO: {op=}")

def parse_string(token: String_t, scope: Scope) -> String | IString:
    """Convert a string token to an AST"""

    if len(token.body) == 1 and isinstance(token.body[0], str):
        return String(token.body[0])

    # else handle interpolation strings
    parts = []
    for chunk in token.body:
        if isinstance(chunk, str):
            parts.append(chunk)
        elif isinstance(chunk, Escape_t):
            parts.append(chunk.to_str())
        else:
            # put any interpolation expressions in a new scope
            ast = parse(chunk.body, scope)
            if isinstance(ast, Block):
                parts.append(ast)
            elif isinstance(ast, ListOfASTs):
                pdb.set_trace()
                # not sure if this should ever come up, might be a parse bug
                # or might just need to convert to a block...
            else:
                parts.append(Block([ast]))

    # combine any adjacent Strings into a single string (e.g. if there were escapes)
    parts = iterchain(*((''.join(g),) if issubclass(t, str) else (*g,) for t, g in groupby(parts, type)))
    # convert any free strings to ASTs
    parts = [p if not isinstance(p, str) else String(p) for p in parts]

    # cast because pyright complains
    parts = cast(list[AST], parts)
    return IString(parts)


def as_dict_inners(items:list[AST]) -> list[PointsTo] | None:
    """Determine if the inner items indicate the container is a Dict (i.e. all items are points-to)"""
    if all(isinstance(i, PointsTo) for i in items):
        return cast(list[PointsTo], items)
    return None

def as_bidir_dict_inners(items:list[AST]) -> list[BidirPointsTo] | None:
    """Determine if the inner items indicate the container is a BidirDict (i.e. all items are bidir-points-to)"""
    if all(isinstance(i, BidirPointsTo) for i in items):
        return cast(list[BidirPointsTo], items)
    return None

def as_array_inners(items:list[AST]) -> list[AST] | None:
    """Determine if the inner items indicate the container is an Array (i.e. no points-to, assigns, or declarations)"""
    invalid_types = (Declare, Assign, PointsTo, BidirPointsTo)
    if any(isinstance(i, invalid_types) for i in items):
        return None
    return items

def as_object_inners(items:list[AST]) -> list[AST] | None:
    """determine if the inner items indicate the container is an Object (i.e. no points-to, and should contain at least one assign or declaration)"""
    invalid_types = (PointsTo, BidirPointsTo)
    expected_types = (Assign, Declare)
    if any(isinstance(i, invalid_types) for i in items):
        return None
    if not any(isinstance(i, expected_types) for i in items):
        return None
    return items

def parse_block(block: Block_t, scope: Scope) -> AST:
    """Convert a block token to an AST"""

    # if new scope block, nest the current scope
    newscope = block.left == '{' and block.right == '}'
    if newscope:
        scope = Scope(scope)

    # parse the inside of the block
    inner = parse(block.body, scope)

    delims = block.left + block.right
    match delims, inner:
        case '()' | '{}' | '[]', Void():
            return inner
        case '()', ListOfASTs():
            return Group(inner.asts)
        case '{}', ListOfASTs():
            return Block(inner.asts)
        case '[]', ListOfASTs():
            if (asts:=as_dict_inners(inner.asts)) is not None:
                return Dict(asts)
            elif (asts:=as_bidir_dict_inners(inner.asts)) is not None:
                return BidirDict(asts)
            elif (asts:=as_array_inners(inner.asts)) is not None:
                return Array(asts)
            elif (asts:=as_object_inners(inner.asts)) is not None:
                return Object(inner.asts)
            #error cases
            if any(isinstance(i, PointsTo) for i in inner.asts) and not all(isinstance(i, PointsTo) for i in inner.asts):
                raise ValueError(f"ERROR: cannot mix PointsTo with other types in a dict: {inner=}")
            #TBD other known cases
            #otherwise there is an issue with the parser
            raise ValueError(f"INTERNAL ERROR: could not determine container type for {inner=}. Should have been suitably disambiguated by parser...")
        case '()' | '[]' | '(]' | '[)', BareRange():
            return Range(inner.left, inner.right, delims)

        # catch all cases for any type of AST inside a block or range
        case '()', _:
            return Group([inner])
        case '{}', _:
            return Block([inner])
        case '[]', PointsTo():
            return Dict([inner])
        case '[]', BidirPointsTo():
            return BidirDict([inner])
        case '[]', Assign() | Declare():
            return Object([inner])
        case '[]', _:
            # TODO: handle if this should be an object or dictionary instead of an array
            return Array([inner])
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'block parse not implemented for {block.left+block.right}, {type(inner)}')



def parse_type_param(param: TypeParam_t, scope: Scope) -> TypeParam:
    items = parse(param.body, scope)
    if isinstance(items, ListOfASTs):
        return TypeParam(items.asts)
    return TypeParam([items])


def parse_flow(flow: Flow_t, scope: Scope) -> Flowable:

    # special case for closing else clause in a flow chain. Treat as `<if> <true> <clause>`
    if flow.keyword is None:
        return Default(parse_chain(flow.clause, scope))

    assert flow.condition is not None, f"ERROR: flow condition must be present for {flow=}"
    cond = parse_chain(flow.condition, scope)
    clause = parse_chain(flow.clause, scope)

    match flow.keyword:
        case Keyword_t(src='if'): return If(cond, clause)
        case Keyword_t(src='loop'): return Loop(cond, clause)
        case _:
            pdb.set_trace()
            ...
            raise NotImplementedError('TODO: other flow keywords, namely lazy')
    pdb.set_trace()
    ...


def parse_declare(declare: Declare_t, scope: Scope) -> Declare:
    expr = parse_chain(declare.expr, scope)
    assert isinstance(expr, (PrototypeIdentifier, Identifier, TypedIdentifier, TypedGroup, UnpackTarget, Assign)), f'ERROR: expected identifier, typed-identifier, or unpack target for declare expression, got {expr=}'
    match declare:
        case Declare_t(keyword=Keyword_t(src='let')): return Declare(DeclarationType.LET, expr)
        case Declare_t(keyword=Keyword_t(src='const')): return Declare(DeclarationType.CONST, expr)
        # case Declare_t(keyword=Keyword_t(src='local_const')): return Declare(DeclarationType.LOCAL_CONST, expr)
        # case Declare_t(keyword=Keyword_t(src='fixed_type')): return Declare(DeclarationType.FIXED_TYPE, expr)
        case _:
            raise ValueError(f"ERROR: unknown declare keyword {declare.keyword=}. Expected one of {DeclarationType.__members__}. {declare=}")
    pdb.set_trace()
    raise NotImplementedError






################################ Docs Markdown Helpers ################################
opname_map = {
    '@': 'reference',
    '.': 'access',
    '^': 'power',
    '*': 'multiply',
    '/': 'divide',
    '%': 'modulus',
    '+': 'add',
    '-': 'subtract',
    '<<': 'left shift',
    '>>': 'right shift',
    '>>>': 'rotate left no carry',
    '<<<': 'rotate right no carry',
    '<<!': 'rotate left with carry',
    '!>>': 'rotate right with carry',
    '>?': 'greater than',
    '<?': 'less than',
    '>=?': 'greater than or equal',
    '<=?': 'less than or equal',
    '=?': 'equal',
    'and': 'and',
    'nand': 'nand',
    '&': 'and',
    'xor': 'xor',
    'xnor': 'xnor',
    'or': 'or',
    'nor': 'nor',
    '|': 'or',
    '=>': 'function arrow',
    '=': 'bind',
    'else': 'flow alternate',
    ';': 'semicolon',
    'in': 'in',
    'as': 'as',
    'transmute': 'transmute',
    '|>': 'pipe',
    '<|': 'reverse pipe',
    '->': 'right pointer',
    '<->': 'bidir pointer',
    '<-': 'left pointer',
    ':': 'type annotation',

    Comma_t(None): 'comma',
    Juxtapose_t(None): 'unknown juxtapose',
    EllipsisJuxtapose_t(None): 'ellipsis juxtapose',
    RangeJuxtapose_t(None): 'range juxtapose',
    TypeParamJuxtapose_t(None): 'type param juxtapose',
}


def get_precedence_table_markdown() -> str:
    """return a string that is the markdown table for the docs containing all the operators"""
    header = '| Precedence | Operator | Name | Associativity |\n| --- | --- | --- | --- |'

    def get_ops_str(ops: list[Operator_t | ShiftOperator_t | Juxtapose_t | Comma_t]) -> str:
        return '<br>'.join(f'`{op.op if isinstance(op, (Operator_t, ShiftOperator_t)) else op.__class__.__name__[:-2].lower()}`' for op in ops)

    def get_opnames_str(ops: list[Operator_t | ShiftOperator_t | Juxtapose_t | Comma_t]) -> str:
        return '<br>'.join(f'{opname_map.get(op.op, None) if isinstance(op, (Operator_t, ShiftOperator_t)) else op.__class__.__name__[:-2].lower()}' for op in ops)

    def get_row_str(row: tuple[Associativity, list[Operator_t | ShiftOperator_t | Juxtapose_t | Comma_t]]) -> str:
        assoc, group = row
        return f'{get_ops_str(group)} | {get_opnames_str(group)} | {assoc.name}'

    rows = [
        f'| {i} | {get_row_str(row)} |'
        for i, row in reversed([*enumerate(operator_groups)])
    ]

    return header + '\n' + '\n'.join(rows)
18:T53e7,from .tokenizer import (
    tokenize, tprint, full_traverse_tokens,
    unary_prefix_operators,
    unary_postfix_operators,
    binary_operators,
    opchain_starters,
    Token,
    Keyword_t, Undefined_t, Void_t, End_t,
    WhiteSpace_t, Escape_t,
    Identifier_t, Hashtag_t,
    Block_t, TypeParam_t,
    RawString_t, String_t,
    Integer_t, BasedNumber_t, Boolean_t,
    DotDot_t, DotDotDot_t,
    Juxtapose_t, Operator_t, ShiftOperator_t, Comma_t,
)

from typing import Generator, overload, cast
from abc import ABC


import pdb


# A chain is just a list of tokens that is known to be directly parsable as an expression without any other syntax
# i.e. it is the result of calls to `get_next_chain()`
# all other syntax is wrapped up into compound tokens
# it should literally just be a sequence of atoms and operators
from typing import TypeVar
T = TypeVar('T', bound=Token)
class Chain(list[T]):
    """class for explicitly annotating that a token list is a single chain"""

# class Chain[T](list[T]):
#     """class for explicitly annotating that a token list is a single chain"""


############### NEW TOKENS CREATED BY POST-TOKENIZATION PROCESS ###############

class Flow_t(Token):
    @overload
    def __init__(self, keyword: None, condition: None, clause: Chain[Token]): ...  # closing else
    @overload
    def __init__(self, keyword: Keyword_t, condition: Chain[Token], clause: Chain[Token]): ...  # if, loop, lazy

    def __init__(self, keyword: Keyword_t | None, condition: Chain[Token] | None, clause: Chain[Token]):
        if keyword is None and condition is not None:
            raise ValueError("closing else should have no condition. `keyword` and `condition` should both be None")
        self.keyword = keyword
        self.condition = condition
        self.clause = clause

    def __repr__(self) -> str:
        return f"<Flow_t: {self.keyword}: {self.condition} {self.clause}>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        if self.condition is not None:
            yield self.condition
        yield self.clause


# class Do_t(Token):...
# class Return_t(Token):...
# class Express_t(Token):...


class Declare_t(Token):
    def __init__(self, keyword: Keyword_t, expr: Chain[Token]):
        self.keyword = keyword
        self.expr = expr

    def __repr__(self) -> str:
        return f"<Declare_t: {self.keyword} {self.expr}>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield [self.keyword] #appraently flow doesn't yield the keyword. tbd if it matters...
        yield self.expr


class RangeJuxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __repr__(self) -> str:
        return "<RangeJuxtapose_t>"

    def __hash__(self) -> int:
        return hash(RangeJuxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, RangeJuxtapose_t)


class EllipsisJuxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __repr__(self) -> str:
        return "<EllipsisJuxtapose_t>"

    def __hash__(self) -> int:
        return hash(EllipsisJuxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, EllipsisJuxtapose_t)


class TypeParamJuxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __repr__(self) -> str:
        return "<TypeParamJuxtapose_t>"

    def __hash__(self) -> int:
        return hash(TypeParamJuxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, TypeParamJuxtapose_t)

class OpChain_t(Token):
    def __init__(self, ops:list[Operator_t]):
        assert len(ops) > 1, f"OpChain_t must have at least 2 operators. Got {len(ops)} operators"
        self.ops = ops

    def __repr__(self) -> str:
        return f"<OpChain_t: {''.join(op.op for op in self.ops)}>"

    def __hash__(self) -> int:
        return hash((OpChain_t, tuple(self.ops)))

    def __eq__(self, other) -> bool:
        return isinstance(other, OpChain_t) and self.ops == other.ops

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield cast(list[Token], self.ops)

class VectorizedOp_t(Token):
    def __init__(self, dot:Operator_t, op:Operator_t|OpChain_t):
        assert isinstance(dot, Operator_t) and dot.op == '.', f"VectorizedOp_t must have a '.' operator. Got {dot}"
        self.dot = dot
        self.op = op

    def __repr__(self) -> str:
        return f"<VectorizedOp_t: {self.dot}, {self.op}>"

    def __hash__(self) -> int:
        return hash((VectorizedOp_t, self.dot, self.op))

    def __eq__(self, other) -> bool:
        return isinstance(other, VectorizedOp_t) and self.dot == other.dot and self.op == other.op

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield [self.dot]
        yield [self.op]


class CombinedAssignmentOp_t(Token):
    def __init__(self, op:Operator_t, assign:Operator_t):
        assert isinstance(assign, Operator_t) and assign.op == '=', f"CombinedAssignmentOp_t must have an '=' operator. Got {assign}"
        self.op = op
        self.assign = assign

    def __repr__(self) -> str:
        return f"<CombinedAssignmentOp_t: {self.op}, {self.assign}>"

    def __hash__(self) -> int:
        return hash((CombinedAssignmentOp_t, self.op, self.assign))

    def __eq__(self, other) -> bool:
        return isinstance(other, CombinedAssignmentOp_t) and self.op == other.op and self.assign == other.assign

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield [self.op]
        yield [self.assign]


atom_tokens = (
    Identifier_t,
    Integer_t,
    Boolean_t,
    BasedNumber_t,
    RawString_t,
    String_t,
    Block_t,
    TypeParam_t,
    Hashtag_t,
    DotDot_t,
    DotDotDot_t,
    Flow_t,
    Undefined_t,
)


class ShouldBreakTracker(ABC):
    def op_breaks_chain(self, token: Token) -> bool:
        raise NotImplementedError("op_breaks_chain must be implemented by subclass")

    def view(self, tokens: list[Token]) -> None:
        raise NotImplementedError("view must be implemented by subclass")


class ShouldBreakFlowTracker(ShouldBreakTracker):
    def __init__(self):
        self.flows_seen = 0

    def op_breaks_chain(self, token: Token) -> bool:
        # should only be operators
        if isinstance(token, Operator_t) and token.op == 'else':
            if self.flows_seen == 0:
                return True
            self.flows_seen -= 1

        return False

    def view(self, tokens: list[Token]) -> None:
        # view each token without any ability to do anything
        # keep track of how many flows we've seen
        for token in tokens:
            if isinstance(token, Flow_t) and token.keyword is not None:
                self.flows_seen += 1
            if isinstance(token, Operator_t) and token.op == 'else':
                raise ValueError("should not be seeing else here")
            if isinstance(token, Keyword_t) and token.src in ('if', 'loop', 'lazy'):
                raise ValueError("should not be seeing if/loop/lazy here. Everything should be bundled up into a flow")


def invert_whitespace(tokens: list[Token]) -> None:
    """
    removes all whitespace tokens, and insert juxtapose tokens between adjacent pairs (i.e. not separated by whitespace)

    Args:
        tokens (list[Token]): list of tokens to modify. This is modified in place.
    """

    # juxtapose singleton token so we aren't wasting memory
    jux = Juxtapose_t(None)

    i = 0
    while i < len(tokens):
        # delete whitespace if it comes up
        if isinstance(tokens[i], WhiteSpace_t):
            tokens.pop(i)
            continue

        # recursively handle inverting whitespace for blocks
        if isinstance(tokens[i], (Block_t, TypeParam_t)):
            invert_whitespace(tokens[i].body)
        elif isinstance(tokens[i], String_t):
            for child in tokens[i].body:
                if isinstance(child, Block_t):
                    invert_whitespace(child.body)

        # insert juxtapose if no whitespace between tokens
        if i + 1 < len(tokens) and not isinstance(tokens[i + 1], WhiteSpace_t):
            tokens.insert(i + 1, jux)
            i += 1
        i += 1

    # finally, remove juxtapose tokens next to operators that are not whitespace sensitive
    i = 1
    while i < len(tokens) - 1:
        left, middle, right = tokens[i-1:i+2]
        if isinstance(middle, Juxtapose_t) and (isinstance(left, (Operator_t, ShiftOperator_t, Comma_t)) or isinstance(right, (Operator_t, ShiftOperator_t, Comma_t))):
            tokens.pop(i)
            continue
        i += 1


def _get_next_prefixes(tokens: list[Token]) -> tuple[list[Token], list[Token]]:
    prefixes = []
    # isinstance(tokens[0], Operator_t) and tokens[0].op in unary_prefix_operators:
    while len(tokens) > 0 and is_unary_prefix_op(tokens[0]):
        prefixes.append(tokens.pop(0))
    return prefixes, tokens


def _get_next_postfixes(tokens: list[Token]) -> tuple[list[Token], list[Token]]:
    postfixes = []
    # isinstance(tokens[0], Operator_t) and tokens[0].op in unary_postfix_operators - {';'}:
    while len(tokens) > 0 and is_unary_postfix_op(tokens[0], exclude_semicolon=True):
        postfixes.append(tokens.pop(0))
    return postfixes, tokens


def _get_next_atom(tokens: list[Token]) -> tuple[Token, list[Token]]:
    if len(tokens) == 0:
        raise ValueError(f"ERROR: expected atom, got {tokens=}")

    # TODO: this is going to be unnecessary as expressions will have been bundled up into single tokens
    if isinstance(tokens[0], Keyword_t):
        return _get_next_keyword_expr(tokens)

    # (Integer_t, BasedNumber_t, String_t, RawString_t, Identifier_t, Hashtag_t, Block_t, TypeParam_t, DotDot_t)):
    if isinstance(tokens[0], atom_tokens):
        return tokens[0], tokens[1:]

    raise ValueError(f"ERROR: expected atom, got {tokens[0]=}")


def _get_next_chunk(tokens: list[Token]) -> tuple[list[Token], list[Token]]:
    chunk = []
    t, tokens = _get_next_prefixes(tokens)
    chunk.extend(t)

    t, tokens = _get_next_atom(tokens)
    if t is None:
        raise ValueError(f"ERROR: expected atom, got {tokens[0]=}")
    chunk.append(t)

    t, tokens = _get_next_postfixes(tokens)
    chunk.extend(t)

    return chunk, tokens


def is_unary_prefix_op(token: Token) -> bool:
    """
    Determines if a token could be a unary prefix operator.
    Note that this is not mutually exclusive with being a postfix operator or a binary operator.
    """
    return isinstance(token, Operator_t) and token.op in unary_prefix_operators


def is_unary_postfix_op(token: Token, exclude_semicolon: bool = False) -> bool:
    """
    Determines if a token could be a unary postfix operator.
    Optionally can exclude semicolon from the set of operators.
    Note that this is not mutually exclusive with being a prefix operator or a binary operator.
    """
    if exclude_semicolon:
        return isinstance(token, Operator_t) and token.op in unary_postfix_operators - {';'}
    return isinstance(token, Operator_t) and token.op in unary_postfix_operators


def is_binop(token: Token) -> bool:
    """
    Determines if a token could be a binary operator.
    Note that this is not mutually exclusive with being a prefix operator or a postfix operator.
    """
    return isinstance(token, Operator_t) and token.op in binary_operators or isinstance(token, (ShiftOperator_t, Comma_t, Juxtapose_t, RangeJuxtapose_t, EllipsisJuxtapose_t, TypeParamJuxtapose_t))


def is_op(token: Token) -> bool:
    return is_binop(token) or is_unary_prefix_op(token) or is_unary_postfix_op(token)


def is_opchain_starter(token: Token) -> bool:
    return isinstance(token, Operator_t) and token.op in opchain_starters


def _get_next_keyword_expr(tokens: list[Token]) -> tuple[Token, list[Token]]:
    """package up the next keyword expression into a single token"""
    if len(tokens) == 0:
        raise ValueError(f"ERROR: expected keyword expression, got {tokens=}")
    t, tokens = tokens[0], tokens[1:]

    if not isinstance(t, Keyword_t):
        raise ValueError(f"ERROR: expected keyword expression, got {t=}")

    match t:
        case Keyword_t(src='if' | 'loop' | 'lazy'):
            cond, tokens = get_next_chain(tokens)
            clause, tokens = get_next_chain(tokens, tracker=ShouldBreakFlowTracker())
            return Flow_t(t, cond, clause), tokens
        case Keyword_t(src='closing_else'):
            clause, tokens = get_next_chain(tokens, tracker=ShouldBreakFlowTracker())
            return Flow_t(None, None, clause), tokens
        case Keyword_t(src='do'):
            clause, tokens = get_next_chain(tokens)
            # assert next token is a do_keyward
            # depending on the keyward, get a condition, or condition+clause
            pdb.set_trace()
            ...
        case Keyword_t(src='return'):
            # TBD how to do this one...
            pdb.set_trace()
            ...
        case Keyword_t(src='express'):
            pdb.set_trace()
            ...
        case Keyword_t(src='let' | 'const' | 'local_const' | 'fixed_type'):
            expr, tokens = get_next_chain(tokens)
            return Declare_t(t, expr), tokens


    raise NotImplementedError("TODO: handle keyword based expressions")
    # return #chain?
    # yield #chain
    # (break | continue) #hashtag? //note the hashtag should be an entire chain if present
    # (let | const) #chain


def get_next_chain(tokens: list[Token], *, tracker: ShouldBreakTracker = None, op_blacklist: set[Token] = None) -> tuple[Chain[Token], list[Token]]:
    """
    grab the next single expression chain of tokens from the given list of tokens

    Also wraps up keyword-based expressions (if loop etc.) into a single token

    A chain is represented by the following grammar:
        #chunk = #prefix_op* #atom_expr (#postfix_op - ';')*
        #chain = #chunk (#binary_op #chunk)* ';'?

    Args:
        tokens (list[Token]): list of tokens to grab the next chain from
        tracker (ShouldBreakTracker, optional): tracker for complex analysis to determine if an operator should break the chain. Defaults to None.
        op_blacklist (set[Token], optional): simpler handler for operators that should break the chain. Defaults to None.

    Returns:
        next, rest (list[Token], list[Token]): the next chain of tokens, and the remaining tokens
    """

    if op_blacklist is None:
        op_blacklist = set()

    chain = []

    # grab the first chunk and let the tracker view it
    chunk, tokens = _get_next_chunk(tokens)
    chain.extend(chunk)
    if tracker is not None:
        tracker.view(chunk)

    while len(tokens) > 0 and is_binop(tokens[0]) and (tracker is None or not tracker.op_breaks_chain(tokens[0])) and tokens[0] not in op_blacklist:
        # get the operator, and continuing chunk, then let the tracker view it
        chain.append(tokens.pop(0))
        chunk, tokens = _get_next_chunk(tokens)
        chain.extend(chunk)
        if tracker is not None:
            tracker.view(chunk)

    # if there's a semicolon, it ends the chain
    if len(tokens) > 0 and isinstance(tokens[0], Operator_t) and tokens[0].op == ';':
        chain.append(tokens.pop(0))

    return Chain(chain), tokens


def narrow_juxtapose(tokens: list[Token]) -> None:
    """
    range juxtapose:
    convert [<token>, <jux>, <..>] into [<token>, <range_jux>, <..>]
    convert [<..>, <jux>, <token>] into [<..>, <range_jux>, <token>]
    if .. doesn't connect to anything on the left or right, connect it to undefined

    ellipsis juxtapose:
    convert [<...>, <jux>, <token>] into [<...>, <ellipsis_jux>, <token>]

    type param juxtapose:
    convert [<token>, <jux>, <type_param>] into [<token>, <type_param_jux>, <type_param>]
    convert [<type_param>, <jux>, <token>] into [<type_param>, <type_param_jux>, <token>]
    """
    range_jux = RangeJuxtapose_t(None)
    ellipsis_jux = EllipsisJuxtapose_t(None)
    type_param_jux = TypeParamJuxtapose_t(None)
    undefined = Undefined_t(None)
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        # handle range jux
        if isinstance(token, DotDot_t):
            if i + 1 < len(stream):
                if isinstance(stream[i+1], Juxtapose_t):
                    stream[i+1] = range_jux
                else:
                    stream[i+1:i+1] = [range_jux, undefined]
            if i > 0:
                if isinstance(stream[i-1], Juxtapose_t):
                    stream[i-1] = range_jux
                else:
                    stream[i:i] = [undefined, range_jux]
                    gen.send(i+3)

        # handle ellipsis jux
        elif isinstance(token, DotDotDot_t):
            if i + 1 < len(stream) and isinstance(stream[i+1], Juxtapose_t):
                stream[i+1] = ellipsis_jux

        # handle type param jux
        elif isinstance(token, TypeParam_t):
            if i > 0 and isinstance(stream[i-1], Juxtapose_t):
                stream[i-1] = type_param_jux
            elif i + 1 < len(stream) and isinstance(stream[i+1], Juxtapose_t):
                stream[i+1] = type_param_jux


def convert_bare_else(tokens: list[Token]) -> None:
    """
    convert any instances of `else` without a flow keyword after, and convert to `else` `if` `true`
    """
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if isinstance(token, Operator_t) and token.op == 'else':
            if i+1 < len(stream) and isinstance(stream[i+1], Keyword_t) and stream[i+1].src in ('if', 'loop', 'lazy'):
                continue
            # stream[i+1:i+1] = [Keyword_t('if'), Boolean_t('true')]
            stream.insert(i+1, Keyword_t('closing_else'))


def bundle_conditionals(tokens: list[Token]) -> None:
    """
    Convert sequences of tokens that represent conditionals (if, loop, etc.) into a single expression token
    """
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if isinstance(token, Keyword_t) and token.src in ('if', 'loop', 'lazy'):
            flow_chain, tokens = get_next_chain(stream[i:])
            stream[i] = flow_chain[0]
            stream[i+1:] = [*flow_chain[1:], *tokens]


def chain_operators(tokens: list[Token]) -> None:
    """Convert consecutive operator tokens into a single opchain token"""
    """
    A chain is represented by the following grammar:
        #chunk = #prefix_op* #atom_expr (#postfix_op - ';')*
        #chain = #chunk (#binary_op #chunk)* ';'?

        #prefix_op = '+' | '-' | '*' | '/' | 'not' | '@' | '...'
        #postfix_op = '?' | '`' | ';'
        #binary_op = '+' | '-' | '*' | '/' | '%' | '^'
          | '=?' | '>?' | '<?' | '>=?' | '<=?' | 'in?' | 'is?' | 'isnt?' | '<=>'
          | '|' | '&'
          | 'and' | 'or' | 'nand' | 'nor' | 'xor' | 'xnor' | '??'
          | '=' | ':=' | 'as' | 'in' | 'transmute'
          | '@?'
          | '|>' | '<|' | '=>'
          | '->' | '<->' | '<-'
          | '.' | ':'
    """

    # TODO: skip for now. not needed by hello world
    # also may not be necessary if we use a pratt parser. was necessary for split by lowest precedence parser

    for i, token, stream in (gen := full_traverse_tokens(tokens)):

        # TODO: this is not a correct way to detect these. need to verify that the operators are in between two #chunks
        #   this will be conservative, but for now it will let us do a hello world happy path
        if is_opchain_starter(token):
            j = 1
            while i+j < len(stream) and is_unary_prefix_op(stream[i+j]):
                j += 1
            if j > 1:
                pdb.set_trace()
                raise NotImplementedError('opchaining has not been implemented yet')


def post_process(tokens: list[Token]) -> None:
    """post process the tokens to make them ready for parsing"""

    # remove whitespace, and insert juxtapose tokens
    invert_whitespace(tokens)

    if len(tokens) == 0:
        return

    # find any instances of <else> without a flow keyword after, and convert to <else> <if> <true>
    convert_bare_else(tokens)

    # bundle up conditionals into single token expressions
    bundle_conditionals(tokens)

    # combine operator chains into a single operator token
    chain_operators(tokens)

    # convert juxtapose tokens to more specific types if possible
    narrow_juxtapose(tokens)


    # make the actual list of chains

    # based on types, replace jux with jux_mul or jux_call
    # TODO: actually this probably would need to be done during parsing, since we can't get a type for a complex/compound expression...


def test():
    with open('../../../examples/hello.dewy') as f:
        src = f.read()

    tokens = tokenize(src)

    # chainer process
    post_process(tokens)

    pdb.set_trace()
    ...


def test2():
    """gauntlet of multiple tests from example file"""
    with open('../../../examples/syntax3.dewyl') as f:
        lines = f.readlines()

    # filter out empty lines
    lines = [l for line in lines if (l := line.strip())]

    for line in lines:
        tokens = tokenize(line)

        # chainer process
        post_process(tokens)

        # other stuff? pass to the parser? etc.

    pdb.set_trace()
    ...


def test_hello():
    line = "printl'Hello, World!'"

    tokens = tokenize(line)
    post_process(tokens)

    pdb.set_trace()
    ...


if __name__ == '__main__':
    # test()
    # test2()
    test_hello()
19:T17d0,from .syntax import (
    AST,
    Access,
    Declare,
    PointsTo, BidirPointsTo,
    Type,
    ListOfASTs, Tuple, Block, BareRange, Ellipsis, Spread, Array, Group, Range, Object, Dict, BidirDict, TypeParam,
    Void, Undefined, void, undefined, untyped,
    String, IString,
    Flowable, Flow, If, Loop, Default,
    FunctionLiteral, PrototypePyAction, PyAction, Call,
    Index,
    PrototypeIdentifier, Express, Identifier, TypedIdentifier, TypedGroup, UnpackTarget, Assign,
    Int, Bool,
    Range, IterIn,
    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,
    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv,
    DeclarationType,
    DeclareGeneric, Parameterize,
)
from typing import Generator

"""after the main parsing, post parse to handle any remaining prototype asts within the main ast"""
import pdb


def post_parse(ast: AST) -> AST:

    # any conversions should probably run simplest to most complex
    ast = convert_prototype_tuples(ast)
    ast = convert_bare_ranges(ast)
    ast = convert_prototype_identifiers(ast)

    # at the end of the post parse process
    if not ast.is_settled():
        raise ValueError(f'INTERNAL ERROR: Parse was incomplete. AST still has prototypes\n{ast!r}')

    return ast


def convert_prototype_identifiers(ast: AST) -> AST:
    """Convert all PrototypeIdentifiers to either Identifier or Express, depending on the context"""
    ast = Group([ast])
    for i in (gen := ast.__full_traversal_iter__()):

        # skip ASTs that don't have any Prototypes
        if i.is_settled():
            continue

        match i:
            # if we ever get to a bare identifier, treat it like an express
            case PrototypeIdentifier(name=name):
                gen.send(Express(Identifier(name)))
            case Call(f=PrototypeIdentifier(name=name), args=args):
                gen.send(Call(Identifier(name), args))
            case Call(args=None): ...
            # case Call(args=): ... #TODO: handling when args is not none... generally will be a list of identifiers that need to be converted directly to Identifier
            case Call():
                pdb.set_trace()
                ...
            case Assign(left=PrototypeIdentifier(name=name), right=right):
                gen.send(Assign(Identifier(name), right))
            case Assign(left=Array() as arr, right=right):
                target = convert_prototype_to_unpack_target(arr)
                gen.send(Assign(target, right))
            case Assign():
                pdb.set_trace()
                ...
            case IterIn(left=PrototypeIdentifier(name=name), right=right):
                gen.send(IterIn(Identifier(name), right))
            case IterIn(left=Array() as arr, right=right):
                target = convert_prototype_to_unpack_target(arr)
                gen.send(IterIn(target, right))
            case IterIn():
                pdb.set_trace()
                ...
            case UnpackTarget():
                pdb.set_trace()
                ...
            case Declare():
                pdb.set_trace()
                ...
            case Index():
                pdb.set_trace()
                ...
            case Access():
                pdb.set_trace()
                ...

            # cases that themselves don't get adjusted but may contain nested children that need to be converted
            case IString() | Group() | Block() | Tuple() | Array() | Object() | Dict() | BidirDict() | FunctionLiteral() | Range() | Loop() | If() | Flow() | Default() \
                | PointsTo() | BidirPointsTo() | Equal() | Less() | LessEqual() | Greater() | GreaterEqual() | LeftShift() | RightShift() | LeftRotate() | RightRotate() | LeftRotateCarry() | RightRotateCarry() | Add() | Sub() | Mul() | Div() | IDiv() | Mod() | Pow() | And() | Or() | Xor() | Nand() | Nor() | Xnor() | MemberIn() \
                | Not() | UnaryPos() | UnaryNeg() | UnaryMul() | UnaryDiv():
                ...
            #TBD cases: Type() | ListOfASTs() | BareRange() | Ellipsis() | Spread() | TypeParam() | Flowable() | Flow() | PrototypePyAction() | PyAction() | Express() | TypedIdentifier() | TypedGroup() | SequenceUnpackTarget() | ObjectUnpackTarget() | DeclarationType() | DeclareGeneric() | Parameterize():
            case _:  # all others are traversed as normal
                raise ValueError(f'Unhandled case {type(i)}')
            #     pdb.set_trace()
            #     ...

    return ast.items[0]


def convert_prototype_to_unpack_target(ast: Array) -> UnpackTarget:
    """Convert an Array of PrototypeIdentifiers or other ASTs to an UnpackTarget"""
    for i in (gen := ast.__full_traversal_iter__()):
        if i.is_settled():
            continue

        match i:
            case PrototypeIdentifier(name=name):
                gen.send(Identifier(name))
            case Assign(left=PrototypeIdentifier(name=name), right=right):
                gen.send(Assign(Identifier(name), right))
            case Array() as arr:
                gen.send(convert_prototype_to_unpack_target(arr))
            case Spread():
                gen.send(i)
            case TypedIdentifier():
                gen.send(i)
            case _:
                raise NotImplementedError(f'Unhandled case {type(i)} in convert_prototype_to_unpack_target')

    return UnpackTarget(ast.items)


def convert_prototype_tuples(ast: AST) -> AST:
    """For now, literally just turn all tuples into arrays"""
    ast = Group([ast])
    for i in (gen := ast.__full_traversal_iter__()):
        if isinstance(i, Tuple):
            gen.send(Array(i.items))
    return ast.items[0]

def convert_bare_ranges(ast: AST) -> AST:
    """Convert all BareRanges to Ranges with inclusive bounds"""
    ast = Group([ast])
    for i in (gen := ast.__full_traversal_iter__()):
        if isinstance(i, BareRange):
            gen.send(Range(i.left, i.right, '[]'))
    return ast.items[0]1a:T4eaa,from abc import ABC, abstractmethod, ABCMeta
from typing import get_args, get_origin, Generator, Any, Literal, Union, Type as TypingType, dataclass_transform, Callable as TypingCallable
from types import UnionType
from dataclasses import dataclass, field, fields
from enum import Enum, auto
# from fractions import Fraction

from .tokenizer import Operator_t, escape_whitespace  # TODO: move into utils

import pdb


@dataclass_transform()
class AST(ABC):
    def __init_subclass__(cls: TypingType['AST'], **kwargs):
        """
        - automatically applies the dataclass decorator with repr=False to AST subclasses
        """
        super().__init_subclass__(**kwargs)

        # Apply the dataclass decorator with repr=False to the subclass
        dataclass(repr=False)(cls)

    # TODO: add property to all ASTs for function complete/locked/etc. meaning it and all children are settled
    @abstractmethod
    def __str__(self) -> str:
        """Return a string representation of the AST in a canonical dewy code format"""

    def __repr__(self) -> str:
        """
        Returns a string representation of the AST tree with correct indentation for each sub-component

        e.g.
        SomeAST(prop0=..., prop1=...)
        ├── child0=SomeSubAST(...)
        ├── child1=SomeOtherAST(...)
        │   ├── a=ThisAST(...)
        │   └── b=ThatAST(...)
        └── child2=AST2(...)
            └── something=ThisLastAST(...)

        Where all non-ast attributes of a node are printed on the same line as the node itself
        and all children are recursively indented a level and printed on their own line
        """
        return '\n'.join(self._gentree())

    def _gentree(self, prefix: str = '') -> Generator[str, None, None]:
        """
        a recursive generator helper function for __repr__

        Args:
            prefix: str - the string to prepend to each child line (root line already has prefix)
            name: str - the name of the current node in the tree
            # draw_branches: bool - whether each item should be drawn with branches or only use whitespace

        Returns:
            str: the string representation of the AST tree
        """
        # prefix components:
        space = '    '
        branch = '│   '
        # pointers:
        tee = '├── '
        last = '└── '

        attrs_str = ', '.join(f'{k}={v}' for k, v in self.__iter_members__() if not isinstance(v, AST))
        yield f'{self.__class__.__name__}({attrs_str})'
        children = tuple((k, v) for k, v in self.__iter_members__() if isinstance(v, AST))
        pointers = [tee] * (len(children) - 1) + [last]
        for (k, v), pointer in zip(children, pointers):
            extension = branch if pointer == tee else space
            gen = v._gentree(f'{prefix}{extension}')
            name = f'{k}=' if k else ''
            yield f'{prefix}{pointer}{name}{next(gen)}'     # first line gets name and pointer
            yield from gen                                  # rest of lines already have a prefix

    def __iter_members__(self) -> Generator[tuple[str, Any], 'AST', None]:
        """
        A method for getting all properties on the AST instance (including child ASTs, and non-AST properties).
        Returns a generator of tuples of the form (property_name, property_value)
        Allows replacing the current AST with a new one during iteration via .send()
        NOTE: Does not recurse into child ASTs
        """
        for key, value in self.__dict__.items():
            # any direct children are ASTs
            if isinstance(value, AST):
                replacement = yield key, value
                if replacement is not None:
                    setattr(self, key, replacement)
                    yield

            # any direct children are containers of ASTs
            elif is_ast_container(self.__class__.__annotations__.get(key)):
                if value is None:
                    continue
                if isinstance(value, list):
                    for i, item in enumerate(value):
                        replacement = yield '', item
                        if replacement is not None:
                            value[i] = replacement
                            yield
                # elif isinstance(value, some_other_container_type): ...
                else:
                    raise NotImplementedError(f'__iter_members__ over {type(value)} (from member "{key}") of {self} is not yet implemented')

            # properties that are not ASTs
            else:
                _ = yield key, value
                assert _ is None, f'ILLEGAL: attempted to replace non-AST value "{key}" during __iter_members__ for ast {self}'

    def __iter__(self) -> Generator['AST', None, None]:
        """DEPRECATED: Use __iter_asts__ instead"""
        raise DeprecationWarning(f'__iter__ is deprecated. Use __iter_asts__ instead')

    def __iter_asts__(self) -> Generator['AST', None, None]:
        """Return a generator of the direct children ASTs of the AST"""
        for _, child in self.__iter_members__():
            if isinstance(child, AST):
                yield child


    def __full_traversal_iter__(self) -> Generator['AST', 'AST', None]:
        """
        Recursive in-order traversal of all child ASTs of the current AST instance
        Has ability to replace the current AST with a new one during iteration via .send()
        """
        for _, child in (gen := self.__iter_members__()):
            if isinstance(child, AST):
                replacement = yield child
                if replacement is not None:
                    gen.send(replacement)
                    yield
                    child = replacement # allow traversal over all children of the replacement
                yield from child.__full_traversal_iter__()


    def is_settled(self) -> bool:
        """Return True if the neither the AST, nor any of its descendants, are prototypes"""
        for child in self.__iter_asts__():
            if not child.is_settled():
                return False
        return True


def is_ast_container(type_hint: TypingType | None) -> bool:
    """
    Determine if the type hint is a container of ASTs.
    e.g. list[AST], set[SomeSubclassOfAST|OtherSubclassOfAST], etc.

    Args:
        type_hint: TypingType | None - the type hint to check. If None, returns False

    Returns:
        bool: True if any of the contained types are subclasses of AST, False otherwise
    """
    if type_hint is None:
        return False

    # Iterate over all contained types
    args = get_args(type_hint)
    for arg in args:
        # Handle Union types (e.g., Union[B, C] or B | C)
        if get_origin(arg) is Union:
            if any(issubclass(sub_arg, AST) for sub_arg in get_args(arg) if isinstance(sub_arg, type)):
                return True
        elif isinstance(arg, UnionType):
            if any(issubclass(sub_arg, AST) for sub_arg in arg.__args__ if isinstance(sub_arg, type)):
                return True
        # Check if the argument itself is a subclass of the base class
        elif isinstance(arg, type) and issubclass(arg, AST):
            return True

    # no AST subclasses found
    return False


class PrototypeAST(AST, ABC):
    """Used to represent AST nodes that are not complete, and must be removed before the whole AST is evaluated"""

    def is_settled(self) -> bool:
        """By definition, prototypes are not settled"""
        return False


class Delimited(ABC):
    """used to track which ASTs are printed with their own delimiter so they can be juxtaposed without extra parentheses"""

class Type(AST):
    name: str
    parameters: list = field(default_factory=list)

    def __str__(self) -> str:
        if self.parameters:
            return f'{self.name}<{", ".join(map(str, self.parameters))}>'
        return self.name


# TODO: turn into a singleton...
# untyped type for when a declaration doesn't specify a type
untyped = Type('untyped')


class Undefined(AST):
    """undefined singleton"""
    def __new__(cls):
        if not hasattr(cls, 'instance'):
            cls.instance = super(Undefined, cls).__new__(cls)
        return cls.instance

    def __str__(self) -> str:
        return 'undefined'


# undefined shorthand, for convenience
undefined = Undefined()


class Void(AST):
    """void singleton"""
    def __new__(cls):
        if not hasattr(cls, 'instance'):
            cls.instance = super(Void, cls).__new__(cls)
        return cls.instance

    def __str__(self) -> str:
        return 'void'


# void shorthand, for convenience
void = Void()


# assign is just a binop?
# perhaps bring this one back since it's syntax that distinguishes it, not type checking
# class Assign(AST):
#     # TODO: allow bind to take in an unpack structure
#     target: Declare | Identifier | UnpackTarget
#     value: AST

#     def __str__(self):
#         return f'{self.target} = {self.value}'


class ListOfASTs(PrototypeAST):
    """Intermediate step for holding a list of ASTs that are probably captured by a container"""
    asts: list[AST]

    def __str__(self):
        return f'{", ".join(map(str, self.asts))}'


class Tuple(PrototypeAST):
    """
    A comma separated list of expressions (not wrapped in parentheses) e.g. 1, 2, 3
    There is no special in-memory representation of a tuple, it is literally just a const list
    """
    items: list[AST]

    def __str__(self):
        return f'{", ".join(map(str, self.items))}'


# class Container(PrototypeAST, Delimited):
#     items: list[AST]
#     brackets: Literal['{}', '[]', '(]', '[)', '()', '<>']

#     def __str__(self):
#         return f'{self.brackets[0]}{" ".join(map(str, self.items))}{self.brackets[1]}'


# class Number(AST):
#     val: int | float | Fraction

class Bool(AST):
    val: bool

    def __str__(self) -> str:
        return str(self.val).lower()


class Int(AST):
    val: int

    def __str__(self) -> str:
        return str(self.val)


class String(AST, Delimited):
    val: str

    def __str__(self) -> str:
        return f'"{escape_whitespace(self.val)}"'


class IString(AST, Delimited):
    parts: list[AST]

    def __str__(self):
        s = ''
        for part in self.parts:
            if isinstance(part, String):
                s += part.val
            else:
                s += f'{part}'
        return f'"{s}"'


class Flowable(AST, ABC):
    ...
    # def was_entered(self) -> bool:
    #     """Determine if the flowable branch was entered. Should reset before performing calls to flow and checking this."""
    #     raise NotImplementedError(f'flowables must implement `was_entered()`. No implementation found for {self.__class__}')

    # def reset_was_entered(self) -> None:
    #     """reset the state of was_entered, in preparation for executing branches in a flow"""
    #     raise NotImplementedError(f'flowables must implement `reset_was_entered()`. No implementation found for {self.__class__}')


class Flow(AST):
    branches: list[Flowable]

    def __str__(self):
        return ' else '.join(map(str, self.branches))


class If(Flowable):
    condition: AST
    body: AST

    def __str__(self):
        return f'if {self.condition} {self.body}'


class Loop(Flowable):
    condition: AST
    body: AST

    def __str__(self):
        return f'loop {self.condition} {self.body}'


class Default(Flowable):
    body: AST

    def __str__(self):
        return f'{self.body}'


class FunctionLiteral(AST):
    args: AST
    body: AST

    def __str__(self):
        if isinstance(self.args, Delimited):
            return f'{self.args} => {self.body}'
        return f'({self.args}) => {self.body}'


class PrototypePyAction(PrototypeAST):
    args: AST
    return_type: AST

    def __str__(self):
        return f'({self.args}): {self.return_type} => ...'

class PyAction(AST):
    args: AST
    action: TypingCallable
    return_type: AST

    def __str__(self):
        return f'({self.args}): {self.return_type} => {self.action}'


class Call(AST):
    f: AST
    args: None | AST = None

    def __str__(self):
        if self.args is None:
            return f'{self.f}()'
        if isinstance(self.args, Delimited):
            return f'{self.f}{self.args}'
        return f'{self.f}({self.args})'


class BinOp(AST, ABC):
    left: AST
    right: AST

class Assign(BinOp):
    def __str__(self): return f'{self.left} = {self.right}'

class PointsTo(BinOp):
    def __str__(self): return f'{self.left} -> {self.right}'

class BidirPointsTo(BinOp):
    def __str__(self): return f'{self.left} <-> {self.right}'

class Access(BinOp):
    def __str__(self): return f'{self.left}.{self.right}'

class Equal(BinOp):
    def __str__(self): return f'{self.left} =? {self.right}'

# covered by OpChain([Not, Equal])
# class NotEqual(BinOp):
#     def __str__(self): return f'{self.left} not=? {self.right}'

class Less(BinOp):
    def __str__(self): return f'{self.left} <? {self.right}'

class LessEqual(BinOp):
    def __str__(self): return f'{self.left} <=? {self.right}'

class Greater(BinOp):
    def __str__(self): return f'{self.left} >? {self.right}'

class GreaterEqual(BinOp):
    def __str__(self): return f'{self.left} >=? {self.right}'

class  LeftShift(BinOp):
    def __str__(self): return f'{self.left} << {self.right}'

class  RightShift(BinOp):
    def __str__(self): return f'{self.left} >> {self.right}'

class LeftRotate(BinOp):
    def __str__(self): return f'{self.left} <<< {self.right}'

class RightRotate(BinOp):
    def __str__(self): return f'{self.left} >>> {self.right}'

class LeftRotateCarry(BinOp):
    def __str__(self): return f'{self.left} <<! {self.right}'

class RightRotateCarry(BinOp):
    def __str__(self): return f'{self.left} !>> {self.right}'

class Add(BinOp):
    def __str__(self): return f'{self.left} + {self.right}'

class Sub(BinOp):
    def __str__(self): return f'{self.left} - {self.right}'

class Mul(BinOp):
    def __str__(self): return f'{self.left} * {self.right}'

class Div(BinOp):
    def __str__(self): return f'{self.left} / {self.right}'

class IDiv(BinOp):
    def __str__(self): return f'{self.left} ÷ {self.right}'

class Mod(BinOp):
    def __str__(self): return f'{self.left} % {self.right}'

class Pow(BinOp):
    def __str__(self): return f'{self.left} ^ {self.right}'

class And(BinOp):
    def __str__(self): return f'{self.left} and {self.right}'

class Or(BinOp):
    def __str__(self): return f'{self.left} or {self.right}'

class Xor(BinOp):
    def __str__(self): return f'{self.left} xor {self.right}'

class Nand(BinOp):
    def __str__(self): return f'{self.left} nand {self.right}'

class Nor(BinOp):
    def __str__(self): return f'{self.left} nor {self.right}'

class Xnor(BinOp):
    def __str__(self): return f'{self.left} xnor {self.right}'

class IterIn(BinOp):
    def __str__(self): return f'{self.left} in {self.right}'

class MemberIn(BinOp):
    def __str__(self): return f'{self.left} in? {self.right}'

class UnaryPrefixOp(AST, ABC):
    operand: AST

class Not(UnaryPrefixOp):
    def __str__(self): return f'not {self.operand}'

class UnaryNeg(UnaryPrefixOp):
    def __str__(self): return f'-{self.operand}'

class UnaryPos(UnaryPrefixOp):
    def __str__(self): return f'+{self.operand}'

class UnaryMul(UnaryPrefixOp):
    def __str__(self): return f'*{self.operand}'

class UnaryDiv(UnaryPrefixOp):
    def __str__(self): return f'/{self.operand}'


class UnaryPostfixOp(AST, ABC):
    operand: AST


class Group(AST, Delimited):
    items: list[AST]

    def __str__(self):
        return f'({" ".join(map(str, self.items))})'


class Block(AST, Delimited):
    items: list[AST]

    def __str__(self):
        return f'{{{" ".join(map(str, self.items))}}}'


class BareRange(PrototypeAST):
    left: AST
    right: AST

    def __str__(self) -> str:
        return f'{self.left}..{self.right}'


class Ellipsis(AST):
    def __str__(self) -> str:
        return '...'


class Spread(AST):
    right: AST

    def __str__(self) -> str:
        return f'...{self.right}'


class Range(AST):
    left: AST
    right: AST
    brackets: Literal['[]', '[)', '(]', '()']

    def __str__(self) -> str:
        return f'{self.brackets[0]}{self.left}..{self.right}{self.brackets[1]}'


class Array(AST, Delimited):
    items: list[AST] # list[T] where T is not Declare or Assign or PointsTo or BidirPointsTo

    def __str__(self):
        return f'[{" ".join(map(str, self.items))}]'


class Dict(AST, Delimited):
    items: list[PointsTo]

    def __str__(self):
        return f'[{" ".join(map(str, self.items))}]'


class BidirDict(AST, Delimited):
    items: list[BidirPointsTo]

    def __str__(self):
        return f'[{" ".join(map(str, self.items))}]'


class Object(AST, Delimited):
    items: list[AST] # list[Declare|Assign|AST] has to have at least 1 declare or assignment

    def __str__(self):
        return f'[{" ".join(map(str, self.items))}]'


class TypeParam(AST, Delimited):
    items: list[AST]

    def __str__(self):
        return f'<{" ".join(map(str, self.items))}>'


class DeclareGeneric(AST):
    left: TypeParam
    right: AST

    def __str__(self):
        return f'{self.left}{self.right}'


class Parameterize(AST):
    left: AST
    right: TypeParam

    def __str__(self):
        return f'{self.left}{self.right}'


#TODO: maybe this should just be a binop, i.e. does right need to be restricted to Range|Array?
# perhaps keep since to parse an index, the right must be a Range|Array
class Index(AST):
    left: AST
    right: Range | Array

    def __str__(self):
        return f'{self.left}{self.right}'


class PrototypeIdentifier(PrototypeAST):
    name: str
    def __str__(self) -> str:
        return f'{self.name}'

class Identifier(AST):
    name: str
    def __str__(self) -> str:
        return f'{self.name}'


class Express(AST):
    id: Identifier

    def __str__(self) -> str:
        return f'{self.id}'

class TypedIdentifier(AST):
    id: Identifier
    type: AST

    def __str__(self) -> str:
        return f'{self.id}:{self.type}'


class TypedGroup(AST):
    group: Group
    type: AST

    def __str__(self) -> str:
        return f'{self.group}:{self.type}'

class UnpackTarget(AST):
    target: 'list[Identifier | TypedIdentifier | UnpackTarget | Assign | Spread]'
    def __str__(self) -> str:
        return f'[{" ".join(map(str, self.target))}]'

class DeclarationType(Enum):
    LET = auto()
    CONST = auto()
    # LOCAL_CONST = auto()
    # FIXED_TYPE = auto()

    # default for binding without declaring
    # DEFAULT = LET


class Declare(AST):
    decltype: DeclarationType
    target: Identifier | TypedIdentifier | TypedGroup | UnpackTarget | Assign

    def __str__(self):
        return f'{self.decltype.name.lower()} {self.target}'



if __name__ == '__main__':
    # DEBUG testing tree string printing
    class _Add(AST):
        l: AST
        r: AST

        def __str__(self) -> str:
            return f'{self.l} + {self.r}'

    class _Mul(AST):
        l: AST
        r: AST

        def __str__(self) -> str:
            return f'{self.l} * {self.r}'

    class _List(AST):
        items: list[AST]

        def __str__(self) -> str:
            return f'[{", ".join(map(str, self.items))}]'

    class _Int(AST):
        value: int

        def __str__(self) -> str:
            return str(self.value)

    # big long test ast
    test = _Add(
        _Add(
            _Int(1),
            _List([_Int(2), _Int(3), _Int(4), _Int(5)])
        ),
        _Mul(
            _Int(2),
            _Add(
                _Mul(
                    _Int(3),
                    _Int(4)
                ),
                _Mul(
                    _Int(5),
                    _Int(6)
                )
            )
        )
    )

    print(repr(test))
    print(str(test))
    # class Broken(AST):
    #     num: int
    #     def __str__(self) -> str:
    #         return f'{self.num}'
    #     def __iter__(self) -> Generator['AST', None, None]:
    #         yield Int(self.num)
1b:T9908,from abc import ABC
import inspect
from typing import Callable, Type, Generator
from types import UnionType
from functools import lru_cache
from .utils import CoordString

import pdb


"""
[tasks]
- clean up eat_block
    - general cleanup
    - break out eat_ matching into smaller functions?
    - figure out tie breaking process:
        1. prefer @full_eat over @peek_eat ---> TODO: implement
        2. prefer longest matches
        3. prefer higher precedence
        4. error
- make all tokens keep the source they come from (for error reporting/keeping track of row/col of the token)
"""


class Token(ABC):
    def __repr__(self) -> str:
        """default repr for tokens is just the class name"""
        return f"<{self.__class__.__name__}>"

    def __hash__(self) -> int:
        raise NotImplementedError(f'hash is not implemented for token type {type(self)}')

    def __eq__(self, __value: object) -> bool:
        raise NotImplementedError(f'equals is not implemented for token type {type(self)}')

    def __iter__(self) -> Generator['list[Token]', None, None]:
        """
        Iter is used by full_traverse_tokens for iterating over any contained tokens.
        e.g. Block_t.body TypeParam_t.body, String_t.body (interpolation blocks only), etc.
        """
        raise NotImplementedError(f'iter is not implemented for token type {type(self)}')


class WhiteSpace_t(Token):
    def __init__(self, _): ...


class Operator_t(Token):
    def __init__(self, op: str):
        self.op = op

    def __repr__(self) -> str:
        return f"<Operator_t: `{self.op}`>"

    def __hash__(self) -> int:
        return hash((Operator_t, self.op))

    def __eq__(self, other) -> bool:
        return isinstance(other, Operator_t) and self.op == other.op


class Juxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __hash__(self) -> int:
        return hash(Juxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, Juxtapose_t)


class Keyword_t(Token):
    def __init__(self, src: str):
        self.src = src.lower()

    def __repr__(self) -> str:
        return f"<Keyword_t: {self.src}>"

    def __hash__(self) -> int:
        return hash((Keyword_t, self.src))

    def __eq__(self, other) -> bool:
        return isinstance(other, Keyword_t) and self.src == other.src


class Identifier_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Identifier_t: {self.src}>"


class Hashtag_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Hashtag_t: {self.src}>"


class Block_t(Token):
    def __init__(self, body: list[Token], left: str, right: str):
        self.body = body
        self.left = left
        self.right = right

    def __repr__(self) -> str:
        body_str = ', '.join(repr(token) for token in self.body)
        return f"<Block_t: {self.left}{body_str}{self.right}>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield self.body


class TypeParam_t(Token):
    def __init__(self, body: list[Token]):
        self.body = body

    def __repr__(self) -> str:
        body_str = ', '.join(repr(token) for token in self.body)
        return f"<TypeParam_t: `<{body_str}>`>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield self.body


class Escape_t(Token):
    escape_map = {
        '\\n': '\n', '\\r': '\r', '\\t': '\t', '\\b': '\b', '\\f': '\f', '\\v': '\v', '\\a': '\a', '\\0': '\0', '\\\\': '\\'
    }

    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Escape_t: {self.src}>"

    def to_str(self) -> str:
        """Convert the escape sequence to the character it represents"""

        # unicode escape (may be several characters long)
        if self.src.startswith('\\U') or self.src.startswith('\\u'):
            return chr(int(self.src[2:], 16))
        assert len(self.src) == 2 and self.src[0] == '\\', "internal error. Ill-posed escape sequence"

        # known escape sequence
        if self.src in self.escape_map:
            esc = self.escape_map[self.src]
            # construct a CoordString at the position of the original escape
            return CoordString.from_existing(esc, self.src[:len(esc)].row_col_map)

        # unknown escape sequence (i.e. just replicate the character)
        return self.src[1]


class RawString_t(Token):
    def __init__(self, body: str):
        self.body = body

    def __repr__(self) -> str:
        return f"<RawString_t: {self.body}>"

    def to_str(self) -> str:
        body = self.body
        if body.startswith('r"""') or body.startswith("r'''"):
            body = body[4:-3]
        elif body.startswith('r"') or body.startswith("r'"):
            body = body[2:-1]
        else:
            raise ValueError(f"Internal Error: unrecognized delimiters on raw string: {repr(self)}")
        return body


class String_t(Token):
    def __init__(self, body: list[str | Escape_t | Block_t]):
        self.body = body

    def __repr__(self) -> str:
        return f"<String_t: {self.body}>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        for token in self.body:
            if isinstance(token, Block_t):
                yield token.body

# class Number_t(Token, ABC):...


class Integer_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Integer_t: {self.src}>"


class BasedNumber_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<BasedNumber_t: {self.src}>"


class Undefined_t(Token):
    def __init__(self, _): ...

    def __hash__(self) -> int:
        return hash(Undefined_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, Undefined_t)

    def __repr__(self) -> str:
        return "<Undefined_t>"


class Void_t(Token):
    def __init__(self, _): ...

    def __hash__(self) -> int:
        return hash(Void_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, Void_t)

    def __repr__(self) -> str:
        return "<Void_t>"


class End_t(Token):
    def __init__(self, _): ...

    def __hash__(self) -> int:
        return hash(End_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, End_t)

    def __repr__(self) -> str:
        return "<End_t>"


class Boolean_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Boolean_t: {self.src}>"


class ShiftOperator_t(Operator_t):
    def __init__(self, op: str):
        self.op = op

    def __repr__(self) -> str:
        return f"<ShiftOperator_t: `{self.op}`>"

    def __hash__(self) -> int:
        return hash((ShiftOperator_t, self.op))

    def __eq__(self, other) -> bool:
        return isinstance(other, ShiftOperator_t) and self.op == other.op


class Comma_t(Operator_t):
    def __init__(self, op: str):
        self.op = op

    def __hash__(self) -> int:
        return hash(Comma_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, Comma_t)


class DotDot_t(Token):
    def __init__(self, src: str):
        self.src = src


class DotDotDot_t(Token):
    def __init__(self, src: str):
        self.src = src

# #TODO: these should probably each be their own class/token, or a single class..
# these should all be case insensitive
# reserved_values = ['true', 'false', 'void', 'undefined', 'end']


# identify token classes that should take precedence over others when tokenizing
# each row is a list of token types that are confusable in their precedence order. e.g. [Keyword, Unit, Identifier] means Keyword > Unit > Identifier
# only confusable token classes need to be included in the table
precedence_table = [
    [Keyword_t, Undefined_t, Void_t, End_t, Boolean_t, Operator_t, DotDot_t, Identifier_t],
]
precedence = {cls: len(row)-i for row in precedence_table for i, cls in enumerate(row)}

# mark which tokens cannot be repeated in a list of tokens. E.g. whitespace should always be merged into a single token
idempotent_tokens = {
    WhiteSpace_t
}

# paired delimiters for blocks, ranges, groups, etc.
pair_opening_delims = '{(['
pair_closing_delims = '})]'

# which closing delimiters are allowed for each opening delimiter
valid_delim_closers = {
    '{': '}',
    '(': ')]',
    '[': '])',
    # '<': '>'
}

# list of all operators sorted from longest to shortest
# TODO: make @ and ... into expressions (perhaps with lower precedence calling than regular calls?)
unary_prefix_operators = {'+', '-', '*', '/', 'not', '@'}#, '...'}
unary_postfix_operators = {'?', '`', ';'}
binary_operators = {
    '+', '-', '*', '/', '%', '^',
    '=?', '>?', '<?', '>=?', '<=?', 'in?', 'is?', 'isnt?', '<=>',
    '|', '&',
    'and', 'or', 'nand', 'nor', 'xor', 'xnor', '??',
    'else',
    '=', ':=', 'as', 'in', 'transmute',
    '@?',
    '|>', '<|', '=>',
    '->', '<->', #'<-', #reverse arrow is dumb
    '.', ':'
}
opchain_starters = {'+', '-', '*', '/', '%', '^'}
operators = sorted(
    [*(unary_prefix_operators | unary_postfix_operators | binary_operators)],
    key=len,
    reverse=True
)
# TODO: may need to separate |> from regular operators since it may confuse type param
shift_operators = sorted(['<<', '>>', '<<<', '>>>', '<<!', '!>>'], key=len, reverse=True)
keywords = ['loop', 'lazy', 'do', 'if', 'match', 'return', 'yield', 'break', 'continue',
            'async', 'await', 'import', 'from', 'let', 'const', 'local_const', 'fixed_type']
# TODO: what about language values, e.g. void, undefined, end, units, etc.? probably define at compile time, rather than in the compiler

# note that the prefix is case insensitive, so call .lower() when matching the prefix
# numbers may have _ as a separator (if _ is not in the set of digits)
number_bases = {
    '0b': {*'01'},  # binary
    '0t': {*'012'},  # ternary
    '0q': {*'0123'},  # quaternary
    '0s': {*'012345'},  # seximal
    '0o': {*'01234567'},  # octal
    '0d': {*'0123456789'},  # decimal
    '0z': {*'0123456789xeXE'},  # dozenal
    '0x': {*'0123456789abcdefABCDEF'},  # hexadecimal
    '0u': {*'0123456789abcdefghijklmnopqrstuvABCDEFGHIJKLMNOPQRSTUV'},  # base 32 (duotrigesimal)
    '0r': {*'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'},  # base 36 (hexatrigesimal)
    '0y': {*'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!$'},  # base 64 (tetrasexagesimal)
}


# units = #actually units should probably not be specific tokens, but recognized identifiers since the user can make their own units


def peek_eat(cls: Type[Token], whitelist: list[Type[Token]] | None = None, blacklist: list[Type[Token]] | None = None):
    """
    Decorator for functions that eat tokens, but only return how many characters would make up the token.
    Makes function return include constructor for token class that it tries to eat, in tupled with return.

    whitelist and blacklist can be used to specify parent token contexts that may or may not consume this type as a child
    """
    assert issubclass(cls, Token), f"cls must be a subclass of Token, but got {cls}"
    if whitelist is not None and blacklist is not None:
        raise ValueError("cannot specify both whitelist and blacklist")

    def decorator(eat_func: Callable[[str], int | None]):
        def wrapper(src: str) -> tuple[int | None, Type[Token]]:
            return eat_func(src), cls
        wrapper._is_peek_eat_decorator = True  # make it easy to check if a function has this decorator
        wrapper._eat_func = eat_func
        wrapper._token_cls = cls
        wrapper._whitelist = whitelist
        wrapper._blacklist = blacklist
        return wrapper
    return decorator

# TODO: full eat probably won't need to take the class as an argument, since the function will know how to construct the token itself


def full_eat(whitelist: list[Type[Token]] | None = None, blacklist: list[Type[Token]] | None = None):
    def decorator(eat_func: Callable[[str], tuple[int, Token] | None]):
        """
        Decorator for functions that eat tokens, and return the token itself if successful.
        TBD what this actually does...for now, largely keep unmodified, but attach the metadata to the wrapped function
        """
        # pull cls it from the return type of eat_func (which should be a Union[tuple[int, Token], None])
        cls = inspect.signature(eat_func).return_annotation.__args__[0].__args__[1]
        assert issubclass(cls, Token), f"cls must be a subclass of Token, but got {cls}"
        if whitelist is not None and blacklist is not None:
            raise ValueError("cannot specify both whitelist and blacklist")

        def wrapper(*args, **kwargs):
            return eat_func(*args, **kwargs), cls
        wrapper._is_full_eat_decorator = True  # make it easy to check if a function has this decorator
        wrapper._eat_func = eat_func
        wrapper._token_cls = cls
        wrapper._whitelist = whitelist
        wrapper._blacklist = blacklist

        return wrapper
    return decorator


def get_peek_eat_funcs_with_name() -> tuple[tuple[str, Callable], ...]:
    return tuple((name, func) for name, func in globals().items() if callable(func) and getattr(func, '_is_peek_eat_decorator', False))


def get_full_eat_funcs_with_name() -> tuple[tuple[str, Callable], ...]:
    return tuple((name, func) for name, func in globals().items() if callable(func) and getattr(func, '_is_full_eat_decorator', False))


def get_eat_funcs() -> tuple[Callable, ...]:
    return tuple(func for name, func in get_peek_eat_funcs_with_name() + get_full_eat_funcs_with_name())


@lru_cache()
def get_contextual_eat_funcs(context: Type[Token]) -> tuple[Callable, ...]:
    """Get all the eat functions that are valid in the given context"""
    return tuple(func for func in get_eat_funcs() if (func._whitelist is None or context in func._whitelist) and (func._blacklist is None or context not in func._blacklist))


@lru_cache()
def get_func_precedences(funcs: tuple[Callable]) -> tuple[int, ...]:
    assert isinstance(funcs, tuple)
    return tuple(precedence.get(func._token_cls, 0) for func in funcs)


@peek_eat(WhiteSpace_t)
def eat_line_comment(src: str) -> int | None:
    """eat a line comment, return the number of characters eaten"""
    if src.startswith('//'):
        try:
            return src.index('\n') + 1
        except ValueError:
            return len(src)
    return None


@peek_eat(WhiteSpace_t)
def eat_block_comment(src: str) -> int | None:
    """
    Eat a block comment, return the number of characters eaten
    Block comments are of the form /{ ... }/ and can be nested.
    """
    if not src.startswith("/{"):
        return None

    nesting_level = 0
    i = 0

    while i < len(src):
        if src[i:].startswith('/{'):
            nesting_level += 1
            i += 2
        elif src[i:].startswith('}/'):
            nesting_level -= 1
            i += 2

            if nesting_level == 0:
                return i
        else:
            i += 1

    raise ValueError("unterminated block comment")
    # return None


@peek_eat(WhiteSpace_t)
def eat_whitespace(src: str) -> int | None:
    """Eat whitespace, return the number of characters eaten"""
    i = 0
    while i < len(src) and src[i].isspace():
        i += 1
    return i if i > 0 else None


@peek_eat(Keyword_t)
def eat_keyword(src: str) -> int | None:
    """
    Eat a reserved keyword, return the number of characters eaten

    #keyword = {in} | {as} | {loop} | {lazy} | {if} | {and} | {or} | {xor} | {nand} | {nor} | {xnor} | {not};# | {true} | {false};

    noting that keywords are case insensitive
    """

    max_len = max(len(keyword) for keyword in keywords)

    lower_src = src[:max_len].lower()
    for keyword in keywords:
        if lower_src.startswith(keyword):
            # TBD if we need to check that the next character is not an identifier character
            return len(keyword)

    return None


# TODO: expand the list of valid identifier characters
digits = set('0123456789')
alpha = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz')
greek = set('ΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩαβγδεζηθικλμνξοπρςστυφχψω')
misc = set('_?!$&°')

start_characters = (alpha | greek | misc) - {'?'}
continue_characters = (alpha | digits | greek | misc)


@peek_eat(Identifier_t)
def eat_identifier(src: str) -> int | None:
    """
    Eat an identifier, return the number of characters eaten

    Identifiers:
    - may not start with a number or a question mark
    - may not end with a question mark
    - may use (TODO enumerate the full chars list somewhere. for now copying from python)

    """
    if not src[0] in start_characters:
        return None

    i = 1
    while i < len(src) and src[i] in continue_characters:
        i += 1

    # while last character is ?, remove it
    while i > 1 and src[i-1] == '?':
        i -= 1

    return i


@peek_eat(Hashtag_t)
def eat_hashtag(src: str) -> int | None:
    """
    Eat a hashtag, return the number of characters eaten

    hashtags are special identifiers that start with #
    """

    if src.startswith('#'):
        i, _ = eat_identifier(src[1:])
        if i is not None:
            return i + 1

    return None


@peek_eat(Escape_t, whitelist=[String_t])
def eat_escape(src: str) -> int | None:
    r"""
    Eat an escape sequence, return the number of characters eaten
    Escape sequences must be either a known escape sequence:
    - \n newline
    - \r carriage return
    - \t tab
    - \b backspace
    - \f form feed
    - \v vertical tab
    - \a alert
    - \0 null
    - \u##..# or \U##..# for an arbitrary unicode character. May have any number of hex digits

    or a \ followed by an unknown character. In this case, the escape converts to just the unknown character
    This is how to insert characters that are otherwise illegal inside a string, e.g.
    - \' converts to just a single quote '
    - \{ converts to just a single open brace {
    - \\ converts to just a single backslash \
    - \m converts to just a single character m
    - etc.
    """
    if not src.startswith('\\'):
        return None

    if len(src) == 1:
        raise ValueError("unterminated escape sequence")

    if src[1] in 'uU':
        i = 2
        while i < len(src) and src[i].isxdigit():
            i += 1
        if i == 2:
            raise ValueError("invalid unicode escape sequence")
        return i

    # if src[1] in 'nrtbfva0':
    #     return 2

    # all other escape sequences (known or unknown) are just a single character
    return 2


@full_eat()
def eat_string(src: str) -> tuple[int, String_t] | None:
    r"""
    strings are delimited with either single (') or double quotes (")
    the character portion of a string may contain any character except the delimiter, \, or {.
    strings may be multiline
    strings may contain escape sequences of the form \s where s is either a known escape sequence or a single character
    strings may interpolation blocks which open with { and close with }

    Tokenizing of escape sequences and interpolation blocks is handled as sub-tokenization task via eat_block and eat_escape

    returns the number of characters eaten and an instance of the String token, containing the list of tokens/string chunks/escape sequences
    """

    # determine the starting delimiter, or exit if there is none
    if src.startswith('"""') or src.startswith("'''"):
        delim = src[:3]
        i = 3
    elif src.startswith('"') or src.startswith("'"):
        delim = src[0]
        i = 1
    else:
        return None

    # keep track of chunks, and the start index of the current chunk
    chunk_start = i
    body = []

    # add character sequences, escapes, and block sections until the end of the string
    while i < len(src) and not src[i:].startswith(delim):

        # regular characters
        if src[i] not in '\\{':
            i += 1
            continue

        # add the previous chunk before handling the escape/interpolation block
        if i > chunk_start:
            body.append(src[chunk_start:i])

        if src[i] == '\\':
            res, _ = eat_escape(src[i:])
            if res is None:
                raise ValueError("invalid escape sequence")
            body.append(Escape_t(src[i:i+res]))
            i += res

        else:  # src[i] == '{':
            assert src[i] == '{', "internal error"
            res, _ = eat_block(src[i:])
            if res is None:
                raise ValueError("invalid block")
            n_eaten, block = res
            body.append(block)
            i += n_eaten

        # update the chunk start
        chunk_start = i

    if i == len(src):
        raise ValueError("unterminated string")

    # add the final chunk
    if i > chunk_start:
        body.append(src[chunk_start:i])

    return i + len(delim), String_t(body)


@peek_eat(RawString_t)
def eat_raw_string(src: str) -> int | None:
    """
    raw strings start with `r`, followed by a delimiter, one of ' " ''' or \"""
    raw strings may contain any character except the delimiter.
    Escapes and interpolations are ignored.
    The string ends at the first instance of the delimiter
    """
    if not src.startswith('r'):
        return None
    i = 1

    if src[i:].startswith('"""') or src[i:].startswith("'''"):
        delim = src[i:i+3]
        i += 3
    elif src[i:].startswith('"') or src[i:].startswith("'"):
        delim = src[i]
        i += 1
    else:
        return None

    while i < len(src) and not src[i:].startswith(delim):
        i += 1

    if i == len(src):
        raise ValueError("unterminated raw string")

    return i + len(delim)


@peek_eat(Integer_t)
def eat_integer(src: str) -> int | None:
    """
    eat an integer, return the number of characters eaten
    integers are of the form [0-9]+
    """
    if not src[0].isdigit():
        return None

    i = 1
    while i < len(src) and src[i].isdigit() or src[i] == '_':
        i += 1
    return i


@peek_eat(BasedNumber_t)
def eat_based_number(src: str) -> int | None:
    """
    eat a based number, return the number of characters eaten

    based numbers have a (case-insensitive) prefix (0p) identifying the base, and (case-sensitive) allowed digits
    """
    try:
        digits = number_bases[src[:2].lower()]
    except KeyError:
        return None

    i = 2
    while i < len(src) and src[i] in digits or src[i] == '_':
        i += 1

    return i if i > 2 else None


@peek_eat(Undefined_t)
def eat_undefined(src: str) -> int | None:
    """
    eat the undefined token, return the number of characters eaten
    """
    sample = src[:9].lower()
    if sample.startswith('undefined'):
        return 9
    return None


@peek_eat(Void_t)
def eat_void(src: str) -> int | None:
    """
    eat the void token, return the number of characters eaten
    """
    sample = src[:4].lower()
    if sample.startswith('void'):
        return 4
    return None


@peek_eat(End_t)
def eat_end(src: str) -> int | None:
    """
    eat the end token, return the number of characters eaten
    """
    sample = src[:3].lower()
    if sample.startswith('end'):
        return 3
    return None


@peek_eat(Boolean_t)
def eat_boolean(src: str) -> int | None:
    """
    eat a boolean, return the number of characters eaten

    booleans are either true or false (case-insensitive)
    """
    sample = src[:5].lower()
    if sample.startswith('true'):
        return 4
    elif sample.startswith('false'):
        return 5

    return None


@peek_eat(Operator_t)
def eat_operator(src: str) -> int | None:
    """
    eat a unary or binary operator, return the number of characters eaten

    picks the longest matching operator

    see `operators` for full list of operators
    """
    for op in operators:
        if src.startswith(op):
            return len(op)
    return None


@peek_eat(ShiftOperator_t, blacklist=[TypeParam_t])
def eat_shift_operator(src: str) -> int | None:
    """
    eat a shift operator, return the number of characters eaten

    picks the longest matching operator.
    Shift operators are not allowed in type parameters, e.g. `>>` is not recognized in `Foo<Bar<Baz<T>>, U>`

    see `shift_operators` for full list of operators
    """
    for op in shift_operators:
        if src.startswith(op):
            return len(op)
    return None


@peek_eat(Comma_t)
def eat_comma(src: str) -> int | None:
    """
    eat a comma, return the number of characters eaten
    """
    return 1 if src.startswith(',') else None


@peek_eat(DotDot_t)
def eat_dotdot(src: str) -> int | None:
    """
    eat a dotdot, return the number of characters eaten
    """
    return 2 if src.startswith('..') else None


@peek_eat(DotDotDot_t)
def eat_dotdotdot(src: str) -> int | None:
    """
    eat a dotdotdot, return the number of characters eaten
    """
    return 3 if src.startswith('...') else None


class EatTracker:
    i: int
    tokens: list[Token]


@full_eat()
def eat_type_param(src: str) -> tuple[int, TypeParam_t] | None:
    """
    eat a type parameter, return the number of characters eaten and an instance of the TypeParam token

    type parameters are of the form <...> where ... is a sequence of tokens.
    Type parameters may not start with `<<` or contain any shift operators (`<<`, `<<<`, `>>`, `>>>`)
    Internally encountered shift operators are considered to be delimiters for the type parameter
    """
    if not src.startswith('<') or src.startswith('<<'):
        return None

    i = 1
    body: list[Token] = []

    while i < len(src) and src[i] != '>':

        funcs = get_contextual_eat_funcs(TypeParam_t)
        precedences = get_func_precedences(funcs)
        res = get_best_match(src[i:], funcs, precedences)

        if res is None:
            return None
        n_eaten, token = res

        if isinstance(token, Token):
            # add the already-eaten token to the list of tokens
            body.append(token)
        else:
            # add a new instance of the token to the list of tokens (handling idempotent token cases)
            token_cls = token
            if not body or token_cls not in idempotent_tokens or not isinstance(body[-1], token_cls):
                body.append(token_cls(src[i:i+n_eaten]))

        # increment the index
        i += n_eaten

    if i == len(src):
        return None

    return i + 1, TypeParam_t(body)


@full_eat()
def eat_block(src: str, tracker: EatTracker | None = None) -> tuple[int, Block_t] | None:
    """
    Eat a block, return the number of characters eaten and an instance of the Block token

    blocks are { ... } or ( ... ) and may contain sequences of any other tokens including other blocks

    if return_partial is True, then returns (i, body) in the case where the eat process fails, instead of None
    """

    if not src or src[0] not in pair_opening_delims:
        return None

    # save the opening delimiter
    left = src[0]

    i = 1
    body: list[Token] = []

    if tracker:
        tracker.i = i
        tracker.tokens = body

    while i < len(src) and src[i] not in pair_closing_delims:
        # run all root eat functions
        # if multiple, resolve for best match (TBD... current is longest match + precedence)
        # if no match, return None

        # TODO: probably break this inner part into a function that eats the next token, given a list of eat functions
        # could also think about ways to specify other multi-match resolutions, other than longest match + precedence...
        # run all the eat functions on the current src
        funcs = get_contextual_eat_funcs(Block_t)
        precedences = get_func_precedences(funcs)
        res = get_best_match(src[i:], funcs, precedences)

        # if we didn't match anything, return None
        if res is None:
            return None

        n_eaten, token = res

        if isinstance(token, Token):
            # add the already-eaten token to the list of tokens
            body.append(token)
        else:
            # add a new instance of the token to the list of tokens (handling idempotent token cases)
            token_cls = token
            if not body or token_cls not in idempotent_tokens or not isinstance(body[-1], token_cls):
                body.append(token_cls(src[i:i+n_eaten]))

        # increment the index
        i += n_eaten
        if tracker:
            tracker.i = i

    if i == len(src):
        if tracker:  # only return an exception for the top level block. nested blocks can return None
            raise ValueError("unterminated block")
        return None

    # closing delim (doesn't need to match opening delim)
    right = src[i]
    assert left in pair_opening_delims and right in pair_closing_delims, f"invalid block delimiters: {left} {right}"

    # include closing delimiter in character count
    i += 1
    if tracker:
        tracker.i = i

    return i, Block_t(body, left=left, right=right)


def get_best_match(src: str, eat_funcs: list, precedences: list[int]) -> tuple[int, Type[Token] | Token] | None:
    # TODO: handle selecting between full_eat and peek_eat functions that were successful...
    #      general, just need to clarify the selection order precedence

    # may return none if no match
    # may return (i, token_cls) if peek match
    # may return (i, token) if full match

    matches = [eat_func(src) for eat_func in eat_funcs]

    # find the longest token that matched. if multiple tied for longest, use the one with the highest precedence.
    # raise an error if multiple tokens tied, and they have the same precedence
    def key(x):
        (res, _cls), precedence = x
        if res is None:
            return 0, precedence
        if isinstance(res, tuple):
            res, _token = res  # full_eat functions return a tuple of (num_chars_eaten, token)
        return res, precedence

    matches = [*zip(matches, precedences)]
    best = max(matches, key=key)
    ties = [match for match in matches if key(match) == key(best)]
    if len(ties) > 1:
        raise ValueError(f"multiple tokens matches tied {[match[0][1].__name__ for match in ties]}: {repr(src)}\nPlease disambiguate by providing precedence levels for these tokens.")

    (res, token_cls), _ = best

    # force the type annotations
    res: tuple[int, Token] | int | None
    token_cls: type[Token]

    if res is None:
        return None

    if isinstance(res, int):
        return res, token_cls

    if isinstance(res, tuple):
        return res

    raise ValueError(f"Internal Error: invalid return type from eat function: {res}")


def tokenize(src: str) -> list[Token]:

    # insert src into a block
    src = f'{{\n{src}\n}}'

    # convert string to a coordinate string (for keeping track of row/col numbers)
    src = CoordString(src, anchor=(-1, 0))

    # eat tokens for a block
    tracker = EatTracker()
    try:
        res, _cls = eat_block(src, tracker=tracker)
    except Exception as e:
        raise ValueError(f"failed to tokenize: ```{escape_whitespace(src[tracker.i:])}```.\nCurrent tokens: {tracker.tokens}") from e

    # check if the process failed
    if res is None:
        raise ValueError(f"failed to tokenize: ```{escape_whitespace(src[tracker.i:])}```.\nCurrent tokens: {tracker.tokens}")

    (i, block) = res
    tokens = block.body

    # ensure that all blocks have valid open/close pairs
    validate_block_braces(tokens)

    return tokens


def full_traverse_tokens(tokens: list[Token]) -> Generator[tuple[int, Token, list[Token]], int, None]:
    """
    Walk all tokens recursively, allowing for modification of the tokens list as it is traversed.

    So long as modifications do not occur before the current token, this will safely iterate over all tokens.
    This will not yield string or escape chunks in strings, but will yield interpolated blocks.

    While traversing, the user can overwrite the current index by calling .send(new_index).

    e.g.
    ```python
    gen = full_traverse_tokens(tokens)
    for i, token, stream in gen:
        #do something with current token
        #...

        #maybe overwrite the current index
        if should_overwrite:
            gen.send(new_index)
    ```

    Do not call .send() twice in a row without calling next() in between. This will cause unexpected behavior.

    Args:
        tokens: the list of tokens to traverse

    Yields:
        i: the index of the current token in the current token list
        token: the current token
        stream: the current token list
    """

    i = 0

    while i < len(tokens):
        """
        1. get next token
        2. send current to user
        3. increment index (or overwrite it)
        4. recurse into blocks
        """

        # get the current token
        token = tokens[i]

        # send the current index to the user. possibly receive a new index to continue from
        overwrite_i = yield i, token, tokens

        # only calls to next() will continue execution. calls to .send do nothing wait
        if overwrite_i is not None:
            assert (yield) is None, ".send() may only be called once per iteration."
            i = overwrite_i
        else:
            i += 1

        # for tokens that have defined __iter__ methods, yield their contents
        try:
            for children in token:
                yield from full_traverse_tokens(children)
        except NotImplementedError:
            pass


def traverse_tokens(tokens: list[Token]) -> Generator[Token, None, None]:
    """
    Convenience function over full_traverse_tokens. Walk all tokens recursively

    Does not allow for modification of the tokens list as it is traversed.
    To modify during traversal, use `full_traverse_tokens` instead.

    Args:
        tokens: the list of tokens to traverse

    Yields:
        token: the current token
    """
    for _, token, _ in full_traverse_tokens(tokens):
        yield token


def validate_block_braces(tokens: list[Token]) -> None:
    """
    Checks that all blocks have valid open/close pairs.

    For example, ranges may have differing open/close pairs, e.g. [0..10), (0..10], etc.
    But regular blocks must have matching open/close pairs, e.g. { ... }, ( ... ), [ ... ]
    Performs some validation, without knowing if the block is a range or a block.
    So more validation is needed when the actual block type is known.

    Raises:
        AssertionError: if a block is found with an invalid open/close pair
    """
    for token in traverse_tokens(tokens):
        if isinstance(token, Block_t):
            assert token.left in valid_delim_closers, f'INTERNAL ERROR: left block opening token is not a valid token. Expected one of {[*valid_delim_closers.keys()]}. Got \'{token.left}\''
            assert token.right in valid_delim_closers[token.left], f'ERROR: mismatched opening and closing braces. For opening brace \'{token.left}\', expected one of \'{valid_delim_closers[token.left]}\''


def validate_functions():

    # Validate the @peek_eat function signatures
    peek_eat_functions = get_peek_eat_funcs_with_name()
    for name, wrapper_func in peek_eat_functions:
        func = wrapper_func._eat_func
        signature = inspect.signature(func)
        param_types = [param.annotation for param in signature.parameters.values()
                       if param.default is inspect.Parameter.empty]
        return_type = signature.return_annotation

        # Check if the function has the correct signature
        if len(param_types) != 1 or param_types[0] != str or return_type != int | None:
            pdb.set_trace()
            raise ValueError(f"{func.__name__} has an invalid signature: `{signature}`. Expected `(src: str) -> int | None`")

    # Validate the @full_eat function signatures
    full_eat_functions = get_full_eat_funcs_with_name()
    for name, wrapper_func in full_eat_functions:
        func = wrapper_func._eat_func
        signature = inspect.signature(func)
        param_types = [param.annotation for param in signature.parameters.values()
                       if param.default is inspect.Parameter.empty]
        return_type = signature.return_annotation

        # Check if the function has the correct signature
        error_message = f"{func.__name__} has an invalid signature: `{signature}`. Expected `(src: str) -> tuple[int, Token] | None`"
        if not (isinstance(return_type, UnionType) and len(return_type.__args__) == 2 and type(None) in return_type.__args__):
            raise ValueError(error_message)
        A, B = return_type.__args__
        if B is not type(None):
            B, A = A, B
        if not (isinstance(A, type(tuple)) and len(A.__args__) == 2 and A.__args__[0] is int and issubclass(A.__args__[1], Token)):
            raise ValueError(error_message)
        if len(param_types) != 1 or param_types[0] != str:
            pdb.set_trace()
            raise ValueError(error_message)

    # check for any functions that start with eat_ but are not decorated with @eat
    peek_eat_func_names = {name for name, _ in peek_eat_functions}
    full_eat_func_names = {name for name, _ in full_eat_functions}
    for name, func in globals().items():
        if name.startswith("eat_") and callable(func) and name not in peek_eat_func_names and name not in full_eat_func_names:
            raise ValueError(f"`{name}()` function is not decorated with @peek_eat or @full_eat")


def escape_whitespace(s: str):
    """convert a string to one where all non-space whitespace is escaped"""
    escape_map = {
        '\t': '\\t',
        '\r': '\\r',
        '\f': '\\f',
        '\v': '\\v',
        '\n': '\\n',
    }
    return ''.join(escape_map.get(c, c) for c in s)


def tprint(token: Token, level=0):
    """
    print a token with a certain indentation level.

    If tokens contain nested tokens, they will be printed recursively with an increased indentation level
    """
    print(f'{"    "*level}', end='')
    if isinstance(token, Block_t):
        print(f'<Block {token.left}{token.right}>')
        for t in token.body:
            tprint(t, level=level+1)
    elif isinstance(token, String_t):
        print(f'<String>')
        for t in token.body:
            tprint(t, level=level+1)
    elif isinstance(token, TypeParam_t):
        print(f'<TypeParam>')
        for t in token.body:
            tprint(t, level=level+1)
    else:
        print(token)


def test():
    import sys
    """simple test dewy program"""

    try:
        path = sys.argv[1]
    except IndexError:
        raise ValueError("Usage: `python tokenizer.py path/to/file.dewy>`")

    with open(path) as f:
        src = f.read()

    tokens = tokenize(src)
    print(f'matched tokens:')
    tprint(Block_t(left='{', right='}', body=tokens))
    # for t in tokens:
    #     tprint(t, level=1)


if __name__ == "__main__":
    validate_functions()
    test()
1c:T226c,from typing import TypeVar, Generic, Callable
import pdb


def wrap_coords(method: Callable):
    def wrapped_method(self, *args, **kwargs):
        result = method(self, *args, **kwargs)
        if isinstance(result, str) and len(result) == len(self):
            custom_str = CoordString(result)
            custom_str.row_col_map = self.row_col_map
            return custom_str
        else:
            raise ValueError("coord_string_method must return a string of the same length as the original string")
        return result

    return wrapped_method


def fail_coords(method: Callable):
    def wrapped_method(self, *args, **kwargs):
        raise ValueError(f"coord_string_method {method} cannot be called on a CoordString, as it will not return a CoordString")
    return wrapped_method


class CoordString(str):
    """
    Drop-in replacement for str that keeps track of the coordinates of each character in the string

    Identical to normal strings, but attaches the `row_col(i:int) -> tuple[int, int]` method
    which returns the (row, column) of the character at index i

    Args:
        anchor (tuple[int,int], optional): The row and column of the top left of the string. Defaults to (0, 0).
    """
    def __new__(cls, *args, anchor: tuple[int, int] = (0, 0), **kwargs):
        self = super().__new__(cls, *args, **kwargs)
        row, col = anchor
        self.row_col_map = self._generate_row_col_map(row, col)

        return self

    # TODO: make init so that class recognized .row_col_map as property on instances
    # def __init__(self, s:str, row_col_map:list[tuple[int,int]]):

    def _generate_row_col_map(self, row=0, col=0) -> list[tuple[int, int]]:
        row_col_map = []
        for c in self:
            if c == '\n':
                row_col_map.append((row, col))
                row += 1
                col = 0
            else:
                row_col_map.append((row, col))
                col += 1
        return row_col_map

    def __getitem__(self, key):
        if isinstance(key, slice):
            sliced_str = super().__getitem__(key)
            sliced_row_col_map = self.row_col_map[key]
            custom_str = CoordString(sliced_str)
            custom_str.row_col_map = sliced_row_col_map
            return custom_str
        return super().__getitem__(key)

    def loc(self, index):
        return self.row_col_map[index]

    @staticmethod
    def from_existing(new_str: str, old_coords: list[tuple[int, int]]) -> 'CoordString':
        new_coord_str = CoordString(new_str)
        new_coord_str.row_col_map = old_coords
        return new_coord_str

    # wrappers for string methods that should return CoordStrings
    def lstrip(self, *args, **kwargs):
        result = super().lstrip(*args, **kwargs)
        custom_str = CoordString(result)
        custom_str.row_col_map = self.row_col_map[len(self)-len(result):]
        return custom_str

    def rstrip(self, *args, **kwargs):
        result = super().rstrip(*args, **kwargs)
        custom_str = CoordString(result)
        custom_str.row_col_map = self.row_col_map[:len(result)]
        return custom_str

    def strip(self, *args, **kwargs):
        return self.lstrip(*args, **kwargs).rstrip(*args, **kwargs)

    @wrap_coords
    def capitalize(self): return super().capitalize()

    @wrap_coords
    def casefold(self): return super().casefold()

    @wrap_coords
    def lower(self): return super().lower()

    @wrap_coords
    def upper(self): return super().upper()

    @wrap_coords
    def swapcase(self): return super().swapcase()

    @wrap_coords
    def title(self): return super().title()

    @wrap_coords
    def translate(self, table): return super().translate(table)

    @wrap_coords
    def replace(self, old, new, count=-1): return super().replace(old, new, count)

    @fail_coords
    def center(self, *args, **kwargs): ...

    @fail_coords
    def expandtabs(self, *args, **kwargs): ...

    @fail_coords
    def ljust(self, *args, **kwargs): ...

    @fail_coords
    def zfill(self, *args, **kwargs): ...


# TODO: maybe make adding this string with other regular str illegal


int_parsable_base_prefixes = {
    '0b': 2,  '0B': 2,
    '0t': 3,  '0T': 3,
    '0q': 4,  '0Q': 4,
    '0s': 6,  '0S': 6,
    '0o': 8,  '0O': 8,
    '0d': 10, '0D': 10,
    # '0z':12, #uses different digits Z/z and X/x (instead of A/a and B/b expected by int())
    '0x': 16, '0X': 16,
    '0u': 32, '0U': 32,
    '0r': 36, '0R': 36,
    # '0y':64, #more than int's max parsable base (36)
}


def based_number_to_int(src: str) -> int:
    """
    convert a number in a given base to an int
    """
    prefix, digits = src[:2], src[2:]
    if prefix in int_parsable_base_prefixes:
        return int(digits, int_parsable_base_prefixes[prefix])
    elif prefix == '0z':
        raise NotImplementedError(f"base {prefix} is not supported")
    elif prefix == '0y':
        raise NotImplementedError(f"base {prefix} is not supported")
    else:
        raise ValueError(f"INTERNAL ERROR: base {prefix} is not a valid base")


def bool_to_bool(src: str) -> bool:
    """
    convert a (case-insensitive) bool literal to a bool
    """
    try:
        return bool(['false', 'true'].index(src.lower()))
    except ValueError:
        raise ValueError(f"INTERNAL ERROR: bool {src} is not a valid bool") from None


class CaselessStr(str):
    def __init__(self, s: str):
        super().__init__()

    def __eq__(self, other: str):
        return self.casefold() == other.casefold()

    def __hash__(self):
        return hash(self.casefold())

    def __repr__(self):
        return f"(caseless)'{self.casefold()}'"

    def __str__(self):
        return self.casefold()


T = TypeVar('T')
U = TypeVar('U')


class CaseSelectiveDict(Generic[T]):
    """A dictionary where keys may be either case sensitive or case insensitive"""

    def __init__(self, **kwargs):
        self.caseless: dict[CaselessStr, any] = {}
        self.caseful: dict[str, any] = {}
        for k, v in kwargs.items():
            self.__setitem__(k, v)  # Use setitem to handle initial assignments

    def __getitem__(self, key: str | CaselessStr) -> T:
        try:
            return self.caseless[CaselessStr(key)]
        except KeyError:
            pass
        try:
            return self.caseful[key]
        except KeyError:
            pass

        raise KeyError(f"Key {key} not found") from None

    def __setitem__(self, key: str | CaselessStr, value: T):
        if not isinstance(key, (str, CaselessStr)):
            raise TypeError(f"Key must be of type str or CaselessStr, not '{type(key)}'")

        if CaselessStr(key) in self.caseless:
            if not isinstance(key, CaselessStr):
                key = CaselessStr(key)
            self.caseless[key] = value
            return

        if isinstance(key, CaselessStr):
            # need to check if the new key was in the caseful keys. This is an ERROR
            caseful_keys = [*self.caseful.keys()]
            folded_caseful_keys = [k.casefold() for k in caseful_keys]
            if key in folded_caseful_keys:
                existing_key = caseful_keys[folded_caseful_keys.index(key)]
                raise KeyError(f"Cannot insert key {repr(key)}. Caseful version {repr(existing_key)} already exists")
            self.caseless[key] = value
            return

        self.caseful[key] = value

    def __delitem__(self, key: str | CaselessStr):
        key = str(key)
        try:
            del self.caseless[CaselessStr(key)]
            return
        except KeyError:
            pass
        try:
            del self.caseful[key]
            return
        except KeyError:
            pass

        raise KeyError(f"Key {key} not found") from None

    def __contains__(self, key: str | CaselessStr):
        try:
            return CaselessStr(key) in self.caseless or key in self.caseful
        except TypeError:
            return False

    def has_key(self, key: str | CaselessStr):
        return key in self

    def get(self, key: str | CaselessStr, default: U = None) -> T | U:
        try:
            return self[key]
        except KeyError:
            return default

    def __len__(self):
        return len(self.caseless) + len(self.caseful)

    def __iter__(self):
        return iter(self.keys())

    def items(self):
        return {**self.caseless, **self.caseful}.items()

    def keys(self) -> list[str | CaselessStr]:
        return [*self.caseless.keys(), *self.caseful.keys()]

    def values(self) -> list[T]:
        return [*self.caseless.values(), *self.caseful.values()]

    def __repr__(self):
        return f"CaseSelectiveDict({self.caseless}, {self.caseful})"

    def __str__(self):
        items = {**self.caseless, **self.caseful}
        return f'{items}'
1d:T4d7,"""
Collection of all the Dewy Language backends
"""

from .python import python_interpreter
from .qbe import qbe_compiler
from .llvm import llvm_compiler
from .c import c_compiler
from .x86_64 import x86_64_compiler
from .arm import arm_compiler
from .riscv import riscv_compiler
from .shell import shell_compiler
from typing import Protocol
from pathlib import Path

class Backend(Protocol):
    def __call__(self, path: Path, args: list[str]) -> None:
        ...


backend_map: dict[str, Backend] = {
    'python': python_interpreter,
    'qbe': qbe_compiler,
    'llvm': llvm_compiler,
    'c': c_compiler,
    'x86_64': x86_64_compiler,
    'arm': arm_compiler,
    'riscv': riscv_compiler,
    'sh': shell_compiler,
    'zsh': shell_compiler,
    'bash': shell_compiler,
    'fish': shell_compiler,
    'posix': shell_compiler,
    'powershell': shell_compiler,
}
backend_names = [*backend_map.keys()]


def get_backend(name: str) -> Backend:
    try:
        return backend_map[name.lower()]
    except:
        raise ValueError(f'Unknown backend "{name}"') from None


def get_version() -> str:
    """Return the semantic version of the language"""
    return (Path(__file__).parent.parent.parent / 'VERSION').read_text().strip()
1e:T64a3,from ..postparse import post_parse
from ..tokenizer import tokenize
from ..postok import post_process
from ..parser import top_level_parse, Scope as ParserScope
from ..syntax import (
    AST,
    Type,
    PointsTo, BidirPointsTo,
    ListOfASTs, Tuple, Block, Array, Group, Range, Object, Dict, BidirDict, UnpackTarget,
    TypedIdentifier,
    Void, void, Undefined, undefined,
    String, IString,
    Flowable, Flow, If, Loop, Default,
    PrototypeIdentifier, Identifier, Express,
    FunctionLiteral, PrototypePyAction, PyAction, Call,
    Assign,
    Int, Bool,
    Range, IterIn,
    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,
    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv,
    Spread,
    # DeclarationType,
)

from dataclasses import dataclass, field
from pathlib import Path
from typing import Protocol, cast, Callable, Any
from functools import cache
from collections import defaultdict
from types import SimpleNamespace
from argparse import ArgumentParser

import pdb



def python_interpreter(path: Path, args: list[str]):
    arg_parser = ArgumentParser(description='Dewy Compiler: Python Interpreter Backend')
    arg_parser.add_argument('--verbose', action='store_true', help='Print verbose output')
    args = arg_parser.parse_args(args)

    # get the source code and tokenize
    src = path.read_text()
    tokens = tokenize(src)
    post_process(tokens)

    # parse tokens into AST
    ast = top_level_parse(tokens)
    ast = post_parse(ast)

    # debug printing
    if args.verbose:
        print_ast(ast)
        print(repr(ast))

    # run the program
    res = top_level_evaluate(ast)
    if res is not void:
        print(res)

def print_ast(ast: AST):
    """little helper function to print out the equivalent source code of an AST"""
    print('```dewy')
    if isinstance(ast, (Block, Group)):
        for i in ast.__iter_asts__(): print(i)
    else:
        print(ast)
    print('```')


def top_level_evaluate(ast:AST) -> AST:
    scope = Scope.default()
    insert_pyactions(scope)
    return evaluate(ast, scope)


############################ Runtime helper classes ############################

class MetaNamespace(SimpleNamespace):
    """A simple namespace for storing AST meta attributes for use at runtime"""
    def __getattribute__(self, key: str) -> Any | None:
        """Get the attribute associated with the key, or None if it doesn't exist"""
        try:
            return super().__getattribute__(key)
        except AttributeError:
            return None

    def __setattr__(self, key: str, value: Any) -> None:
        """Set the attribute associated with the key"""
        super().__setattr__(key, value)


class MetaNamespaceDict(defaultdict):
    """A defaultdict that preprocesses AST keys to use the classname + memory address as the key"""
    def __init__(self):
        super().__init__(MetaNamespace)

    # add preprocessing to both __getitem__ and __setitem__ to handle AST keys
    # apparently __setitem__ always calls __getitem__ so we only need to override __getitem__
    def __getitem__(self, item: AST) -> Any | None:
        key = f'::{item.__class__.__name__}@{hex(id(item))}'
        return super().__getitem__(key)

@dataclass
class Scope(ParserScope):
    """An extension of the Scope used during parsing to support runtime"""
    meta: dict[AST, MetaNamespace] = field(default_factory=MetaNamespaceDict)


class Iter(AST):
    item: AST
    i: int

    def __str__(self):
        return f'Iter({self.item}, i={self.i})'


############################ Evaluation functions ############################

#DEBUG supporting py3.11
from typing import TypeVar
T = TypeVar('T', bound=AST)
class EvalFunc(Protocol):
    def __call__(self, ast: T, scope: Scope) -> AST: ...

def no_op(ast: T, scope: Scope) -> T:
    """For ASTs that just return themselves when evaluated"""
    return ast

# class EvalFunc[T](Protocol):
#     def __call__(self, ast: T, scope: Scope) -> AST: ...


# def no_op[T](ast: T, scope: Scope) -> T:
#     """For ASTs that just return themselves when evaluated"""
#     return ast

def cannot_evaluate(ast: AST, scope: Scope) -> AST:
    raise ValueError(f'INTERNAL ERROR: evaluation of type {type(ast)} is not possible')


@cache
def get_eval_fn_map() -> dict[type[AST], EvalFunc]:
    return {
        Call: evaluate_call,
        Block: evaluate_block,
        Group: evaluate_group,
        Array: evaluate_array,
        Dict: evaluate_dict,
        PointsTo: evaluate_points_to,
        BidirDict: evaluate_bidir_dict,
        BidirPointsTo: evaluate_bidir_points_to,
        Assign: evaluate_assign,
        IterIn: evaluate_iter_in,
        FunctionLiteral: evaluate_function_literal,
        Closure: evaluate_closure,
        PyAction: evaluate_pyaction,
        String: no_op,
        IString: evaluate_istring,
        Identifier: cannot_evaluate,
        Express: evaluate_express,
        Int: no_op,
        Bool: no_op,
        Range: no_op,
        Flow: evaluate_flow,
        Default: evaluate_default,
        If: evaluate_if,
        Loop: evaluate_loop,
        Less: evaluate_less,
        Equal: evaluate_equal,
        And: evaluate_and,
        Or: evaluate_or,
        Not: evaluate_not,
        Add: evaluate_add,
        Mul: evaluate_mul,
        Mod: evaluate_mod,
        Undefined: no_op,
        Void: no_op,
        #TODO: other AST types here
    }

def evaluate(ast:AST, scope:Scope) -> AST:
    eval_fn_map = get_eval_fn_map()

    ast_type = type(ast)
    if ast_type in eval_fn_map:
        return eval_fn_map[ast_type](ast, scope)

    raise NotImplementedError(f'evaluation not implemented for {ast_type}')



def evaluate_call(ast: Call, scope: Scope) -> AST:
    f = ast.f
    if isinstance(f, Group):
        f = evaluate(f, scope)
    if isinstance(f, Identifier):
        f = scope.get(f.name).value
    assert isinstance(f, (PyAction, Closure)), f'expected Function or PyAction, got {f}'

    if isinstance(f, PyAction):
        args, kwargs = collect_args(ast.args, scope)
        return f.action(*args, **kwargs, scope=scope)

    if isinstance(f, Closure):
        args, kwargs = collect_args(ast.args, scope)
        return evaluate(f.fn.body, f.scope)

    pdb.set_trace()
    raise NotImplementedError(f'Function evaluation not implemented yet')

def collect_args(args: AST | None, scope: Scope) -> tuple[list[AST], dict[str, AST]]:
    match args:
        case None: return [], {}
        case Identifier(name): return [scope.get(name).value], {}
        case Assign(): raise NotImplementedError('Assign not implemented yet')
        # case Tuple(items): raise NotImplementedError('Tuple not implemented yet')
        case String() | IString(): return [args], {}
        case Group(items): return [evaluate(i, scope) for i in items], {}
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'collect_args not implemented yet for {args}')


    raise NotImplementedError(f'collect_args not implemented yet for {args}')

def evaluate_group(ast: Group, scope: Scope):

    expressed: list[AST] = []
    for expr in ast.items:
        res = evaluate(expr, scope)
        if res is not void:
            expressed.append(res)
    if len(expressed) == 0:
        return void
    if len(expressed) == 1:
        return expressed[0]
    raise NotImplementedError(f'Block with multiple expressions not yet supported. {ast=}, {expressed=}')


def evaluate_block(ast: Block, scope: Scope):
    scope = Scope(scope)
    return evaluate_group(Group(ast.items), scope)

def evaluate_array(ast: Array, scope: Scope) -> Array:
    return Array([evaluate(i, scope) for i in ast.items])

def evaluate_dict(ast: Dict, scope: Scope) -> Dict:
    return Dict([evaluate(kv, scope) for kv in ast.items])

def evaluate_points_to(ast: PointsTo, scope: Scope) -> PointsTo:
    return PointsTo(evaluate(ast.left, scope), evaluate(ast.right, scope))

def evaluate_bidir_dict(ast: BidirDict, scope: Scope) -> BidirDict:
    return BidirDict([evaluate(kv, scope) for kv in ast.items])

def evaluate_bidir_points_to(ast: BidirPointsTo, scope: Scope) -> BidirPointsTo:
    return BidirPointsTo(evaluate(ast.left, scope), evaluate(ast.right, scope))

def evaluate_assign(ast: Assign, scope: Scope):
    match ast:
        case Assign(left=Identifier(name), right=right):
            right = evaluate(right, scope)
            scope.assign(name, right)
            return void
        case Assign(left=UnpackTarget() as target, right=right):
            right = evaluate(right, scope)
            unpack_assign(target, right, scope)
            return void
    pdb.set_trace()
    raise NotImplementedError('Assign not implemented yet')


def evaluate_iter_in(ast: IterIn, scope: Scope):

    # helper function for progressing the iterator
    def step_iter_in(iter_props: tuple[Callable, Iter], scope: Scope) -> AST:
        binder, iterable = iter_props
        cond, val = iter_next(iterable).items
        binder(val)
        return cond

    # if the iterator properties are already in the scope, use them
    if (res := scope.meta[ast].props) is not None:
        return step_iter_in(cast(tuple[Callable, Iter], res), scope)

    # otherwise initialize since this is the first time we're hitting this IterIn
    match ast:
        case IterIn(left=Identifier(name), right=right):
            right = evaluate(right, scope)
            props = lambda x: scope.assign(name, x), Iter(item=right, i=0)
            scope.meta[ast].props = props
            return step_iter_in(props, scope)
        case IterIn(left=UnpackTarget() as target, right=right):
            right = evaluate(right, scope)
            props = lambda x: unpack_assign(target, x, scope), Iter(item=right, i=0)
            scope.meta[ast].props = props
            return step_iter_in(props, scope)

    pdb.set_trace()
    raise NotImplementedError('IterIn not implemented yet')

# def determine_how_many_to_take
def take_n(gen, n:int):
    return [next(gen) for _ in range(n)]

#TODO: this is really only for array unpacking. need to handle object unpacking as well...
#      need to check what type value
def unpack_assign(target: UnpackTarget, value: AST, scope: Scope):

    # current inefficient hack to unpack strings
    if isinstance(value, String):
        value = Array([String(c) for c in value.val])

    # current types supporting unpacking
    if not isinstance(value, (Array, Dict, PointsTo, BidirDict, BidirPointsTo, Undefined)):
        raise NotImplementedError(f'unpack_assign() is not yet implemented for {value=}')

    # determine how many targets will be assigned, and if spread is present
    num_targets = len(target.target)
    num_spread = sum(isinstance(t, Spread) for t in target.target)
    if num_spread > 1: raise RuntimeError(f'Only one spread is allowed in unpacking. {target=}, {value=}')

    # undefined unpacks as many undefineds as there are non-spread targets
    if isinstance(value, Undefined):
        value = Array([undefined for _ in range(num_targets - num_spread)])

    # verify if enough values to unpack, and set up generator (using built in iteration over ASTs children)
    num_values = len([*value.__iter_asts__()])
    spread_size = num_values - num_targets + 1  # if a spread is present, how many elements it will take
    if num_targets - num_spread > num_values: raise RuntimeError(f'Not enough values to unpack. {num_targets=}, {target=}, {value=}')
    gen = value.__iter_asts__()

    for left in target.target:
        match left:
            case Identifier(name):
                scope.assign(name, next(gen))
            # #TODO: object member renamed unpack. need to get the member of the object and assign it to the new name
            # case Assign(left=Identifier(name), right=right):
            #     scope.assign(name, right)
            case UnpackTarget():
                unpack_assign(left, next(gen), scope)
            case Spread(right=Identifier(name)):
                scope.assign(name, Array([next(gen) for _ in range(spread_size)]))
            case Spread(right=UnpackTarget() as left):
                unpack_assign(left, Array([next(gen) for _ in range(spread_size)]), scope)
            case _:
                pdb.set_trace()
                raise NotImplementedError(f'unpack_assign not implemented for {left=} and right={next(gen)}')

    # if there are any remaining values, raise an error
    if (remaining := [*gen]):
        raise RuntimeError(f'Too many values to unpack. {num_targets=}, {target=}, {value=}, {remaining=}')

# TODO: probably break this up into one function per type of iterable
def iter_next(iter: Iter):
    match iter.item:
        case Array(items):
            if iter.i >= len(items):
                cond, val = Bool(False), undefined
            else:
                cond, val = Bool(True), items[iter.i]
            iter.i += 1
            return Array([cond, val])
        case Dict(items):
            if iter.i >= len(items):
                cond, val = Bool(False), undefined
            else:
                cond, val = Bool(True), items[iter.i]
            iter.i += 1
            return Array([cond, val])
        case Range(left=Int(val=l), right=Void()|Undefined(), brackets=brackets):
            offset = int(brackets[0] == '(') # handle if first value is exclusive
            cond, val = Bool(True), Int(l + iter.i + offset)
            iter.i += 1
            return Array([cond, val])
        case Range(left=Int(val=l), right=Int(val=r), brackets=brackets):
            offset = int(brackets[0] == '(') # handle if first value is exclusive
            end_offset = int(brackets[1] == ']')
            i = l + iter.i + offset
            if i > r + end_offset - 1:
                cond, val = Bool(False), undefined
            else:
                cond, val = Bool(True), Int(i)
            iter.i += 1
            return Array([cond, val])
        case Range(left=Array(items=[Int(val=r0), Int(val=r1)]), right=Void()|Undefined(), brackets=brackets):
            offset = int(brackets[0] == '(') # handle if first value is exclusive
            step = r1 - r0
            cond, val = Bool(True), Int(r0 + (iter.i + offset) * step)
            iter.i += 1
            return Array([cond, val])
        case Range(left=Array(items=[Int(val=r0), Int(val=r1)]), right=Int(val=r2), brackets=brackets):
            offset = int(brackets[0] == '(') # handle if first value is exclusive
            end_offset = int(brackets[1] == ']')
            step = r1 - r0
            i = r0 + (iter.i + offset) * step
            if i > r2 + end_offset - 1:
                cond, val = Bool(False), undefined
            else:
                cond, val = Bool(True), Int(i)
            iter.i += 1
            return Array([cond, val])
        #TODO: other range cases...
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'iter_next not implemented yet for {iter.item=}')


class Closure(AST):
    fn: FunctionLiteral
    scope: Scope
    # call_args: AST|None=None # TBD how to handle
    def __str__(self):
        return f'Closure({self.fn}, scope={self.scope})'

def evaluate_function_literal(ast: FunctionLiteral, scope: Scope):
    return Closure(fn=ast, scope=scope)

def evaluate_closure(ast: Closure, scope: Scope):
    fn_scope = Scope(ast.scope)
    #TODO: for now we assume everything is 0 args. need to handle args being attached to the closure
    return evaluate(ast.fn.body, fn_scope)

    #grab arguments from scope and put them in fn_scope
    pdb.set_trace()
    ast.fn.args
    raise NotImplementedError('Closure not implemented yet')

def evaluate_pyaction(ast: PyAction, scope: Scope):
    # fn_scope = Scope(ast.scope)
    #TODO: currently just assuming 0 args in and no return
    return ast.action(scope)


def evaluate_istring(ast: IString, scope: Scope) -> String:
    parts = (py_stringify(i, scope) for i in ast.parts)
    return String(''.join(parts))


def evaluate_express(ast: Express, scope: Scope):
    val = scope.get(ast.id.name).value
    return evaluate(val, scope)

def evaluate_flow(ast: Flow, scope: Scope):
    for branch in ast.branches:
        #TODO: slightly hacky way to get the child scope created by the branch (so we can check if it was entered)
        child_scope = None
        def save_child_scope(scope: Scope):
            nonlocal child_scope
            child_scope = scope

        match branch:
            case Default(): res = evaluate_default(branch, scope, save_child_scope)
            case If(): res = evaluate_if(branch, scope, save_child_scope)
            case Loop(): res = evaluate_loop(branch, scope, save_child_scope)
            case _:
                pdb.set_trace()
                raise NotImplementedError(f'evaluate_flow not implemented for flow type {branch=}')

        # if the branch was entered, return its result
        assert child_scope.meta[branch].was_entered is not None, f'INTERNAL ERROR: {branch=} .was_entered was not set'
        if child_scope.meta[branch].was_entered:
            return res

    # if no branches were entered, return void
    return void

def evaluate_default(ast: Default, scope: Scope, save_child_scope:Callable[[Scope], None]=lambda _: None):
    scope = Scope(scope)
    save_child_scope(scope)
    scope.meta[ast].was_entered = True
    return evaluate(ast.body, scope)

#TODO: this needs improvements!
#Issue URL: https://github.com/david-andrew/dewy-lang/issues/2
def evaluate_if(ast: If, scope: Scope, save_child_scope:Callable[[Scope], None]=lambda _: None):
    scope = Scope(scope)
    save_child_scope(scope)
    scope.meta[ast].was_entered = False
    if cast(Bool, evaluate(ast.condition, scope)).val:
        scope.meta[ast].was_entered = True
        return evaluate(ast.body, scope)

    # is this correct if the If isn't entered?
    return void

#TODO: this needs improvements!
def evaluate_loop(ast: Loop, scope: Scope, save_child_scope:Callable[[Scope], None]=lambda _: None):
    scope = Scope(scope)
    save_child_scope(scope)
    scope.meta[ast].was_entered = False
    while cast(Bool, evaluate(ast.condition, scope)).val:
        scope.meta[ast].was_entered = True
        evaluate(ast.body, scope)

    # for now loops can't return anything
    return void
    # ast.body
    # ast.condition
    # pdb.set_trace()

class Comparable(Protocol):
    def __lt__(self, other: "Comparable") -> bool: ...
    def __le__(self, other: "Comparable") -> bool: ...
    def __gt__(self, other: "Comparable") -> bool: ...
    def __ge__(self, other: "Comparable") -> bool: ...
    def __eq__(self, other: "Comparable") -> bool: ...
    def __ne__(self, other: "Comparable") -> bool: ...

def evaluate_comparison_op(op: Callable[[Comparable, Comparable], bool], ast: AST, scope: Scope):
    left = evaluate(ast.left, scope)
    right = evaluate(ast.right, scope)
    match left, right:
        case Int(val=l), Int(val=r): return Bool(op(l, r))
        case String(val=l), String(val=r): return Bool(op(l, r))
        case _:
            raise NotImplementedError(f'{op.__name__} not implemented for {left=} and {right=}')

def evaluate_less(ast: Less, scope: Scope):
    return evaluate_comparison_op(lambda l, r: l < r, ast, scope)

def evaluate_equal(ast: Equal, scope: Scope):
    return evaluate_comparison_op(lambda l, r: l == r, ast, scope)



# TODO: op depends on what type of operands. bools use built-in and/or/etc, but ints need to use the bitwise operators
def evaluate_logical_op(logical_op: Callable[[bool, bool], bool], bitwise_op: Callable[[int, int], int], ast: AST, scope: Scope):
    left = evaluate(ast.left, scope)
    right = evaluate(ast.right, scope)
    match left, right:
        case Bool(val=l), Bool(val=r): return Bool(logical_op(l, r))
        case Int(val=l), Int(val=r): return Int(bitwise_op(l, r))
        case _:
            raise NotImplementedError(f'evaluate logical op not implemented for {left=} and {right=}')

def evaluate_and(ast: And, scope: Scope):
    return evaluate_logical_op(lambda l, r: l and r, lambda l, r: l & r, ast, scope)

def evaluate_or(ast: Or, scope: Scope):
    return evaluate_logical_op(lambda l, r: l or r, lambda l, r: l | r, ast, scope)

def evaluate_not(ast: Not, scope: Scope):
    val = evaluate(ast.operand, scope)
    match val:
        case Bool(val=v): return Bool(not v)
        case Int(val=v): return Int(~v) #TODO: bitwise not depends on the size of the int...
        case _:
            raise NotImplementedError(f'Not not implemented for {val=}')

#TODO: long term, probably convert this into a matrix for all the input types and ops, where pairs can register to it
# def evaluate_arithmetic_op[T](op: Callable[[T, T], T], ast: AST, scope: Scope):
#     left = evaluate(ast.left, scope)
#     right = evaluate(ast.right, scope)
#     match left, right:
#         case Int(val=l), Int(val=r): return Int(op(l, r))
#         case Array(), Array(): return Array(op(left.items, right.items)) #TODO: restrict this to add only...
#         case _:
#             raise NotImplementedError(f'{op.__name__} not implemented for {left=} and {right=}')

#TODO: unified arithmetic evaluation function
def evaluate_add(ast: Add, scope: Scope):
    left = evaluate(ast.left, scope)
    right = evaluate(ast.right, scope)
    match left, right:
        case Int(val=l), Int(val=r): return Int(l + r)
        case Array(items=l), Array(items=r): return Array(l + r)
        case _:
            raise NotImplementedError(f'Add not implemented for {left=} and {right=}')

def evaluate_mul(ast: Mul, scope: Scope):
    left = evaluate(ast.left, scope)
    right = evaluate(ast.right, scope)
    match left, right:
        case Int(val=l), Int(val=r): return Int(l * r)
        case _:
            raise NotImplementedError(f'Mul not implemented for {left=} and {right=}')

def evaluate_mod(ast: Mod, scope: Scope):
    left = evaluate(ast.left, scope)
    right = evaluate(ast.right, scope)
    match left, right:
        case Int(val=l), Int(val=r): return Int(l % r)
        case _:
            raise NotImplementedError(f'Mod not implemented for {left=} and {right=}')

############################ Builtin functions and helpers ############################

# all references to python functions go through this interface to allow for easy swapping
from functools import partial
class BuiltinFuncs:
    printl=print
    print=partial(print, end='')
    readl=input

#TODO: consider adding a flag repr vs str, where initially str is used, but children get repr. 
# as is, stringifying should put quotes around strings that are children of other objects 
# but top level printed strings should not show their quotes
def py_stringify(ast: AST, scope: Scope) -> str:
    ast = evaluate(ast, scope)
    match ast:
        # types that require special handling (i.e. because they have children that need to be stringified)
        case String(val): return val #TODO: should get quotes if stringified as a child
        case Array(items): return f"[{' '.join(py_stringify(i, scope) for i in items)}]"
        case Dict(items): return f"[{' '.join(py_stringify(kv, scope) for kv in items)}]"
        case PointsTo(left, right): return f'{py_stringify(left, scope)}->{py_stringify(right, scope)}'
        case BidirDict(items): return f"[{' '.join(py_stringify(kv, scope) for kv in items)}]"
        case BidirPointsTo(left, right): return f'{py_stringify(left, scope)}<->{py_stringify(right, scope)}'

        # can use the built-in __str__ method for these types
        case Int() | Bool() | Undefined(): return str(ast)

        # TBD what other types need to be handled
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'stringify not implemented for {type(ast)}')
    pdb.set_trace()


    raise NotImplementedError('stringify not implemented yet')

#TODO: fix the function signatures here! they should not be keyword only for scope.
#Issue URL: https://github.com/david-andrew/dewy-lang/issues/8
#      fixing will involve being able to set default arguments for regular functions
#      then making the pyaction have a default for the string s=''
def py_printl(s:String|IString=None, *, scope: Scope) -> Void:
    # TODO: hacky way of handling no-arg case
    if s is None:
        s = String('')
    py_print(s, scope=scope)
    BuiltinFuncs.printl()
    return void

def py_print(s:String|IString=None, *, scope: Scope) -> Void:
    # TODO: hacky way of handling no-arg case
    if s is None:
        s = String('')
    if not isinstance(s, (String, IString)):
        s = String(py_stringify(s, scope))
        # raise ValueError(f'py_print expected String or IString, got {type(s)}:\n{s!r}')
    if isinstance(s, IString):
        s = cast(String, evaluate(s, scope))
    BuiltinFuncs.print(s.val)
    return void

def py_readl(scope: Scope) -> String:
    return String(input())

def insert_pyactions(scope: Scope):
    """replace pyaction stubs with actual implementations"""
    if 'printl' in scope.vars:
        assert isinstance((proto:=scope.vars['printl'].value), PrototypePyAction)
        scope.vars['printl'].value = PyAction(proto.args, py_printl, proto.return_type)
    if 'print' in scope.vars:
        assert isinstance((proto:=scope.vars['print'].value), PrototypePyAction)
        scope.vars['print'].value = PyAction(proto.args, py_print, proto.return_type)
    if 'readl' in scope.vars:
        assert isinstance((proto:=scope.vars['readl'].value), PrototypePyAction)
        scope.vars['readl'].value = PyAction(proto.args, py_readl, proto.return_type)
1f:T8e5,//TODO: uncomment when types are supported
/{
// simple xorshift+ generator
state:uint64 = 123456789
rand = ():uint64 => {
    state xor= state >> 21
    state xor= state << 35
    state xor= state >> 4
    
    return state * 2_685821_657736_338717 //TODO: divide by uint64.MAX (18_446744_073709_551615)
}
half = 9_223372_036854_775807
a = rand <? half
b = rand <? half
c = rand <? half
d = rand <? half
e = rand <? half
f = rand <? half
g = rand <? half
h = rand <? half
i = rand <? half
j = rand <? half
k = rand <? half
l = rand <? half
m = rand <? half
n = rand <? half
o = rand <? half
p = rand <? half
q = rand <? half
r = rand <? half
s = rand <? half
t = rand <? half
u = rand <? half
v = rand <? half
w = rand <? half
x = rand <? half
y = rand <? half
z = rand <? half
}/

//manually specify bools
a = false
b = true
c = true
d = false
e = true
f = false
g = true
h = false
i = true
j = false
k = true
l = false
m = true
n = false
o = true
p = false
q = true
r = false
s = true
t = false
u = true
v = false
w = true
x = false
y = true
z = false



if a
    printl'a'
else if b
    if c
        if d
            if e
                printl'bcde'
            else if f
                if g
                    if h
                        printl'bcdfgh'
                    else if i
                        printl'bcdfgi'
                    else if j
                        printl'bcdfgj'
                    else
                        printl'bcdfg[]'
                else if k
                    printl'bcdfk'
                else if l
                    if m
                        printl'bcdflm'
                    else
                        printl'bcdfl[]'
                else
                    printl'bcdf[]'
            else if n
                printl'bcdn'
            else
                printl'bcd[]'
        else if o
            printl'bco'
        else if p
            if q
                printl'bcpq'
            else
                printl'bcp[]'
        else
            printl'bc[]'
    else
        printl'b[]'
else if r
    printl'r'
else if s
    if t
        printl'st'
    else if u
        printl'su'
if v
    if w
        printl'vw'
    else if x
        printl'x'
if y
    printl'y'
else if z
    printl'z'
else
    printl'[]'
20:Tbe6,/{ 
    Dewy Docs mdbook preprocessor:
    find code blocks labelled 'dewy' or 'dewy, editable' and convert them to iframes
}/

#main = () => {
    // mdbook calls preprocessor twice, first with args ["supports", <renderer>], 
    // and then if the first call exited with 0, the actual preprocessor run occurs
    // ignore the second argument (meaning support for all renderers)
    if sys.argv.length >? 1 and sys.argv[1] =? "supports"
        sys.exit(0)

    // get and parse the json input from stdin
    // TODO: need to make a json parser
    context, book = parse_json(read())

    loop section in book['sections']
    {
        section['Chapter']['content'] |>= process_markdown

        loop subitem in section['Chapter']['sub_items']
            subitem['Chapter']['content'] |>= process_markdown

        print(dump_json(book))
    }
}


counter = iter[0..]
process_markdown = (input_markdown) => {
    lines = input_markdown.split('\n')

    // lines starting with ```dewy
    starts = [
        loop i in [0..] and line in lines 
            if line[..6] =? '```dewy' 
                i
    ]

    // early return if no dewy code blocks
    if starts.length =? 0
        return input_markdown

    // ```dewy lines that are followed by ', editable' (whitespace invariant)
    editables = [
        loop i in starts
        {
            remainder = lines[i][7..].strip
            remainder[0] = ',' and remainder[1..].strip =? 'editable'
        }
    ]

    // match closing ``` lines
    stops = [
        loop i in starts
            loop line in lines[i..]
                if line =? '```'
                {
                    j
                    break
                }
    ]

    if starts.length not=? stops.length
        printl'Error: mismatched dewy code block starts and ends'
        sys.exit(1)

    return [
        loop 
            start in starts 
            and stop in stops 
            and editable in editables
            and prev_stop in [0 ...(stops.+1)]
        {
            //push the previous non-code block content
            if start >? prev_stop
                lines[prev_stop..start).join('\n')

            i = next(counter) // to give each iframe a unique id
            page = if editable 'demo_only' else 'src_only'
            code = lines(start..stop).join('\n')
            encoded_code = url_quote(code)
            
            // push the iframe replacement
            f'
                <iframe
                    src="https://david-andrew.github.io/iframes/dewy/{page}?src={encoded_code}&id=DewyIframe{i}"
                    style="width: 100%; border-radius: 0.5rem;"
                    id="DewyIframe{i}"
                    frameBorder="0"
                ></iframe>
            '
        }

        // push the last non-code block content
        if stops[-1]+1 <? lines.length
            lines[stops[-1]+1..].join('\n')
 
    ].join('\n')
}


//TODO: implement these functions
let parse_json = () => {}
let dump_json = () => {}
let url_quote = () => {}
let f = () => {}21:T1093,//simple, fast, high quality, dependency-free RNG generation
//uniform distribution via XORSHIFT*
//normal distribution via PPND16


RNG = (s:uint64) => [
    next_u64 = () => {
        s ^= s >> 21
        s ^= s << 35
        s ^= s >> 4
        s * 2685821657736338717
    }
    next_uniform = () => fast_to_uniform(next_u64())
    next_normal = () => ppnd16(fast_to_uniform(next_u64()))
    
    /{
        Use bit hacks to quickly convert a 64-bit number to a double in the range [0, 1)

        @param x the number to convert. Only the lowest 23 bits are used.
        @return the number in the range [0, 1)
    }/
    fast_to_uniform = (x:uint64):float64 => {
        const mask1 = 0x3FF0_0000_0000_0000
        const mask2 = 0x3FFF_FFFF_FFFF_FFFF
        out: uint64 = (x or mask1) and mask2
        (out as float64) - 1
    }

    full_to_uniform = (x:uint64):float64 => truediv(x uint64.max float64)

    /{
        Convert a uniformly distributed double in the range (0, 1) to a normally distributed double
        Uses the PPND16 algorithm from Algorithm AS241: The Percentage Points of the Normal Distribution

        @param x the uniformly distributed double in the range (0, 1)
        @return the normally distributed double
    }/
    ppnd16 = (x:float64) => {

        // zero area at x=0/x=1
        if x <=? 0 or x >=? 1
            return 0

        const split1:float64 = 0.425
        const split2:float64 = 5.0
        const C1:float64 = 0.180625
        const C2:float64 = 1.6

        // Cofficients for x close to 0.5
        const A:float64[] =
        [
            3.3871328727963665
            133.14166789178438
            1971.5909503065514
            13731.693765509461
            45921.95393154987
            67265.7709270087
            33430.575583588128
            2509.0809287301227            
        ]
        const B:float64[] =
        [
            1.0
            42.313330701600911
            687.18700749205789
            5394.1960214247511
            21213.794301586597
            39307.895800092709
            28729.085735721943
            5226.4952788528544
        ]
        
        // Coefficients for x not close to 0, 0.5 or 1
        const C:float64[] =
        [
            1.4234371107496835
            4.6303378461565456
            5.769497221460691
            3.6478483247632045
            1.2704582524523684
            0.24178072517745061
            0.022723844989269184
            0.00077454501427834139
        ]
        const D:float64[] =
        [
            1.0
            2.053191626637759
            1.6763848301838038
            0.6897673349851
            0.14810397642748008
            0.015198666563616457
            0.00054759380849953455
            0.0000000010507500716444169
        ]

        // Coefficients for x near 0 or 1
        const E:float64[] =
        [
            6.6579046435011033
            5.4637849111641144
            1.7848265399172913
            0.29656057182850487
            0.026532189526576124
            0.0012426609473880784
            0.000027115555687434876
            0.00000020103343992922882
        ]
        const F:float64[] =
        [
            1.0
            0.599832206555888
            0.13692988092273581
            0.014875361290850615
            0.00078686913114561329
            0.000018463183175100548
            0.0000001421511758316446
            0.0000000000000020442631033899397
        ]

        let r:float64

        // shift x to the range (-0.5, 0.5)
        const q = x - 0.5
        if abs(q) <=? split1
        {
            r = C1 - q^2
            powers = r.^[0..7]
            return q * (A .* powers).sum / (B .* powers).sum
        }
        
        // shift x back to (0,1) and invert if it was positive
        r = if q <? 0 x else 1 - x
        r = sqrt(-log(r))
        
        if r <=? split2
        {
            r -= C2
            powers = r.^[0..7]
            return sign(q) * (C .* powers).sum / (D .* powers).sum
        }
        else
        {
            r -= split2
            powers = r.^[0..7]
            return sign(q) * (E .* powers).sum / (F .* powers).sum
        }
    }
]



r = RNG(42)
loop i in 0..100 printl(r.next_normal())22:Tc05,// examples of syntax used in dewy
// line comments
/{ block/multiline comments }/

// typed declaration
apple: uint64
banana: map<int string>
peach: array<int length=N>  //array of ints with length N...
let pear: set<range>  // let indicates that this is definitely a new declaration, even if the identifier already exists



// unpack assignment examples
A = 1..10
B = [loop a in A -a]
loop [a b] in [A B] {}

// object with nested objects to unpack
my_obj = [
    apple = [1 2 3 4 [
        ultimate_answer = 42
    ]]
    banana = 10
    peach = [
        purple = 23
        blue = 12
        orange = 'orange'
    ]
]

// nested unpack assignment. tbd if the top level is `[unpack, params, etc] = obj`, or `obj as [unpack, params, etc]`
[
    [
        a1
        a2 
        a3 
        a4 
        [answer = ultimate_answer] = a5
    ] = apple
    renamed_banana = banana
    [purple blue orange] = peach
] = my_obj
// unpacked variables are:
//   a1 = 1, a2 = 2, a3 = 3, a4 = 4
//   answer = 42
//   renamed_banana = 10
//   purple = 23, blue = 12, orange = 'orange'

// unpacking dictionaries probably treats them as just the list of key -> value pairs
// unpacking sets, probably just treats the elements like a normal array
// unpacking ranges treats them as a normal array

// `...` can be used in unpack to coalesce extra elements for list-like containers
// there may only be 1 `...` in an unpack (otherwise it would be ambiguous which elements to collect)
// the variable receiving the `...` will be of the same type as the original object being unpacked
my_arr = [1 2 3 4 5 6 7 8 9]
[a1 a2 a3 ...my_arr a8 a9] = my_arr  //a1 = 1, a2 = 2, a3 = 3, my_arr = [4, 5, 6, 7], a8 = 8, a9 = 9

my_dict = ['apple' -> 1 'banana' -> 2 'peach' -> 3 'pie' -> 4]
[d1 ...dict_left] = my_dict //d1 = ('apple' -> 1), dict_left = ['banana' -> 2 'peach' -> 3 'pie' -> 4]

// random range note: for step sizes other than +1, use range_iter constructor e.g. range_iter(start to stop, step=5)
my_range = 1..inf
loop my_range.length >? 0 ( [i ...my_range] = my_range )
// first iteration: i = 1, my_range = 2..inf
// second iteration: i = 2, my_range = 3..inf
// third iteration: i = 3, my_range = 4..inf
// ...
// for forever

//[...my_range i] = my_range //will probably set my_range = 1 to inf, i = inf

// unpacking too many values, or named values that don't exist just sets them to undefined



// assignment expressions (i.e. python's walrus operator from https://www.python.org/dev/peps/pep-0572/)
// Handle a matched regex
if (match = pattern.search(data); match) not =? undefined
{
    // Do something with match
}

// A loop that can't be trivially rewritten using 2-arg iter()
loop (chunk = file.read(8192) chunk.length >? 0)
{
   process(chunk)
}

// Reuse a value that's expensive to compute
[y=f(x) y y**2 y**3]

// Share a subexpression between a comprehension filter clause and its output
filtered_data = [for x in data if (y=f(x) y) not =? undefined y]
//though you could also just write this like so
filtered_data = [for x in data {y=f(x) if y not =? undefined y}]
23:T14e1,///////////////////// STRING INTERPOLATION /////////////////////

/{Todo: probably break each section for different syntaxes into different files?}/

//silly example with keyword vs identifier
loop i i

r'this is a raw string \'  expr  'a separate string later'

// simple blocks
{   }
( /{comment inside}/ )
{ 2+2 }
( 2+2 )


//string interpolation
my_string = '2 + 2 = {2+2}'

//complex interpolation
s = "first 10 primes are: {
    primes = [2]
    loop i in [3, 5..) and primes.length <? 10
        if i .% primes |> product not =? 0 
            primes.push(i)
    primes
}"


//alternative prime generator + getting first 10 primes
#ctx
primes = [
    2
    lazy i in [3, 5..)
        if i .% #ctx.primes .=? 0 |> @reduce(, (prev, v) => prev and v)
            i
][..10)

//TBD if there is a parallel way to do this where the i .% primes dispatches each operation, and fails immediately on any returning false
#label
primes = [
    2
    lazy i in [3, 5..)
        if not parallel_or(p => i % p =? 0, #label.primes)
            i
][..10)
//parallel or is like goroutines with cancel once any is true...should have it be more flexible, e.g. able to use any of the boolean keywords that can short circuit
//actually probably don't want to need to specify that it's parallel. Instead if there's an operation over a vector, it gets parallelized if possible.

//nested interpolation
s2 = 'this is an outer string, and {'this is an interior string with "{my_string}" in it'}'





const add = (a:int b:int): int => { /{return sum of a and b}/ }
let div = (a:real b:real): real? => { /{return a / b}/ }

// function type with named default argument
my_func = (s:string kwarg:bool=false): void => {}

//you probably can do the verbose version as well (probably useful for when you're just defining the interface without the implementation)
my_func: (s:string kwarg:bool=false): void



// example annotations for function types
() => ()
() => void
() => bool
int => bool
a: int => bool
(int int) => int
<T>(T T) => T
<T>(a:T b:T) => T

// object type
[a:int? b:string]

//? (optional) is sugar for |void
[a:int|void b:string]

// operators juxtaposed to identifiers
aorb
a or b
a+b


//based number literals
0b1010_0011_0101_0110_1001_1010_1100_1111
0B0101_1111_1010_1110_0011_0101_1001_1100

0t012_221_012_221_012_221_012_221
0t211_001_211_001_211_001_211_001

0q331_231_223_131_331_231_223_131
0Q123_321_123_321_123_321_123_321

0s123_450_123_450_123_450_123_450
0S543_210_543_210_543_210_543_210

0o123_456_701_234_567_012_345_670
0O012_345_670_123_456_701_234_567

0d123_456_789_012_345_678_901_234
0D987_654_321_098_765_432_109_876

0z123_456_789_xe0_123_456_789_xe0
0ZEX9_876_543_210_987_654_321_09E

0x1234_5678_9abc_def0_1234_5678_9abc_def0
0XFEDC_BA98_7654_3210_fedc_ba98_7654_3210

0u0123456789abcdefghijklmnopqrstuv0123456
0UVUTSRQPONMLKJIHGFEDCBA9876543210vutsrq

0r0123456789abcdefghijklmnopqrstuvwxyz012345
0RZYXWVUTSRQPONMLKJIHGFEDCBA9876543210zyxwv

0y0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!$
0Y$!ZYXWVUTSRQPONMLKJIHGFEDCBA9876543210zyxwvutsrqponmlkjihgfedcba

//Units TODO




[a b c] = [1 2 3]                                       //a=1, b=2, c=3
[a [b c]] = [1 [2 3]]                                   //a=1, b=2, c=3
[a [b c] d] = [1 [2 3] 4]                               //a=1, b=2, c=3, d=4
[a ...b] = [1 2 3 4]                                    //a=1, b=[2 3 4]
[a ...b c] = [1 2 3 4 5]                                //a=1, b=[2 3 4], c=5
[a ...b [c [...d e f]]] = [1 2 3 4 [5 [6 7 8 9 10]]]    //a=1, b=[2 3 4], c=5, d=[6 7 8], e=9, f=10



// silly things that are technically valid
x = loop i in [0..10] i //x=10

y = [
    if something 
        x
    else loop i in something_else
        y
    else if z
        z
    else loop i in last_thing
        w
    else
        ()
]


apple & banana
apple&banana
apple | banana
apple|banana





///////////// String prefixes ////////////////
p = s:string => [
    //process s based on / and \ separators
    //store result in this object
    route:array<string> = ...
    filename:string? = ...
    extension:string? = ...
]

p"this/is/a/file/path.ext"

//other prefixes
re"[^i*&2@]"                            // regex literal
t'my_token'                             //token literal. probably my version of enums
r'this is a raw string'                 //raw string. technically handled during tokenizing, there is no r function
(dewy)r'''printl("Hello, World!")'''    //dewy source code literal. uses raw string so that we don't have to worry about {}.

ipa"ɛt vɔkavit dɛus aɾidam tɛɾam kɔngɾɛgatsiɔnɛskwɛ akwaɾum apɛlavit maɾia ɛt vidit dɛus kwɔd ɛsɛt bɔnum" //international phonetic alphabet literal
(apl)r"life ← {⊃1 ⍵ ∨.∧ 3 4 = +/ +⌿ ¯1 0 1 ∘.⊖ ¯1 0 1 ⌽¨ ⊂⍵}"  //apl expression literal
apl<|r"life ← {⊃1 ⍵ ∨.∧ 3 4 = +/ +⌿ ¯1 0 1 ∘.⊖ ¯1 0 1 ⌽¨ ⊂⍵}"  //same as above

'''this is a regular string with triple quotes'''
"""this is a regular string with triple quotes"""

///////////// object prefixes ////////////////
//doubly linked list
dll[1 2 3 4 6 5 3 6 3 2]

//set literal syntax
set[4 3 6 4 6 4 2 2 4 5]



// silly example for generating a list of ones
ones = n => {l = [...[1..n]] l.=1 l}
ones(10) // [1 1 1 1 1 1 1 1 1 1]
//alternate
ones = n => [loop i in 1..n 1]11:[["$","$13",null,{"fallback":null,"children":["$","$L14",null,{"children":["$","$L15",null,{"dewy_interpreter_source":[{"name":"src/__init__.py","code":""},{"name":"src/frontend.py","code":"$16"},{"name":"src/parser.py","code":"$17"},{"name":"src/postok.py","code":"$18"},{"name":"src/postparse.py","code":"$19"},{"name":"src/syntax.py","code":"$1a"},{"name":"src/tokenizer.py","code":"$1b"},{"name":"src/utils.py","code":"$1c"},{"name":"src/backend/__init__.py","code":"$1d"},{"name":"src/backend/arm.py","code":"from pathlib import Path\n\ndef arm_compiler(path: Path, args: list[str]):\n    raise NotImplementedError('ARM backend is not yet supported')\n"},{"name":"src/backend/c.py","code":"from pathlib import Path\n\ndef c_compiler(path: Path, args: list[str]):\n    raise NotImplementedError('C backend is not yet supported')\n"},{"name":"src/backend/llvm.py","code":"from pathlib import Path\n\ndef llvm_compiler(path: Path, args: list[str]):\n    raise NotImplementedError('LLVM backend is not yet supported')\n"},{"name":"src/backend/python.py","code":"$1e"},{"name":"src/backend/qbe.py","code":"from pathlib import Path\n\ndef qbe_compiler(path: Path, args: list[str]):\n    raise NotImplementedError('QBE backend is not yet supported')\n"},{"name":"src/backend/riscv.py","code":"from pathlib import Path\n\ndef riscv_compiler(path: Path, args: list[str]):\n    raise NotImplementedError('RISC-V backend is not yet supported')\n"},{"name":"src/backend/shell.py","code":"from pathlib import Path\n\ndef shell_compiler(path: Path, args: list[str]):\n    \"\"\"this would target sh/powershell/etc. all simultaneously\"\"\"\n    # TODO: find the explanation of how this works\n    # https://en.wikipedia.org/wiki/Polyglot_(computing)\n    raise NotImplementedError('Shell backend is not yet supported')\n"},{"name":"src/backend/x86_64.py","code":"from pathlib import Path\n\ndef x86_64_compiler(path: Path, args: list[str]):\n    raise NotImplementedError('x86_64 backend is not yet supported')\n"}],"dewy_examples":{"good_examples":[{"name":"hello.dewy","code":"printl'Hello, World!'"},{"name":"hello_func.dewy","code":"main = () => printl'Hello, World!'\nmain"},{"name":"hello_name.dewy","code":"print\"What's your name? \"\nname = readl\nprintl'Hello {name}!'"},{"name":"hello_loop.dewy","code":"print\"What's your name? \"\nname = readl\ni = 0\nloop i <? 10 {\n    printl'Hello {name}!'\n    i = i + 1\n}"},{"name":"anonymous_func.dewy","code":"(() => printl'Hello from an anonymous function!')()"},{"name":"if_else.dewy","code":"print\"What's your name? \"\nname = readl\nif name =? 'Alice' printl'Hello Alice!'\nelse printl'Hello stranger!'"},{"name":"if_else_if.dewy","code":"print\"What's your name? \"\nname = readl\nif name =? 'Alice' printl'Hello Alice!'\nelse if name =? 'Bob' printl'Hello Bob!'\nelse printl'Hello stranger!'"},{"name":"dangling_else.dewy","code":"// if a then if b then s else s2\n// See: https://en.wikipedia.org/wiki/Dangling_else\n\na = false\nb = true\n\nif a\n    if b\n        printl's'\n    else\n        printl's2'\nelse\n    printl's3'"},{"name":"if_tree.dewy","code":"$1f"},{"name":"loop_in_iter.dewy","code":"loop i in 0,2..20\n    printl(i)"},{"name":"loop_and_iters.dewy","code":"loop i in 0.. and j in (2..20)\n    printl'{i} and {j}'"},{"name":"enumerate_list.dewy","code":"// example enumerating a list\nfruits = ['apple' 'banana' 'peach' 'pear' 'pineapple']\nloop i in 0.. and fruit in fruits\n    printl'{i}: {fruit}'"},{"name":"loop_or_iters.dewy","code":"loop i in (0..20] or j in [0,2..10) \n    printl'{i} or {j}'"},{"name":"nested_loop.dewy","code":"loop i in 0,2..10\n    loop j in 0,2..10\n        printl'{i},{j}'"},{"name":"block_printing.dewy","code":"loop i in 0,2..5 {\n    loop j in 0,2..5 {\n        loop k in 0,2..5 {\n            loop l in 0,2..5 {\n                loop m in 0,2..5 {\n                    printl'{i},{j},{k},{l},{m}'\n                }\n            }\n        }\n    }\n}"},{"name":"row_vs_col.dewy","code":"unit = [1 2 3]\nrow = [1,2,3]\ncol = [[1] [2] [3]] // tbd if better way to make this, but generally not necessary since 1d arrays are treated as column vectors\nmat = [1,2,3 4,5,6 7,8,9]\ntensor = [(1,2,3),(4,5,6),(7,8,9) (10,11,12),(13,14,15),(16,17,18) (19,20,21),(22,23,24),(25,26,27)]\n\n// currently not supported\nmat2 = [\n    1 2 3\n    4 5 6\n    7 8 9\n]\n\n\nprintl(unit)\nprintl(row)\nprintl(col)\nprintl(mat)\nprintl(tensor)\nprintl(mat2)"},{"name":"unpack_array.dewy","code":"s = ['Hello' ['World' '!'] 5 10]\nprintl's={s}'\n\na, b, c, d = s\nprintl'a={a} b={b} c={c} d={d}'\n\na, ...b = s\nprintl'a={a} b={b}'\n\n...a, b = s\nprintl'a={a} b={b}'\n\na, [b c], ...d = s\nprintl'a={a} b={b} c={c} d={d}'\n\na, ...b, c, d, e = s\nprintl'a={a} b={b} c={c} d={d} e={e}'\n\n//error tests\n//a, b, c, d, e = s         //error: not enough values to unpack\n//a, b = s                  //error: too many values to unpack\n//a, ...b, c, d, e, f = s   //error: too many values to unpack\n"},{"name":"unpack_dict.dewy","code":"d1 = ['a' -> 1 'b' -> 2 'c' -> 3]\n\na, b, c = d1\nprintl'a={a} b={b} c={c}'\n\na, ...b = d1\nprintl'a={a} b={b}'\n\n...a, b = d1\nprintl'a={a} b={b}'\n\na, [b c], ...d = d1\nprintl'a={a} b={b} c={c} d={d}'\n\n\n\nd2 = ['a' <-> 1 'b' <-> 2 'c' <-> 3 'd' <-> ['e' -> 4 'f' -> 5]]\n\na, b, c, d = d2\nprintl'a={a} b={b} c={c} d={d}'\n\na, ...b, c, d, e = d2\nprintl'a={a} b={b} c={c} d={d} e={e}'\n\n[ka va], [kb vb], [kc vc], [kd [ke vf]] = d2\nprintl'ka={ka} va={va} kb={kb} vb={vb} kc={kc} vc={vc} kd={kd} ke={ke} vf={vf}'"},{"name":"fizzbuzz-1.dewy","code":"// fizbuzz that works with the current version of dewy\nmultiples = [3 5]\nwords = ['Fizz' 'Buzz']\nloop i in [0..100)\n{\n    printed_words = false\n    loop multiple in multiples and word in words\n    {\n        if i % multiple =? 0 \n        { \n            print(word)\n            printed_words = true\n        }\n    }\n    if not printed_words print(i)\n    printl()\n}"},{"name":"fizzbuzz0.dewy","code":"taps = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nloop i in [0..100)\n{\n    printed_words = false\n    loop [tap string] in taps \n    {\n        if i % tap =? 0 \n        { \n            print(string)\n            printed_words = true\n        }\n    }\n    if not printed_words print(i)\n    printl()\n}"},{"name":"primes.dewy","code":"/{simple program for generating prime numbers}/\nprintl(2)\nprimes = [2]\nloop i in 0.. and candidate in 3,5..100 {\n    no_factors = true\n    loop p in primes and p*p <? candidate+1 {\n        if candidate % p =? 0 {\n            no_factors = false\n            // break //not supported yet..\n        }\n    }\n    if no_factors {\n        printl(candidate)\n        primes = primes + [candidate]\n        //primes = [...primes candidate]\n        //primes.push(i) // not supported yet.. ambiguous parse since jux has qint precedence, while . equals the higher precedence\n    }\n}"}],"bad_examples":[{"name":"tensors.dewy","code":"\ntensor1 = [(1,2,3),(4,5,6),(7,8,9) (10,11,12),(13,14,15),(16,17,18) (19,20,21),(22,23,24),(25,26,27)]\n\n\ntensor2 = [\n     1  2  3\n     4  5  6\n     7  8  9\n\n    10 11 12\n    13 14 15\n    16 17 18\n\n    19 20 21\n    22 23 24\n    25 26 27\n]\n\ntensor3 = [\n     1  2  3\n     4  5  6\n     7  8  9\n\n    10 11 12\n    13 14 15\n    16 17 18\n\n    19 20 21\n    22 23 24\n    25 26 27\n\n\n    28 29 30\n    31 32 33\n    34 35 36\n\n    37 38 39\n    40 41 42\n    43 44 45\n\n    46 47 48\n    49 50 51\n    52 53 54\n\n\n    55 56 57\n    58 59 60\n    61 62 63\n\n    64 65 66\n    67 68 69\n    70 71 72\n\n    73 74 75\n    76 77 78\n    79 80 81\n]\n\n// multiline expression means non-whitespace sensitive for outer array, but inner array is whitespace sensitive\ntensor4 = [\n    loop i in 0..3 9i .+\n    [\n        1 2 3\n        4 5 6\n        7 8 9\n    ]\n]\n\n\nprintl(tensor1)\nprintl(tensor2)\nprintl(tensor3)\nprintl(tensor4)"},{"name":"unpack_object.dewy","code":"o1 = [a='Hello' b=['World' '!'] c=5 d=10]\n\na, b, c, d = o1\nprintl'a={a} b={b} c={c} d={d}'\n\nb, c = o1\nprintl'b={b} c={c}'\n\na, ...rest = o1\nprintl'a={a} rest={rest}'\n\nd, c, ...rest, b = o1\nprintl'd={d} c={c} rest={rest} b={b}'\n\n[a [b1 b2]=b] = o1\nprintl'a={a} b1={b1} b2={b2}'"},{"name":"declare.dewy","code":"//some examples of declarations\nlet x\nlet y = 10\nlet z: int = 100 + 1000\nlet w: SomeType<a=10 b=20> = 15\nlet v = (a:int b:int): int => a + b + 10\n\nconst a\nconst b = '{1000/10}'\nconst c: int = { 10000 }\n\nlocal_const α\nlocal_const β = 100(100)\nlocal_const γ: int = 1_000_000\n\nfixed_type A\nfixed_type B = 0x1000\nfixed_type C: int = 0b1000\n"},{"name":"loop_iter_manual.dewy","code":"it = [0,2..10].iter\n[cond i] = it.next\nloop cond {\n    printl(i)\n    [cond i] = it.next\n}"},{"name":"range_iter_test.dewy","code":"r = 0,2..20\nit = r.iter\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next) //last iteration. should return [true, 20]\nprintl(it.next) //should return [false, undefined]\nprintl(it.next)\nprintl(it.next)"},{"name":"functions.dewy","code":"f0 = () => 0\nprintl('f0 = {f0}')\n\nf1 = x => x + 1\nprintl('f1(5) = {f1(5)}')\n\nf1b = (x) => x + 1\nprintl('f1b(5) = {f1b(5)}')\n\nf1c = (x=2) => x + 1\nprintl('f1c = {f1c}')\nprintl('f1c(x=5) = {f1c(x=5)}')\n\nf2 = (x y) => x + y\nprintl('f2(5 6) = {f2(5 6)}')\n\nf2b = (x y=2) => x + y\nprintl('f2b(5) = {f2b(5)}')\nprintl('f2b(5 y=6) = {f2b(5 y=6)}')\n\nf3 = (x y z) => x + y + z\nprintl('f3(5 6 7) = {f3(5 6 7)}')\n"},{"name":"partial_functions.dewy","code":"let add = (a b) => a + b\nlet add5 = @add(5)\nlet add7 = @add(a=7)\nlet add10 = @add(b=10)\n\nprintl(add(3 5))\nprintl(add5(3))\nprintl(add7(3))\nprintl(add10(3))\n\nlet fortytwo = @add10(32)\nlet fortythree = @add10(0 b=43)\nlet fortyfour = @add10(a=34)\nlet fortyfive = @add5(40)\nlet fortysix = @add5(a=6 40)\n\nprintl(fortytwo)\nprintl(fortythree)\nprintl(fortyfour)\nprintl(fortyfive)\nprintl(fortysix)"},{"name":"closure.dewy","code":"x = 10\nmy_closure = y => x + y\nmy_closure(5)\n"},{"name":"functions.dewy","code":"f0 = () => 0\nprintl('f0 = {f0}')\n\nf1 = x => x + 1\nprintl('f1(5) = {f1(5)}')\n\nf1b = (x) => x + 1\nprintl('f1b(5) = {f1b(5)}')\n\nf1c = (x=2) => x + 1\nprintl('f1c = {f1c}')\nprintl('f1c(x=5) = {f1c(x=5)}')\n\nf2 = (x y) => x + y\nprintl('f2(5 6) = {f2(5 6)}')\n\nf2b = (x y=2) => x + y\nprintl('f2b(5) = {f2b(5)}')\nprintl('f2b(5 y=6) = {f2b(5 y=6)}')\n\nf3 = (x y z) => x + y + z\nprintl('f3(5 6 7) = {f3(5 6 7)}')\n"},{"name":"shebang.dewy","code":"#!//home/david/dev/dewy-lang/dewy\n\n//shows how shebangs can be syntactically valid as a hashtag `#!` followed by a comment\nprintl'this code was invoked as an executable script with a shebang line!'"},{"name":"fizzbuzz1.dewy","code":"taps = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nrange = [0..100)\n\n//indexing at [new ..] and [.. new] adds singleton dimensions wherever there is new\nword_bools = range[new ..] .% taps.keys[.. new] .=? 0\n\n// ` means transpose, which behaves like python's zip()\nwords_grid = [taps.values word_bools]`.map(\n    [word bools] => bools.map(b => if b word else '')\n)\n\nraw_lines = word_grid`.map(line_words => line_words.join(''))\n\nlines = [raw_lines range]`.map(\n    (raw_line i) => if raw_line.length =? 0 '{i}' else raw_line\n)\n\nlines.join'\\n' |> printl\n"},{"name":"primes2.dewy","code":"// generating primes with more advanced features\n#ctx\nprimes = [\n    2\n    loop i in [3, 5..)\n        if i .% #ctx.primes .=? 0 |> @any |> @not\n            i\n][..10)"},{"name":"mdbook_preprocessor.dewy","code":"$20"},{"name":"random.dewy","code":"$21"},{"name":"fast_inverse_sqrt.dewy","code":"\n\n// fast inverse square-root. see: https://en.wikipedia.org/wiki/Fast_inverse_square_root#Overview_of_the_code\nfast_isqrt = (x:f32) => {\n    let y:f32, i:u32\n    \n    i = 0.5x transmute u32      // evil floating point bit level hacking\n    i = 0x5f3759df - (i >> 1)   // what the fuck?\n    y = i transmute f32\n    y *= 1.5 - (0.5x)y^2        // 1st iteration of newton's method\n    //y *= 1.5 - (0.5x)y^2      // 2nd iteration (optional)\n\n    return y\n}\n\n\n//TODO: use autodiff to calculate the derivative automatically?\n//isqrt = (x:number) => 1/x^0.5\n//diff(isqrt)"},{"name":"rule110.dewy","code":"// proof that dewy is turing complete\n// rule 110 would grow the vector from the front, so instead we reverse everything for efficiency\n// for now use parenthesis where precedence filter needed. eventually should be able to remove with precedence filter\n\nprogress = world:vector<bit> => {\n    update:bit = 0\n    loop i in 0..world.length\n    {\n        if i >? 0 world[i-1] = update //TODO: #notfirst handled by compiler unrolling the loop into prelude, interludes, and postlude\n        update = 0b01110110 << (world[i-1..i+1] .?? 0 .<< [2 1 0])\n    }\n    world.push(update)\n}\n\nworld: vector<bit> = [1]\nloop true\n{\n    printl(world)\n    progress(world)\n}"},{"name":"dewy_syntax_examples.dewy","code":"$22"},{"name":"syntax.dewy","code":"$23"},{"name":"tokenizer.dewy","code":"//demo of manual dewy tokenizer written in dewy\n\n// (template) instance of the token class\nTokenBase = [\n    name = 'Token'\n    __repr__ = () => '<{name}>'\n]\nToken = type(TokenBase)\n\n\n// class constructor for Keyword token type\nKeyword = src:string => [\n    ...TokenBase\n    name = 'Keyword'\n    __repr__ = () => '<{name}: {value}>'\n]\n\n\neat_fn_type = func<src:string> => int? \n\n\n/{\n    Eat a reserved keyword, return the number of characters eaten\n\n    #keyword = {in} | {as} | {loop} | {lazy} | {if} | {and} | {or} | {xor} | {nand} | {nor} | {xnor} | {not}; \n\n    noting that keywords are case insensitive\n}/\neat_keyword: func<src:string> => int? = src:string => {\n    keywords = ['in' 'as' 'loop' 'lazy' 'if' 'and' 'or' 'xor' 'nand' 'nor' 'xnor' 'not']\n    max_len = [loop k in keywords k.length].max\n    lower_src = src[..max_len].lowercase()\n    loop k in keywords\n        if lower_src.startswith(k)\n            return k.length\n    return undefined // tbd if this is optional\n}\n"}]}}]}]}],["$","h3",null,{"className":"text-2xl mt-6 mb-2 font-quadon","children":"About"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"Dewy is a programming language I have been developing off and on since 2016. It is a general purpose language, designed with engineering applications in mind. Think the functionality and ease of use of matlab or python combined with the speed of a compiled language like C or Rust, but with its own unique flare."}],["$","p",null,{"className":"text-xl font-gentona text-justify mb-2","children":"Some key planned features include:"}],["$","ul",null,{"className":"list-disc mb-6 pl-10 text-xl font-gentona","children":[["$","li",null,{"children":[["$","strong",null,{"children":"Functional and Imperative"}]," - Dewy is an imperative language with strong support for functional programming. This allows for a very flexible programming style, where you can use the best tool for the job."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Expression based syntax"}]," - Dewy uses an expression based syntax, meaning that everything is an expression. This allows for a very simple yet powerful syntax, where common language features often are just a free consequence of the syntax"]}],["$","li",null,{"children":[["$","strong",null,{"children":"Garbage-collector-free memory management"}]," - Dewy uses a unique memory management system, allowing for fast and efficient memory management without the need for a garbage collector."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Strong type system"}]," - Dewy has a powerful static type system with inference, reminiscent of those in Typescript and Julia."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Built in unit system"}]," - Dewy has a built in unit system, allowing you to easily work with units and convert between them. This is especially useful for engineering applications."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Strong math support"}]," - Dewy has strong support many math features, including complex numbers, quaternions, vectors, matrices, and more. This is especially useful for engineering applications."]}]]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"An example of the common FizzBuzz program implemented in Dewy might look like this:"}],["$","$L24",null,{"src":"multiples = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nloop i in [0..100)\n{\n    printed_words = false\n    loop [multiple word] in multiples \n    {\n        if i % multiple =? 0 \n        { \n            print(multiple)\n            printed_words = true\n        }\n    }\n    if not printed_words print(i)\n    printl()\n}"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"Or a more functional style implementation might look like this:"}],["$","$L24",null,{"src":"multiples = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nrange = [0..100)\n\n//indexing at [, ..] and [..,] adds singleton dimensions\nword_bools = range[, ..] .% multiples.keys[..,] .=? 0\n\n// ` means transpose, which behaves like python's zip()\nword_grid = [multiples.values word_bools]`.map(\n[word bools] => bools.map(b => if b word else '')\n)\n\nraw_lines = word_grid`.map(line_words => line_words.join(''))\n\nlines = [raw_lines range]`.map(\n    (raw_line, i) => if raw_line.length =? 0 '{i}' else raw_line\n)\n\nlines.join('\\n') |> printl"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"For clarity, the variables at each step look like so:"}],["$","$L24",null,{"src":"word_bools = [[true false false true false false true false ...]\n             [true false false false false true false false ...]]\n\nword_grid = [['Fizz' '' '' 'Fizz' '' '' 'Fizz' '' '' 'Fizz' '' '' ...]\n            ['Buzz' '' '' '' '' 'Buzz' '' '' '' '' 'Buzz' '' '' ...]]\n\nraw_lines = ['FizzBuzz' '' '' 'Fizz' '' 'Buzz' 'Fizz' '' '' 'Fizz' 'Buzz' '' ...]\n\nlines = ['FizzBuzz' '1' '2' 'Fizz' '4' 'Buzz' 'Fizz' '7' '8' 'Fizz' 'Buzz' '11' ...]"}],["$","h3",null,{"className":"text-2xl mt-6 mb-2 font-quadon","children":"Current Status"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"Currently I'm working through a simple interpreter for the language (powering the demo above). I've got a tokenizer, a basic interpreter backend, and handling of a few basic types of syntaxes via the parser. But the majority of the syntax is still unimplemented, hence the long list of \"Broken Examples\". Thus the current focus is finishing parser support for the rest of the syntax features."}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["Previously I had been doing a lot of development on bleeding edge"," ",["$","$L25",null,{"href":"/projects/dewy_old","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"parser generators"}],", but that ended up being too big of a time sink for not much visible progress. Instead, for the time being, I ended up just hand rolling a parser in python, which has led to actually runnable code! I'll definitely revisit parser generators in the future when the language is further along."]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["After the parser is complete, the next steps will be working on compiling to"," ",["$","$L25",null,{"href":"https://en.wikipedia.org/wiki/LLVM#Intermediate_representation","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"LLVM IR"}]," (and possibly to C as a portable alternative), as well as starting to build out the standard library, and then bootstrapping the compiler to be able to compile itself."]}],["$","h3",null,{"className":"text-2xl mt-6 mb-2 font-quadon","children":"About the Demo"}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["The demo above was actually pretty complex to put together. The current interpreter is written in python, and this website is statically hosted, which meant the demo required some way to statically run python code without a server. For this, I used ",["$","$L25",null,{"href":"https://pyodide.org/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Pyodide"}],", which is basically ",["$","$L25",null,{"href":"https://github.com/python/cpython","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"CPython"}]," compiled to"," ",["$","$L25",null,{"href":"https://webassembly.org/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"WebAssembly"}]," via"," ",["$","$L25",null,{"href":"https://emscripten.org/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Emscripten."}]]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["Pyodide itself isn't too difficult to use, except for the fact that it doesn't have good support for asynchronous standard input—it really wants to halt the entire UI while you type input into a stock browser popup prompt. To get around this, I found a"," ",["$","$L25",null,{"href":"https://www.npmjs.com/package/sync-message","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"handy library"}]," where you run pyodide in a web worker, and then any time it wants to read input, the worker makes a synchronous XHR request to a service worker, blocking the pyodide web worker until the service worker receives a response from the main thread with the input, which the service worker can then pass back to the pyodide worker. Suffice it to say, I don't think I ever want to deal with service workers again."]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["Now that python is handled, the next aspect is getting the Dewy interpreter itself to run. For this, I fetch (at website build time) the source code directly from"," ",["$","$L25",null,{"href":"https://github.com/david-andrew/dewy-lang/tree/master/src/compiler","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"github"}],". I then abuse the python import lib to allow loading \"modules\" directly from strings, and then pass all of the dewy source in as modules. Then I have a little wrapper function for the entry point which receives the source code string, and runs the program. The entry point can then be called from the browser via a javascript wrapper function."]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":["The final piece of the puzzle is the text entry, and terminal emulator. For text input, I'm using the ",["$","$L25",null,{"href":"https://codemirror.net/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Code Mirror Library"}]," with a custom syntax highlighter. For the terminal, I use the ",["$","$L25",null,{"href":"https://xtermjs.org/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"xterm.js"}]," library. I then hooked up stdin and stdout from pyodide to interact with the terminal, and voila! A Dewy interpreter running in the browser."]}],["$","p",null,{"className":"mb-6 text-xl font-gentona text-justify","children":"There are definitely some rough edges, and the parser only supports a small handful of features, but it runs! It's probably the easiest way to try out the language, and I'm looking forward to getting all of the broken example programs working!"}],["$","h3",null,{"className":"text-2xl mt-6 mb-2 font-quadon","children":"Links"}],["$","div",null,{"className":"flex flex-col space-y-3","children":[["$","div",null,{"className":"flex flex-row text-sm items-center","children":[["$","$L26",null,{"src":{"src":"/_next/static/media/github.8d24eda3.svg","height":484,"width":496,"blurWidth":0,"blurHeight":0},"alt":"github icon","className":"inline-block w-8 h-8 mr-2  pointer-events-none select-none","draggable":false}],["$","span",null,{"className":"align-middle","children":["$","$L25",null,{"href":"https://github.com/david-andrew/dewy-lang","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Github Repo"}]}]]}],["$","div",null,{"className":"flex flex-row text-sm items-center","children":[["$","$L26",null,{"src":{"src":"/_next/static/media/docs.71257e0f.svg","height":512,"width":384,"blurWidth":0,"blurHeight":0},"alt":"docs icon","className":"inline-block w-8 h-8 mr-2  pointer-events-none select-none","draggable":false}],["$","span",null,{"className":"align-middle","children":["$","$L25",null,{"href":"https://david-andrew.github.io/dewy-lang/","className":"text-blue-400 hover:text-blue-500 font-gentona text-xl","children":"Language Docs"}]}]]}]]}]]
