1:HL["/_next/static/media/a34f9d1faa5f3315-s.p.woff2",{"as":"font","type":"font/woff2"}]
2:HL["/_next/static/css/1e5c7afc2a6fc00a.css",{"as":"style"}]
0:["-LyvZV8k3cc6pSXDeey36",[[["",{"children":["iframes",{"children":["dewy",{"children":["demo_and_examples",{"children":["__PAGE__",{}]}]}]}]},"$undefined","$undefined",true],"$L3",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/1e5c7afc2a6fc00a.css","precedence":"next"}]],"$L4"]]]]
5:HL["/_next/static/css/85fa6dafca566008.css",{"as":"style"}]
3:[null,"$L6",null]
4:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"David Samson"}],["$","meta","2",{"name":"description","content":"Generated by create next app"}],["$","meta","3",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","link","4",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","meta","5",{"name":"next-size-adjust"}]]
7:I{"id":27250,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","8861:static/chunks/8861-dd878d5de052195d.js","5094:static/chunks/5094-a6ddac3dc3ee8b0f.js","3185:static/chunks/app/layout-f15e5947567490b6.js"],"name":"Navbar","async":false}
8:I{"id":19885,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","8861:static/chunks/8861-dd878d5de052195d.js","5094:static/chunks/5094-a6ddac3dc3ee8b0f.js","3185:static/chunks/app/layout-f15e5947567490b6.js"],"name":"GithubTimestampsProvider","async":false}
9:I{"id":19885,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","8861:static/chunks/8861-dd878d5de052195d.js","5094:static/chunks/5094-a6ddac3dc3ee8b0f.js","3185:static/chunks/app/layout-f15e5947567490b6.js"],"name":"ProjectsContextProvider","async":false}
a:I{"id":47767,"chunks":["2272:static/chunks/webpack-def1c966b90aff50.js","2971:static/chunks/fd9d1056-bc91c3b3fa0b78fd.js","596:static/chunks/596-fc24fa4abd14b1c2.js"],"name":"default","async":false}
b:I{"id":57920,"chunks":["2272:static/chunks/webpack-def1c966b90aff50.js","2971:static/chunks/fd9d1056-bc91c3b3fa0b78fd.js","596:static/chunks/596-fc24fa4abd14b1c2.js"],"name":"default","async":false}
e:I{"id":49488,"chunks":["6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","8861:static/chunks/8861-dd878d5de052195d.js","5094:static/chunks/5094-a6ddac3dc3ee8b0f.js","3185:static/chunks/app/layout-f15e5947567490b6.js"],"name":"ColorPicker","async":false}
6:["$","html",null,{"lang":"en","children":[["$","head",null,{"children":["$","script",null,{"src":"/pyodideCommsService.js","async":true}]}],["$","body",null,{"className":"__className_d65c78 overflow-hidden bg-slate-900","children":["$","div",null,{"className":"w-screen h-screen bg-black overflow-hidden","children":[["$","$L7",null,{}],["$","div",null,{"style":{"height":"calc(100vh - var(--navbar-height))"},"className":"overflow-x-hidden","children":["$","$L8",null,{"children":["$","$L9",null,{"children":["$","$La",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":["$","div",null,{"className":"fixed w-screen h-screen bg-black flex justify-center items-center","children":["$","div",null,{"role":"status","children":[["$","svg",null,{"aria-hidden":"true","className":"w-32 h-32 mr-2 text-gray-200 animate-spin dark:text-gray-600 fill-accent","viewBox":"0 0 100 101","fill":"none","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M100 50.5908C100 78.2051 77.6142 100.591 50 100.591C22.3858 100.591 0 78.2051 0 50.5908C0 22.9766 22.3858 0.59082 50 0.59082C77.6142 0.59082 100 22.9766 100 50.5908ZM9.08144 50.5908C9.08144 73.1895 27.4013 91.5094 50 91.5094C72.5987 91.5094 90.9186 73.1895 90.9186 50.5908C90.9186 27.9921 72.5987 9.67226 50 9.67226C27.4013 9.67226 9.08144 27.9921 9.08144 50.5908Z","fill":"currentColor"}],["$","path",null,{"d":"M93.9676 39.0409C96.393 38.4038 97.8624 35.9116 97.0079 33.5539C95.2932 28.8227 92.871 24.3692 89.8167 20.348C85.8452 15.1192 80.8826 10.7238 75.2124 7.41289C69.5422 4.10194 63.2754 1.94025 56.7698 1.05124C51.7666 0.367541 46.6976 0.446843 41.7345 1.27873C39.2613 1.69328 37.813 4.19778 38.4501 6.62326C39.0873 9.04874 41.5694 10.4717 44.0505 10.1071C47.8511 9.54855 51.7191 9.52689 55.5402 10.0491C60.8642 10.7766 65.9928 12.5457 70.6331 15.2552C75.2735 17.9648 79.3347 21.5619 82.5849 25.841C84.9175 28.9121 86.7997 32.2913 88.1811 35.8758C89.083 38.2158 91.5421 39.6781 93.9676 39.0409Z","fill":"currentFill"}]]}],["$","span",null,{"className":"sr-only","children":"Loading..."}]]}]}],"loadingStyles":[],"hasLoading":true,"error":"$undefined","errorStyles":"$undefined","template":["$","$Lb",null,{}],"templateStyles":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"childProp":{"current":["$","$La",null,{"parallelRouterKey":"children","segmentPath":["children","iframes","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$Lb",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$","$La",null,{"parallelRouterKey":"children","segmentPath":["children","iframes","children","dewy","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$Lb",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$","$La",null,{"parallelRouterKey":"children","segmentPath":["children","iframes","children","dewy","children","demo_and_examples","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$Lb",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$Lc","$Ld",null],"segment":"__PAGE__"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/85fa6dafca566008.css","precedence":"next"}]]}],"segment":"demo_and_examples"},"styles":[]}],"segment":"dewy"},"styles":[]}],"segment":"iframes"},"styles":[]}]}]}]}],["$","$Le",null,{}]]}]}]]}]
c:null
f:"$Sreact.suspense"
10:I{"id":33699,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","9659:static/chunks/9659-468d9b7f9560cf55.js","6413:static/chunks/6413-9fe1335479bb3ecd.js","6896:static/chunks/6896-8f1241fc1c380e01.js","3215:static/chunks/3215-a5826776a257bdd1.js","3205:static/chunks/app/iframes/dewy/demo_and_examples/page-9917e68d1930bf4e.js"],"name":"NoSSR","async":false}
11:I{"id":43215,"chunks":["6401:static/chunks/363642f4-e868532fae175454.js","9838:static/chunks/30d07d85-388c23c5cce2d192.js","6685:static/chunks/6685-d55f1e0fd750c950.js","4769:static/chunks/4769-afbb25c509513e18.js","9659:static/chunks/9659-468d9b7f9560cf55.js","6413:static/chunks/6413-9fe1335479bb3ecd.js","6896:static/chunks/6896-8f1241fc1c380e01.js","3215:static/chunks/3215-a5826776a257bdd1.js","3205:static/chunks/app/iframes/dewy/demo_and_examples/page-9917e68d1930bf4e.js"],"name":"","async":false}
12:T11a8,from pathlib import Path
from argparse import REMAINDER
from .argparse_monkeypatch import CustomArgumentParser, CustomHelpFormatter, FlagOrEqualsValueAction
from .backend import backend_names, get_backend, python_repl, get_version
from .utils import try_install_rich
import sys

import pdb


default_backend_name = 'qbe'


def main():

    # base argparser without backend-specific options
    _base_parser = CustomArgumentParser(description='Dewy Compiler', add_help=False)

    # positional argument for the file to compile
    _base_parser.add_argument('file', nargs='?', help='.dewy file to run. If not provided, enter REPL mode')

    # mutually exclusive flags for specifying the backend to use
    group = _base_parser.add_mutually_exclusive_group()
    group.add_argument('-i', '--interpret', action='store_true', help=f'Run in interpreter mode with the python backend')
    group.add_argument('-c', '--compile', action='store_true', help=f'Run in compiler mode with the QBE backend')
    group.add_argument('--backend', choices=backend_names, type=str.lower, help=f'Specify a backend compiler/interpreter to use (default: {default_backend_name})')

    # other args for the base 
    _base_parser.add_argument('-v', '--version', action='version', version=f'Dewy {get_version()}', help='Print version information and exit')
    _base_parser.add_argument('-p', '--disable-rich-print', action='store_true', help='Disable using rich for printing stack traces')
    _base_parser.add_argument('--verbose', action='store_true', help='Print verbose output')
    _base_parser.add_argument('--tokens', action='store_true', help='Print tokens for the input expression')
    
    # all remaining args will be passed to the program
    _base_parser.add_argument('remaining', nargs=REMAINDER, help='Arguments after the file are passed directly to program')
    

    # make a first pass parser that includes a fake --help option so we can detect it without printing it out
    pre_parser = CustomArgumentParser(parents=[_base_parser], description='Dewy Compiler', add_help=False)
    pre_parser.add_argument('-h', '--help', action='store_true') # placeholder to be replaced by help option

    # initial pass over the args to figure out which backend to use
    argv = sys.argv[1:]
    args, _ = pre_parser.parse_known_args(argv)

    # verify any file specified exists
    if args.file and not Path(args.file).exists():
        print(f"Error: file '{args.file}' does not exist")
        _base_parser.print_help()
        sys.exit(1)
    
    # if no file was provided, we enter REPL mode (backend is ignored)
    if args.file is None and not args.help:
        if args.interpret or args.compile or args.backend:
            print("Warning: backend selection flags [--interpret --compile --backend] are ignored in REPL mode")
        args.backend = 'python'
        args.interpret = False
        args.compile = False

    # identify the backend based on any flags provided
    if args.compile:
        args.backend = 'qbe'
    elif args.interpret:
        args.backend = 'python'
    if args.backend is None:
        args.backend = default_backend_name

    # augment the base argparser with backend-specific options
    backend = get_backend(args.backend)
    main_parser = CustomArgumentParser(parents=[_base_parser], description=f'Dewy Compiler - Backend: {args.backend}', formatter_class=CustomHelpFormatter)
    main_parser.register('action', 'flag_or_explicit', FlagOrEqualsValueAction)
    backend.make_argparser(main_parser)

    # reparse now that all the args have been specified
    args = main_parser.parse_args(argv)

    # if no file is provided, we enter REPL mode (and no remaining args allowed)
    # TODO: this branch probably isn't possible to happen since positional args must be specified for there to be remaining args, and the first is taken as the file
    if not args.file and args.remaining:
        print("Error: unrecognized arguments:", " ".join(args.remaining))
        main_parser.print_help()
        sys.exit(1)
    
    # use rich for pretty traceback printing
    if not args.disable_rich_print:
        try_install_rich()


    # get the options object for the backend
    options = backend.make_options(args)


    # if no file is provided, enter REPL mode
    if args.file is None:
        python_repl(args.remaining, options)
        return
    
    # run with the selected backend
    backend.exec(Path(args.file), args.remaining, options)






if __name__ == '__main__':
    main()
13:T95f8,from typing import Generator, Sequence, cast, Callable as TypingCallable
from enum import Enum, auto
from dataclasses import dataclass
from itertools import groupby, chain as iterchain

from .syntax import (
    AST,
    Access,
    Declare,
    PointsTo, BidirPointsTo,
    Type,
    ListOfASTs, PrototypeTuple, Block, BareRange, DotDotDot, CollectInto, SpreadOutFrom, Array, Group, Range, ObjectLiteral, Dict, BidirDict, TypeParam,
    Void, Undefined, void, undefined, untyped,
    String, IString,
    Flowable, Flow, If, Loop, Default,
    PrototypeFunctionLiteral, PrototypeBuiltin, Call,
    Index,
    PrototypeIdentifier, Identifier, TypedIdentifier, ReturnTyped, SubTyped, UnpackTarget, Assign, CompiletimeAssign,
    Int, Bool,
    Range, IterIn,
    BinOp,
    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,
    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv, AtHandle,
    Backticks, CycleLeft, CycleRight, Suppress,
    BroadcastOp,
    DeclarationType,
    MakeGeneric, Parameterize,
)
from .tokenizer import (
    Token,
    Block_t,
    Operator_t,
    ShiftOperator_t,
    Juxtapose_t,
    Comma_t,
    String_t,
    Escape_t,
    TypeParam_t,
    Undefined_t,
    Identifier_t,
    Integer_t,
    Boolean_t,
    BasedNumber_t,
    RawString_t,
    DotDot_t, DotDotDot_t, Backticks_t,
    Keyword_t,
)
from .postok import (
    RangeJuxtapose_t,
    EllipsisJuxtapose_t,
    TypeParamJuxtapose_t,
    BackticksJuxtapose_t,
    get_next_chain,
    Chain,
    is_op, is_binop, is_unary_prefix_op, is_unary_postfix_op,
    Flow_t,
    Declare_t,
    OpChain_t,
    BroadcastOp_t,
    CombinedAssignmentOp_t,
)
from .utils import (
    bool_to_bool,
    based_number_to_int,
)

import pdb





def top_level_parse(tokens: list[Token]) -> AST:
    """Main entrypoint to kick off parsing a sequence of tokens"""

    ast = parse(tokens)
    if isinstance(ast, ListOfASTs):
        ast = Group(ast.asts)

    return ast

def parse_generator(tokens: list[Token]) -> Generator[AST, None, None]:
    """
    Parse all tokens into a sequence of ASTs
    """

    while len(tokens) > 0:
        chain, tokens = get_next_chain(tokens)
        yield parse_chain(chain)


def parse(tokens: list[Token]) -> AST:
    items = [*parse_generator(tokens)]

    # depending on how many expressions were parsed, return an AST or container
    if len(items) == 0:
        ast = void # literally nothing was parsed
    elif len(items) == 1:
        ast = items[0]
    else:
        ast = ListOfASTs(items)

    return ast

@dataclass
class qint:
    """
    quantum int for dealing with precedences that are multiple values at the same time
    qint's can only be strictly greater or strictly less than other values. Otherwise it's ambiguous
    In the case of ambiguous precedences, the symbol table is needed for helping resolve the ambiguity
    """
    values: set[int]

    def __post_init__(self):
        assert len(self.values) > 1, f'qint must have more than one value. Got {self.values}'

    def __gt__(self, other: 'int|qint') -> bool:
        if isinstance(other, int):
            return all(v > other for v in self.values)
        return all(v > other for v in self.values)

    def __lt__(self, other: 'int|qint') -> bool:
        if isinstance(other, int):
            return all(v < other for v in self.values)
        return all(v < other for v in self.values)

    def __ge__(self, other: 'int|qint') -> bool: return self.__gt__(other)
    def __le__(self, other: 'int|qint') -> bool: return self.__lt__(other)
    def __eq__(self, other: object) -> bool: return False


# class QAST(AST):
#     """
#     Quantum AST for dealing with ambiguous precedence
#     Simplest usage will just look see which expression passes typechecking
#     More complex versions can include something (lambdas?) to determine which case should be selected
#     """
#     asts: list[AST]
#     # predicates: list[TypingCallable] | None = None  # uncomment if we actually use this

#     def __post_init__(self):
#         assert len(self.asts) > 1, f'QAST must have more than one value. Got {self.asts}'

#     def __str__(self):
#         return f'QAST([{", ".join(str(i) for i in self.asts)}])'

class QJux(AST):
    """
    Quantum Juxtapose for dealing with the three operators vanilla juxtapose could be:
    - call e.g. a(b)
    - index e.g. a[b]
    - multiply e.g. a * b
    """
    call: Call|None   # syntactically we might know if left is not callable
    index: Index|None # syntactically we definitely know if it's not index if right is not Array or Range
    mul: Mul          # generally we cannot tell until type checking if mul is valid or not

    def __str__(self):
        call = 'call | ' if self.call is not None else ''
        index = 'index | ' if self.index is not None else ''
        return f'QJux(({call}{index}multiply): {self.mul.left}{self.mul.right})'
        # call = f'{self.call}, ' if self.call is not None else ''
        # index = f'{self.index}, ' if self.index is not None else ''
        # return f'QJux({call}{index}{self.mul})'

######### Operator Precedence Table #########
# TODO: class for compund operators, e.g. += -= .+= .-= not=? not>? etc.
# TODO: how to handle unary operators in the table? perhaps make PrefixOperator_t/PostfixOperator_t classes?
# TODO: add specification of associativity for each row
class Associativity(Enum):
    left = auto()  # left-to-right
    right = auto()  # right-to-left
    unary = auto()  # out-to-in
    # prefix = auto()
    # postfix = auto()
    none = auto()
    fail = auto()


"""
[HIGHEST PRECEDENCE]
    (prefix) @
    . <jux call> <jux index access>
    <jux ellipsis>                      //e.g. [...args]
    <jux type param>                    //e.g. <T>(x:T):>T
    (prefix) ` (postfix) `
    (prefix) not
    ^                                   //right-associative
    <jux mul>
    / * %
    + -
    << >> <<< >>> <<! !>>
    ,                                   //tuple maker
    <jux range>                         //e.g. [first,second..last]
    in
    =? >? <? >=? <=? not=? <=> is? isnt? @?
    (postfix) ?
    and nand &
    xor xnor                            //following C's precedence: and > xor > or
    or nor |
    as transmute
    :                                   //e.g. let x:int
    :>                                  //e.g. let x:():>int => 42
    =>
    |>                                  //function pipe operators
    <|
    -> <->                              //dict pointers
    = .= <op>= .<op>=  (e.g. += .+=)    //right-associative (but technically causes a type error since assignments can't be chained)
    else
    (postfix) ;
    <seq> (i.e. space)
[LOWEST PRECEDENCE]

[Notes]
.. for ranges is not an operator, it is an expression. it uses juxtapose to bind to left/right arguments (or empty), and type-checks left and right
if-else-loop chain expr is more like a single unit, so it doesn't really have a precedence. but they act like they have the lowest precedence since the expressions they capture will be full chains only broken by space/seq
the unary versions of + - * / % have the same precedence as their binary versions
"""
operator_groups: list[tuple[Associativity, Sequence[Operator_t]]] = list(reversed([
    (Associativity.unary, [Operator_t('@')]),
    (Associativity.left, [Operator_t('.'), Juxtapose_t(None)]),  # jux-call, jux-index
    (Associativity.none, [TypeParamJuxtapose_t(None)]),
    (Associativity.none, [EllipsisJuxtapose_t(None)]),  # jux-ellipsis
    (Associativity.none, [BackticksJuxtapose_t(None)]),  # jux-backticks
    (Associativity.unary, [Operator_t('not'), Operator_t('~')]),
    (Associativity.right,  [Operator_t('^')]),
    (Associativity.left, [Juxtapose_t(None)]),  # jux-multiply
    (Associativity.left, [Operator_t('*'), Operator_t('/'), Operator_t('%')]),
    (Associativity.left, [Operator_t('+'), Operator_t('-')]),
    (Associativity.left, [*map(ShiftOperator_t, ['<<', '>>', '<<<', '>>>', '<<!', '!>>'])]),
    (Associativity.none,  [Comma_t(',')]),
    (Associativity.left, [RangeJuxtapose_t(None)]),  # jux-range
    (Associativity.none, [Operator_t('in')]),
    (Associativity.left, [Operator_t('=?'), Operator_t('>?'), Operator_t('<?'), Operator_t('>=?'), Operator_t('<=?')]),
    (Associativity.unary, [Operator_t('?')]),
    (Associativity.left, [Operator_t('and'), Operator_t('nand'), Operator_t('&')]),
    (Associativity.left, [Operator_t('xor'), Operator_t('xnor')]),
    (Associativity.left, [Operator_t('or'), Operator_t('nor'), Operator_t('|')]),
    (Associativity.none,  [Operator_t('as'), Operator_t('transmute')]),
    (Associativity.fail, [Operator_t('of')]),
    (Associativity.none, [Operator_t(':')]),
    (Associativity.right, [Operator_t(':>')]),
    (Associativity.right,  [Operator_t('=>')]),  # () => () => () => 42
    (Associativity.right, [Operator_t('|>')]),
    (Associativity.left, [Operator_t('<|')]),
    (Associativity.fail,  [Operator_t('->'), Operator_t('<->')]),
    (Associativity.fail,  [Operator_t('='), Operator_t('::')]),
    (Associativity.none,  [Operator_t('else')]),
]))
precedence_table: dict[Operator_t, int | qint] = {}
associativity_table: dict[int, Associativity] = {}
for i, (assoc, group) in enumerate(operator_groups):

    # mark precedence level i as the specified associativity
    associativity_table[i] = assoc

    # insert all ops in the row into the precedence table at precedence level i
    for op in group:
        if op not in precedence_table:
            precedence_table[op] = i
            continue

        val = precedence_table[op]
        if isinstance(val, int):
            precedence_table[op] = qint({val, i})
        else:
            precedence_table[op] = qint(val.values | {i})


def operator_precedence(op: Operator_t|OpChain_t|BroadcastOp_t|CombinedAssignmentOp_t) -> int | qint:

    # for complex operators, extract the actual operator that determines precedence
    if isinstance(op, CombinedAssignmentOp_t):
        op = op.assign # combined assignment has same precedence as regular assignment
    if isinstance(op, BroadcastOp_t):
        op = op.op # precedence should be based on the operator attached to the . operator
    if isinstance(op, OpChain_t):
        op = op.ops[0] # opchain precedence is determined by the first operator in the chain

    try:
        return precedence_table[op]
    except:
        raise ValueError(f"ERROR: expected operator, got {op=} which failed to return a value from the operator precedence table") from None


def operator_associativity(op: Operator_t | int) -> Associativity|set[Associativity]:
    if not isinstance(op, int):
        i = operator_precedence(op)
        # assert isinstance(i, int), f'Cannot determine associativity of operator ({op}) with multiple precedence levels ({i})'
    else:
        i = op
    try:
        if isinstance(i, int):
            return associativity_table[i]
        return {associativity_table[v] for v in i.values}
    except:
        raise ValueError(f"Error: failed to determine associativity for operator {op}") from None



def parse_chain(chain: Chain[Token]) -> AST:
    assert isinstance(chain, Chain), f"ERROR: parse chain must be called on Chain[Token], got {type(chain)}"

    if len(chain) == 0:
        return void
    if len(chain) == 1:
        return parse_single(chain[0])

    try:
        left, op, right = split_by_lowest_precedence(chain)
    except AmbiguousPrecedenceError as e:
        pdb.set_trace()
        ... #TODO: handle ambiguous precedence error by making a QAST
        return build_quantum_expr(e.ops, e.ranks, e.assocs, e.tokens)

    left, right = parse_chain(left), parse_chain(right)

    assert not (left is void and right is void), f"Internal Error: both left and right returned void during parse chain, implying both left and right side of operator were empty, i.e. chain was invalid: {chain}"

    # 3 cases are prefix expr, postfix expr, or binary expr
    if left is void:
        return build_unary_prefix_expr(op, right)
    if right is void:
        return build_unary_postfix_expr(left, op)
    return build_bin_expr(left, op, right)


class AmbiguousPrecedenceError(ValueError):
    def __init__(self, ops: list[Operator_t], ranks: list[int], assocs: list[Associativity], tokens: Chain[Token]):
        self.ops = ops
        self.ranks = ranks
        self.assocs = assocs
        self.tokens = tokens
        super().__init__(f"Ambiguous precedence for operators {ops=} with ranks {ranks=} in token stream {tokens=}")


def split_by_lowest_precedence(tokens: Chain[Token]) -> tuple[Chain[Token], Token, Chain[Token]]:
    """
    return the integer index/indices of the lowest precedence operator(s) in the given list of tokens
    """
    assert isinstance(tokens, Chain), f"ERROR: `split_by_lowset_precedence()` may only be called on explicitly known Chain[Token], got {type(tokens)}"

    # collect all operators, their indices, and their associativity from the list of tokens
    idxs, ops = zip(*[(i, token) for i, token in enumerate(tokens) if is_op(token)])
    idxs, ops = cast(list[int], idxs), cast(list[Token], ops)
    assocs = [operator_associativity(op) for op in ops]

    # simple cases of none or one operator
    if len(ops) == 0:
        raise ValueError("INTERNAL ERROR: Attempted to split chain with no operators which shouldn't happen")
    if len(ops) == 1:
        i, = idxs
        op, = ops
        return Chain(tokens[:i]), op, Chain(tokens[i+1:])

    # when more than one op present, find the lowest precedence one

    # case of all unary operators has different splitting logic
    if all(assoc == Associativity.unary for assoc in assocs):
        return unary_split_by_lowest_precedence(tokens, ops, idxs)

    # filter out any unary operators
    assocs, idxs, ops = zip(*[(a, i, op) for a, i, op in zip(assocs, idxs, ops) if a is not Associativity.unary])
    assocs, idxs, ops = cast(list[Associativity], assocs), cast(list[int], idxs), cast(list[Token], ops)

    # continue handling binary operators as before
    ranks = [operator_precedence(op) for op in ops]
    min_rank = min(ranks)
    min_idx = ranks.index(min_rank)

    # verify that the min is strictly less than or equal to all other ranks
    if not all(min_rank <= r for r in ranks[:min_idx] + ranks[min_idx+1:]):
        raise AmbiguousPrecedenceError(ops, ranks, assocs, tokens)

    # find operators with precedence equal to the current minimum
    op_idxs = [i for i, r in zip(idxs, ranks) if r == min_rank or r is min_rank]

    if len(op_idxs) == 1:
        i, = op_idxs
        return Chain(tokens[:i]), tokens[i], Chain(tokens[i+1:])

    # handling when multiple ops have the same precedence, select based on associativity rules
    if isinstance(min_rank, qint):
        assocs_set = set(assocs) #{operator_associativity(i) for i in min_rank.values}
        if len(assocs_set) > 1:
            raise NotImplementedError(f'TODO: need to type check to deal with multiple/ambiguous operator associativities: {assocs_set}')
        assoc, = assocs_set
    else:
        assoc = operator_associativity(min_rank)

    match assoc:
        case Associativity.left: i = op_idxs[-1]
        case Associativity.right: i = op_idxs[0]
        case Associativity.unary: raise ValueError(f'INTERNAL ERROR: there should not be any unary operators in the list of operators at this point')
        case Associativity.none: i = op_idxs[-1]  # default to left. handled later in parsing
        case Associativity.fail: raise ValueError(f'Cannot handle multiple given operators in chain {tokens}, as lowest precedence operator is marked as un-associable.')

    return Chain(tokens[:i]), tokens[i], Chain(tokens[i+1:])


def unary_split_by_lowest_precedence(tokens: Chain[Token], ops: list[Token], idxs:list[int]) -> tuple[Chain[Token], Token, Chain[Token]]:
    """
    split the list of tokens by the lowest precedence unary operator
    """
    # unary split looks at the left=leftmost prefix operator and the right=rightmost postfix operator
    # if left is None, then it's right. if right is None, then it's left
    # otherwise, it's determined by which has the lower precedence
    # if both have the same precedence (shouldn't generally happen), probably just do left to right

    #TODO: this might actually fail for the jux operators because to determine if they are prefix or postfix requires looking at the left and right token...
    pdb.set_trace()


    # get the leftmost and rightmost operators
    left_op = ops[0] if is_unary_prefix_op(ops[0]) else None
    left_idx = idxs[0]
    if left_op is not None:
        assert left_idx == 0, f'INTERNAL ERROR: expected left operator to be at the start of the list of tokens, got {left_idx=}, {tokens=}'
    
    right_op = ops[-1] if is_unary_postfix_op(ops[-1]) else None
    right_idx = idxs[-1]
    if right_op is not None:
        assert right_idx == len(tokens) - 1, f'INTERNAL ERROR: expected right operator to be at the end of the list of tokens, got {right_idx=}, {tokens=}'

    if left_op is None and right_op is None:
        raise ValueError(f'INTERNAL ERROR: no unary operators found in list of operators {ops=}')

    # determine which operator is the lowest precedence
    if left_op is None:
        i = right_idx
    elif right_op is None:
        i = left_idx
    else:
        # use precedence to determine lower precedence op
        left_rank = operator_precedence(left_op)
        right_rank = operator_precedence(right_op)
        i = left_idx if left_rank <= right_rank else right_idx

    return Chain(tokens[:i]), tokens[i], Chain(tokens[i+1:])


def parse_single(token: Token) -> AST:
    """Parse a single token into an AST"""
    match token:
        case Undefined_t(): return undefined
        case Identifier_t(): return PrototypeIdentifier(token.src)
        case Integer_t(): return Int(int(token.src))
        case Boolean_t(): return Bool(bool_to_bool(token.src))
        case BasedNumber_t(): return Int(based_number_to_int(token.src))
        case RawString_t(): return String(token.to_str())
        case DotDot_t(): return BareRange(void, void)
        case DotDotDot_t(): return DotDotDot()
        case Backticks_t(src=src): return Backticks(src)
        case String_t(): return parse_string(token)
        case Block_t(): return parse_block(token)
        case TypeParam_t(): return parse_type_param(token)
        case Flow_t(): return parse_flow(token)
        case Declare_t(): return parse_declare(token)

        case _:
            # TODO handle other types...
            pdb.set_trace()
            ...

    pdb.set_trace()
    raise NotImplementedError()
    ...


def build_bin_expr(left: AST, op: Token, right: AST) -> AST:
    """create a unary prefix expression AST from the op and right AST"""

    match op:
        #TODO: replace vanilla juxtapose with prototype?
        # when split_by_lowest_precedence is ambiguous we will create a QAST which has all possible ASTs, and disambiguation will happen at runtime/compiletime
        # then we will replace Juxtapose_t here with JuxtaposeCall_t | JuxtaposeIndex_t | JuxtaposeMul_t
        case Juxtapose_t(): return build_quantum_juxtapose(left, right) #return QAST([Call(left, right), Index(left, right), Mul(left, right)])

        case Operator_t(op='|>'): return Call(right, left)
        case Operator_t(op='<|'): return Call(left, right)
        case Operator_t(op='='): return Assign(left, right)
        case Operator_t(op='::'): return CompiletimeAssign(left, right)
        case Operator_t(op='=>'): return PrototypeFunctionLiteral(left, right)
        case Operator_t(op='->'): return PointsTo(left, right)
        case Operator_t(op='<->'): return BidirPointsTo(left, right)
        case Operator_t(op='.'): return Access(left, right)

        # a bunch of simple cases:
        case ShiftOperator_t(op='<<'):  return LeftShift(left, right)
        case ShiftOperator_t(op='>>'):  return RightShift(left, right)
        case ShiftOperator_t(op='<<<'): return LeftRotate(left, right)
        case ShiftOperator_t(op='>>>'): return RightRotate(left, right)
        case ShiftOperator_t(op='<<!'): return LeftRotateCarry(left, right)
        case ShiftOperator_t(op='!>>'): return RightRotateCarry(left, right)
        case Operator_t(op='+'): return Add(left, right)
        case Operator_t(op='-'): return Sub(left, right)
        case Operator_t(op='*'): return Mul(left, right)
        case Operator_t(op='/'): return Div(left, right)
        case Operator_t(op='รท'): return IDiv(left, right)
        case Operator_t(op='%'): return Mod(left, right)
        case Operator_t(op='^'): return Pow(left, right)

        # comparison operators
        case Operator_t(op='=?'): return Equal(left, right)
        case Operator_t(op='>?'): return Greater(left, right)
        case Operator_t(op='<?'): return Less(left, right)
        case Operator_t(op='>=?'): return GreaterEqual(left, right)
        case Operator_t(op='<=?'): return LessEqual(left, right)
        case Operator_t(op='in?'): return MemberIn(left, right)
        # case Operator_t(op='is?'): return Is(left, right)
        # case Operator_t(op='isnt?'): return Isnt(left, right)
        # case Operator_t(op='<=>'): return ThreewayCompare(left, right)

        # Logical Operators. TODO: outtype=Bool is not flexible enough...
        case Operator_t(op='and'|'&'): return And(left, right)
        case Operator_t(op='or'|'|'): return Or(left, right)
        case Operator_t(op='nand'): return Nand(left, right)
        case Operator_t(op='nor'): return Nor(left, right)
        case Operator_t(op='xor'): return Xor(left, right)
        case Operator_t(op='xnor'): return Xnor(left, right)

        # Misc Operators
        case Operator_t(op=':'):
            if isinstance(left, PrototypeIdentifier): return TypedIdentifier(Identifier(left.name), right)
            #TBD if there are other things that can have type annotations beyond identifiers
            raise ValueError(f'ERROR: can only apply a type to an identifier. Got {left=}, {right=}')
        case Operator_t(op=':>'): return ReturnTyped(left, right)
        case Operator_t(op='of'): return SubTyped(left, right)

        case TypeParamJuxtapose_t():
            if isinstance(left, TypeParam):
                return MakeGeneric(left, right)
            if isinstance(right, TypeParam):
                return Parameterize(left, right)
            raise ValueError(f"INTERNAL ERROR: TypeParamJuxtapose must be attached to a type param. {left=}, {right=}")

        case EllipsisJuxtapose_t():
            if isinstance(left, DotDotDot):
                return CollectInto(right)
            if isinstance(right, DotDotDot):
                return SpreadOutFrom(left)
            raise ValueError(f'INTERNAL ERROR: EllipsisJuxtapose must be attached to an ellipsis token. {left=}, {right=}')

        case RangeJuxtapose_t():
            if isinstance(right, BareRange):
                assert right.left is void, f"ERROR: can't attach expression to range, range already has values. Got {left=}, {right=}"
                right.left = left
                return right

            if isinstance(left, BareRange):
                assert left.right is void, f"ERROR: can't attach expression to range, range already has values. Got {left=}, {right=}"
                left.right = right
                return left

            raise ValueError(f'INTERNAL ERROR: Range Juxtapose must be next to a range. Got {left=}, {right=}')

        case BackticksJuxtapose_t():
            assert isinstance(left, Backticks) or isinstance(right, Backticks), f'INTERNAL ERROR: BackticksJuxtapose must be attached to a backticks token. Got {left=}, {right=}'
            if isinstance(left, Backticks):
                return CycleLeft(right, len(left.backticks))
            return CycleRight(left, len(right.backticks))

        case Comma_t():
            # TODO: combine left or right tuples into a single tuple
            if isinstance(left, PrototypeTuple) and isinstance(right, PrototypeTuple):
                return PrototypeTuple([*left.items, *right.items])
            elif isinstance(left, PrototypeTuple):
                return PrototypeTuple([*left.items, right])
            elif isinstance(right, PrototypeTuple):
                return PrototypeTuple([left, *right.items])
            else:
                return PrototypeTuple([left, right])

        case Operator_t(op='else'):
            if isinstance(left, Flow) and isinstance(right, Flow):
                # merge left+right as single flow
                return Flow([*left.branches, *right.branches])
            elif isinstance(left, Flow):
                # append right to left
                assert not isinstance(left.branches[-1], Default), f"ERROR: can't merge default branch into middle of flow. Got: {left=}, {right=}"
                if isinstance(right, Flowable):
                    return Flow([*left.branches, right])
                return Flow([*left.branches, Default(right)])

            elif isinstance(right, Flow):
                # prepend left to right
                assert isinstance(left, Flowable), f"ERROR: can only prepend Flowables to left of a Flow. Got: {left=}, {right=}"
                return Flow([left, *right.branches])
            else:
                # create a new flow out of the left and right
                assert isinstance(left, Flowable), f"ERROR: can only create a Flow from Flowables. Got: {left=}, {right=}"
                if isinstance(right, Flowable):
                    return Flow([left, right])
                return Flow([left, Default(right)])

        case Operator_t(op='in'):
            return IterIn(left, right)
        
        case OpChain_t(ops=list() as ops) if len(ops) > 1: #TODO: shouldn't be possible to make an OpChain with 1 or 0 ops
            for unary_op in reversed(ops[1:]):
                right = build_unary_prefix_expr(unary_op, right)
            return build_bin_expr(left, ops[0], right)

        case CombinedAssignmentOp_t(op=op):
            return Assign(left, build_bin_expr(left, op, right))

        case BroadcastOp_t(op=op):
            expr = build_bin_expr(left, op, right)
            assert isinstance(expr, BinOp), f'INTERNAL ERROR: expected BinOp, got {expr=}'
            return BroadcastOp(expr)

        case _:
            pdb.set_trace()
            raise NotImplementedError(f'Parsing of operator {op} has not been implemented yet')

_non_callables = (Int, Bool, String, IString, Array, Range, Dict, BidirDict, ObjectLiteral, Void, DotDotDot, BareRange, Backticks)
def build_quantum_juxtapose(left: AST, right: AST) -> QJux | Mul:
    call = Call(left, right) if not isinstance(left, _non_callables) else None
    index = Index(left, right) if isinstance(right, (Array, Range)) else None
    mul = Mul(left, right)

    # default to just mul if the other options definitely don't work
    if call is None and index is None:
        return mul

    return QJux(call=call, index=index, mul=mul)


def build_unary_prefix_expr(op: Token, right: AST) -> AST:
    """create a unary prefix expression AST from the op and right AST"""
    match op:
        # normal prefix operators
        case Operator_t(op='+'): return UnaryPos(right)
        case Operator_t(op='-'): return UnaryNeg(right)
        case Operator_t(op='*'): return UnaryMul(right)
        case Operator_t(op='/'): return UnaryDiv(right)
        case Operator_t(op='not'|'~'): return Not(right)  # TODO: don't want to hardcode Bool here!
        case Operator_t(op='@'): return AtHandle(right)

        # binary operators that appear to be unary because the left can be void
        # => called as unary prefix op means left was ()/void
        case Operator_t(op='=>'): return PrototypeFunctionLiteral(void, right)

        case OpChain_t(ops=list() as ops):
            for unary_op in reversed(ops):
                right = build_unary_prefix_expr(unary_op, right)
            return right

        case _:
            raise ValueError(f"INTERNAL ERROR: {op=} is not a known unary prefix operator")


def build_unary_postfix_expr(left: AST, op: Token) -> AST:
    """create a unary postfix expression AST from the left AST and op token"""
    match op:
        # normal postfix operators
        case Operator_t(op='!'): raise NotImplementedError(f"TODO: postfix op: {op=}")  # return Fact(left)

        # binary operators that appear to be unary because the right can be void
        # anything juxtaposed with void is treated as a zero-arg call()
        case Juxtapose_t():
            return Call(left)

        case _:
            pdb.set_trace()
            raise NotImplementedError(f"TODO: {op=}")

def parse_string(token: String_t) -> String | IString:
    """Convert a string token to an AST"""

    if len(token.body) == 1 and isinstance(token.body[0], str):
        return String(token.body[0])

    # else handle interpolation strings
    parts = []
    for chunk in token.body:
        if isinstance(chunk, str):
            parts.append(chunk)
        elif isinstance(chunk, Escape_t):
            parts.append(chunk.to_str())
        else:
            ast = parse(chunk.body)
            if isinstance(ast, Block):
                parts.append(ast)
            elif isinstance(ast, ListOfASTs):
                pdb.set_trace()
                # not sure if this should ever come up, might be a parse bug
                # or might just need to convert to a block...
            else:
                parts.append(Block([ast]))

    # combine any adjacent Strings into a single string (e.g. if there were escapes)
    parts = iterchain(*((''.join(g),) if issubclass(t, str) else (*g,) for t, g in groupby(parts, type)))
    # convert any free strings to ASTs
    parts = [p if not isinstance(p, str) else String(p) for p in parts]

    # cast because pyright complains
    parts = cast(list[AST], parts)
    return IString(parts)


def as_dict_inners(items:list[AST]) -> list[PointsTo] | None:
    """Determine if the inner items indicate the container is a Dict (i.e. all items are points-to)"""
    if all(isinstance(i, PointsTo) for i in items):
        return cast(list[PointsTo], items)
    return None

def as_bidir_dict_inners(items:list[AST]) -> list[BidirPointsTo] | None:
    """Determine if the inner items indicate the container is a BidirDict (i.e. all items are bidir-points-to)"""
    if all(isinstance(i, BidirPointsTo) for i in items):
        return cast(list[BidirPointsTo], items)
    return None

def as_array_inners(items:list[AST]) -> list[AST] | None:
    """Determine if the inner items indicate the container is an Array (i.e. no points-to, assigns, or declarations)"""
    invalid_types = (Declare, Assign, PointsTo, BidirPointsTo)
    if any(isinstance(i, invalid_types) for i in items):
        return None
    return items

def as_object_inners(items:list[AST]) -> list[AST] | None:
    """determine if the inner items indicate the container is an Object (i.e. no points-to, and should contain at least one assign or declaration)"""
    invalid_types = (PointsTo, BidirPointsTo)
    expected_types = (Assign, Declare)
    if any(isinstance(i, invalid_types) for i in items):
        return None
    if not any(isinstance(i, expected_types) for i in items):
        return None
    return items

def parse_block(block: Block_t) -> AST:
    """Convert a block token to an AST"""

    # parse the inside of the block
    inner = parse(block.body)

    delims = block.left + block.right
    match delims, inner:
        case '()' | '{}' | '[]', Void():
            return inner
        case '()', ListOfASTs():
            return Group(inner.asts)
        case '{}', ListOfASTs():
            return Block(inner.asts)
        case '[]', ListOfASTs():
            if (asts:=as_dict_inners(inner.asts)) is not None:
                return Dict(asts)
            elif (asts:=as_bidir_dict_inners(inner.asts)) is not None:
                return BidirDict(asts)
            elif (asts:=as_array_inners(inner.asts)) is not None:
                return Array(asts)
            elif (asts:=as_object_inners(inner.asts)) is not None:
                return ObjectLiteral(inner.asts)
            # elif (asts:=as_array_generator_inners(inner.asts)) is not None:
            #     return ArrayGenerator(asts)
            # elif (asts:=as_dict_generator_inners(inner.asts)) is not None:
            #     return DictGenerator(asts)
            # elif (asts:=as_bidict_generator_inners(inner.asts)) is not None:
            #     return BidirDictGenerator(asts)
            #error cases
            if any(isinstance(i, PointsTo) for i in inner.asts) and not all(isinstance(i, PointsTo) for i in inner.asts):
                raise ValueError(f"ERROR: cannot mix PointsTo with other types in a dict: {inner=}")
            #TBD other known cases
            #otherwise there is an issue with the parser
            raise ValueError(f"INTERNAL ERROR: could not determine container type for {inner=}. Should have been suitably disambiguated by parser...")
        case '()' | '[]' | '(]' | '[)', BareRange():
            return Range(inner.left, inner.right, delims)

        # catch all cases for any type of AST inside a block or range
        case '()', _:
            return Group([inner])
        case '{}', _:
            return Block([inner])
        case '[]', PointsTo():
            return Dict([inner])
        case '[]', BidirPointsTo():
            return BidirDict([inner])
        case '[]', Assign() | Declare():
            return ObjectLiteral([inner])
        case '[]', _:
            # TODO: handle if this should be an object or dictionary instead of an array
            return Array([inner])
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'block parse not implemented for {block.left+block.right}, {type(inner)}')



def parse_type_param(param: TypeParam_t) -> TypeParam:
    items = parse(param.body)
    if isinstance(items, ListOfASTs):
        return TypeParam(items.asts)
    return TypeParam([items])


def parse_flow(flow: Flow_t) -> Flowable:

    # special case for closing else clause in a flow chain. Treat as `<if> <true> <clause>`
    if flow.keyword is None:
        return Default(parse_chain(flow.clause))

    assert flow.condition is not None, f"ERROR: flow condition must be present for {flow=}"
    cond = parse_chain(flow.condition)
    clause = parse_chain(flow.clause)

    match flow.keyword:
        case Keyword_t(src='if'): return If(cond, clause)
        case Keyword_t(src='loop'): return Loop(cond, clause)
        case _:
            pdb.set_trace()
            ...
            raise NotImplementedError('TODO: other flow keywords, namely lazy')
    pdb.set_trace()
    ...


def parse_declare(declare: Declare_t) -> Declare:
    expr = parse_chain(declare.expr)
    assert isinstance(expr, (PrototypeIdentifier, Identifier, TypedIdentifier, ReturnTyped, UnpackTarget, Assign, CompiletimeAssign)), f'ERROR: expected identifier, typed-identifier, or unpack target for declare expression, got {expr=}'
    match declare:
        case Declare_t(keyword=Keyword_t(src='let')): return Declare(DeclarationType.LET, expr)
        case Declare_t(keyword=Keyword_t(src='const')): return Declare(DeclarationType.CONST, expr)
        # case Declare_t(keyword=Keyword_t(src='local_const')): return Declare(DeclarationType.LOCAL_CONST, expr)
        # case Declare_t(keyword=Keyword_t(src='fixed_type')): return Declare(DeclarationType.FIXED_TYPE, expr)
        case _:
            raise ValueError(f"ERROR: unknown declare keyword {declare.keyword=}. Expected one of {DeclarationType.__members__}. {declare=}")
    pdb.set_trace()
    raise NotImplementedError






################################ Docs Markdown Helpers ################################
opname_map = {
    '@': 'reference',
    '.': 'access',
    '^': 'power',
    '*': 'multiply',
    '/': 'divide',
    '%': 'modulus',
    '+': 'add',
    '-': 'subtract',
    '<<': 'left shift',
    '>>': 'right shift',
    '>>>': 'rotate left no carry',
    '<<<': 'rotate right no carry',
    '<<!': 'rotate left with carry',
    '!>>': 'rotate right with carry',
    '>?': 'greater than',
    '<?': 'less than',
    '>=?': 'greater than or equal',
    '<=?': 'less than or equal',
    '=?': 'equal',
    'and': 'and',
    'nand': 'nand',
    '&': 'and',
    'xor': 'xor',
    'xnor': 'xnor',
    'or': 'or',
    'nor': 'nor',
    '|': 'or',
    '=>': 'function arrow',
    '=': 'bind',
    'else': 'flow alternate',
    ';': 'semicolon',
    'in': 'in',
    'as': 'as',
    'transmute': 'transmute',
    '|>': 'pipe',
    '<|': 'reverse pipe',
    '->': 'right pointer',
    '<->': 'bidir pointer',
    '<-': 'left pointer',
    ':': 'type annotation',

    Comma_t(None): 'comma',
    Juxtapose_t(None): 'unknown juxtapose',
    EllipsisJuxtapose_t(None): 'ellipsis juxtapose',
    RangeJuxtapose_t(None): 'range juxtapose',
    TypeParamJuxtapose_t(None): 'type param juxtapose',
}


def get_precedence_table_markdown() -> str:
    """return a string that is the markdown table for the docs containing all the operators"""
    header = '| Precedence | Operator | Name | Associativity |\n| --- | --- | --- | --- |'

    def get_ops_str(ops: list[Operator_t | ShiftOperator_t | Juxtapose_t | Comma_t]) -> str:
        return '<br>'.join(f'`{op.op if isinstance(op, (Operator_t, ShiftOperator_t)) else op.__class__.__name__[:-2].lower()}`' for op in ops)

    def get_opnames_str(ops: list[Operator_t | ShiftOperator_t | Juxtapose_t | Comma_t]) -> str:
        return '<br>'.join(f'{opname_map.get(op.op, None) if isinstance(op, (Operator_t, ShiftOperator_t)) else op.__class__.__name__[:-2].lower()}' for op in ops)

    def get_row_str(row: tuple[Associativity, list[Operator_t | ShiftOperator_t | Juxtapose_t | Comma_t]]) -> str:
        assoc, group = row
        return f'{get_ops_str(group)} | {get_opnames_str(group)} | {assoc.name}'

    rows = [
        f'| {i} | {get_row_str(row)} |'
        for i, row in reversed([*enumerate(operator_groups)])
    ]

    return header + '\n' + '\n'.join(rows)
14:T5b2f,from .tokenizer import (
    tokenize, tprint, full_traverse_tokens,
    unary_prefix_operators,
    unary_postfix_operators,
    binary_operators,
    opchain_starters,
    Token,
    Keyword_t, Undefined_t, Void_t, End_t, New_t,
    WhiteSpace_t, Escape_t,
    Identifier_t, Hashtag_t,
    Block_t, TypeParam_t,
    RawString_t, String_t,
    Integer_t, BasedNumber_t, Boolean_t,
    DotDot_t, DotDotDot_t, Backticks_t,
    Juxtapose_t, Operator_t, ShiftOperator_t, Comma_t,
)

from typing import Generator, overload, cast
from abc import ABC, abstractmethod


import pdb


# A chain is just a list of tokens that is known to be directly parsable as an expression without any other syntax
# i.e. it is the result of calls to `get_next_chain()`
# all other syntax is wrapped up into compound tokens
# it should literally just be a sequence of atoms and operators
from typing import TypeVar
T = TypeVar('T', bound=Token)
class Chain(list[T]):
    """class for explicitly annotating that a token list is a single chain"""

# class Chain[T](list[T]):
#     """class for explicitly annotating that a token list is a single chain"""


############### NEW TOKENS CREATED BY POST-TOKENIZATION PROCESS ###############

class Flow_t(Token):
    @overload
    def __init__(self, keyword: None, condition: None, clause: Chain[Token]): ...  # closing else
    @overload
    def __init__(self, keyword: Keyword_t, condition: Chain[Token], clause: Chain[Token]): ...  # if, loop, lazy

    def __init__(self, keyword: Keyword_t | None, condition: Chain[Token] | None, clause: Chain[Token]):
        if keyword is None and condition is not None:
            raise ValueError("closing else should have no condition. `keyword` and `condition` should both be None")
        self.keyword = keyword
        self.condition = condition
        self.clause = clause

    def __repr__(self) -> str:
        return f"<Flow_t: {self.keyword}: {self.condition} {self.clause}>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        if self.condition is not None:
            yield self.condition
        yield self.clause


# class Do_t(Token):...
# class Return_t(Token):...
# class Express_t(Token):...


class Declare_t(Token):
    def __init__(self, keyword: Keyword_t, expr: Chain[Token]):
        self.keyword = keyword
        self.expr = expr

    def __repr__(self) -> str:
        return f"<Declare_t: {self.keyword} {self.expr}>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield [self.keyword] #appraently flow doesn't yield the keyword. tbd if it matters...
        yield self.expr


class RangeJuxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __repr__(self) -> str:
        return "<RangeJuxtapose_t>"

    def __hash__(self) -> int:
        return hash(RangeJuxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, RangeJuxtapose_t)


class EllipsisJuxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __repr__(self) -> str:
        return "<EllipsisJuxtapose_t>"

    def __hash__(self) -> int:
        return hash(EllipsisJuxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, EllipsisJuxtapose_t)


class BackticksJuxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __repr__(self) -> str:
        return "<BackticksJuxtapose_t>"

    def __hash__(self) -> int:
        return hash(BackticksJuxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, BackticksJuxtapose_t)


class TypeParamJuxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __repr__(self) -> str:
        return "<TypeParamJuxtapose_t>"

    def __hash__(self) -> int:
        return hash(TypeParamJuxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, TypeParamJuxtapose_t)

class OpChain_t(Token):
    def __init__(self, ops:list[Operator_t]):
        assert len(ops) > 1, f"OpChain_t must have at least 2 operators. Got {len(ops)} operators"
        self.ops = ops

    def __repr__(self) -> str:
        return f"<OpChain_t: {''.join(op.op for op in self.ops)}>"

    def __hash__(self) -> int:
        return hash((OpChain_t, tuple(self.ops)))

    def __eq__(self, other) -> bool:
        return isinstance(other, OpChain_t) and self.ops == other.ops

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield cast(list[Token], self.ops)

class BroadcastOp_t(Token):
    def __init__(self, dot:Operator_t, op:Operator_t|OpChain_t):
        assert isinstance(dot, Operator_t) and dot.op == '.', f"VectorizedOp_t must have a '.' operator. Got {dot}"
        self.dot = dot
        self.op = op

    def __repr__(self) -> str:
        return f"<VectorizedOp_t: {self.dot}, {self.op}>"

    def __hash__(self) -> int:
        return hash((BroadcastOp_t, self.dot, self.op))

    def __eq__(self, other) -> bool:
        return isinstance(other, BroadcastOp_t) and self.dot == other.dot and self.op == other.op

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield [self.dot]
        yield [self.op]


class CombinedAssignmentOp_t(Token):
    def __init__(self, op:Operator_t|OpChain_t|BroadcastOp_t, assign:Operator_t):
        assert isinstance(assign, Operator_t) and assign.op == '=', f"CombinedAssignmentOp_t must have an '=' operator. Got {assign}"
        self.op = op
        self.assign = assign

    def __repr__(self) -> str:
        return f"<CombinedAssignmentOp_t: {self.op}, {self.assign}>"

    def __hash__(self) -> int:
        return hash((CombinedAssignmentOp_t, self.op, self.assign))

    def __eq__(self, other) -> bool:
        return isinstance(other, CombinedAssignmentOp_t) and self.op == other.op and self.assign == other.assign

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield [self.op]
        yield [self.assign]


atom_tokens = (
    Identifier_t,
    Integer_t,
    Boolean_t,
    BasedNumber_t,
    RawString_t,
    String_t,
    Block_t,
    TypeParam_t,
    Hashtag_t,
    DotDot_t,
    DotDotDot_t,
    Backticks_t,
    Flow_t,
    Undefined_t,
)

# atoms that can be juxtaposed (so juxtaposes next to them shouldn't be removed)
jux_atoms = (
    DotDot_t,
    DotDotDot_t,
    Backticks_t,
)

non_jux_ops = (
    Operator_t,
    ShiftOperator_t,
    Comma_t
)



class ShouldBreakTracker(ABC):
    @abstractmethod
    def op_breaks_chain(self, token: Token) -> bool: ...

    @abstractmethod
    def view(self, tokens: list[Token]) -> None: ...


class ShouldBreakFlowTracker(ShouldBreakTracker):
    def __init__(self):
        self.flows_seen = 0

    def op_breaks_chain(self, token: Token) -> bool:
        # should only be operators
        if isinstance(token, Operator_t) and token.op == 'else':
            if self.flows_seen == 0:
                return True
            self.flows_seen -= 1

        return False

    def view(self, tokens: list[Token]) -> None:
        # view each token without any ability to do anything
        # keep track of how many flows we've seen
        for token in tokens:
            if isinstance(token, Flow_t) and token.keyword is not None:
                self.flows_seen += 1
            if isinstance(token, Operator_t) and token.op == 'else':
                raise ValueError("should not be seeing else here")
            if isinstance(token, Keyword_t) and token.src in ('if', 'loop', 'lazy'):
                raise ValueError("should not be seeing if/loop/lazy here. Everything should be bundled up into a flow")


def invert_whitespace(tokens: list[Token]) -> None:
    """
    removes all whitespace tokens, and insert juxtapose tokens between adjacent pairs (i.e. not separated by whitespace)

    Args:
        tokens (list[Token]): list of tokens to modify. This is modified in place.
    """

    # juxtapose singleton token so we aren't wasting memory
    jux = Juxtapose_t(None)

    i = 0
    while i < len(tokens):
        # delete whitespace if it comes up
        if isinstance(tokens[i], WhiteSpace_t):
            tokens.pop(i)
            continue

        # recursively handle inverting whitespace for blocks
        if isinstance(tokens[i], (Block_t, TypeParam_t)):
            invert_whitespace(tokens[i].body)
        elif isinstance(tokens[i], String_t):
            for child in tokens[i].body:
                if isinstance(child, Block_t):
                    invert_whitespace(child.body)

        # insert juxtapose if no whitespace between tokens
        if i + 1 < len(tokens) and not isinstance(tokens[i + 1], WhiteSpace_t):
            tokens.insert(i + 1, jux)
            i += 1
        i += 1

    # finally, remove juxtapose tokens next to operators that are not whitespace sensitive
    i = 1
    while i < len(tokens) - 1:
        left, middle, right = tokens[i-1:i+2]
        #TODO: somewhere around here, need to fix how @ isn't juxtaposable but should be on the left depending on lots of stuff...
        if isinstance(middle, Juxtapose_t) \
        and (isinstance(left, non_jux_ops) or isinstance(right, non_jux_ops))\
        and not isinstance(left, jux_atoms) and not isinstance(right, jux_atoms):
            tokens.pop(i)
            continue
        i += 1


def _get_next_prefixes(tokens: list[Token]) -> tuple[list[Token], list[Token]]:
    prefixes = []
    while len(tokens) > 0 and is_unary_prefix_op(tokens[0]):
        prefixes.append(tokens.pop(0))

    return prefixes, tokens


def _get_next_postfixes(tokens: list[Token]) -> tuple[list[Token], list[Token]]:
    postfixes = []
    while len(tokens) > 0 and is_unary_postfix_op(tokens[0], exclude_semicolon=True):
        postfixes.append(tokens.pop(0))

    return postfixes, tokens


def _get_next_atom(tokens: list[Token]) -> tuple[Token, list[Token]]:
    if len(tokens) == 0:
        raise ValueError(f"ERROR: expected atom, got {tokens=}")

    # TODO: this is going to be unnecessary as expressions will have been bundled up into single tokens
    if isinstance(tokens[0], Keyword_t):
        return _get_next_keyword_expr(tokens)

    if isinstance(tokens[0], atom_tokens):
        return tokens[0], tokens[1:]

    raise ValueError(f"ERROR: expected atom, got {tokens[0]=}")


def _get_next_chunk(tokens: list[Token]) -> tuple[list[Token], list[Token]]:
    chunk = []
    t, tokens = _get_next_prefixes(tokens)
    chunk.extend(t)

    t, tokens = _get_next_atom(tokens)
    if t is None:
        raise ValueError(f"ERROR: expected atom, got {tokens[0]=}")
    chunk.append(t)

    t, tokens = _get_next_postfixes(tokens)
    chunk.extend(t)

    return chunk, tokens


def is_unary_prefix_op(token: Token) -> bool:
    """
    Determines if a token could be a unary prefix operator.
    Note that this is not mutually exclusive with being a postfix operator or a binary operator.
    """
    return isinstance(token, Operator_t) and token.op in unary_prefix_operators \
        or isinstance(token, OpChain_t) and token.ops[0].op in unary_prefix_operators


def is_unary_postfix_op(token: Token, exclude_semicolon: bool = False) -> bool:
    """
    Determines if a token could be a unary postfix operator.
    Optionally can exclude semicolon from the set of operators.
    Note that this is not mutually exclusive with being a prefix operator or a binary operator.
    """
    if exclude_semicolon:
        return isinstance(token, Operator_t) and token.op in unary_postfix_operators - {';'}
    return isinstance(token, Operator_t) and token.op in unary_postfix_operators


def is_binop(token: Token) -> bool:
    """
    Determines if a token could be a binary operator.
    Note that this is not mutually exclusive with being a prefix operator or a postfix operator.
    """
    return isinstance(token, Operator_t) and token.op in binary_operators or isinstance(token, (ShiftOperator_t, Comma_t, Juxtapose_t, RangeJuxtapose_t, EllipsisJuxtapose_t, BackticksJuxtapose_t, TypeParamJuxtapose_t, OpChain_t, BroadcastOp_t, CombinedAssignmentOp_t))


def is_op(token: Token) -> bool:
    return is_binop(token) or is_unary_prefix_op(token) or is_unary_postfix_op(token)


def is_opchain_starter(token: Token) -> bool:
    return isinstance(token, Operator_t) and token.op in opchain_starters


def _get_next_keyword_expr(tokens: list[Token]) -> tuple[Token, list[Token]]:
    """package up the next keyword expression into a single token"""
    if len(tokens) == 0:
        raise ValueError(f"ERROR: expected keyword expression, got {tokens=}")
    t, tokens = tokens[0], tokens[1:]

    if not isinstance(t, Keyword_t):
        raise ValueError(f"ERROR: expected keyword expression, got {t=}")

    match t:
        case Keyword_t(src='if' | 'loop' | 'lazy'):
            cond, tokens = get_next_chain(tokens)
            clause, tokens = get_next_chain(tokens, tracker=ShouldBreakFlowTracker())
            return Flow_t(t, cond, clause), tokens
        case Keyword_t(src='closing_else'):
            clause, tokens = get_next_chain(tokens, tracker=ShouldBreakFlowTracker())
            return Flow_t(None, None, clause), tokens
        case Keyword_t(src='do'):
            clause, tokens = get_next_chain(tokens)
            # assert next token is a do_keyward
            # depending on the keyward, get a condition, or condition+clause
            pdb.set_trace()
            ...
        case Keyword_t(src='return'):
            # TBD how to do this one...
            pdb.set_trace()
            ...
        case Keyword_t(src='express'):
            pdb.set_trace()
            ...
        case Keyword_t(src='let' | 'const' | 'local_const' | 'fixed_type'):
            expr, tokens = get_next_chain(tokens)
            return Declare_t(t, expr), tokens


    raise NotImplementedError("TODO: handle keyword based expressions")
    # return #chain?
    # yield #chain
    # (break | continue) #hashtag? //note the hashtag should be an entire chain if present
    # (let | const) #chain


def get_next_chain(tokens: list[Token], *, tracker: ShouldBreakTracker = None, op_blacklist: set[Token] = None) -> tuple[Chain[Token], list[Token]]:
    """
    grab the next single expression chain of tokens from the given list of tokens

    Also wraps up keyword-based expressions (if loop etc.) into a single token

    A chain is represented by the following grammar:
        #chunk = #prefix_op* #atom_expr (#postfix_op - ';')*
        #chain = #chunk (#binary_op #chunk)* ';'?

    Args:
        tokens (list[Token]): list of tokens to grab the next chain from
        tracker (ShouldBreakTracker, optional): tracker for complex analysis to determine if an operator should break the chain. Defaults to None.
        op_blacklist (set[Token], optional): simpler handler for operators that should break the chain. Defaults to None.

    Returns:
        next, rest (list[Token], list[Token]): the next chain of tokens, and the remaining tokens
    """

    if op_blacklist is None:
        op_blacklist = set()

    chain = []

    # grab the first chunk and let the tracker view it
    chunk, tokens = _get_next_chunk(tokens)
    chain.extend(chunk)
    if tracker is not None:
        tracker.view(chunk)

    while len(tokens) > 0 and is_binop(tokens[0]) and (tracker is None or not tracker.op_breaks_chain(tokens[0])) and tokens[0] not in op_blacklist:
        # get the operator, and continuing chunk, then let the tracker view it
        chain.append(tokens.pop(0))
        chunk, tokens = _get_next_chunk(tokens)
        chain.extend(chunk)
        if tracker is not None:
            tracker.view(chunk)

    # if there's a semicolon, it ends the chain
    if len(tokens) > 0 and isinstance(tokens[0], Operator_t) and tokens[0].op == ';':
        chain.append(tokens.pop(0))

    return Chain(chain), tokens


def narrow_juxtapose(tokens: list[Token]) -> None:
    """
    range juxtapose:
    convert [<token>, <jux>, <..>] into [<token>, <range_jux>, <..>]
    convert [<..>, <jux>, <token>] into [<..>, <range_jux>, <token>]
    if .. doesn't connect to anything on the left or right, connect it to undefined

    ellipsis juxtapose:
    convert [<...>, <jux>, <token>] into [<...>, <ellipsis_jux>, <token>]

    type param juxtapose:
    convert [<token>, <jux>, <type_param>] into [<token>, <type_param_jux>, <type_param>]
    convert [<type_param>, <jux>, <token>] into [<type_param>, <type_param_jux>, <token>]
    """
    range_jux = RangeJuxtapose_t(None)
    ellipsis_jux = EllipsisJuxtapose_t(None)
    backticks_jux = BackticksJuxtapose_t(None)
    type_param_jux = TypeParamJuxtapose_t(None)
    undefined = Undefined_t(None)
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        left_is_jux = i > 0 and isinstance(stream[i-1], Juxtapose_t)
        right_is_jux = i + 1 < len(stream) and isinstance(stream[i+1], Juxtapose_t)

        # handle range jux
        if isinstance(token, DotDot_t):
            if i + 1 < len(stream):
                if isinstance(stream[i+1], Juxtapose_t):
                    stream[i+1] = range_jux
                else:
                    stream[i+1:i+1] = [range_jux, undefined]
            if i > 0:
                if isinstance(stream[i-1], Juxtapose_t):
                    stream[i-1] = range_jux
                else:
                    stream[i:i] = [undefined, range_jux]
                    gen.send(i+3)

        # handle ellipsis jux
        elif isinstance(token, DotDotDot_t):
            # ellipsis can be optionally juxtaposed, but when it is juxtaposed, it may only be juxtaposed on one side
            if left_is_jux and right_is_jux:
                raise ValueError(f"ERROR: ellipsis operator {token} must be juxtaposed on either zero or one side. Got ...{stream[i-2:i+3]}...")
            if left_is_jux:
                stream[i-1] = ellipsis_jux
            if right_is_jux:
                stream[i+1] = ellipsis_jux

        # handle type param jux
        elif isinstance(token, TypeParam_t):
            if left_is_jux:
                stream[i-1] = type_param_jux
            elif right_is_jux:
                stream[i+1] = type_param_jux

        # handle backticks jux
        elif isinstance(token, Backticks_t):
            # only left or right can be juxtaposed, but not both, and not neither
            if (left_is_jux and right_is_jux) or (not left_is_jux and not right_is_jux):
                raise ValueError(f"ERROR: backticks operator {token} must be juxtaposed on a exactly one side. Got ...{stream[i-2:i+3]}...")

            if left_is_jux:
                stream[i-1] = backticks_jux
            elif right_is_jux:
                stream[i+1] = backticks_jux



def convert_bare_else(tokens: list[Token]) -> None:
    """
    convert any instances of `else` without a flow keyword after, and convert to `else` `if` `true`
    """
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if isinstance(token, Operator_t) and token.op == 'else':
            if i+1 < len(stream) and isinstance(stream[i+1], Keyword_t) and stream[i+1].src in ('if', 'loop', 'lazy'):
                continue
            # stream[i+1:i+1] = [Keyword_t('if'), Boolean_t('true')]
            stream.insert(i+1, Keyword_t('closing_else'))


def bundle_conditionals(tokens: list[Token]) -> None:
    """
    Convert sequences of tokens that represent conditionals (if, loop, etc.) into a single expression token
    """
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if isinstance(token, Keyword_t) and token.src in ('if', 'loop', 'lazy'):
            flow_chain, tokens = get_next_chain(stream[i:])
            stream[i] = flow_chain[0]
            stream[i+1:] = [*flow_chain[1:], *tokens]


def make_chain_operators(tokens: list[Token]) -> None:
    """Convert consecutive operator tokens into a single opchain token"""
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if is_opchain_starter(token):
            j = 1
            while i+j < len(stream) and is_unary_prefix_op(stream[i+j]):
                j += 1
            if j > 1:
                # convert the prefix operators into a single token
                stream[i:i+j] = [OpChain_t([stream[i+k] for k in range(j)])]
                gen.send(i+j)
                continue

def make_broadcast_operators(tokens: list[Token]) -> None:
    """Convert any . operator next to a binary operator or opchain into a broadcast operator"""
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if isinstance(token, Operator_t) and token.op == '.':
            if len(stream) > i+1 and is_binop(stream[i+1]) or isinstance(stream[i+1], OpChain_t):
                stream[i:i+2] = [BroadcastOp_t(token, stream[i+1])]
                gen.send(i+2)

def make_combined_assignment_operators(tokens: list[Token]) -> None:
    """Convert any combined assignment operators into a single token"""
    for i, token, stream in (gen := full_traverse_tokens(tokens)):
        if is_binop(token) or isinstance(token, OpChain_t) or isinstance(token, BroadcastOp_t):
            if i+1 < len(stream) and isinstance(stream[i+1], Operator_t) and stream[i+1].op == '=':
                stream[i:i+2] = [CombinedAssignmentOp_t(token, stream[i+1])]
                gen.send(i+2)

def post_process(tokens: list[Token]) -> None:
    """post process the tokens to make them ready for parsing"""

    # remove whitespace, and insert juxtapose tokens
    invert_whitespace(tokens)

    if len(tokens) == 0:
        return

    # find any instances of <else> without a flow keyword after, and convert to <else> <if> <true>
    convert_bare_else(tokens)

    # bundle up conditionals into single token expressions
    bundle_conditionals(tokens)

    # combine operator chains into a single operator token
    make_chain_operators(tokens)

    # convert any . operator next to a binary operator or opchain (e.g. .+ .^/-) into a broadcast operator
    make_broadcast_operators(tokens)

    # convert any combined assignment operators (e.g. += -= etc.) into a single token
    make_combined_assignment_operators(tokens)

    # convert juxtapose tokens to more specific types if possible
    narrow_juxtapose(tokens)


    # make the actual list of chains

    # based on types, replace jux with jux_mul or jux_call
    # TODO: actually this probably would need to be done during parsing, since we can't get a type for a complex/compound expression...


def test():
    with open('../../../examples/hello.dewy') as f:
        src = f.read()

    tokens = tokenize(src)

    # chainer process
    post_process(tokens)

    pdb.set_trace()
    ...


def test2():
    """gauntlet of multiple tests from example file"""
    with open('../../../examples/syntax3.dewyl') as f:
        lines = f.readlines()

    # filter out empty lines
    lines = [l for line in lines if (l := line.strip())]

    for line in lines:
        tokens = tokenize(line)

        # chainer process
        post_process(tokens)

        # other stuff? pass to the parser? etc.

    pdb.set_trace()
    ...


def test_hello():
    line = "printl'Hello, World!'"

    tokens = tokenize(line)
    post_process(tokens)

    pdb.set_trace()
    ...


if __name__ == '__main__':
    # test()
    # test2()
    test_hello()
15:T2f9b,"""after the main parsing, post parse to handle any remaining prototype asts within the main ast"""

from .syntax import (
    AST,
    Access,
    Declare,
    PointsTo, BidirPointsTo,
    Type,
    ListOfASTs, PrototypeTuple, Block, BareRange, Ellipsis, DotDotDot, CollectInto, SpreadOutFrom, Array, Group, Range, ObjectLiteral, Dict, BidirDict, TypeParam,
    Void, Undefined, void, undefined, untyped,
    String, IString,
    Flowable, Flow, If, Loop, Default,
    PrototypeFunctionLiteral, PrototypeBuiltin, Call,
    Index,
    PrototypeIdentifier, Express, Identifier, TypedIdentifier, ReturnTyped, SubTyped, UnpackTarget, Assign,
    Int, Bool,
    Range, IterIn,
    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,
    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv, AtHandle,
    CycleLeft, CycleRight, Suppress,
    BroadcastOp,
    DeclarationType,
    MakeGeneric, Parameterize,
)
from .parser import QJux

from typing import Callable as TypingCallable
from dataclasses import field
import pdb


class Signature(AST):
    pkwargs: list[AST] = field(default_factory=list)
    pargs:   list[AST] = field(default_factory=list)
    kwargs:  list[AST] = field(default_factory=list)
    #TODO: probably keep track of spread args i.e. "spargs"

    def _is_delimited(self) -> bool:
        n_args = len(self.pkwargs) + len(self.pargs) + len(self.kwargs)
        if n_args > 1 or n_args == 0:
            return False
        if any(isinstance(i, Assign) for i in self.pkwargs):
            return False
        if len(self.pargs) > 0 or len(self.kwargs) > 0:
            return False
        return True

    def __str__(self):
        pkwargs = ' '.join(str(i) for i in self.pkwargs)
        pargs   = ' '.join(str(i) for i in self.pargs)
        kwargs  = ' '.join(str(i) for i in self.kwargs)

        if pargs:
            pargs = f' #pos_only {pargs}'
        if kwargs:
            kwargs = f' #kw_only {kwargs}'

        s = f'{pkwargs}{pargs}{kwargs}'.strip()

        if self._is_delimited():
            return s
        return f'({s})'


# basically just convert all the different types of args to a normalized format (i.e. group)
class FunctionLiteral(AST):
    args: Signature
    body: AST
    return_type: AST = field(default_factory=lambda: untyped)

    def __str__(self):
        if self.return_type is untyped:
            return f'{self.args} => {self.body}'
        return f'{self.args}:>{self.return_type} => {self.body}'



def post_parse(ast: AST) -> AST:

    # any conversions should probably run simplest to most complex
    ast = convert_prototype_tuples(ast)
    ast = convert_bare_ranges(ast)
    ast = convert_bare_ellipses(ast)
    ast = convert_prototype_function_literals(ast)
    ast = convert_prototype_identifiers(ast)

    # at the end of the post parse process
    if not ast.is_settled():
        raise ValueError(f'INTERNAL ERROR: Parse was incomplete. AST still has prototypes\n{ast!r}')

    return ast


def convert_prototype_identifiers(ast: AST) -> AST:
    """Convert all PrototypeIdentifiers to either Identifier or Express, depending on the context"""
    ast = Group([ast])
    for i in (gen := ast.__iter_asts_full_traversal__(visit_replacement=False)):

        # skip ASTs that don't have any Prototypes
        if i.is_settled():
            continue

        # if we ever get to a bare identifier, treat it like an express
        if isinstance(i, PrototypeIdentifier):
            gen.send(Express(Identifier(i.name)))
            continue

        match i:
            case Call(f=PrototypeIdentifier(name=name), args=args):
                gen.send(Call(Identifier(name), args))
            case Call(args=None) | Call(f=AtHandle()) | Call(f=Group()): ...
            # case Call(args=args): ... #TODO: handling when args is not none... generally will be a list of identifiers that need to be converted directly to Identifier
            case Call():
                pdb.set_trace()
                ...
            case AtHandle(operand=PrototypeIdentifier(name=name)):
                gen.send(AtHandle(Identifier(name)))
            case AtHandle():
                pdb.set_trace()
                ...          
            case Assign(left=PrototypeIdentifier(name=name), right=right):
                gen.send(Assign(Identifier(name), right))
            case Assign(left=Array() as arr, right=right):
                target = convert_prototype_to_unpack_target(arr)
                gen.send(Assign(target, right))
            case Assign(left=TypedIdentifier(id=_id, type=_type)): ... #typed identifiers are already converted
            case Assign():
                pdb.set_trace()
                ...
            
            case IterIn(left=PrototypeIdentifier(name=name), right=right):
                gen.send(IterIn(Identifier(name), right))
            case IterIn(left=Array() as arr, right=right):
                target = convert_prototype_to_unpack_target(arr)
                gen.send(IterIn(target, right))
            case IterIn():
                pdb.set_trace()
                ...
            case UnpackTarget(): #TODO: may not need this one
                pdb.set_trace()
                ...
            case Declare(decltype=decltype, target=PrototypeIdentifier(name=name)):
                gen.send(Declare(decltype, Identifier(name)))
            case Declare(decltype=decltype, target=Array() as arr):
                pdb.set_trace()
                ...
            case Declare(decltype=decltype, target=Group() as group):
                pdb.set_trace()
                ...
            case Declare(): ... # all other declare cases are handled as normal
            case Index(): ... # I think all index cases are handled as normal
            case Access(left=left, right=PrototypeIdentifier(name=name)):
                gen.send(Access(left, Identifier(name)))
            case Access(left=left, right=AtHandle(operand=PrototypeIdentifier(name=name))):
                gen.send(Access(left, AtHandle(Identifier(name))))
            case Access():
                pdb.set_trace()
                ...

            # cases that themselves don't get adjusted but may contain nested children that need to be converted
            case IString() | Group() | Block() | PrototypeTuple() | Array() | ObjectLiteral() | Dict() | BidirDict() | FunctionLiteral() | Signature() | Range() | Loop() | If() | Flow() | Default() \
                | PointsTo() | BidirPointsTo() | Equal() | Less() | LessEqual() | Greater() | GreaterEqual() | LeftShift() | RightShift() | LeftRotate() | RightRotate() | LeftRotateCarry() | RightRotateCarry() | Add() | Sub() | Mul() | Div() | IDiv() | Mod() | Pow() | And() | Or() | Xor() | Nand() | Nor() | Xnor() | MemberIn() \
                | BroadcastOp() | SpreadOutFrom() | QJux() \
                | Not() | UnaryPos() | UnaryNeg() | UnaryMul() | UnaryDiv() | CycleLeft() | CycleRight() \
                | TypedIdentifier() | ReturnTyped() | SubTyped() | MakeGeneric() | Parameterize() | TypeParam():
                ...
            #TBD cases: Type() | ListOfASTs() | BareRange() | Ellipsis() | Spread() | TypeParam() | Flowable() | Flow() | PrototypeBuiltin() | Builtin() | Express() | ReturnTyped() | SequenceUnpackTarget() | ObjectUnpackTarget() | DeclarationType() | DeclareGeneric() | Parameterize():
            case _:  # all others are traversed as normal
                pdb.set_trace()
                raise ValueError(f'Unhandled case {type(i)}')
            #     pdb.set_trace()
            #     ...

    return ast.items[0]

#TODO: maybe have one of these for Array, Object, Dict, BidirDict depending on what is to be unpacked
#      hard though because also requires the type of whatever is being unpacked
#      because array unpack and object unpack can look the same syntactically
def convert_prototype_to_unpack_target(ast: Array) -> UnpackTarget:
    """Convert an Array of PrototypeIdentifiers or other ASTs to an UnpackTarget"""
    for i in (gen := ast.__iter_asts_full_traversal__()):
        if i.is_settled():
            continue

        match i:
            case PrototypeIdentifier(name=name):
                gen.send(Identifier(name))
            case Assign(left=PrototypeIdentifier(name=name), right=right):
                gen.send(Assign(Identifier(name), right))
            case Array() as arr:
                gen.send(convert_prototype_to_unpack_target(arr))
            case CollectInto(): ...
            case TypedIdentifier(): ...
            case _:
                raise NotImplementedError(f'Unhandled case {type(i)} in convert_prototype_to_unpack_target')

    return UnpackTarget(ast.items)


def convert_prototype_tuples(ast: AST) -> AST:
    """For now, literally just turn all tuples into groups"""
    ast = Group([ast])
    for i in (gen := ast.__iter_asts_full_traversal__()):
        if isinstance(i, PrototypeTuple):
            gen.send(Group(i.items))
    return ast.items[0]

def convert_bare_ranges(ast: AST) -> AST:
    """Convert all BareRanges to Ranges with inclusive bounds"""
    ast = Group([ast])
    for i in (gen := ast.__iter_asts_full_traversal__()):
        if isinstance(i, BareRange):
            gen.send(Range(i.left, i.right, '[]'))
    return ast.items[0]


def convert_bare_ellipses(ast: AST) -> AST:
    """Convert all remaining DotDotDots (that were not juxtaposed) to Ellipsis"""
    ast = Group([ast])
    for i in (gen := ast.__iter_asts_full_traversal__()):
        if isinstance(i, DotDotDot):
            gen.send(Ellipsis())
    return ast.items[0]


def convert_prototype_function_literals(ast: AST) -> AST:
    ast = Group([ast])
    for i in (gen := ast.__iter_asts_full_traversal__()):
        if isinstance(i, PrototypeFunctionLiteral):
            args = normalize_function_args(i.args)
            gen.send(FunctionLiteral(args, i.body))
    return ast.items[0]


def normalize_function_arg(arg: AST) -> tuple[list[AST], list[AST], list[AST]]:
    pkwarg, parg, kwarg = [], [], []
    match arg:
        case Void(): ...
        case PrototypeIdentifier(name=name):
            pkwarg.append(Identifier(name))
        case Identifier() | TypedIdentifier() | Assign():
            pkwarg.append(arg)
        case Array() as arr:
            pdb.set_trace()
            parg.append(convert_prototype_to_unpack_target(arr))
        # case Spread() | UnpackTarget():
        #     pdb.set_trace()
        #     ...
        # case Dict() | BidirDict():
        #     pdb.set_trace()
        #     ...
        #     #copilot suggested these could be kwargs, though I suspect it won't work (i.e. how is default vs no default handled? name -> void)
        #     #think about though. could use identifiers directly instead of strings

        case _:
            raise NotImplementedError(f'normalize_signature not implemented yet for {arg=}')

    return pkwarg, parg, kwarg


# def array_items_to_unpack_target(items: list[AST]) -> UnpackTarget:
#     """Convert an Array of ASTs to an UnpackTarget"""
#     unpack_items = []
#     for i in items:
#         match i:
#             case Identifier() | PrototypeIdentifier() | TypedIdentifier() | Assign() | Spread() | UnpackTarget():
#                 unpack_items.append(i)
#             case Array(items):
#                 unpack_items.append(array_items_to_unpack_target(items))
#             case _:
#                 raise NotImplementedError(f'array_items_to_unpack_target not implemented yet for {i=}')
#     return UnpackTarget(unpack_items)


def normalize_function_args(signature: AST) -> Signature:
    """Convert all the different function arg syntax options to a normalized format (group)"""
    if not isinstance(signature, Group):
        return Signature(*normalize_function_arg(signature))

    pkwargs, pargs, kwargs = [], [], []
    for i in signature.items:
        pkw, p, kw = normalize_function_arg(i)
        pkwargs.extend(pkw)
        pargs.extend(p)
        kwargs.extend(kw)
    return Signature(pkwargs, pargs, kwargs)


16:T5b3f,from abc import ABC, abstractmethod, ABCMeta
from typing import get_args, get_origin, Generator, Any, Literal, Union, dataclass_transform, Callable as TypingCallable, TypeVar
from types import UnionType
from dataclasses import dataclass, field, fields
from enum import Enum, auto
# from fractions import Fraction

from .tokenizer import Operator_t, escape_whitespace  # TODO: move into utils

import pdb


@dataclass_transform()
class AST(ABC):
    def __init_subclass__(cls: type['AST'], **kwargs):
        """
        - automatically applies the dataclass decorator with repr=False to AST subclasses
        """
        super().__init_subclass__(**kwargs)

        # Apply the dataclass decorator with repr=False to the subclass
        dataclass(repr=False)(cls)

    # TODO: add property to all ASTs for function complete/locked/etc. meaning it and all children are settled
    @abstractmethod
    def __str__(self) -> str:
        """Return a string representation of the AST in a canonical dewy code format"""

    def __repr__(self) -> str:
        """
        Returns a string representation of the AST tree with correct indentation for each sub-component

        e.g.
        SomeAST(prop0=..., prop1=...)
        โโโ child0=SomeSubAST(...)
        โโโ child1=SomeOtherAST(...)
        โ   โโโ a=ThisAST(...)
        โ   โโโ b=ThatAST(...)
        โโโ child2=AST2(...)
            โโโ something=ThisLastAST(...)

        Where all non-ast attributes of a node are printed on the same line as the node itself
        and all children are recursively indented a level and printed on their own line
        """
        return '\n'.join(self._gentree())

    def _gentree(self, prefix: str = '') -> Generator[str, None, None]:
        """
        a recursive generator helper function for __repr__

        Args:
            prefix: str - the string to prepend to each child line (root line already has prefix)
            name: str - the name of the current node in the tree
            # draw_branches: bool - whether each item should be drawn with branches or only use whitespace

        Returns:
            str: the string representation of the AST tree
        """
        # prefix components:
        space = '    '
        branch = 'โ   '
        # pointers:
        tee = 'โโโ '
        last = 'โโโ '

        attrs_str = ', '.join(f'{k}={v}' for k, v in self.__iter_non_ast_props__())
        yield f'{self.__class__.__name__}({attrs_str})'
        children: tuple[tuple[str, AST], ...] = tuple((k, v) for k, v in self.__iter_ast_props__())
        pointers = [tee] * (len(children) - 1) + [last]
        for (k, v), pointer in zip(children, pointers):
            extension = branch if pointer == tee else space
            gen = v._gentree(f'{prefix}{extension}')
            name = f'{k}=' if k else ''
            yield f'{prefix}{pointer}{name}{next(gen)}'     # first line gets name and pointer
            yield from gen                                  # rest of lines already have a prefix


    def __iter_ast_props__(self, *, visit_replacement:bool=True) -> Generator[tuple[str, 'AST'], 'AST', None]:
        """
        Iterate over all children ASTs of this AST (including those in containers, e.g. list[AST])
        Allows for in place replacement of the current AST via .send()
        
        Arguments:
            visit_replacement (bool): if True, then anytime .send(ast) is called, the iterator will visit that ast on the next step

        Sends:
            replacement (AST): an AST to replace the current AST with in the parent/container

        Yields:
            ast (AST): The current child AST        
        """
        for key, value in self.__dict__.items():
            # catch cases of something: AST | None
            if value is None:
                continue
            
            if isinstance(value, AST):
                while True:
                    replacement = yield key, value
                    if replacement is not None:
                        value = replacement
                        setattr(self, key, value)
                        assert (yield) is None, 'ILLEGAL: Cannot .send() multiple times in a row. must allow next() (e.g. from loop iter) to be called first'
                        if visit_replacement:
                            continue
                    break
                continue

            if is_ast_container(self.__class__.__annotations__.get(key)):
                if isinstance(value, list):
                    for i, vi in enumerate(value):
                        while True:
                            replacement = yield '', vi
                            if replacement is not None:
                                vi = replacement
                                value[i] = vi
                                assert (yield) is None, 'ILLEGAL: Cannot .send() multiple times in a row. must allow next() (e.g. from loop iter) to be called first'
                                if visit_replacement:
                                    continue
                            break
                    continue
                # elif isinstance(value, some_other_container_type)
                else:
                    pdb.set_trace()
                    raise NotImplementedError(f'__iter_ast_props__ over {type(value)} (from member "{key}") of {self} is not yet implemented')


          
    def __iter_non_ast_props__(self) -> Generator[tuple[str, Any], None, None]:
        """Iterate over the non-AST members of the AST, returning the key and value of each prop"""
        for key, value in self.__dict__.items():
            if isinstance(value, AST):
                continue
            if key.startswith('_'):
                continue
            yield key, value
        
    def __iter_asts__(self, *, visit_replacement:bool=True) -> Generator['AST', 'AST', None]:
        """Return a generator of the direct children ASTs of the AST"""
        yield from map_generator(lambda x: x[1], self.__iter_ast_props__(visit_replacement=visit_replacement))


    def __iter_asts_full_traversal__(self, *, visit_replacement:bool=True):
        """
        Recursively pre-order traversal of all child ASTs of the current AST
        Allows for in place replacement of the current AST via .send()
        """
        for child in (gen:= self.__iter_asts__(visit_replacement=visit_replacement)):
            replacement = yield child
            if replacement is not None:
                child = replacement
                gen.send(child)
                assert (yield) is None, 'ILLEGAL: Cannot .send() multiple times in a row. must allow next() (e.g. from loop iter) to be called first'

            yield from child.__iter_asts_full_traversal__(visit_replacement=visit_replacement)


    def is_settled(self) -> bool:
        """Return True if the neither the AST, nor any of its descendants, are prototypes"""
        for child in self.__iter_asts__():
            if not child.is_settled():
                return False
        return True



T = TypeVar('T')
U = TypeVar('U')
V = TypeVar('V')
def map_generator(f: TypingCallable[[T], U], gen: Generator[T, V, None]) -> Generator[U, V, None]:
    """Map a sendable generator to a new generator using the given function"""
    gen = iter(gen)
    try:
        val = next(gen)
        while True:
            to_send = yield f(val) if val is not None else None
            val = gen.send(to_send)
    except StopIteration:
        return


def is_ast_container(type_hint: type | None) -> bool:
    """
    Determine if the type hint is a container of ASTs.
    e.g. list[AST], set[SomeSubclassOfAST|OtherSubclassOfAST], etc.

    Args:
        type_hint: type | None - the type hint to check. If None, returns False

    Returns:
        bool: True if any of the contained types are subclasses of AST, False otherwise
    """
    if type_hint is None:
        return False

    # class constructors are not containers
    if get_origin(type_hint) is type:
        return False

    # python callables are not containers regardless of if they take in or return ASTs
    if get_origin(type_hint) == get_origin(TypingCallable):
        return False


    # Iterate over all contained types
    args = get_args(type_hint)
    for arg in args:
        # Handle Union types (e.g., Union[B, C] or B | C)
        if get_origin(arg) is Union:
            if any(issubclass(sub_arg, AST) for sub_arg in get_args(arg) if isinstance(sub_arg, type)):
                return True
        elif isinstance(arg, UnionType):
            if any(issubclass(sub_arg, AST) for sub_arg in arg.__args__ if isinstance(sub_arg, type)):
                return True
        # Check if the argument itself is a subclass of the base class
        elif isinstance(arg, type) and issubclass(arg, AST):
            return True

    # no AST subclasses found
    return False


class PrototypeAST(AST, ABC):
    """Used to represent AST nodes that are not complete, and must be removed before the whole AST is evaluated"""

    def is_settled(self) -> bool:
        """By definition, prototypes are not settled"""
        return False


class Delimited(ABC):
    """used to track which ASTs are printed with their own delimiter so they can be juxtaposed without extra parentheses"""

class TypeParam(AST, Delimited):
    items: list[AST]

    def __str__(self):
        return f'<{" ".join(map(str, self.items))}>'
class Type(AST):
    t: type[AST]
    parameters: TypeParam | None = None

    def __str__(self) -> str:
        if self.parameters:
            return f'{self.t.__name__}{self.parameters}'
        return self.t.__name__
    
    # strictly for hashing
    def __eq__(self, other):
        return isinstance(other, Type) and self.t == other.t and self.parameters == other.parameters
    
    def __hash__(self):
        return hash((self.t, self.parameters))




# TODO: turn into a singleton...
# untyped type for when a declaration doesn't specify a type
class Untyped(AST):
    def __str__(self) -> str:
        return 'untyped'
untyped = Type(Untyped)


class Undefined(AST):
    """undefined singleton"""
    def __new__(cls):
        if not hasattr(cls, 'instance'):
            cls.instance = super(Undefined, cls).__new__(cls)
        return cls.instance

    def __str__(self) -> str:
        return 'undefined'


# undefined shorthand, for convenience
undefined = Undefined()


class Void(AST):
    """void singleton"""
    def __new__(cls):
        if not hasattr(cls, 'instance'):
            cls.instance = super(Void, cls).__new__(cls)
        return cls.instance

    def __str__(self) -> str:
        return 'void'


# void shorthand, for convenience
void = Void()


# assign is just a binop?
# perhaps bring this one back since it's syntax that distinguishes it, not type checking
# class Assign(AST):
#     # TODO: allow bind to take in an unpack structure
#     target: Declare | Identifier | UnpackTarget
#     value: AST

#     def __str__(self):
#         return f'{self.target} = {self.value}'


class ListOfASTs(PrototypeAST):
    """Intermediate step for holding a list of ASTs that are probably captured by a container"""
    asts: list[AST]

    def __str__(self):
        return f'{", ".join(map(str, self.asts))}'


class PrototypeTuple(PrototypeAST):
    """
    A comma separated list of expressions (not wrapped in parentheses) e.g. 1, 2, 3
    There is no special in-memory representation of a tuple, it is literally just a const list
    """
    items: list[AST]

    def __str__(self):
        return f'{", ".join(map(str, self.items))}'


class Group(AST, Delimited):
    items: list[AST]

    def __str__(self):
        return f'({" ".join(map(str, self.items))})'


class Block(AST, Delimited):
    items: list[AST]

    def __str__(self):
        return f'{{{" ".join(map(str, self.items))}}}'


# class Number(AST):
#     val: int | float | Fraction

class Bool(AST):
    val: bool

    def __str__(self) -> str:
        return str(self.val).lower()


class Int(AST):
    val: int

    def __str__(self) -> str:
        return str(self.val)


class String(AST, Delimited):
    val: str

    def __str__(self) -> str:
        return f'"{escape_whitespace(self.val)}"'


class IString(AST, Delimited):
    parts: list[AST]

    def __str__(self):
        s = ''
        for part in self.parts:
            if isinstance(part, String):
                s += part.val
            else:
                s += f'{part}'
        return f'"{s}"'


class Flowable(AST, ABC):
    ...
    # def was_entered(self) -> bool:
    #     """Determine if the flowable branch was entered. Should reset before performing calls to flow and checking this."""
    #     raise NotImplementedError(f'flowables must implement `was_entered()`. No implementation found for {self.__class__}')

    # def reset_was_entered(self) -> None:
    #     """reset the state of was_entered, in preparation for executing branches in a flow"""
    #     raise NotImplementedError(f'flowables must implement `reset_was_entered()`. No implementation found for {self.__class__}')


class Flow(AST):
    branches: list[Flowable]

    def __str__(self):
        return ' else '.join(map(str, self.branches))


class If(Flowable):
    condition: AST
    body: AST

    def __str__(self):
        return f'if {self.condition} {self.body}'


class Loop(Flowable):
    condition: AST
    body: AST

    def __str__(self):
        return f'loop {self.condition} {self.body}'


class Default(Flowable):
    body: AST

    def __str__(self):
        return f'{self.body}'


class PrototypeFunctionLiteral(PrototypeAST):
    args: AST
    body: AST

    def __str__(self):
        if isinstance(self.args, Delimited):
            return f'{self.args} => {self.body}'
        return f'({self.args}) => {self.body}'


class PrototypeBuiltin(PrototypeAST):
    args: Group
    return_type: AST

    def __str__(self):
        return f'({self.args}):> {self.return_type} => ...'


class Call(AST):
    f: AST
    args: None | AST = None

    def __str__(self):
        if self.args is None:
            return f'{self.f}()'
        if isinstance(self.args, Delimited):
            return f'{self.f}{self.args}'
        return f'{self.f}({self.args})'

from typing import cast
class BinOp(AST, ABC):
    left: AST
    right: AST

    def __post_init__(self):
        self._space = cast(bool, getattr(self, '_space', True))
        self._op = cast(str, getattr(self, '_op', None))
        assert isinstance(self._op, str), f'BinOp subclass "{self.__class__.__name__}" must define an `_op` attribute'

    def __str__(self) -> str:
        if self._space:
            return f'{self.left} {self._op} {self.right}'
        return f'{self.left}{self._op}{self.right}'

class Assign(BinOp):
    _op = '='
class CompiletimeAssign(BinOp):
    _op = '::'
class PointsTo(BinOp):
    _op = '->'
class BidirPointsTo(BinOp):
    _op = '<->'
class Access(BinOp):
    _op = '.'
    _space = False

class Index(BinOp):
    _op = ''
    _space = False

class Equal(BinOp):
    _op = '=?'

# covered by OpChain([Not, Equal])
# class NotEqual(BinOp):
#     _op = 'not=?'

class Less(BinOp):
    _op = '<?'

class LessEqual(BinOp):
    _op = '<=?'

class Greater(BinOp):
    _op = '>?'

class GreaterEqual(BinOp):
    _op = '>=?'

class  LeftShift(BinOp):
    _op = '<<'

class  RightShift(BinOp):
    _op = '>>'

class LeftRotate(BinOp):
    _op = '<<<'

class RightRotate(BinOp):
    _op = '>>>'

class LeftRotateCarry(BinOp):
    _op = '<<!'

class RightRotateCarry(BinOp):
    _op = '!>>'

class Add(BinOp):
    _op = '+'

class Sub(BinOp):
    _op = '-'

class Mul(BinOp):
    _op = '*'

class Div(BinOp):
    _op = '/'

class IDiv(BinOp):
    _op = 'รท'

class Mod(BinOp):
    _op = '%'

class Pow(BinOp):
    _op = '^'

class And(BinOp):
    _op = 'and'

class Or(BinOp):
    _op = 'or'

class Xor(BinOp):
    _op = 'xor'

class Nand(BinOp):
    _op = 'nand'

class Nor(BinOp):
    _op = 'nor'

class Xnor(BinOp):
    _op = 'xnor'

class IterIn(BinOp):
    _op = 'in'

class MemberIn(BinOp):
    _op = 'in?'

class UnaryPrefixOp(AST, ABC):
    operand: AST

    def __post_init__(self):
        self._space = cast(bool, getattr(self, '_space', False))
        self._op = cast(str, getattr(self, '_op', None))
        assert isinstance(self._op, str), f'UnaryPrefixOp subclass "{self.__class__.__name__}" must define an `_op` attribute'

    def __str__(self) -> str:
        if self._space:
            return f'{self._op} {self.operand}'
        return f'{self._op}{self.operand}'

class Not(UnaryPrefixOp):
    _op = 'not'
    _space = True

class UnaryNeg(UnaryPrefixOp):
    _op = '-'

class UnaryPos(UnaryPrefixOp):
    _op = '+'

class UnaryMul(UnaryPrefixOp):
    _op = '*'

class UnaryDiv(UnaryPrefixOp):
    _op = '/'



class AtHandle(UnaryPrefixOp):
    _op = '@'
    def __str__(self):
        if isinstance(self.operand, (Delimited, Identifier)):
            return f'@{self.operand}'
        return f'@({self.operand})'


class UnaryPostfixOp(AST, ABC):
    operand: AST

    def __post_init__(self):
        self._op = cast(str, getattr(self, '_op', None))
        assert isinstance(self._op, str), f'UnaryPostfixOp subclass "{self.__class__.__name__}" must define an `_op` attribute'

    def __str__(self) -> str:
        return f'{self.operand}{self._op}'

class Suppress(UnaryPostfixOp):
    _op = ';'


class BroadcastOp(AST):
    op: BinOp

    def __str__(self):
        return f'{self.op.left} .{self.op._op} {self.op.right}'

# TBD if need. For now, parser just does `left = left op right` for `left op= right`
#     needed if we wanted to instead actually do slightly different things when doing an update assign
#     ony case I can think of is from overloading:
#     ```
#     myfunc = () => 'first version'
#     // three possible ways overloading would work...
#     myfunc  |= (a:int b:int) => 'second version'  // myfunc  = myfunc  | (a:int b:int) => 'second version' // fails because myfunc needs to be @myfunc on the right side
#     @myfunc |= (a:int b:int) => 'second version'  // @myfunc = @myfunc | (a:int b:int) => 'second version'
#     myfunc  |= (a:int b:int) => 'second version'  // myfunc  = @myfunc | (a:int b:int) => 'second version'
#     ```
# Also note that vectorized ops might be `op: BinOp | CombinedAssign` if we implement this
# class CombinedAssign(AST):
#     op: BinOp

#     def __str__(self) -> str:
#         return f'{self.op.left} {self.op._op}= {self.op.right}'


class BareRange(PrototypeAST):
    left: AST
    right: AST

    def __str__(self) -> str:
        return f'{self.left}..{self.right}'


class Ellipsis(AST):
    def __str__(self) -> str:
        return '...'


class Backticks(PrototypeAST):
    backticks: str
    def __str__(self) -> str:
        return f'{self.backticks}'


class CycleLeft(AST):
    operand: AST
    num_steps: int

    def __str__(self):
        return f'{"`"*self.num_steps}{self.operand}'


class CycleRight(AST):
    operand: AST
    num_steps: int

    def __str__(self):
        return f'{self.operand}{"`"*self.num_steps}'


class DotDotDot(PrototypeAST):
    def __str__(self) -> str:
        return f'...'

class CollectInto(AST):
    right: AST

    def __str__(self):
        return f'...{self.right}'

class SpreadOutFrom(AST):
    left: AST

    def __str__(self):
        return f'{self.left}...'



class Range(AST):
    left: AST
    right: AST
    brackets: Literal['[]', '[)', '(]', '()']

    def __str__(self) -> str:
        return f'{self.brackets[0]}{self.left}..{self.right}{self.brackets[1]}'


class Array(AST, Delimited):
    items: list[AST] # list[T] where T is not PointsTo or BidirPointsTo. Might have Declare or Assign. Must have at least 1 expression!

    def __str__(self):
        return f'[{" ".join(map(str, self.items))}]'


class Dict(AST, Delimited):
    items: list[PointsTo]

    def __str__(self):
        return f'[{" ".join(map(str, self.items))}]'


class BidirDict(AST, Delimited):
    items: list[BidirPointsTo]

    def __str__(self):
        return f'[{" ".join(map(str, self.items))}]'


class ObjectLiteral(AST, Delimited):
    items: list[AST] # list[Declare|Assign|AST] has to have at least 1 declare or assignment, and no expressions!

    def __str__(self):
        return f'[{" ".join(map(str, self.items))}]'




class MakeGeneric(AST):
    # e.g. you are creating something that is generic: myfn = <T>(a:T, b:T) => a + b
    left: TypeParam
    right: AST

    def __str__(self):
        return f'{self.left}{self.right}'


class Parameterize(AST):
    # e.g. you are applying a type to an existing generic object: res = myfn<int64>(1 2)
    left: AST
    right: TypeParam

    def __str__(self):
        return f'{self.left}{self.right}'


class PrototypeIdentifier(PrototypeAST):
    name: str
    def __str__(self) -> str:
        return f'ProtoId<{self.name}>'

class Identifier(AST):
    name: str
    def __str__(self) -> str:
        return f'{self.name}'


class Express(AST):
    id: Identifier

    def __str__(self) -> str:
        return f'{self.id}'

class TypedIdentifier(AST):
    id: Identifier
    type: AST

    def __str__(self) -> str:
        return f'{self.id}:{self.type}'


class ReturnTyped(BinOp):
    _op = ':>'

class SubTyped(BinOp):
    _op = 'of'
    _space = True
    

class UnpackTarget(AST):
    target: 'list[Identifier | TypedIdentifier | UnpackTarget | Assign | CollectInto]'
    def __str__(self) -> str:
        return f'[{" ".join(map(str, self.target))}]'

class DeclarationType(Enum):
    LET = auto()
    CONST = auto()
    # LOCAL_CONST = auto()
    # FIXED_TYPE = auto()

    # default for binding without declaring
    # DEFAULT = LET


class Declare(AST):
    decltype: DeclarationType
    target: Identifier | TypedIdentifier | ReturnTyped | UnpackTarget | Assign | CompiletimeAssign #| CollectInto #TBD if can declare with collect into

    def __str__(self):
        return f'{self.decltype.name.lower()} {self.target}'



if __name__ == '__main__':
    # DEBUG testing tree string printing
    class _Add(AST):
        l: AST
        r: AST

        def __str__(self) -> str:
            return f'{self.l} + {self.r}'

    class _Mul(AST):
        l: AST
        r: AST

        def __str__(self) -> str:
            return f'{self.l} * {self.r}'

    class _List(AST):
        items: list[AST]

        def __str__(self) -> str:
            return f'[{", ".join(map(str, self.items))}]'

    class _Int(AST):
        value: int

        def __str__(self) -> str:
            return str(self.value)

    # big long test ast
    test = _Add(
        _Add(
            _Int(1),
            _List([_Int(2), _Int(3), _Int(4), _Int(5)])
        ),
        _Mul(
            _Int(2),
            _Add(
                _Mul(
                    _Int(3),
                    _Int(4)
                ),
                _Mul(
                    _Int(5),
                    _Int(6)
                )
            )
        )
    )

    print(repr(test))
    print(str(test))
    # class Broken(AST):
    #     num: int
    #     def __str__(self) -> str:
    #         return f'{self.num}'
    #     def __iter__(self) -> Generator['AST', None, None]:
    #         yield Int(self.num)
17:T9c83,from abc import ABC
import inspect
from typing import Callable, Type, Generator
from types import UnionType
from functools import lru_cache
from .utils import CoordString

import pdb


"""
[tasks]
- clean up eat_block
    - general cleanup
    - break out eat_ matching into smaller functions?
    - figure out tie breaking process:
        1. prefer @full_eat over @peek_eat ---> TODO: implement
        2. prefer longest matches
        3. prefer higher precedence
        4. error
- make all tokens keep the source they come from (for error reporting/keeping track of row/col of the token)
"""


class Token(ABC):
    def __repr__(self) -> str:
        """default repr for tokens is just the class name"""
        return f"<{self.__class__.__name__}>"

    def __hash__(self) -> int:
        raise NotImplementedError(f'hash is not implemented for token type {type(self)}')

    def __eq__(self, __value: object) -> bool:
        raise NotImplementedError(f'equals is not implemented for token type {type(self)}')

    def __iter__(self) -> Generator['list[Token]', None, None]:
        """
        Iter is used by full_traverse_tokens for iterating over any contained tokens.
        e.g. Block_t.body TypeParam_t.body, String_t.body (interpolation blocks only), etc.
        """
        raise NotImplementedError(f'iter is not implemented for token type {type(self)}')


class WhiteSpace_t(Token):
    def __init__(self, _): ...


class Operator_t(Token):
    def __init__(self, op: str):
        self.op = op

    def __repr__(self) -> str:
        return f"<Operator_t: `{self.op}`>"

    def __hash__(self) -> int:
        return hash((Operator_t, self.op))

    def __eq__(self, other) -> bool:
        return isinstance(other, Operator_t) and self.op == other.op


class Juxtapose_t(Operator_t):
    def __init__(self, _):
        super().__init__('')

    def __repr__(self) -> str:
        return f"<Juxtapose_t>"

    def __hash__(self) -> int:
        return hash(Juxtapose_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, Juxtapose_t)


class Keyword_t(Token):
    def __init__(self, src: str):
        self.src = src.lower()

    def __repr__(self) -> str:
        return f"<Keyword_t: {self.src}>"

    def __hash__(self) -> int:
        return hash((Keyword_t, self.src))

    def __eq__(self, other) -> bool:
        return isinstance(other, Keyword_t) and self.src == other.src


class Identifier_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Identifier_t: {self.src}>"


class Hashtag_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Hashtag_t: {self.src}>"


class Block_t(Token):
    def __init__(self, body: list[Token], left: str, right: str):
        self.body = body
        self.left = left
        self.right = right

    def __repr__(self) -> str:
        body_str = ', '.join(repr(token) for token in self.body)
        return f"<Block_t: {self.left}{body_str}{self.right}>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield self.body


class TypeParam_t(Token):
    def __init__(self, body: list[Token]):
        self.body = body

    def __repr__(self) -> str:
        body_str = ', '.join(repr(token) for token in self.body)
        return f"<TypeParam_t: `<{body_str}>`>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        yield self.body


class Escape_t(Token):
    escape_map = {
        '\\n': '\n', '\\r': '\r', '\\t': '\t', '\\b': '\b', '\\f': '\f', '\\v': '\v', '\\a': '\a', '\\0': '\0', '\\\\': '\\'
    }

    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Escape_t: {self.src}>"

    def to_str(self) -> str:
        """Convert the escape sequence to the character it represents"""

        # unicode escape (may be several characters long)
        if self.src.startswith('\\U') or self.src.startswith('\\u'):
            return chr(int(self.src[2:], 16))
        assert len(self.src) == 2 and self.src[0] == '\\', "internal error. Ill-posed escape sequence"

        # known escape sequence
        if self.src in self.escape_map:
            esc = self.escape_map[self.src]
            # construct a CoordString at the position of the original escape
            return CoordString.from_existing(esc, self.src[:len(esc)].row_col_map)

        # unknown escape sequence (i.e. just replicate the character)
        return self.src[1]


class RawString_t(Token):
    def __init__(self, body: str):
        self.body = body

    def __repr__(self) -> str:
        return f"<RawString_t: {self.body}>"

    def to_str(self) -> str:
        body = self.body
        if body.startswith('r"""') or body.startswith("r'''"):
            body = body[4:-3]
        elif body.startswith('r"') or body.startswith("r'"):
            body = body[2:-1]
        else:
            raise ValueError(f"Internal Error: unrecognized delimiters on raw string: {repr(self)}")
        return body


class String_t(Token):
    def __init__(self, body: list[str | Escape_t | Block_t]):
        self.body = body

    def __repr__(self) -> str:
        return f"<String_t: {self.body}>"

    def __iter__(self) -> Generator[list[Token], None, None]:
        for token in self.body:
            if isinstance(token, Block_t):
                yield token.body

# class Number_t(Token, ABC):...


class Integer_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Integer_t: {self.src}>"


class BasedNumber_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<BasedNumber_t: {self.src}>"


class Undefined_t(Token):
    def __init__(self, _): ...

    def __hash__(self) -> int:
        return hash(Undefined_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, Undefined_t)

    def __repr__(self) -> str:
        return "<Undefined_t>"


class Void_t(Token):
    def __init__(self, _): ...

    def __hash__(self) -> int:
        return hash(Void_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, Void_t)

    def __repr__(self) -> str:
        return "<Void_t>"


class End_t(Token):
    def __init__(self, _): ...

    def __hash__(self) -> int:
        return hash(End_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, End_t)

    def __repr__(self) -> str:
        return "<End_t>"


class New_t(Token):
    def __init__(self, _): ...

    def __hash__(self) -> int:
        return hash(New_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, New_t)

    def __repr__(self) -> str:
        return "<New_t>"


class Boolean_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Boolean_t: {self.src}>"


class ShiftOperator_t(Operator_t):
    def __init__(self, op: str):
        self.op = op

    def __repr__(self) -> str:
        return f"<ShiftOperator_t: `{self.op}`>"

    def __hash__(self) -> int:
        return hash((ShiftOperator_t, self.op))

    def __eq__(self, other) -> bool:
        return isinstance(other, ShiftOperator_t) and self.op == other.op


class Comma_t(Operator_t):
    def __init__(self, op: str):
        self.op = op

    def __hash__(self) -> int:
        return hash(Comma_t)

    def __eq__(self, other) -> bool:
        return isinstance(other, Comma_t)


class DotDot_t(Token):
    def __init__(self, src: str):
        self.src = src


class DotDotDot_t(Token):
    def __init__(self, src: str):
        self.src = src

class Backticks_t(Token):
    def __init__(self, src: str):
        self.src = src

    def __repr__(self) -> str:
        return f"<Backticks_t: {self.src}>"


# identify token classes that should take precedence over others when tokenizing
# each row is a list of token types that are confusable in their precedence order. e.g. [Keyword, Unit, Identifier] means Keyword > Unit > Identifier
# only confusable token classes need to be included in the table
precedence_table = [
    [Keyword_t, Undefined_t, Void_t, End_t, New_t, Boolean_t, Operator_t, DotDot_t, Backticks_t, Identifier_t],
]
precedence = {cls: len(row)-i for row in precedence_table for i, cls in enumerate(row)}

# mark which tokens cannot be repeated in a list of tokens. E.g. whitespace should always be merged into a single token
idempotent_tokens = {
    WhiteSpace_t
}

# paired delimiters for blocks, ranges, groups, etc.
pair_opening_delims = '{(['
pair_closing_delims = '})]'

# which closing delimiters are allowed for each opening delimiter
valid_delim_closers = {
    '{': '}',
    '(': ')]',
    '[': '])',
    # '<': '>'
}

# list of all operators sorted from longest to shortest
# TODO: make @ and ... into expressions (perhaps with lower precedence calling than regular calls?)
unary_prefix_operators = {'+', '-', '*', '/', 'not', '~', '@'}#, '...'}
unary_postfix_operators = {'?', ';'}
binary_operators = {
    '+', '-', '*', '/', '%', '^',
    '=?', '>?', '<?', '>=?', '<=?', 'in?', 'is?', 'isnt?', '<=>',
    '|', '&',
    'and', 'or', 'nand', 'nor', 'xor', 'xnor', '??',
    'else',
    '=', '::', 'as', 'in', 'transmute', #':=' # walrus operator is redundant. Only add if people really need it, which I find unlikely
    '@?',
    '|>', '<|', '=>',
    '->', '<->', #'<-', #reverse arrow is dumb
    '.', ':', ':>', 'of',
}
opchain_starters = {'+', '-', '*', '/', '%', '^'}
operators = sorted(
    [*(unary_prefix_operators | unary_postfix_operators | binary_operators)],
    key=len,
    reverse=True
)
# TODO: may need to separate |> from regular operators since it may confuse type param
shift_operators = sorted(['<<', '>>', '<<<', '>>>', '<<!', '!>>'], key=len, reverse=True)
keywords = ['loop', 'lazy', 'do', 'if', 'match', 'return', 'yield', 'break', 'continue',
            'async', 'await', 'import', 'from', 'let', 'const', 'local_const', 'fixed_type']
# TODO: what about language values, e.g. void, undefined, end, units, etc.? probably define at compile time, rather than in the compiler

# note that the prefix is case insensitive, so call .lower() when matching the prefix
# numbers may have _ as a separator (if _ is not in the set of digits)
number_bases = {
    '0b': {*'01'},  # binary
    '0t': {*'012'},  # ternary
    '0q': {*'0123'},  # quaternary
    '0s': {*'012345'},  # seximal
    '0o': {*'01234567'},  # octal
    '0d': {*'0123456789'},  # decimal
    '0z': {*'0123456789xeXE'},  # dozenal
    '0x': {*'0123456789abcdefABCDEF'},  # hexadecimal
    '0u': {*'0123456789abcdefghijklmnopqrstuvABCDEFGHIJKLMNOPQRSTUV'},  # base 32 (duotrigesimal)
    '0r': {*'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'},  # base 36 (hexatrigesimal)
    '0y': {*'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!$'},  # base 64 (tetrasexagesimal)
}


# units = #actually units should probably not be specific tokens, but recognized identifiers since the user can make their own units


def peek_eat(cls: Type[Token], whitelist: list[Type[Token]] | None = None, blacklist: list[Type[Token]] | None = None):
    """
    Decorator for functions that eat tokens, but only return how many characters would make up the token.
    Makes function return include constructor for token class that it tries to eat, in tupled with return.

    whitelist and blacklist can be used to specify parent token contexts that may or may not consume this type as a child
    """
    assert issubclass(cls, Token), f"cls must be a subclass of Token, but got {cls}"
    if whitelist is not None and blacklist is not None:
        raise ValueError("cannot specify both whitelist and blacklist")

    def decorator(eat_func: Callable[[str], int | None]):
        def wrapper(src: str) -> tuple[int | None, Type[Token]]:
            return eat_func(src), cls
        wrapper._is_peek_eat_decorator = True  # make it easy to check if a function has this decorator
        wrapper._eat_func = eat_func
        wrapper._token_cls = cls
        wrapper._whitelist = whitelist
        wrapper._blacklist = blacklist
        return wrapper
    return decorator

# TODO: full eat probably won't need to take the class as an argument, since the function will know how to construct the token itself


def full_eat(whitelist: list[Type[Token]] | None = None, blacklist: list[Type[Token]] | None = None):
    def decorator(eat_func: Callable[[str], tuple[int, Token] | None]):
        """
        Decorator for functions that eat tokens, and return the token itself if successful.
        TBD what this actually does...for now, largely keep unmodified, but attach the metadata to the wrapped function
        """
        # pull cls it from the return type of eat_func (which should be a Union[tuple[int, Token], None])
        cls = inspect.signature(eat_func).return_annotation.__args__[0].__args__[1]
        assert issubclass(cls, Token), f"cls must be a subclass of Token, but got {cls}"
        if whitelist is not None and blacklist is not None:
            raise ValueError("cannot specify both whitelist and blacklist")

        def wrapper(*args, **kwargs):
            return eat_func(*args, **kwargs), cls
        wrapper._is_full_eat_decorator = True  # make it easy to check if a function has this decorator
        wrapper._eat_func = eat_func
        wrapper._token_cls = cls
        wrapper._whitelist = whitelist
        wrapper._blacklist = blacklist

        return wrapper
    return decorator


def get_peek_eat_funcs_with_name() -> tuple[tuple[str, Callable], ...]:
    return tuple((name, func) for name, func in globals().items() if callable(func) and getattr(func, '_is_peek_eat_decorator', False))


def get_full_eat_funcs_with_name() -> tuple[tuple[str, Callable], ...]:
    return tuple((name, func) for name, func in globals().items() if callable(func) and getattr(func, '_is_full_eat_decorator', False))


def get_eat_funcs() -> tuple[Callable, ...]:
    return tuple(func for name, func in get_peek_eat_funcs_with_name() + get_full_eat_funcs_with_name())


@lru_cache()
def get_contextual_eat_funcs(context: Type[Token]) -> tuple[Callable, ...]:
    """Get all the eat functions that are valid in the given context"""
    return tuple(func for func in get_eat_funcs() if (func._whitelist is None or context in func._whitelist) and (func._blacklist is None or context not in func._blacklist))


@lru_cache()
def get_func_precedences(funcs: tuple[Callable]) -> tuple[int, ...]:
    assert isinstance(funcs, tuple)
    return tuple(precedence.get(func._token_cls, 0) for func in funcs)


@peek_eat(WhiteSpace_t)
def eat_line_comment(src: str) -> int | None:
    """eat a line comment, return the number of characters eaten"""
    if src.startswith('//'):
        try:
            return src.index('\n') + 1
        except ValueError:
            return len(src)
    return None


@peek_eat(WhiteSpace_t)
def eat_block_comment(src: str) -> int | None:
    """
    Eat a block comment, return the number of characters eaten
    Block comments are of the form /{ ... }/ and can be nested.
    """
    if not src.startswith("/{"):
        return None

    nesting_level = 0
    i = 0

    while i < len(src):
        if src[i:].startswith('/{'):
            nesting_level += 1
            i += 2
        elif src[i:].startswith('}/'):
            nesting_level -= 1
            i += 2

            if nesting_level == 0:
                return i
        else:
            i += 1

    raise ValueError("unterminated block comment")
    # return None


@peek_eat(WhiteSpace_t)
def eat_whitespace(src: str) -> int | None:
    """Eat whitespace, return the number of characters eaten"""
    i = 0
    while i < len(src) and src[i].isspace():
        i += 1
    return i if i > 0 else None


@peek_eat(Keyword_t)
def eat_keyword(src: str) -> int | None:
    """
    Eat a reserved keyword, return the number of characters eaten

    #keyword = {in} | {as} | {loop} | {lazy} | {if} | {and} | {or} | {xor} | {nand} | {nor} | {xnor} | {not};# | {true} | {false};

    noting that keywords are case insensitive
    """

    max_len = max(len(keyword) for keyword in keywords)

    lower_src = src[:max_len].lower()
    for keyword in keywords:
        if lower_src.startswith(keyword):
            # TBD if we need to check that the next character is not an identifier character
            return len(keyword)

    return None


# TODO: expand the list of valid identifier characters
digits = set('0123456789')
alpha = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz')
greek = set('ฮฮฮฮฮฮฮฮฮฮฮฮฮฮฮฮฮกฮฃฮคฮฅฮฆฮงฮจฮฉฮฑฮฒฮณฮดฮตฮถฮทฮธฮนฮบฮปฮผฮฝฮพฮฟฯฯฯฯฯฯฯฯฯฯ')
misc = set('_?!$&ยฐ')

start_characters = (alpha | greek | misc) - {'?'}
continue_characters = (alpha | digits | greek | misc)


@peek_eat(Identifier_t)
def eat_identifier(src: str) -> int | None:
    """
    Eat an identifier, return the number of characters eaten

    Identifiers:
    - may not start with a number or a question mark
    - may not end with a question mark
    - may use (TODO enumerate the full chars list somewhere. for now copying from python)

    """
    if not src[0] in start_characters:
        return None

    i = 1
    while i < len(src) and src[i] in continue_characters:
        i += 1

    # while last character is ?, remove it
    while i > 1 and src[i-1] == '?':
        i -= 1

    return i


@peek_eat(Hashtag_t)
def eat_hashtag(src: str) -> int | None:
    """
    Eat a hashtag, return the number of characters eaten

    hashtags are special identifiers that start with #
    """

    if src.startswith('#'):
        i, _ = eat_identifier(src[1:])
        if i is not None:
            return i + 1

    return None


@peek_eat(Escape_t, whitelist=[String_t])
def eat_escape(src: str) -> int | None:
    r"""
    Eat an escape sequence, return the number of characters eaten
    Escape sequences must be either a known escape sequence:
    - \n newline
    - \r carriage return
    - \t tab
    - \b backspace
    - \f form feed
    - \v vertical tab
    - \a alert
    - \0 null
    - \u##..# or \U##..# for an arbitrary unicode character. May have any number of hex digits

    or a \ followed by an unknown character. In this case, the escape converts to just the unknown character
    This is how to insert characters that are otherwise illegal inside a string, e.g.
    - \' converts to just a single quote '
    - \{ converts to just a single open brace {
    - \\ converts to just a single backslash \
    - \m converts to just a single character m
    - etc.
    """
    if not src.startswith('\\'):
        return None

    if len(src) == 1:
        raise ValueError("unterminated escape sequence")

    if src[1] in 'uU':
        i = 2
        while i < len(src) and src[i].isxdigit():
            i += 1
        if i == 2:
            raise ValueError("invalid unicode escape sequence")
        return i

    # if src[1] in 'nrtbfva0':
    #     return 2

    # all other escape sequences (known or unknown) are just a single character
    return 2


@full_eat()
def eat_string(src: str) -> tuple[int, String_t] | None:
    r"""
    strings are delimited with either single (') or double quotes (")
    the character portion of a string may contain any character except the delimiter, \, or {.
    strings may be multiline
    strings may contain escape sequences of the form \s where s is either a known escape sequence or a single character
    strings may interpolation blocks which open with { and close with }

    Tokenizing of escape sequences and interpolation blocks is handled as sub-tokenization task via eat_block and eat_escape

    returns the number of characters eaten and an instance of the String token, containing the list of tokens/string chunks/escape sequences
    """

    # determine the starting delimiter, or exit if there is none
    if src.startswith('"""') or src.startswith("'''"):
        delim = src[:3]
        i = 3
    elif src.startswith('"') or src.startswith("'"):
        delim = src[0]
        i = 1
    else:
        return None

    # keep track of chunks, and the start index of the current chunk
    chunk_start = i
    body = []

    # add character sequences, escapes, and block sections until the end of the string
    while i < len(src) and not src[i:].startswith(delim):

        # regular characters
        if src[i] not in '\\{':
            i += 1
            continue

        # add the previous chunk before handling the escape/interpolation block
        if i > chunk_start:
            body.append(src[chunk_start:i])

        if src[i] == '\\':
            res, _ = eat_escape(src[i:])
            if res is None:
                raise ValueError("invalid escape sequence")
            body.append(Escape_t(src[i:i+res]))
            i += res

        else:  # src[i] == '{':
            assert src[i] == '{', "internal error"
            res, _ = eat_block(src[i:])
            if res is None:
                raise ValueError("invalid block")
            n_eaten, block = res
            body.append(block)
            i += n_eaten

        # update the chunk start
        chunk_start = i

    if i == len(src):
        raise ValueError("unterminated string")

    # add the final chunk
    if i > chunk_start:
        body.append(src[chunk_start:i])

    return i + len(delim), String_t(body)


@peek_eat(RawString_t)
def eat_raw_string(src: str) -> int | None:
    """
    raw strings start with `r`, followed by a delimiter, one of ' " ''' or \"""
    raw strings may contain any character except the delimiter.
    Escapes and interpolations are ignored.
    The string ends at the first instance of the delimiter
    """
    if not src.startswith('r'):
        return None
    i = 1

    if src[i:].startswith('"""') or src[i:].startswith("'''"):
        delim = src[i:i+3]
        i += 3
    elif src[i:].startswith('"') or src[i:].startswith("'"):
        delim = src[i]
        i += 1
    else:
        return None

    while i < len(src) and not src[i:].startswith(delim):
        i += 1

    if i == len(src):
        raise ValueError("unterminated raw string")

    return i + len(delim)


@peek_eat(Integer_t)
def eat_integer(src: str) -> int | None:
    """
    eat an integer, return the number of characters eaten
    integers are of the form [0-9]+
    """
    if not src[0].isdigit():
        return None

    i = 1
    while i < len(src) and src[i].isdigit() or src[i] == '_':
        i += 1
    return i


@peek_eat(BasedNumber_t)
def eat_based_number(src: str) -> int | None:
    """
    eat a based number, return the number of characters eaten

    based numbers have a (case-insensitive) prefix (0p) identifying the base, and (case-sensitive) allowed digits
    """
    try:
        digits = number_bases[src[:2].lower()]
    except KeyError:
        return None

    i = 2
    while i < len(src) and src[i] in digits or src[i] == '_':
        i += 1

    return i if i > 2 else None


@peek_eat(Undefined_t)
def eat_undefined(src: str) -> int | None:
    """
    eat the undefined token, return the number of characters eaten
    """
    sample = src[:9].lower()
    if sample.startswith('undefined'):
        return 9
    return None


@peek_eat(Void_t)
def eat_void(src: str) -> int | None:
    """
    eat the void token, return the number of characters eaten
    """
    sample = src[:4].lower()
    if sample.startswith('void'):
        return 4
    return None


@peek_eat(End_t)
def eat_end(src: str) -> int | None:
    """
    eat the end token, return the number of characters eaten
    """
    sample = src[:3].lower()
    if sample.startswith('end'):
        return 3
    return None

@peek_eat(New_t)
def eat_new(src: str) -> int | None:
    """
    eat the new token, return the number of characters eaten
    """
    sample = src[:3].lower()
    if sample.startswith('new'):
        return 3
    return None


@peek_eat(Boolean_t)
def eat_boolean(src: str) -> int | None:
    """
    eat a boolean, return the number of characters eaten

    booleans are either true or false (case-insensitive)
    """
    sample = src[:5].lower()
    if sample.startswith('true'):
        return 4
    elif sample.startswith('false'):
        return 5

    return None


@peek_eat(Operator_t)
def eat_operator(src: str) -> int | None:
    """
    eat a unary or binary operator, return the number of characters eaten

    picks the longest matching operator

    see `operators` for full list of operators
    """
    for op in operators:
        if src.startswith(op):
            return len(op)
    return None


@peek_eat(ShiftOperator_t, blacklist=[TypeParam_t])
def eat_shift_operator(src: str) -> int | None:
    """
    eat a shift operator, return the number of characters eaten

    picks the longest matching operator.
    Shift operators are not allowed in type parameters, e.g. `>>` is not recognized in `Foo<Bar<Baz<T>>, U>`

    see `shift_operators` for full list of operators
    """
    for op in shift_operators:
        if src.startswith(op):
            return len(op)
    return None


@peek_eat(Comma_t)
def eat_comma(src: str) -> int | None:
    """
    eat a comma, return the number of characters eaten
    """
    return 1 if src.startswith(',') else None


@peek_eat(DotDot_t)
def eat_dotdot(src: str) -> int | None:
    """
    eat a dotdot, return the number of characters eaten
    """
    return 2 if src.startswith('..') else None


@peek_eat(DotDotDot_t)
def eat_dotdotdot(src: str) -> int | None:
    """
    eat a dotdotdot, return the number of characters eaten
    """
    return 3 if src.startswith('...') else None


@peek_eat(Backticks_t)
def eat_cycle(src: str) -> int | None:
    """
    eat one or more cycle operators, return the number of characters eaten
    """
    i = 0
    while i < len(src) and src[i] == '`':
        i += 1
    return i if i > 0 else None

class EatTracker:
    i: int
    tokens: list[Token]


@full_eat()
def eat_type_param(src: str) -> tuple[int, TypeParam_t] | None:
    """
    eat a type parameter, return the number of characters eaten and an instance of the TypeParam token

    type parameters are of the form <...> where ... is a sequence of tokens.
    Type parameters may not start with `<<` or contain any shift operators (`<<`, `<<<`, `>>`, `>>>`)
    Internally encountered shift operators are considered to be delimiters for the type parameter
    """
    if not src.startswith('<') or src.startswith('<<'):
        return None

    i = 1
    body: list[Token] = []

    while i < len(src) and src[i] != '>':

        funcs = get_contextual_eat_funcs(TypeParam_t)
        precedences = get_func_precedences(funcs)
        res = get_best_match(src[i:], funcs, precedences)

        if res is None:
            return None
        n_eaten, token = res

        if isinstance(token, Token):
            # add the already-eaten token to the list of tokens
            body.append(token)
        else:
            # add a new instance of the token to the list of tokens (handling idempotent token cases)
            token_cls = token
            if not body or token_cls not in idempotent_tokens or not isinstance(body[-1], token_cls):
                body.append(token_cls(src[i:i+n_eaten]))

        # increment the index
        i += n_eaten

    if i == len(src):
        return None

    return i + 1, TypeParam_t(body)


@full_eat()
def eat_block(src: str, tracker: EatTracker | None = None) -> tuple[int, Block_t] | None:
    """
    Eat a block, return the number of characters eaten and an instance of the Block token

    blocks are { ... } or ( ... ) and may contain sequences of any other tokens including other blocks

    if return_partial is True, then returns (i, body) in the case where the eat process fails, instead of None
    """

    if not src or src[0] not in pair_opening_delims:
        return None

    # save the opening delimiter
    left = src[0]

    i = 1
    body: list[Token] = []

    if tracker:
        tracker.i = i
        tracker.tokens = body

    while i < len(src) and src[i] not in pair_closing_delims:
        # run all root eat functions
        # if multiple, resolve for best match (TBD... current is longest match + precedence)
        # if no match, return None

        # TODO: probably break this inner part into a function that eats the next token, given a list of eat functions
        # could also think about ways to specify other multi-match resolutions, other than longest match + precedence...
        # run all the eat functions on the current src
        funcs = get_contextual_eat_funcs(Block_t)
        precedences = get_func_precedences(funcs)
        res = get_best_match(src[i:], funcs, precedences)

        # if we didn't match anything, return None
        if res is None:
            return None

        n_eaten, token = res

        if isinstance(token, Token):
            # add the already-eaten token to the list of tokens
            body.append(token)
        else:
            # add a new instance of the token to the list of tokens (handling idempotent token cases)
            token_cls = token
            if not body or token_cls not in idempotent_tokens or not isinstance(body[-1], token_cls):
                body.append(token_cls(src[i:i+n_eaten]))

        # increment the index
        i += n_eaten
        if tracker:
            tracker.i = i

    if i == len(src):
        if tracker:  # only return an exception for the top level block. nested blocks can return None
            raise ValueError("unterminated block")
        return None

    # closing delim (doesn't need to match opening delim)
    right = src[i]
    assert left in pair_opening_delims and right in pair_closing_delims, f"invalid block delimiters: {left} {right}"

    # include closing delimiter in character count
    i += 1
    if tracker:
        tracker.i = i

    return i, Block_t(body, left=left, right=right)


def get_best_match(src: str, eat_funcs: list, precedences: list[int]) -> tuple[int, Type[Token] | Token] | None:
    # TODO: handle selecting between full_eat and peek_eat functions that were successful...
    #      general, just need to clarify the selection order precedence

    # may return none if no match
    # may return (i, token_cls) if peek match
    # may return (i, token) if full match

    matches = [eat_func(src) for eat_func in eat_funcs]

    # find the longest token that matched. if multiple tied for longest, use the one with the highest precedence.
    # raise an error if multiple tokens tied, and they have the same precedence
    def key(x):
        (res, _cls), precedence = x
        if res is None:
            return 0, precedence
        if isinstance(res, tuple):
            res, _token = res  # full_eat functions return a tuple of (num_chars_eaten, token)
        return res, precedence

    matches = [*zip(matches, precedences)]
    best = max(matches, key=key)
    ties = [match for match in matches if key(match) == key(best)]
    if len(ties) > 1:
        raise ValueError(f"multiple tokens matches tied {[match[0][1].__name__ for match in ties]}: {repr(src)}\nPlease disambiguate by providing precedence levels for these tokens.")

    (res, token_cls), _ = best

    # force the type annotations
    res: tuple[int, Token] | int | None
    token_cls: type[Token]

    if res is None:
        return None

    if isinstance(res, int):
        return res, token_cls

    if isinstance(res, tuple):
        return res

    raise ValueError(f"Internal Error: invalid return type from eat function: {res}")


def tokenize(src: str) -> list[Token]:

    # insert src into a block
    src = f'{{\n{src}\n}}'

    # convert string to a coordinate string (for keeping track of row/col numbers)
    src = CoordString(src, anchor=(-1, 0))

    # eat tokens for a block
    tracker = EatTracker()
    try:
        res, _cls = eat_block(src, tracker=tracker)
    except Exception as e:
        raise ValueError(f"failed to tokenize: ```{escape_whitespace(src[tracker.i:])}```.\nCurrent tokens: {tracker.tokens}") from e

    # check if the process failed
    if res is None:
        raise ValueError(f"failed to tokenize: ```{escape_whitespace(src[tracker.i:])}```.\nCurrent tokens: {tracker.tokens}")

    (i, block) = res
    tokens = block.body

    # ensure that all blocks have valid open/close pairs
    validate_block_braces(tokens)

    return tokens


def full_traverse_tokens(tokens: list[Token]) -> Generator[tuple[int, Token, list[Token]], int, None]:
    """
    Walk all tokens recursively, allowing for modification of the tokens list as it is traversed.

    So long as modifications do not occur before the current token, this will safely iterate over all tokens.
    This will not yield string or escape chunks in strings, but will yield interpolated blocks.

    While traversing, the user can overwrite the current index by calling .send(new_index).

    e.g.
    ```python
    gen = full_traverse_tokens(tokens)
    for i, token, stream in gen:
        #do something with current token
        #...

        #maybe overwrite the current index
        if should_overwrite:
            gen.send(new_index)
    ```

    Do not call .send() twice in a row without calling next() in between. This will cause unexpected behavior.

    Args:
        tokens: the list of tokens to traverse

    Yields:
        i: the index of the current token in the current token list
        token: the current token
        stream: the current token list
    """

    i = 0

    while i < len(tokens):
        """
        1. get next token
        2. send current to user
        3. increment index (or overwrite it)
        4. recurse into blocks
        """

        # get the current token
        token = tokens[i]

        # send the current index to the user. possibly receive a new index to continue from
        overwrite_i = yield i, token, tokens

        # only calls to next() will continue execution. calls to .send do nothing wait
        if overwrite_i is not None:
            assert (yield) is None, ".send() may only be called once per iteration."
            i = overwrite_i
        else:
            i += 1

        # for tokens that have defined __iter__ methods, yield their contents
        try:
            for children in token:
                yield from full_traverse_tokens(children)
        except NotImplementedError:
            pass


def traverse_tokens(tokens: list[Token]) -> Generator[Token, None, None]:
    """
    Convenience function over full_traverse_tokens. Walk all tokens recursively

    Does not allow for modification of the tokens list as it is traversed.
    To modify during traversal, use `full_traverse_tokens` instead.

    Args:
        tokens: the list of tokens to traverse

    Yields:
        token: the current token
    """
    for _, token, _ in full_traverse_tokens(tokens):
        yield token


def validate_block_braces(tokens: list[Token]) -> None:
    """
    Checks that all blocks have valid open/close pairs.

    For example, ranges may have differing open/close pairs, e.g. [0..10), (0..10], etc.
    But regular blocks must have matching open/close pairs, e.g. { ... }, ( ... ), [ ... ]
    Performs some validation, without knowing if the block is a range or a block.
    So more validation is needed when the actual block type is known.

    Raises:
        AssertionError: if a block is found with an invalid open/close pair
    """
    for token in traverse_tokens(tokens):
        if isinstance(token, Block_t):
            assert token.left in valid_delim_closers, f'INTERNAL ERROR: left block opening token is not a valid token. Expected one of {[*valid_delim_closers.keys()]}. Got \'{token.left}\''
            assert token.right in valid_delim_closers[token.left], f'ERROR: mismatched opening and closing braces. For opening brace \'{token.left}\', expected one of \'{valid_delim_closers[token.left]}\''


def validate_functions():

    # Validate the @peek_eat function signatures
    peek_eat_functions = get_peek_eat_funcs_with_name()
    for name, wrapper_func in peek_eat_functions:
        func = wrapper_func._eat_func
        signature = inspect.signature(func)
        param_types = [param.annotation for param in signature.parameters.values()
                       if param.default is inspect.Parameter.empty]
        return_type = signature.return_annotation

        # Check if the function has the correct signature
        if len(param_types) != 1 or param_types[0] != str or return_type != int | None:
            pdb.set_trace()
            raise ValueError(f"{func.__name__} has an invalid signature: `{signature}`. Expected `(src: str) -> int | None`")

    # Validate the @full_eat function signatures
    full_eat_functions = get_full_eat_funcs_with_name()
    for name, wrapper_func in full_eat_functions:
        func = wrapper_func._eat_func
        signature = inspect.signature(func)
        param_types = [param.annotation for param in signature.parameters.values()
                       if param.default is inspect.Parameter.empty]
        return_type = signature.return_annotation

        # Check if the function has the correct signature
        error_message = f"{func.__name__} has an invalid signature: `{signature}`. Expected `(src: str) -> tuple[int, Token] | None`"
        if not (isinstance(return_type, UnionType) and len(return_type.__args__) == 2 and type(None) in return_type.__args__):
            raise ValueError(error_message)
        A, B = return_type.__args__
        if B is not type(None):
            B, A = A, B
        if not (isinstance(A, type(tuple)) and len(A.__args__) == 2 and A.__args__[0] is int and issubclass(A.__args__[1], Token)):
            raise ValueError(error_message)
        if len(param_types) != 1 or param_types[0] != str:
            pdb.set_trace()
            raise ValueError(error_message)

    # check for any functions that start with eat_ but are not decorated with @eat
    peek_eat_func_names = {name for name, _ in peek_eat_functions}
    full_eat_func_names = {name for name, _ in full_eat_functions}
    for name, func in globals().items():
        if name.startswith("eat_") and callable(func) and name not in peek_eat_func_names and name not in full_eat_func_names:
            raise ValueError(f"`{name}()` function is not decorated with @peek_eat or @full_eat")


def escape_whitespace(s: str):
    """convert a string to one where all non-space whitespace is escaped"""
    escape_map = {
        '\t': '\\t',
        '\r': '\\r',
        '\f': '\\f',
        '\v': '\\v',
        '\n': '\\n',
    }
    return ''.join(escape_map.get(c, c) for c in s)


def tprint(token: Token, level=0):
    """
    print a token with a certain indentation level.

    If tokens contain nested tokens, they will be printed recursively with an increased indentation level
    """
    print(f'{"    "*level}', end='')
    if isinstance(token, Block_t):
        print(f'<Block {token.left}{token.right}>')
        for t in token.body:
            tprint(t, level=level+1)
    elif isinstance(token, String_t):
        print(f'<String>')
        for t in token.body:
            tprint(t, level=level+1)
    elif isinstance(token, TypeParam_t):
        print(f'<TypeParam>')
        for t in token.body:
            tprint(t, level=level+1)
    else:
        print(token)


def test():
    import sys
    """simple test dewy program"""

    try:
        path = sys.argv[1]
    except IndexError:
        raise ValueError("Usage: `python tokenizer.py path/to/file.dewy>`")

    with open(path) as f:
        src = f.read()

    tokens = tokenize(src)
    print(f'matched tokens:')
    tprint(Block_t(left='{', right='}', body=tokens))
    # for t in tokens:
    #     tprint(t, level=1)


if __name__ == "__main__":
    validate_functions()
    test()
18:T9eb9,from dataclasses import dataclass, field
from functools import cache
from typing import Protocol, TypeVar, Generator, Sequence, Any, cast, overload, Literal as TypingLiteral
from collections import defaultdict
from types import SimpleNamespace

from .syntax import (
    AST,
    Type, TypeParam,
    PointsTo, BidirPointsTo,
    ListOfASTs, Block, Array, Group, Range, ObjectLiteral, Dict, BidirDict, UnpackTarget,
    TypedIdentifier,
    Void, void, Undefined, undefined, untyped,
    String, IString,
    Flowable, Flow, If, Loop, Default,
    Identifier, Express, Declare,
    PrototypeBuiltin, Call, Access, Index,
    Assign,
    Int, Bool,
    Range, IterIn,
    BinOp,
    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,
    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    UnaryPrefixOp, UnaryPostfixOp,
    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv, AtHandle,
    CycleLeft, CycleRight, Suppress,
    BroadcastOp,
    CollectInto, SpreadOutFrom,
    DeclarationType,
)
from .parser import QJux
from .postparse import FunctionLiteral, Signature



import pdb




class Literal(AST):
    value: AST
    def __str__(self) -> str:
        return f'{self.value}'

class TBD(AST):
    """For representing values where the type is underconstrained"""
    def __str__(self) -> str:
        return '<TBD>'

class Fail(AST):
    """For representing values that typechecking fails on"""
    reason: str|None = None
    def __str__(self) -> str:
        return '<Fail>'

TypeExpr = Type | And | Or | Not | Literal | TBD | Fail


# TODO: consider moving metanamespace stuff into e.g. a utils/etc. file
class MetaNamespace(SimpleNamespace):
    """A simple namespace for storing AST meta attributes for use at runtime"""
    def __getattribute__(self, key: str) -> Any | None:
        """Get the attribute associated with the key, or None if it doesn't exist"""
        try:
            return super().__getattribute__(key)
        except AttributeError:
            return None

    def __setattr__(self, key: str, value: Any) -> None:
        """Set the attribute associated with the key"""
        super().__setattr__(key, value)



def ast_to_key(ast: AST) -> str:
    """Convert an AST to a key for use in a dictionary. Note: relies on ASTs class and memory address"""
    assert isinstance(ast, AST), f'ast_to_key only works on ASTs. {ast=}'
    return f'::{ast.__class__.__name__}@{hex(id(ast))}'

class MetaNamespaceDict(defaultdict):
    """A defaultdict that preprocesses AST keys to use the classname + memory address as the key"""
    def __init__(self):
        super().__init__(MetaNamespace)

    # add preprocessing to both __getitem__ and __setitem__ to handle AST keys
    # apparently __setitem__ always calls __getitem__ so we only need to override __getitem__
    def __getitem__(self, item: AST) -> Any | None:
        return super().__getitem__(ast_to_key(item))

T = TypeVar('T')
from typing import Generic
class ASTDict(dict[AST, T], Generic[T]):
    """A dict that preprocesses AST keys to use the classname + memory address as the key"""
    def __getitem__(self, item: AST) -> T:
        return super().__getitem__(ast_to_key(item))
    def __setitem__(self, item: AST, value: T) -> None:
        super().__setitem__(ast_to_key(item), value)
    def __delitem__(self, item: AST) -> None:
        super().__delitem__(ast_to_key(item))
    def __contains__(self, item: AST) -> bool:
        return super().__contains__(ast_to_key(item))
    def get(self, item: AST, default: T | None = None) -> T | None:
        return super().get(ast_to_key(item), default)
    

# Scope class only used during parsing to keep track of callables
@dataclass
class Scope:
    @dataclass
    class _var():
        # name:str #name is stored in the dict key
        decltype: DeclarationType
        type: TypeExpr
        value: AST

        def __repr__(self) -> str:
            """Overwrite __repr__ so that we can format value with more compact __str__ rather than __repr__"""
            return f'Scope._var(decltype={self.decltype!r}, type={self.type!r}, value={self.value})'

    parent: 'Scope | None' = None
    # callables: dict[str, AST | None] = field(default_factory=dict) #TODO: maybe replace str->AST with str->signature (where signature might be constructed based on the func structure)
    vars: 'dict[str, Scope._var]' = field(default_factory=dict)
    meta: dict[AST, MetaNamespace] = field(default_factory=MetaNamespaceDict)

    @overload
    def get(self, name:str, throw:TypingLiteral[True]=True, search_parents:bool=True) -> 'Scope._var': ...
    @overload
    def get(self, name:str, throw:TypingLiteral[False], search_parents:bool=True) -> 'Scope._var|None': ...
    def get(self, name:str, throw:bool=True, search_parents:bool=True) -> 'Scope._var|None':
        for s in self:
            if name in s.vars:
                return s.vars[name]
            if not search_parents:
                break

        if throw:
            raise KeyError(f'variable "{name}" not found in scope')
        return None

    def assign(self, name:str, value:AST):
        assert len(DeclarationType.__members__) == 2, f'expected only 2 declaration types: let, const. found {DeclarationType.__members__}'

        # cannot assign with void
        assert value is not void, f"Attempted to assign void value to variable: {name=}. {value=}"

        # var is already declared in current scope
        if name in self.vars:
            var = self.vars[name]
            assert var.decltype != DeclarationType.CONST or var.value is void, f"Attempted to assign to constant variable: {name=}. {var=}. {value=}"
            var.value = value
            return

        var = self.get(name, throw=False)

        # var is not declared in any scope
        if var is None:
            self.let(name, value, untyped)
            return

        # var was declared in a parent scope
        if var.decltype == DeclarationType.LET:
            var.value = value
            return

        raise ValueError(f'Attempted to assign to constant variable: {name=}{var=}. {value=}')

    def declare(self, name:str, value:AST, type:Type, decltype:DeclarationType):
        if name in self.vars:
            var = self.vars[name]
            assert var.decltype != DeclarationType.CONST, f"Attempted to {decltype.name.lower()} declare a value that is const in this current scope. {name=}{var=}. {value=}"

        self.vars[name] = Scope._var(decltype, type, value)

    def let(self, name:str, value:AST, type:Type):
        self.declare(name, value, type, DeclarationType.LET)

    def const(self, name:str, value:AST, type:Type):
        self.declare(name, value, type, DeclarationType.CONST)

    def __iter__(self) -> Generator['Scope', None, None]:
        """return an iterator that walks up each successive parent scope. Starts with self"""
        s = self
        while s is not None:
            yield s
            s = s.parent

    def __repr__(self):
        return f'<Scope@{hex(id(self))}>'



class TypeofFunc(Protocol):
    def __call__(self, ast: AST, scope: Scope, params:bool=False) -> TypeExpr:
        """
        Return the type of the given AST node.

        Args:
            ast (AST): the AST node to determine the type of
            scope (Scope): the scope in which the AST node is being evaluated
            params (bool, optional): indicates if full type checking including parameterization should be done. Defaults to False.

        Returns:
            Type: the type of the AST node
        """

class TypeofCallFunc(Protocol):
    def __call__(self, call: AST, args: AST|None, scope: Scope, params:bool=False):
        """
        Return the type that results from calling the given AST node.

        Args:
            call (AST): the AST node to determine the type of
            args (AST|None): the arguments to the call. None means call with no args
            scope (Scope): the scope in which the AST node is being evaluated
            params (bool, optional): indicates if full type checking including parameterization should be done. Defaults to False.
        
        Returns:
            Type: the type of the AST node
        """

def identity(ast: AST, scope: Scope, params:bool=False) -> Type:
    return Type(type(ast))

def short_circuit(ret: type[AST], param_fallback:TypeofFunc|None=None) -> TypeofFunc:
    def inner(ast: AST, scope: Scope, params:bool=False) -> Type:
        if params and param_fallback is not None:
            return param_fallback(ast, scope, params)
        return Type(ret)
    return inner

def cannot_typeof(ast: AST, scope: Scope, params:bool=False):
    raise ValueError(f'INTERNAL ERROR: determining the type of `({type(ast)}) {ast}` is not possible')


_external_typeof_fn_map: dict[type[AST], TypeofFunc] = {}
# TODO: would be cool if this could be a decorator (that figured out the type from the first arg)
def register_typeof(cls: type[AST], fn: TypeofFunc):
    _external_typeof_fn_map[cls] = fn

_external_typeof_call_fn_map: dict[type[AST], TypeofCallFunc] = {}
# TODO: would be cool if this could be a decorator (that figured out the type from the first arg)
def register_typeof_call(cls: type[AST], fn: TypeofCallFunc):
    _external_typeof_call_fn_map[cls] = fn

@cache
def get_typeof_fn_map() -> dict[type[AST], TypeofFunc]:
    return {
        Declare: short_circuit(Void),
        Call: typeof_call,
        Block: typeof_block,
        Group: typeof_group,
        Array: typeof_array,
        # Dict: typeof_dict,
        # PointsTo: typeof_points_to,
        # BidirDict: typeof_bidir_dict,
        # BidirPointsTo: typeof_bidir_points_to,
        # ObjectLiteral: typeof_object_literal,
        # # Object: no_op,
        Access: typeof_access,
        Assign: short_circuit(Void),
        # IterIn: typeof_iter_in,
        # FunctionLiteral: typeof_function_literal,
        # # Closure: typeof_closure,
        # # Builtin: typeof_builtin,
        String: identity,
        IString: short_circuit(String),
        Identifier: typeof_identifier,
        Express: typeof_express,
        Int: identity,
        # # Float: no_op,
        Bool: identity,
        # Range: no_op,
        CycleLeft: lambda ast, scope, params: typeof(ast.operand, scope, params),
        CycleRight: lambda ast, scope, params: typeof(ast.operand, scope, params),
        # Flow: typeof_flow,
        # Default: typeof_default,
        # If: typeof_if,
        # Loop: typeof_loop,
        # UnaryPos: typeof_unary_dispatch,
        # UnaryNeg: typeof_unary_dispatch,
        # UnaryMul: typeof_unary_dispatch,
        # UnaryDiv: typeof_unary_dispatch,
        # Not: typeof_unary_dispatch,
        Greater: short_circuit(Bool),
        GreaterEqual: short_circuit(Bool),
        Less: short_circuit(Bool),
        LessEqual: short_circuit(Bool),
        Equal: short_circuit(Bool),
        And: typeof_logical_binop,
        Or: typeof_logical_binop,
        Xor: typeof_logical_binop,
        Nand: typeof_logical_binop,
        Nor: typeof_logical_binop,
        Xnor: typeof_logical_binop,
        Not: lambda ast, scope, params: typeof(ast.operand, scope, params),
        # Add: typeof_binary_dispatch,
        # Sub: typeof_binary_dispatch,
        Mul: typeof_binary_dispatch,
        # Div: typeof_binary_dispatch,
        # Mod: typeof_binary_dispatch,
        # Pow: typeof_binary_dispatch,
        AtHandle: typeof_at_handle,
        Undefined: identity,
        Void: identity,
        #TODO: other AST types here
    } | _external_typeof_fn_map



def typeof(ast: AST, scope: Scope, params:bool=False) -> TypeExpr:
    """Basically like evaluate, but just returns the type information. Doesn't actually evaluate the AST"""
    typeof_fn_map = get_typeof_fn_map()

    ast_type = type(ast)
    if ast_type in typeof_fn_map:
        return typeof_fn_map[ast_type](ast, scope, params)

    pdb.set_trace()
    raise NotImplementedError(f'typeof not implemented for {ast_type}')


# TODO: split resolution and typechecking into separate functions
#       resolution first, then typechecking since typechecking is hard when there are QASTs
def top_level_typecheck_and_resolve(ast: AST, scope: Scope) -> tuple[AST, ASTDict[Scope]]:
    group = Group([ast])
    gen = group.__iter_asts__(visit_replacement=True)
    scope_map: ASTDict[Scope] = ASTDict()
    scope_map[group] = scope
    inner_typecheck_and_resolve(group, gen, scope_map)

    return group.items[0], scope_map


def inner_typecheck_and_resolve(parent: AST, gen: Generator[AST, AST, None], scope_map: ASTDict[Scope]):

    for ast in gen:
        # if the specific AST has a scope, use it, otherwise use the parent's scope
        # then ensure all ASTs processed have a scope
        scope = scope_map.get(ast) or scope_map[parent]
        scope_map[ast] = scope 

        match ast:
            case Void(): ... 
            case Identifier(): ...
            case Express(): ...
            case Signature(): ...
            case Declare(decltype=decltype, target=target):
                # TODO: check that the type is valid if present...
                # declare the variable in the scope...
                match target:
                    case Identifier(name=name):
                        scope.declare(name, void, untyped, decltype)
                    case TypedIdentifier(name=name, type=type):
                        scope.declare(name, void, type, decltype)
                    # case ReturnTyped(left=left, right=right): ... # TODO
                    case UnpackTarget(target=target): pdb.set_trace(); ...
                    case Assign(left=left):
                        if isinstance(left, Identifier):
                            scope.declare(left.name, void, untyped, decltype) # note the assignment is handled by the child. We just declare the variable here
                        elif isinstance(left, TypedIdentifier):
                            scope.declare(left.id.name, void, left.type, decltype)
                        else:
                            pdb.set_trace()
                            raise NotImplementedError('Declare not implemented for target Assign with non-Identifier left side')
                    case _:
                        raise TypeError(f'INTERNAL ERROR: Declare received unexpected typeof target: {target}.')

            case Assign(left=left, right=right):
                if isinstance(left, Identifier):
                    scope.assign(left.name, right)
                else:
                    pdb.set_trace()
                    raise NotImplementedError('Assign not implemented for non-Identifier left side')
                # # TODO: handling  other assignment targets on the left...

            case Group(items=items): ...

            case Block(items=items):
                # make a new layer to the scope, and assign it to all the children
                inner_scope = Scope(scope)
                for item in items:
                    scope_map[item] = inner_scope

            case FunctionLiteral(): ...

            case QJux(call=call, mul=mul, index=index):
                # print(f'visiting QJux {ast}')
                valid_branches = []
                if call is not None and is_call_valid(call, scope):
                    valid_branches.append(call)
                if index is not None and is_index_valid(index, scope):
                    valid_branches.append(index)
                if mul is not None and is_multiply_valid(mul, scope):
                    valid_branches.append(mul)
                if len(valid_branches) == 0:
                    raise ValueError(f'ERROR: no valid branches for QJux. must have at exactly one. {ast=}')
                if len(valid_branches) > 1:
                    raise ValueError(f'ERROR: multiple valid branches for QJux. must have exactly one. {ast=}')
                
                # overwrite QJux with the valid branch. Then continue so we process the replacement before going to the children
                valid_branch, = valid_branches                
                gen.send(valid_branch)
                continue 

                # pdb.set_trace()
                # ...
                # return valid_branches[0], scope
            case Call(f=f, args=args):
                # TODO: check if the signature of f agrees with the args...
                # print(f'skipping function typechecking...')
                ...
            
            case Int() | String(): ...

            case And() | Or() | Xor() | Nand() | Nor() | Xnor() | Not():
                # TODO: check that the left and right are either int or bool
                ...
            
            case _:
                pdb.set_trace()
                ...
                raise ValueError(f'INTERNAL ERROR: typecheck_and_resolve not implemented for {type(ast)}')

        child_gen = ast.__iter_asts__(visit_replacement=True)
        inner_typecheck_and_resolve(ast, child_gen, scope_map)



# def typecheck_and_resolve(ast: AST, scope: Scope) -> AST | None:
#     match ast:
#         # TODO: all cases here
#         case _: ...

# def top_level_typecheck_and_resolve(root: AST, scope: Scope) -> tuple[AST, Scope]:
#     ast = Group([ast])
#     gen = ast.__iter_asts__()
#     return typecheck_and_resolve_v2(ast, gen, scope)

# def typecheck_and_resolve_v2(ast: AST, gen: Generator[AST, AST, None], scope: Scope) -> tuple[AST, Scope]:
#     match ast:
#         case Void(): ... 
#             # return ast, scope
#         case Identifier(): ... # TODO...
#         case Express(): ...
#         case Signature(): ...
#         case Declare():
#             # TODO: check that the type is valid if present...
#             ...
#         case Assign(left=left, right=right):
#             if isinstance(left, Identifier):
#                 scope.assign(left.name, right)
#             else:
#                 pdb.set_trace()
#                 raise NotImplementedError('Assign not implemented for non-Identifier left side')
#             # TODO: put the identifier into the scope with the type of the right...
#             ...

#             # # TODO: handling identifier or other assignment targets on the left...
#             # typecheck_and_resolve(ast.right, scope)
#             # return ast, scope
#         case Group(items=items): ...
#             # pdb.set_trace()
#             # ...
#             # results = []
#             # for item in items:
#             #     result = typecheck_and_resolve(item, scope)
#             #     results.append(result)
#             # pdb.set_trace()
#             # ...
#         case Block(items=items):
#             # TODO: make a new layer to the scope
#             ...
#         case FunctionLiteral(): ...
#             # typecheck_and_resolve(ast.body, scope)
#             # return ast, scope
#         case QJux(call=call, mul=mul, index=index):
#             # print(f'visiting QJux {ast}')
#             valid_branches = []
#             if call is not None and is_call_valid(call, scope):
#                 valid_branches.append(call)
#             if index is not None and is_index_valid(index, scope):
#                 valid_branches.append(index)
#             if mul is not None and is_multiply_valid(mul, scope):
#                 valid_branches.append(mul)
#             if len(valid_branches) == 0:
#                 raise ValueError(f'ERROR: no valid branches for QJux. must have at exactly one. {ast=}')
#             if len(valid_branches) > 1:
#                 raise ValueError(f'ERROR: multiple valid branches for QJux. must have exactly one. {ast=}')
            
#             # overwrite QJux with the valid branch
#             valid_branch, = valid_branches
#             # typecheck_and_resolve(valid_branch, scope) #manually typechecking here since current __full_traversal_iter doesn't recurse into newly sent children... consider adjusting
#             gen.send(valid_branch)

#             # pdb.set_trace()
#             # ...
#             # return valid_branches[0], scope
#         case Call(f=f, args=args):
#             # TODO: check if the signature of f agrees with the args...
#             # print(f'skipping function typechecking...')
#             ...
        
#         case Int(): ...
        
#         case _:
#             pdb.set_trace()
#             ...
#             raise ValueError(f'INTERNAL ERROR: typecheck_and_resolve not implemented for {type(ast)}')


#     for child in (gen:=ast.__iter_asts__()):
#         typecheck_and_resolve(child, gen, scope)
#     pdb.set_trace()

# # def promote/coerce() -> Type: #or make_compatible
# # promotion_table = {...}
# # type_tree = {...}
# # TODO: perhaps ctx should be a dataclass that contains scope but also any other context needed? TBD...
# def typecheck_and_resolve(ast: AST, scope: Scope, ctx=None) -> tuple[AST, Scope]:
#     """Typecheck the given AST, and resolve any quantum ASTs to concrete selections based on the types"""
#     # ctx_stack: list[Literal['assigning', 'etc']] = []
#     # scope_stack = [scope]
#     # def pre_effect():
#     #     nonlocal scope, scope_stack
#     #     scope_stack.append(scope)
#     # def post_effect():
#     #     nonlocal scope, scope_stack
#     #     scope = scope_stack.pop()

#     ast = Group([ast])
#     for i in (gen := ast.__full_traversal_iter__(visit_replacement=True)):#pre_effect=pre_effect, post_effect=post_effect)):

#         print(f'visiting {i}')
#         match i:
#             case Void(): ... 
#                 # return ast, scope
#             case Identifier(): ... # TODO...
#             case Express(): ...
#             case Signature(): ...
#             case Declare():
#                 # TODO: check that the type is valid if present...
#                 ...
#             case Assign(left=left, right=right):
#                 if isinstance(left, Identifier):
#                     scope.assign(left.name, right)
#                 else:
#                     pdb.set_trace()
#                     raise NotImplementedError('Assign not implemented for non-Identifier left side')
#                 # TODO: put the identifier into the scope with the type of the right...
#                 ...

#                 # # TODO: handling identifier or other assignment targets on the left...
#                 # typecheck_and_resolve(ast.right, scope)
#                 # return ast, scope
#             case Group(items=items): ...
#                 # pdb.set_trace()
#                 # ...
#                 # results = []
#                 # for item in items:
#                 #     result = typecheck_and_resolve(item, scope)
#                 #     results.append(result)
#                 # pdb.set_trace()
#                 # ...
#             case Block(items=items):
#                 # TODO: make a new layer to the scope
#                 ...
#             case FunctionLiteral(): ...
#                 # typecheck_and_resolve(ast.body, scope)
#                 # return ast, scope
#             case QJux(call=call, mul=mul, index=index):
#                 # print(f'visiting QJux {ast}')
#                 valid_branches = []
#                 if call is not None and is_call_valid(call, scope):
#                     valid_branches.append(call)
#                 if index is not None and is_index_valid(index, scope):
#                     valid_branches.append(index)
#                 if mul is not None and is_multiply_valid(mul, scope):
#                     valid_branches.append(mul)
#                 if len(valid_branches) == 0:
#                     raise ValueError(f'ERROR: no valid branches for QJux. must have at exactly one. {ast=}')
#                 if len(valid_branches) > 1:
#                     raise ValueError(f'ERROR: multiple valid branches for QJux. must have exactly one. {ast=}')
                
#                 # overwrite QJux with the valid branch
#                 valid_branch, = valid_branches
#                 # typecheck_and_resolve(valid_branch, scope) #manually typechecking here since current __full_traversal_iter doesn't recurse into newly sent children... consider adjusting
#                 gen.send(valid_branch)

#                 # pdb.set_trace()
#                 # ...
#                 # return valid_branches[0], scope
#             case Call(f=f, args=args):
#                 # TODO: check if the signature of f agrees with the args...
#                 # print(f'skipping function typechecking...')
#                 ...
            
#             case Int(): ...
            
#             case _:
#                 pdb.set_trace()
#                 ...
#                 raise ValueError(f'INTERNAL ERROR: typecheck_and_resolve not implemented for {type(ast)}')

        

#     pdb.set_trace()
#     return ast.items[0], scope
#     ...
# #     """Check if the given AST is well-formed from a type perspective"""
# #     match ast:
# #         case Call(): return typecheck_call(ast, scope)
# #         case Index(): return typecheck_index(ast, scope)
# #         case Mul(): return typecheck_binary_dispatch(ast, scope)
# #         case _: raise NotImplementedError(f'typecheck not implemented for {type(ast)}')

# # def infer_types(ast: AST, scope: Scope) -> AST:

def is_call_valid(ast: Call, scope: Scope) -> bool:
    f_type = typeof(ast.f, scope)
    if issubclass(f_type.t, (FunctionLiteral, CallableBase)):
        # TODO: check if args match the function signature.
        #       for now, just skip
        return True
    return False


def is_index_valid(ast: Index, scope: Scope) -> bool:
    pdb.set_trace()
    ...

def is_multiply_valid(ast: Mul, scope: Scope) -> bool:
    # early short circuit for common case
    left_type = typeof(ast.left, scope)
    if issubclass(left_type.t, (FunctionLiteral, CallableBase)):
        return False
    
    # TBD this is technically true, but it implies you can call functions on the right side when you can't
    # instead at this point it probably should just be a general case...
    right_type = typeof(ast.right, scope)
    if issubclass(right_type.t, (FunctionLiteral, CallableBase)):
        return False


    # TODO: probably need some sort of proper table for keeping track of multipliable types
    pdb.set_trace()
    ...



def typeof_identifier(ast: Identifier, scope: Scope, params:bool=False) -> TypeExpr:
    var = scope.get(ast.name)
    if var is None:
        raise KeyError(f'variable "{ast.name}" not found in scope')
    if var.type is not untyped:
        return var.type

    return typeof(var.value, scope, params)



# abstract base type to register new callable types
class CallableBase(AST): ...
#TODO: this is used by the interpreter. remove when compiler type checking is complete and can replace interpreter stuff
_callable_types = (PrototypeBuiltin, FunctionLiteral, CallableBase)  
# def register_callable(cls: type[AST]):
#     _callable_types.append(cls)

def typeof_call(ast: Call, scope: Scope, params:bool=False) -> TypeExpr:
    f_type = type(ast.f)
    if f_type in _external_typeof_call_fn_map:
        return _external_typeof_call_fn_map[f_type](ast.f, ast.args, scope, params)
    
    if not isinstance(ast.f, Identifier):
        raise NotImplementedError(f"calling non-identifier functions is not implemented yet. {ast.f} called with args {ast.args}")
        # get the return type of the function
    
    f_var = scope.get(ast.f.name)
    pdb.set_trace()
    # match f_var.value:
    #     case FunctionLiteral(return_type=dewy_return_type):# | Builtin(return_type=dewy_return_type) | Closure(FunctionLiteral(return_type=dewy_return_type)):
    #         # dewy_return_type = return_type
    #         if not isinstance(dewy_return_type, Type):
    #             dewy_return_type = typeof(dewy_return_type, scope)
    #         ret_type = dewy_qbe_type_map[dewy_return_type]
    #     case _:
    #         raise ValueError(f'Unrecognized AST type to call: {f_var.value!r}')
    

# def simple_typecheck_resolve_ast(ast: AST, scope: Scope) -> AST:
#     """Resolve the AST to a type. This is a simple version that doesn't do any complex type checking"""
#     if isinstance(ast, Identifier):
#         var = scope.get(ast.name)
#         return var.value
#     if isinstance(ast, AtHandle):
#         return ast.operand
#     if isinstance(ast, Group):
#         pdb.set_trace()
#         ...
#     if isinstance(ast, Access):
#         pdb.set_trace()
#         ...

#     # no resolving possible
#     return ast

def typecheck_call(ast: Call, scope: Scope) -> bool:
    #For now, just the simplest check. is f callable. ignore rest of type checking
    f_type = typeof(ast.f, scope)
    if not isinstance(f_type, Type):
        pdb.set_trace()
        ...
        #TBD how to handle the different TypeExpr's

    if isinstance(f_type, Type) and issubclass(f_type.t, _callable_types):
        return True

    return False
    pdb.set_trace()
    # f = ast.f
    # f = simple_typecheck_resolve_ast(f, scope) # resolve to a value
    # if isinstance(f, Identifier):
    #     var = scope.get(f.name)
    #     f = var.value
    # if isinstance(f, AtHandle):
    #     f = f.operand

    if isinstance(f, tuple(_callable_types)):
        #TODO: longer term, want to check that the expected args match the given args
        return True

    # if isinstance(f, Group):
    #     pdb.set_trace()
    #     # get the type of the group items... handling void, and if multiple, then answer is False...
    #     ...
    # if isinstance(f, Access):
    #     pdb.set_trace()
    #     ...
    #TODO: replace all this with full typechecking...
    # t = typeof(f, scope)
    # if t in _callable_types: return True
    # return False
    return False


# Abstract base types to register new indexable/indexer types
class IndexableBase(AST): ...
class IndexerBase(AST): ...

_indexable_types = (Array, Range, IndexableBase)
_indexer_types = (Array, Range, IndexerBase)
# def register_indexable(cls: type[AST]):
#     _indexable_types.append(cls)
# def register_indexer(cls: type[AST]):
#     _indexer_types.append(cls)

def typeof_index(ast: Index, scope: Scope, params:bool=False) -> TypeExpr:
    pdb.set_trace()
    ...

def typecheck_index(ast: Index, scope: Scope) -> bool:
    # left = simple_typecheck_resolve_ast(ast.left, scope)
    # right = simple_typecheck_resolve_ast(ast.right, scope)
    # if isinstance(left, _indexable_types) and isinstance(right, _indexer_types):
    #     return True


    #for now super simple checks on left and right
    left_type = typeof(ast.left, scope)
    right_type = typeof(ast.right, scope)
    if not isinstance(left_type, Type) or not isinstance(right_type, Type):
        pdb.set_trace()
        #TODO: more complex cases involving type expressions...
        raise NotImplementedError('typecheck_index not implemented for non-Type left side')

    if issubclass(left_type.t, _indexable_types) and issubclass(right_type.t, _indexer_types):
        return True

    return False



# abstract base type to register new multipliable types
class MultipliableBase(AST): ...
_multipliable_types = (Int, Array, Range, MultipliableBase) #TODO: add more types
# def register_multipliable(cls: type[AST]):
#     _multipliable_types.append(cls)
def typecheck_multiply(ast: Mul, scope: Scope) -> bool:
    # left = simple_typecheck_resolve_ast(ast.left, scope)
    # right = simple_typecheck_resolve_ast(ast.right, scope)
    # if isinstance(left, _multipliable_types) and isinstance(right, _multipliable_types):
    #     return True

    #TODO: full type checking to check if values are multipliable
    # pdb.set_trace()
    left_type = typeof(ast.left, scope)
    right_type = typeof(ast.right, scope)
    if not isinstance(left_type, Type) or not isinstance(right_type, Type):
        pdb.set_trace()
        raise NotImplementedError('typecheck_multiply not implemented for non-Type left side')

    if issubclass(left_type.t, _multipliable_types) and issubclass(right_type.t, _multipliable_types):
        return True

    return False



def typeof_group(ast: Group, scope: Scope, params:bool=False) -> TypeExpr:
    expressed: list[TypeExpr] = []
    for expr in ast.items:
        res = typeof(expr, scope, params)
        if res is not void:
            expressed.append(res)
    if len(expressed) == 0:
        return Type(Void)
    if len(expressed) == 1:
        return expressed[0]
    return Type(Group, parameters=TypeParam(expressed))


def typeof_block(ast: Block, scope: Scope, params:bool=False) -> TypeExpr:
    scope = Scope(scope)
    return typeof_group(Group(ast.items), scope, params)






def typeof_at_handle(ast: AtHandle, scope: Scope, params:bool=False) -> TypeExpr:
    if not isinstance(ast.operand, Identifier):
        raise NotImplementedError('typeof_at_handle only implemented for Identifiers')
    return typeof_identifier(ast.operand, scope, params)


def typeof_express(ast: Express, scope: Scope, params:bool=False) -> TypeExpr:
    var = scope.get(ast.id.name)

    # if we were told what the type is, return that (as it should be the main source of truth)
    if isinstance(var.type, Type) and var.type is not untyped:
        return var.type

    return typeof(var.value, scope, params)

logical_binop_typemap = {
    (Int, Int): Int,
    (Bool, Bool): Bool,
}
def typeof_logical_binop(ast: And|Or|Xor|Nand|Nor|Xnor, scope: Scope, params:bool=False) -> TypeExpr:
    """and or xor nand nor xnor"""
    left_type = typeof(ast.left, scope)
    right_type = typeof(ast.right, scope)

    if not isinstance(left_type, Type) or not isinstance(right_type, Type):
        raise NotImplementedError('typeof_logical_binop not implemented for non-Type left side')

    key = (left_type.t, right_type.t)
    if not key in logical_binop_typemap:
        raise NotImplementedError(f'logical_binop_typemap not implemented for {key}. {ast=}')
    
    return Type(logical_binop_typemap[key])

def typeof_binary_dispatch(ast: BinOp, scope: Scope, params:bool=False) -> TypeExpr:
    pdb.set_trace()
    raise NotImplementedError('typeof_binary_dispatch not implemented')

def typecheck_binary_dispatch(ast: BinOp, scope: Scope) -> bool:
    op = type(ast)
    if op not in binary_dispatch_table:
        return False
    left_type = typeof(ast.left, scope)
    right_type = typeof(ast.right, scope)
    if not isinstance(left_type, Type) or not isinstance(right_type, Type):
        pdb.set_trace()
        raise NotImplementedError('typecheck_binary_dispatch not implemented for non-Type left side')

    if (left_type.t, right_type.t) not in binary_dispatch_table[op]:
        return False

    return True

def typeof_array(ast: Array, scope: Scope, params:bool=False) -> TypeExpr:
    if not params:
        return Type(Array)
    pdb.set_trace()
    ...
    raise NotImplementedError('typeof_array not implemented when params=True')






class ObjectBase(AST): ...

def typeof_access(ast: Access, scope: Scope, params:bool=False) -> TypeExpr:
    left = typeof(ast.left, scope, params)

    # happy path: left was an object
    if isinstance(left, Type) and issubclass(left.t, ObjectBase) and left.parameters is not None:
        parameters = left.parameters
        assert len(parameters.items) == 1, f'expected only one parameter for object access. {parameters=}'
        scope, = parameters.items
        assert isinstance(scope, Scope), f'expected parameter to be a scope. {parameters.items[0]=}'
        if isinstance(ast.right, Identifier):
            handle, id = False, ast.right
        elif isinstance(ast.right, AtHandle):
            handle, id = True, ast.right.operand
            assert isinstance(id, Identifier), f'expected id to be an Identifier. {id=}. Other types not yet supported'
        elif isinstance(ast.right, Access):
            raise ValueError('Right hand side should not be access. Access should be left associative')
        else:
            raise NotImplementedError(f'Access right-hand-side not implemented for {type(ast.right)}')

        if handle:
            return typeof_identifier(id, scope, params)
        return typeof_express(Express(id), scope, params)



    pdb.set_trace()
    raise NotImplementedError(f'typeof_access not implemented for {type(ast)}')





# TODO: for now, just a super simple dispatch table
binary_dispatch_table = {
    Mul: {
        (Int, Int): Int,
        # (Int, Float): Float,
        # (Float, Float): Float,
    }
}




# UnaryDispatchKey =  tuple[type[UnaryPrefixOp]|type[UnaryPostfixOp], type[SimpleValue[T]]]
# unary_dispatch_table: dict[UnaryDispatchKey[T], TypingCallable[[T], AST]] = {
#     (Not, Int): lambda l: Int(~l),
#     (Not, Bool): lambda l: Bool(not l),
#     (UnaryPos, Int): lambda l: Int(l),
#     (UnaryNeg, Int): lambda l: Int(-l),
#     (UnaryMul, Int): lambda l: Int(l),
#     (UnaryDiv, Int): lambda l: Int(1/l),
# }

# BinaryDispatchKey = tuple[type[BinOp], type[SimpleValue[T]], type[SimpleValue[U]]]
# # These are all symmetric meaning you can swap the operand types and the same function will be used (but the arguments should not be swapped)
# binary_dispatch_table: dict[BinaryDispatchKey[T, U], TypingCallable[[T, U], AST]|TypingCallable[[U, T], AST]] = {
#     (And, Int, Int): lambda l, r: Int(l & r),
#     (And, Bool, Bool): lambda l, r: Bool(l and r),
#     (Or, Int, Int): lambda l, r: Int(l | r),
#     (Or, Bool, Bool): lambda l, r: Bool(l or r),
#     (Xor, Int, Int): lambda l, r: Int(l ^ r),
#     (Xor, Bool, Bool): lambda l, r: Bool(l != r),
#     (Nand, Int, Int): lambda l, r: Int(~(l & r)),
#     (Nand, Bool, Bool): lambda l, r: Bool(not (l and r)),
#     (Nor, Int, Int): lambda l, r: Int(~(l | r)),
#     (Nor, Bool, Bool): lambda l, r: Bool(not (l or r)),
#     (Add, Int, Int): lambda l, r: Int(l + r),
#     (Add, Int, Float): lambda l, r: Float(l + r),
#     (Add, Float, Float): lambda l, r: Float(l + r),
#     (Sub, Int, Int): lambda l, r: Int(l - r),
#     (Sub, Int, Float): lambda l, r: Float(l - r),
#     (Sub, Float, Float): lambda l, r: Float(l - r),
#     (Mul, Int, Int): lambda l, r: Int(l * r),
#     (Mul, Int, Float): lambda l, r: Float(l * r),
#     (Mul, Float, Float): lambda l, r: Float(l * r),
#     (Div, Int, Int): int_int_div,
#     (Div, Int, Float): float_float_div,
#     (Div, Float, Float): float_float_div,
#     (Mod, Int, Int): lambda l, r: Int(l % r),
#     (Mod, Int, Float): lambda l, r: Float(l % r),
#     (Mod, Float, Float): lambda l, r: Float(l % r),
#     (Pow, Int, Int): lambda l, r: Int(l ** r),
#     (Pow, Int, Float): lambda l, r: Float(l ** r),
#     (Pow, Float, Float): lambda l, r: Float(l ** r),
#     (Less, Int, Int): lambda l, r: Bool(l < r),
#     (Less, Int, Float): lambda l, r: Bool(l < r),
#     (Less, Float, Float): lambda l, r: Bool(l < r),
#     (LessEqual, Int, Int): lambda l, r: Bool(l <= r),
#     (LessEqual, Int, Float): lambda l, r: Bool(l <= r),
#     (LessEqual, Float, Float): lambda l, r: Bool(l <= r),
#     (Greater, Int, Int): lambda l, r: Bool(l > r),
#     (Greater, Int, Float): lambda l, r: Bool(l > r),
#     (Greater, Float, Float): lambda l, r: Bool(l > r),
#     (GreaterEqual, Int, Int): lambda l, r: Bool(l >= r),
#     (GreaterEqual, Int, Float): lambda l, r: Bool(l >= r),
#     (GreaterEqual, Float, Float): lambda l, r: Bool(l >= r),
#     (Equal, Int, Int): lambda l, r: Bool(l == r),
#     (Equal, Float, Float): lambda l, r: Bool(l == r),
#     (Equal, Bool, Bool): lambda l, r: Bool(l == r),
#     (Equal, String, String): lambda l, r: Bool(l == r),
#     # (NotEqual, Int, Int): lambda l, r: Bool(l != r),

# }

# unsymmetric_binary_dispatch_table: dict[BinaryDispatchKey[T, U], ] = {
#     #e.g. (Mul, String, Int): lambda l, r: String(l * r), # if we follow python's behavior
# }

# # dispatch table for more complicated values that can't be automatically unpacked by the dispatch table
# # TODO: actually ideally just have a single table
# CustomBinaryDispatchKey = tuple[type[BinOp], type[T], type[U]]
# custom_binary_dispatch_table: dict[CustomBinaryDispatchKey[T, U], TypingCallable[[T, U], AST]] = {
#     (Add, Array, Array): lambda l, r: Array(l.items + r.items), #TODO: this will be removed in favor of spread. array add will probably be vector add
#     # (BroadcastOp, Array, Array): broadcast_array_op,
#     # (BroadcastOp, NpArray, NpArray): broadcast_array_op,
#     # (BroadcastOp, Int, Array): broadcast_array_op,
#     # (BroadcastOp, Float, Array): broadcast_array_op,

# }
19:T2610,from typing import TypeVar, Generic, Callable
from dataclasses import dataclass
from typing import Protocol, TypeVar
from pathlib import Path
from argparse import ArgumentParser, Namespace


import pdb

@dataclass
class BaseOptions:
    tokens: bool
    verbose: bool
    #TODO: other command line options

T = TypeVar('T', bound=BaseOptions)
class BackendExecutor(Protocol[T]):
    def __call__(self, path: Path, args: list[str], options: T) -> None: ...
class ArgumentParserMaker(Protocol):
    def __call__(self, parent:ArgumentParser) -> None: ...
class OptionsMaker(Protocol[T]):
    def __call__(self, args: Namespace) -> T: ...

@dataclass
class Backend(Generic[T]):
    name: str
    exec: BackendExecutor[T]
    make_argparser: ArgumentParserMaker
    make_options: OptionsMaker[T]


def try_install_rich():
    try:
        from rich import traceback
        traceback.install(show_locals=True)
    except:
        print('rich unavailable for import. using built-in printing')




def wrap_coords(method: Callable):
    def wrapped_method(self, *args, **kwargs):
        result = method(self, *args, **kwargs)
        if isinstance(result, str) and len(result) == len(self):
            custom_str = CoordString(result)
            custom_str.row_col_map = self.row_col_map
            return custom_str
        else:
            raise ValueError("coord_string_method must return a string of the same length as the original string")
        return result

    return wrapped_method


def fail_coords(method: Callable):
    def wrapped_method(self, *args, **kwargs):
        raise ValueError(f"coord_string_method {method} cannot be called on a CoordString, as it will not return a CoordString")
    return wrapped_method


class CoordString(str):
    """
    Drop-in replacement for str that keeps track of the coordinates of each character in the string

    Identical to normal strings, but attaches the `row_col(i:int) -> tuple[int, int]` method
    which returns the (row, column) of the character at index i

    Args:
        anchor (tuple[int,int], optional): The row and column of the top left of the string. Defaults to (0, 0).
    """
    def __new__(cls, *args, anchor: tuple[int, int] = (0, 0), **kwargs):
        self = super().__new__(cls, *args, **kwargs)
        row, col = anchor
        self.row_col_map = self._generate_row_col_map(row, col)

        return self

    # TODO: make init so that class recognized .row_col_map as property on instances
    # def __init__(self, s:str, row_col_map:list[tuple[int,int]]):

    def _generate_row_col_map(self, row=0, col=0) -> list[tuple[int, int]]:
        row_col_map = []
        for c in self:
            if c == '\n':
                row_col_map.append((row, col))
                row += 1
                col = 0
            else:
                row_col_map.append((row, col))
                col += 1
        return row_col_map

    def __getitem__(self, key):
        if isinstance(key, slice):
            sliced_str = super().__getitem__(key)
            sliced_row_col_map = self.row_col_map[key]
            custom_str = CoordString(sliced_str)
            custom_str.row_col_map = sliced_row_col_map
            return custom_str
        return super().__getitem__(key)

    def loc(self, index):
        return self.row_col_map[index]

    @staticmethod
    def from_existing(new_str: str, old_coords: list[tuple[int, int]]) -> 'CoordString':
        new_coord_str = CoordString(new_str)
        new_coord_str.row_col_map = old_coords
        return new_coord_str

    # wrappers for string methods that should return CoordStrings
    def lstrip(self, *args, **kwargs):
        result = super().lstrip(*args, **kwargs)
        custom_str = CoordString(result)
        custom_str.row_col_map = self.row_col_map[len(self)-len(result):]
        return custom_str

    def rstrip(self, *args, **kwargs):
        result = super().rstrip(*args, **kwargs)
        custom_str = CoordString(result)
        custom_str.row_col_map = self.row_col_map[:len(result)]
        return custom_str

    def strip(self, *args, **kwargs):
        return self.lstrip(*args, **kwargs).rstrip(*args, **kwargs)

    @wrap_coords
    def capitalize(self): return super().capitalize()

    @wrap_coords
    def casefold(self): return super().casefold()

    @wrap_coords
    def lower(self): return super().lower()

    @wrap_coords
    def upper(self): return super().upper()

    @wrap_coords
    def swapcase(self): return super().swapcase()

    @wrap_coords
    def title(self): return super().title()

    @wrap_coords
    def translate(self, table): return super().translate(table)

    @wrap_coords
    def replace(self, old, new, count=-1): return super().replace(old, new, count)

    @fail_coords
    def center(self, *args, **kwargs): ...

    @fail_coords
    def expandtabs(self, *args, **kwargs): ...

    @fail_coords
    def ljust(self, *args, **kwargs): ...

    @fail_coords
    def zfill(self, *args, **kwargs): ...


# TODO: maybe make adding this string with other regular str illegal


int_parsable_base_prefixes = {
    '0b': 2,  '0B': 2,
    '0t': 3,  '0T': 3,
    '0q': 4,  '0Q': 4,
    '0s': 6,  '0S': 6,
    '0o': 8,  '0O': 8,
    '0d': 10, '0D': 10,
    # '0z':12, #uses different digits Z/z and X/x (instead of A/a and B/b expected by int())
    '0x': 16, '0X': 16,
    '0u': 32, '0U': 32,
    '0r': 36, '0R': 36,
    # '0y':64, #more than int's max parsable base (36)
}


def based_number_to_int(src: str) -> int:
    """
    convert a number in a given base to an int
    """
    prefix, digits = src[:2], src[2:]
    if prefix in int_parsable_base_prefixes:
        return int(digits, int_parsable_base_prefixes[prefix])
    elif prefix == '0z':
        raise NotImplementedError(f"base {prefix} is not supported")
    elif prefix == '0y':
        raise NotImplementedError(f"base {prefix} is not supported")
    else:
        raise ValueError(f"INTERNAL ERROR: base {prefix} is not a valid base")


def bool_to_bool(src: str) -> bool:
    """
    convert a (case-insensitive) bool literal to a bool
    """
    try:
        return bool(['false', 'true'].index(src.lower()))
    except ValueError:
        raise ValueError(f"INTERNAL ERROR: bool {src} is not a valid bool") from None


class CaselessStr(str):
    def __init__(self, s: str):
        super().__init__()

    def __eq__(self, other: str):
        return self.casefold() == other.casefold()

    def __hash__(self):
        return hash(self.casefold())

    def __repr__(self):
        return f"(caseless)'{self.casefold()}'"

    def __str__(self):
        return self.casefold()


T = TypeVar('T')
U = TypeVar('U')


class CaseSelectiveDict(Generic[T]):
    """A dictionary where keys may be either case sensitive or case insensitive"""

    def __init__(self, **kwargs):
        self.caseless: dict[CaselessStr, any] = {}
        self.caseful: dict[str, any] = {}
        for k, v in kwargs.items():
            self.__setitem__(k, v)  # Use setitem to handle initial assignments

    def __getitem__(self, key: str | CaselessStr) -> T:
        try:
            return self.caseless[CaselessStr(key)]
        except KeyError:
            pass
        try:
            return self.caseful[key]
        except KeyError:
            pass

        raise KeyError(f"Key {key} not found") from None

    def __setitem__(self, key: str | CaselessStr, value: T):
        if not isinstance(key, (str, CaselessStr)):
            raise TypeError(f"Key must be of type str or CaselessStr, not '{type(key)}'")

        if CaselessStr(key) in self.caseless:
            if not isinstance(key, CaselessStr):
                key = CaselessStr(key)
            self.caseless[key] = value
            return

        if isinstance(key, CaselessStr):
            # need to check if the new key was in the caseful keys. This is an ERROR
            caseful_keys = [*self.caseful.keys()]
            folded_caseful_keys = [k.casefold() for k in caseful_keys]
            if key in folded_caseful_keys:
                existing_key = caseful_keys[folded_caseful_keys.index(key)]
                raise KeyError(f"Cannot insert key {repr(key)}. Caseful version {repr(existing_key)} already exists")
            self.caseless[key] = value
            return

        self.caseful[key] = value

    def __delitem__(self, key: str | CaselessStr):
        key = str(key)
        try:
            del self.caseless[CaselessStr(key)]
            return
        except KeyError:
            pass
        try:
            del self.caseful[key]
            return
        except KeyError:
            pass

        raise KeyError(f"Key {key} not found") from None

    def __contains__(self, key: str | CaselessStr):
        try:
            return CaselessStr(key) in self.caseless or key in self.caseful
        except TypeError:
            return False

    def has_key(self, key: str | CaselessStr):
        return key in self

    def get(self, key: str | CaselessStr, default: U = None) -> T | U:
        try:
            return self[key]
        except KeyError:
            return default

    def __len__(self):
        return len(self.caseless) + len(self.caseful)

    def __iter__(self):
        return iter(self.keys())

    def items(self):
        return {**self.caseless, **self.caseful}.items()

    def keys(self) -> list[str | CaselessStr]:
        return [*self.caseless.keys(), *self.caseful.keys()]

    def values(self) -> list[T]:
        return [*self.caseless.values(), *self.caseful.values()]

    def __repr__(self):
        return f"CaseSelectiveDict({self.caseless}, {self.caseful})"

    def __str__(self):
        items = {**self.caseless, **self.caseful}
        return f'{items}'
1a:T58d,"""
Collection of all the Dewy Language backends
"""

from .python import python_backend, python_repl
from .qbe import qbe_backend
from .llvm import llvm_backend
from .c import c_backend
from .x86_64 import x86_64_backend
from .arm import arm_backend
from .riscv import riscv_backend
from .shell import shell_backend
from pathlib import Path
from ..utils import Backend



# TODO: perhaps we could automatically discover these backends based on files in the backend/ folder
backends: list[Backend] = [
    python_backend,
    qbe_backend,
    llvm_backend,
    c_backend,
    x86_64_backend,
    arm_backend,
    riscv_backend,
    shell_backend,
]

backend_map = {
    backend.name.lower(): backend for backend in backends
}

# TODO: for now we're gonna skip this and all shell backends are accessed under the name 'shell'
# backend_map = {
#     **backend_map,
#     'sh': shell_compiler,
#     'zsh': shell_compiler,
#     'bash': shell_compiler,
#     'fish': shell_compiler,
#     'posix': shell_compiler,
#     'powershell': shell_compiler,
# }


backend_names = [*backend_map.keys()]


def get_backend(name: str) -> Backend:
    try:
        return backend_map[name.lower()]
    except:
        raise ValueError(f'Unknown backend "{name}"') from None


def get_version() -> str:
    """Return the semantic version of the language"""
    return (Path(__file__).parent.parent.parent / 'VERSION').read_text().strip()
1b:Tbdb1,from ..tokenizer import tokenize
from ..postok import post_process
from ..typecheck import (
    Scope as TypecheckScope,
    typecheck_call, typecheck_index, typecheck_multiply,
    register_typeof, short_circuit,
    CallableBase, IndexableBase, IndexerBase, MultipliableBase, ObjectBase,
)
from ..parser import top_level_parse, QJux
from ..syntax import (
    AST,
    Type, TypeParam,
    PointsTo, BidirPointsTo,
    ListOfASTs, PrototypeTuple, Block, Array, Group, Range, ObjectLiteral, Dict, BidirDict, UnpackTarget,
    TypedIdentifier,
    Void, void, Undefined, undefined, untyped,
    String, IString,
    Flowable, Flow, If, Loop, Default,
    Identifier, Express, Declare,
    PrototypeBuiltin, Call, Access, Index,
    Assign,
    Int, Bool,
    Range, IterIn,
    BinOp,
    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,
    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    UnaryPrefixOp, UnaryPostfixOp,
    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv, AtHandle,
    CycleLeft, CycleRight, Suppress,
    BroadcastOp,
    CollectInto, SpreadOutFrom,
    DeclarationType,
)

from ..postparse import post_parse, FunctionLiteral, Signature, normalize_function_args
from ..utils import Backend, BaseOptions

from dataclasses import dataclass, field
from pathlib import Path
from typing import Protocol, cast, Callable as TypingCallable, Any, Generic
from functools import cache
from argparse import ArgumentParser, Namespace
# from collections import defaultdict
# from types import SimpleNamespace


import pdb


@dataclass
class Options(BaseOptions):
    test_option: bool = False


def make_argparser(parent: ArgumentParser) -> None:
    parent.add_argument('--test-option', action='store_true', help='Test option for the python backend')

def make_options(args: Namespace) -> Options:
    return Options(
        tokens=args.tokens,
        verbose=args.verbose,

        # TODO: collect other options here
        test_option=args.test_option,
    )


def python_interpreter(path: Path, args:list[str], options: Options) -> None:
    # get the source code and tokenize
    src = path.read_text()
    tokens = tokenize(src)
    post_process(tokens)

    # parse tokens into AST
    ast = top_level_parse(tokens)
    ast = post_parse(ast)

    # debug printing
    if options.verbose:
        print_ast(ast)
        print(repr(ast))

    # run the program
    res = top_level_evaluate(ast)
    if res is not void:
        print(res)


python_backend = Backend[Options](
    name = 'python',
    exec = python_interpreter,
    make_argparser = make_argparser,
    make_options = make_options
)

    

def python_repl(args: list[str], options: Options):
    try:
        from easyrepl import REPL
    except ImportError:
        print('easyrepl is required for REPL mode. Install with `pip install easyrepl`')
        return

    # Set up scope to share between REPL calls
    scope = Scope.default()

    # get the source code and tokenize
    for src in REPL(history_file='~/.dewy/repl_history'):
        # Check for custom commands
        match src:
            case 'exit' | 'quit':
                return
            case 'help':
                print('Commands:')
                print('  exit|quit: exit the REPL')
                print('  help: display this help message')
                continue

        try:
            tokens = tokenize(src)
            post_process(tokens)
            if options.tokens:
                print(tokens)

            # parse tokens into AST
            ast = top_level_parse(tokens)
            ast = post_parse(ast)

            # debug printing
            if options.verbose:
                print_ast(ast)
                print(repr(ast))

            # run the program (sharing the same scope)
            res = evaluate(ast, scope)
            if res is not void:
                print(res)
        except Exception as e:
            print(f'Error: {e}')

    print() # newline after exiting REPL with ctrl+d

def print_ast(ast: AST):
    """little helper function to print out the equivalent source code of an AST"""
    print('```dewy')
    if isinstance(ast, (Block, Group)):
        for i in ast.__iter_asts__(): print(i)
    else:
        print(ast)
    print('```')


def top_level_evaluate(ast:AST) -> AST:
    scope = Scope.default()
    return evaluate(ast, scope)


############################ Runtime helper classes ############################


@dataclass
class Scope(TypecheckScope):
    """An extension of the Scope used during parsing to support runtime"""

    @staticmethod
    def default() -> 'Scope':
        return Scope(vars={
            'printl': Scope._var(
                DeclarationType.CONST,
                Type(Builtin),
                Builtin(
                    normalize_function_args(Group([Assign(TypedIdentifier(Identifier('s'), Type(String)), String(''))])),
                    preprocess_py_print_args,
                    py_printl,
                    Type(Void)
                ),
            ),
            'print': Scope._var(
                DeclarationType.CONST,
                Type(Builtin),
                Builtin(
                    normalize_function_args(Group([Assign(TypedIdentifier(Identifier('s'), Type(String)), String(''))])),
                    preprocess_py_print_args,
                    py_print,
                    Type(Void)
                )
            ),
            'readl': Scope._var(
                DeclarationType.CONST,
                Type(Builtin),
                Builtin(
                    normalize_function_args(Group([])),
                    lambda *a, **kw: ([],{}),
                    py_readl,
                    Type(String)
                )
            )
        })

# def insert_builtins(scope: Scope):
#     """replace prototype builtin stubs with actual implementations"""
#     if 'printl' in scope.vars:
#         assert isinstance((proto:=scope.vars['printl'].value), PrototypeBuiltin)
#         scope.vars['printl'].value = Builtin.from_prototype(proto, preprocess_py_print_args, py_printl)
#     if 'print' in scope.vars:
#         assert isinstance((proto:=scope.vars['print'].value), PrototypeBuiltin)
#         scope.vars['print'].value = Builtin.from_prototype(proto, preprocess_py_print_args, py_print)
#     if 'readl' in scope.vars:
#         assert isinstance((proto:=scope.vars['readl'].value), PrototypeBuiltin)
#         scope.vars['readl'].value = Builtin.from_prototype(proto, lambda *a: ([],{}), py_readl)



class Iter(AST):
    item: AST
    i: int

    def __str__(self):
        return f'Iter({self.item}, i={self.i})'

class BuiltinArgsPreprocessor(Protocol):
    def __call__(self, args: list[AST], kwargs: dict[str, AST], scope: Scope) -> tuple[list[Any], dict[str, Any]]: ...

class Builtin(CallableBase):
    signature: Signature
    preprocessor: BuiltinArgsPreprocessor
    action: TypingCallable[..., AST]
    return_type: AST

    def __str__(self):
        return f'{self.signature}:> {self.return_type} => {self.action}'

    # def from_prototype(proto: PrototypeBuiltin, preprocessor: BuiltinArgsPreprocessor, action: TypingCallable[..., AST]) -> 'Builtin':
    #     return Builtin(
    #         signature=normalize_function_args(proto.args),
    #         preprocessor=preprocessor,
    #         action=action,
    #         return_type=proto.return_type,
    #     )

# hacky for now. longer term, want full signature type checking for functions!
register_typeof(Builtin, short_circuit(Builtin))

class Closure(CallableBase):
    fn: FunctionLiteral
    scope: Scope

    def __str__(self):
        return f'{self.fn} with <Scope@{hex(id(self.scope))}>'
        # scope_lines = []
        # for name, var in self.scope.vars.items():
        #     line = f'{var.decltype.name.lower()} {name}'
        #     if var.type is not untyped:
        #         line += f': {var.type}'
        #     if var.value is self:
        #         line += ' = <self>'
        #     elif var.value is not void:
        #         line += f' = {var.value}'
        #     scope_lines.append(line)
        # scope_contents = ', '.join(scope_lines)
        # return f'{self.fn} with scope=[{scope_contents}]'

# register_callable(Builtin)
# register_callable(Closure)

register_typeof(Closure, short_circuit(Closure))

class Object(ObjectBase):
    scope: Scope

    def __str__(self):
        chunks = []
        for name, var in self.scope.vars.items():
            chunk = f'{var.decltype.name.lower()} {name}'
            if var.type is not untyped:
                chunk += f': {var.type}'
            #TODO: need to handle any recursion loops...
            elif var.value is not void:
                chunk += f' = {var.value}'
            chunks.append(chunk)
        if len(chunks) < 5:
            return f'[{" ".join(chunks)}]'
        newline = '\n'
        #TODO: py3.12 remove {newline} and replace with direct \n
        return f'[{newline}    {f"{newline}    ".join(chunks)}{newline}]'


def typeof_object(obj: Object, scope: Scope, params:bool=False) -> Type:
    return Type(Object, TypeParam([obj.scope]))
register_typeof(Object, typeof_object)

class Float(MultipliableBase):
    val: float

    def __str__(self):
        return f'{self.val}'

############################ Evaluation functions ############################

#DEBUG supporting py3.11
from typing import TypeVar
T = TypeVar('T', bound=AST)
U = TypeVar('U', bound=AST)
class EvalFunc(Protocol):
    def __call__(self, ast: T, scope: Scope) -> AST: ...

def no_op(ast: T, scope: Scope) -> T:
    """For ASTs that just return themselves when evaluated"""
    return ast

# #py3.12 version
# class EvalFunc[T](Protocol):
#     def __call__(self, ast: T, scope: Scope) -> AST: ...
#
# def no_op[T](ast: T, scope: Scope) -> T:
#     """For ASTs that just return themselves when evaluated"""
#     return ast

def cannot_evaluate(ast: AST, scope: Scope) -> AST:
    raise ValueError(f'INTERNAL ERROR: evaluation of type {type(ast)} is not possible')


@cache
def get_eval_fn_map() -> dict[type[AST], EvalFunc]:
    return {
        Declare: evaluate_declare,
        QJux: evaluate_qjux,
        Call: evaluate_call,
        Block: evaluate_block,
        Group: evaluate_group,
        Array: evaluate_array,
        Dict: evaluate_dict,
        PointsTo: evaluate_points_to,
        BidirDict: evaluate_bidir_dict,
        BidirPointsTo: evaluate_bidir_points_to,
        ObjectLiteral: evaluate_object_literal,
        Object: no_op,
        Access: evaluate_access,
        Index: evaluate_index,
        Assign: evaluate_assign,
        IterIn: evaluate_iter_in,
        FunctionLiteral: evaluate_function_literal,
        Closure: evaluate_closure,
        Builtin: evaluate_builtin,
        String: no_op,
        IString: evaluate_istring,
        Identifier: cannot_evaluate,
        Express: evaluate_express,
        Int: no_op,
        Float: no_op,
        Bool: no_op,
        Range: no_op,
        Flow: evaluate_flow,
        Default: evaluate_default,
        If: evaluate_if,
        Loop: evaluate_loop,
        UnaryPos: evaluate_unary_dispatch,
        UnaryNeg: evaluate_unary_dispatch,
        UnaryMul: evaluate_unary_dispatch,
        UnaryDiv: evaluate_unary_dispatch,
        Not: evaluate_unary_dispatch,
        Greater: evaluate_binary_dispatch,
        GreaterEqual: evaluate_binary_dispatch,
        Less: evaluate_binary_dispatch,
        LessEqual: evaluate_binary_dispatch,
        Equal: evaluate_binary_dispatch,
        And: evaluate_binary_dispatch,
        Or: evaluate_binary_dispatch,
        Xor: evaluate_binary_dispatch,
        Nand: evaluate_binary_dispatch,
        Nor: evaluate_binary_dispatch,
        Xnor: evaluate_binary_dispatch,
        Add: evaluate_binary_dispatch,
        Sub: evaluate_binary_dispatch,
        Mul: evaluate_binary_dispatch,
        Div: evaluate_binary_dispatch,
        Mod: evaluate_binary_dispatch,
        Pow: evaluate_binary_dispatch,
        LeftShift: evaluate_binary_dispatch,
        RightShift: evaluate_binary_dispatch,
        LeftRotate: evaluate_binary_dispatch,
        RightRotate: evaluate_binary_dispatch,
        LeftRotateCarry: evaluate_binary_dispatch,
        RightRotateCarry: evaluate_binary_dispatch,
        CycleLeft: evaluate_binary_dispatch,
        AtHandle: evaluate_at_handle,
        Undefined: no_op,
        Void: no_op,
        #TODO: other AST types here
    }

def evaluate(ast:AST, scope:Scope) -> AST:
    eval_fn_map = get_eval_fn_map()

    ast_type = type(ast)
    if ast_type in eval_fn_map:
        return eval_fn_map[ast_type](ast, scope)

    raise NotImplementedError(f'evaluation not implemented for {ast_type}')


def suspend(ast:AST, scope:Scope) -> Closure:
    """Wrap an AST in a Closure to be evaluated later"""
    return Closure(fn=FunctionLiteral(args=Signature(), body=ast), scope=scope)


def evaluate_declare(ast: Declare, scope: Scope):
    match ast.target:
        case Identifier(name):
            value = void
            type = untyped
        case TypedIdentifier(id=Identifier(name), type=type):
            value = void
        case Assign(left=Identifier(name), right=right):
            value = evaluate(right, scope)
            type = untyped
        case Assign(left=TypedIdentifier(id=Identifier(name), type=type), right=right):
            value = evaluate(right, scope)
        case Assign(left=UnpackTarget() as target, right=right):
            right = evaluate(right, scope)
            #TODO: need to declare each value being unpacked. something with this effect (but also makes new declarations for each):
            # unpack_assign(target, right, scope)
            pdb.set_trace()
            ...

        case _:
            raise NotImplementedError(f'Declare not implemented yet for {ast.target=}')

    scope.declare(name, value, type, ast.decltype)
    return void

def evaluate_qjux(ast: QJux, scope: Scope) -> AST:
    if ast.call is not None and typecheck_call(ast.call, scope):
        return evaluate_call(ast.call, scope)
    if ast.index is not None and typecheck_index(ast.index, scope):
        return evaluate_index(ast.index, scope)
    if typecheck_multiply(ast.mul, scope):
        return evaluate_binary_dispatch(ast.mul, scope)

    raise ValueError(f'Typechecking failed to match a valid evaluation for QJux. {ast=}')

# def evaluate_qast(ast: QAST, scope: Scope):
#     # use type checking to determine which branch of the QAST to evaluate
#     candidates: list[AST] = [a for a in ast.asts if typecheck(a, scope)]
#     if len(candidates) == 0:
#         raise ValueError(f'No valid candidates for QAST. {ast=}')
#     if len(candidates) > 1:
#         raise ValueError(f'Multiple valid candidates for QAST. {ast=}, {candidates=}')
#     selected, = candidates
#     return evaluate(selected, scope)


def evaluate_call(call: Call, scope: Scope) -> AST:
    f = call.f

    # get the expression of the group
    if isinstance(f, Group):
        f = evaluate(f, scope)

    # get the value pointed to by the identifier
    if isinstance(f, Identifier):
        f = scope.get(f.name).value

    # if this is a handle, do a partial evaluation rather than a call
    if isinstance(f, AtHandle):
        return apply_partial_eval(f.operand, call.args, scope)

    # AST being called must be TypingCallable
    assert isinstance(f, (Builtin, Closure)), f'expected Function or Builtin, got {f}'

    # save the args of the call as metadata for the function AST
    call_args, call_kwargs = collect_calling_args(call.args, scope)
    scope.meta[f].call_args = call_args, call_kwargs

    # run the function and return the result
    if isinstance(f, Builtin):
        return evaluate_builtin(f, scope)
    if isinstance(f, Closure):
        return evaluate_closure(f, scope)

    pdb.set_trace()
    raise NotImplementedError(f'Function evaluation not implemented yet')

#TODO: longer term this might also return a list/dict of spread args passed into the function
def collect_calling_args(args: AST | None, scope: Scope) -> tuple[list[AST], dict[str, AST]]:
    """
    Collect the arguments that a function is being called with
    e.g. `let fn = (a b c) => a + b + c; fn(1 c=2 3)`
    then the calling args are [1, 3] and {c: 2}

    Args:
        args: the arguments being passed to the function. If None, then treat as a no-arg call
        scope: the scope in which the function is being called

    Returns:
        a tuple of the positional arguments and keyword arguments
    """
    match args:
        case None | Void(): return [], {}
        case Identifier(name): return [scope.get(name).value], {}
        case Assign(left=Identifier(name)|TypedIdentifier(id=Identifier(name)), right=right): return [], {name: right}
        # case Assign(left=UnpackTarget() as target, right=right): raise NotImplementedError('UnpackTarget not implemented yet')
        case Assign(): raise NotImplementedError('Assign not implemented yet') #called recursively if a calling arg was an keyword arg rather than positional
        case CollectInto(right=right):
            pdb.set_trace()
            ... #right should be iterable, so extend with the values it expresses
                #whether to add to args or kwargs depends on each type from right
            val = evaluate(right, scope)
            match val:
                case Array(items): ... #return [collect_calling_args(i, scope) for i in items]
        case Group(items):
            call_args, call_kwargs = [], {}
            for i in items:
                a, kw = collect_calling_args(i, scope)
                call_args.extend(a)
                call_kwargs.update(kw)
            return call_args, call_kwargs

        #TODO: eventually it should just be anything that is left over is positional args rather than specifying them all out
        case Int() | String() | IString() | Range() | Call() | Access() | Index() | Express() | QJux() | UnaryPrefixOp() | UnaryPostfixOp() | BinOp() | BroadcastOp():
            return [args], {}
        # case Call(): return [args], {}
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'collect_args not implemented yet for {args}')


    raise NotImplementedError(f'collect_args not implemented yet for {args}')


def get_arg_name(arg: AST) -> str:
    """little helper function to get the name of an argument"""
    match arg:
        case Identifier(name): return name
        case TypedIdentifier(id=Identifier(name)): return name
        case Assign(left=Identifier(name)): return name
        case Assign(left=TypedIdentifier(id=Identifier(name))): return name
        case _:
            raise NotImplementedError(f'get_arg_name not implemented for {arg=}')

# TODO: this should maybe take in 2 scopes, one for the closure scope, and one for the callings scope... or closure scope args should already be evaluated...
# TODO: resolve args should handle the full gamut of possible types of arguments in a function
#       position or keyword arguments (with or without defaults)
#       position only arguments (with or without defaults)
#       keyword only arguments (with or without defaults)
# Note: the function signature will stay the same since calling a function or builtin just amounts to setting args and kwargs
# TODO: probably expand to include a container for unpack targets
def resolve_calling_args(signature: Signature, args: list[AST], kwargs: dict[str, AST], caller_scope: Scope, closure_scope: Scope = Scope()) -> tuple[dict[str, AST], dict[str, AST]]:
    """
    Resolve the final list of arguments the function actually receives
    Properly handles when the signature includes defaults, keyword args, positional args, partial evaluation, etc.

    e.g. if we have:

    ```dewy
    let fn = (a, b, c) => a + b + c
    fn = @fn(c=3)
    fn(1, a=2)
    ```

    then we would resolve to args={b: 1} kwargs={a: 2, c: 3}

    This is mainly for interfacing with python functions which want *args, **kwargs
    """
    # for now, just assume all args are position or keyword args
    # partial eval converts that particular arg to keyword only
    sig_pkwargs, sig_pargs, sig_kwargs = signature.pkwargs, signature.pargs, signature.kwargs
    dewy_args, dewy_kwargs = {}, {}

    # evaluate all the args and kwargs
    args = [evaluate(arg, caller_scope) for arg in args]
    kwargs = {name: evaluate(arg, caller_scope) for name, arg in kwargs.items()}


    # first pull out the calling keyword arguments
    dewy_kwargs.update(kwargs)
    sig_pkwargs = [*filter(lambda item: get_arg_name(item) not in kwargs, sig_pkwargs)]
    sig_kwargs = [*filter(lambda item: get_arg_name(item) not in kwargs, sig_kwargs)]

    #remaining kwargs are added to the dewy_kwargs
    for arg in sig_kwargs:
        assert isinstance(arg, Assign), f'INTERNAL ERROR: {arg=} is not an Assign'
        name = get_arg_name(arg)
        dewy_kwargs[name] = evaluate(arg.right, closure_scope)

    if len(sig_pargs) + len(sig_pkwargs) < len(args):
        raise ValueError(f'Too many positional arguments for function. {signature=}, {args=}, {kwargs=}')

    # next, pair up the positional arguments
    for spec, arg in zip(sig_pargs + sig_pkwargs, args):
        name = get_arg_name(spec)
        dewy_args[name] = arg

    # all remaining arguments must have a value provided by the signature
    for arg in (sig_pargs + sig_pkwargs)[len(args):]:
        #TODO: handle unpacking/spread
        name = get_arg_name(arg)
        if not isinstance(arg, Assign):
            pdb.set_trace()
            raise ValueError(f'Non-Assign arguments remaining unpaired in signature. {signature=}, {args=}, {kwargs=}')
        dewy_kwargs[name] = evaluate(arg.right, closure_scope)

    return dewy_args, dewy_kwargs



def update_signature(signature: Signature, args: list[AST], scope: Scope) -> Signature:
    """Given values to partially apply to a function, update the call signature to reflect the new values"""
    call_args, call_kwargs = collect_calling_args(args, scope)
    sig_pkwargs, sig_pargs, sig_kwargs = signature.pkwargs.copy(), signature.pargs.copy(), signature.kwargs.copy()
    for item in sig_kwargs:
        name = get_arg_name(item)
        if name in call_kwargs:
            sig_kwargs = [*filter(lambda i: get_arg_name(i) != name, sig_kwargs)]
            right = suspend(call_kwargs[name], scope) #((lambda ast, scope: lambda: evaluate(ast, scope))(call_kwargs[name], scope))
            sig_kwargs.append(Assign(left=Identifier(name), right=right))
    for item in sig_pkwargs:
        name = get_arg_name(item)
        if name in call_kwargs:
            sig_pkwargs = [*filter(lambda i: get_arg_name(i) != name, sig_pkwargs)]
            right = suspend(call_kwargs[name], scope) #Suspense((lambda ast, scope: lambda: evaluate(ast, scope))(call_kwargs[name], scope))
            # any pkwargs become kwargs when a value is given by keyword or position
            sig_kwargs.append(Assign(left=Identifier(name), right=right))

    # update the positional arguments
    for item in call_args:
        if len(sig_pargs) > 0:
            parg, sig_pargs = sig_pargs[0], sig_pargs[1:]
            name = get_arg_name(parg)
        elif len(sig_pkwargs) > 0:
            parg, sig_pkwargs = sig_pkwargs[0], sig_pkwargs[1:]
            name = get_arg_name(parg)
        else:
            raise ValueError(f'Too many positional arguments for function. {signature=}, {args=}')
        right = suspend(item, scope) #Suspense((lambda ast, scope: lambda: evaluate(ast, scope))(item, scope))
        sig_kwargs.append(Assign(left=Identifier(name), right=right))

    return Signature(pkwargs=sig_pkwargs, pargs=sig_pargs, kwargs=sig_kwargs)

def apply_partial_eval(f: AST, args: list[AST], scope: Scope) -> AST:
    match f:
        # # this case shouldn't really be possible since you have to wrap a function literal in parenthesis to @ it, turning it into a Closure
        # case FunctionLiteral(args=signature, body=body):
        #     new_signature = update_signature(signature, args, scope)
        #     return FunctionLiteral(args=new_signature, body=body)

        case Closure(fn=FunctionLiteral(args=signature, body=body), scope=closure_scope):
            new_signature = update_signature(signature, args, scope)
            return Closure(fn=FunctionLiteral(args=new_signature, body=body), scope=closure_scope)

        case Builtin(signature=signature, preprocessor=preprocessor, action=action, return_type=return_type):
            new_signature = update_signature(signature, args, scope)
            return Builtin(signature=new_signature, preprocessor=preprocessor, action=action, return_type=return_type)

        case Identifier(name):
            f = scope.get(name).value
            return apply_partial_eval(f, args, scope)
        case _:
            raise NotImplementedError(f'Partial evaluation not implemented yet for {f=}')


def evaluate_group(ast: Group, scope: Scope):

    expressed: list[AST] = []
    for expr in ast.items:
        res = evaluate(expr, scope)
        if res is not void:
            expressed.append(res)
    if len(expressed) == 0:
        return void
    if len(expressed) == 1:
        return expressed[0]
    raise NotImplementedError(f'Block with multiple expressions not yet supported. {ast=}, {expressed=}')


def evaluate_block(ast: Block, scope: Scope):
    scope = Scope(scope)
    return evaluate_group(Group(ast.items), scope)

def evaluate_array(ast: Array, scope: Scope) -> Array:
    return Array([evaluate(i, scope) for i in ast.items])

def evaluate_dict(ast: Dict, scope: Scope) -> Dict:
    return Dict([evaluate(kv, scope) for kv in ast.items])

def evaluate_points_to(ast: PointsTo, scope: Scope) -> PointsTo:
    return PointsTo(evaluate(ast.left, scope), evaluate(ast.right, scope))

def evaluate_bidir_dict(ast: BidirDict, scope: Scope) -> BidirDict:
    return BidirDict([evaluate(kv, scope) for kv in ast.items])

def evaluate_bidir_points_to(ast: BidirPointsTo, scope: Scope) -> BidirPointsTo:
    return BidirPointsTo(evaluate(ast.left, scope), evaluate(ast.right, scope))

def evaluate_object_literal(ast: ObjectLiteral, scope: Scope) -> Object:
    obj_scope = Scope(scope)
    evaluate(Group(ast.items), obj_scope)
    return Object(scope=obj_scope)

def evaluate_access(ast: Access, scope: Scope) -> AST:
    left = evaluate(ast.left, scope)
    right = ast.right
    match right:
        case Identifier():
            return evaluate_id_access(left, right, scope, evaluate_right=True)
        case AtHandle(operand=Identifier() as id):
            return evaluate_id_access(left, id, scope, evaluate_right=False)
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'evaluate_access not implemented yet for {right=}. {left=}')

def evaluate_index(ast: Index, scope: Scope) -> AST:
    left = evaluate(ast.left, scope)
    right = evaluate(ast.right, scope)
    match left, right:
        case Array(items), Array(items=[Int(i)]):
            return items[i]
        case _:
            pdb.set_trace()
    pdb.set_trace()
    ...

def evaluate_id_access(left: AST, right: Identifier, scope: Scope, evaluate_right=True) -> AST:
    match left:
        case Object(scope):
            access = scope.get(right.name, search_parents=False).value
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'evaluate_id_access not implemented yet for {left=}, {right=}')

    if evaluate_right:
        return evaluate(access, scope)
    return access

# def evaluate_at_handle_access(left: AST, right: AtHandle, scope: Scope) -> AST:
#     pdb.set_trace()
#     raise NotImplementedError(f'evaluate_at_handle_access not implemented yet for {left=}, {right=}')

#TODO: other access types (which are probably more like vectorized ops)
#      vectorized_call. perhaps this is just the catch all case for anything not an identifier? to use an identifier as an arg, just wrap in parens
#      vectorized_index? is that a coherent concept? honestly probably just let regular multidimensional indexing handle that case

def evaluate_assign(ast: Assign, scope: Scope):
    match ast:
        case Assign(left=Identifier(name), right=right):
            right = evaluate(right, scope)
            scope.assign(name, right)
            return void
        case Assign(left=UnpackTarget() as target, right=right):
            right = evaluate(right, scope)
            unpack_assign(target, right, scope)
            return void
    pdb.set_trace()
    raise NotImplementedError('Assign not implemented yet')


def evaluate_iter_in(ast: IterIn, scope: Scope):

    # helper function for progressing the iterator
    def step_iter_in(iter_props: tuple[TypingCallable, Iter], scope: Scope) -> AST:
        binder, iterable = iter_props
        cond, val = iter_next(iterable).items
        binder(val)
        return cond

    # if the iterator properties are already in the scope, use them
    if (res := scope.meta[ast].props) is not None:
        return step_iter_in(cast(tuple[TypingCallable, Iter], res), scope)

    # otherwise initialize since this is the first time we're hitting this IterIn
    match ast:
        case IterIn(left=Identifier(name), right=right):
            right = evaluate(right, scope)
            props = lambda x: scope.assign(name, x), Iter(item=right, i=0)
            scope.meta[ast].props = props
            return step_iter_in(props, scope)
        case IterIn(left=UnpackTarget() as target, right=right):
            right = evaluate(right, scope)
            props = lambda x: unpack_assign(target, x, scope), Iter(item=right, i=0)
            scope.meta[ast].props = props
            return step_iter_in(props, scope)

    pdb.set_trace()
    raise NotImplementedError('IterIn not implemented yet')


#TODO: this is really only for array unpacking. need to handle object unpacking as well...
#      need to check what type value
def unpack_assign(target: UnpackTarget, value: AST, scope: Scope):

    # current inefficient hack to unpack strings
    if isinstance(value, String):
        value = Array([String(c) for c in value.val])

    # current types supporting unpacking
    if not isinstance(value, (Array, Dict, PointsTo, BidirDict, BidirPointsTo, Undefined)):
        raise NotImplementedError(f'unpack_assign() is not yet implemented for {value=}')

    # determine how many targets will be assigned, and if spread is present
    num_targets = len(target.target)
    num_spread = sum(isinstance(t, CollectInto) for t in target.target)
    if num_spread > 1: raise RuntimeError(f'Only one collect-into is allowed in unpacking. {target=}, {value=}')

    # undefined unpacks as many undefineds as there are non-spread targets
    if isinstance(value, Undefined):
        value = Array([undefined for _ in range(num_targets - num_spread)])

    # verify if enough values to unpack, and set up generator (using built in iteration over ASTs children)
    num_values = len([*value.__iter_asts__()])
    spread_size = num_values - num_targets + 1  # if a spread is present, how many elements it will take
    if num_targets - num_spread > num_values: raise RuntimeError(f'Not enough values to unpack. {num_targets=}, {target=}, {value=}')
    gen = value.__iter_asts__()

    for left in target.target:
        match left:
            case Identifier(name):
                scope.assign(name, next(gen))
            # #TODO: object member renamed unpack. need to get the member of the object and assign it to the new name
            # case Assign(left=Identifier(name), right=right):
            #     scope.assign(name, right)
            case UnpackTarget():
                unpack_assign(left, next(gen), scope)
            case CollectInto(right=Identifier(name)):
                scope.assign(name, Array([next(gen) for _ in range(spread_size)]))
            case CollectInto(right=UnpackTarget() as left):
                unpack_assign(left, Array([next(gen) for _ in range(spread_size)]), scope)
            case _:
                pdb.set_trace()
                raise NotImplementedError(f'unpack_assign not implemented for {left=} and right={next(gen)}')

    # if there are any remaining values, raise an error
    if (remaining := [*gen]):
        raise RuntimeError(f'Too many values to unpack. {num_targets=}, {target=}, {value=}, {remaining=}')

# TODO: probably break this up into one function per type of iterable
def iter_next(iter: Iter):
    match iter.item:
        case Array(items):
            if iter.i >= len(items):
                cond, val = Bool(False), undefined
            else:
                cond, val = Bool(True), items[iter.i]
            iter.i += 1
            return Group([cond, val])
        case Dict(items):
            if iter.i >= len(items):
                cond, val = Bool(False), undefined
            else:
                cond, val = Bool(True), items[iter.i]
            iter.i += 1
            return Group([cond, val])
        case Range(left=Int(val=l), right=Void()|Undefined(), brackets=brackets):
            offset = int(brackets[0] == '(') # handle if first value is exclusive
            cond, val = Bool(True), Int(l + iter.i + offset)
            iter.i += 1
            return Group([cond, val])
        case Range(left=Int(val=l), right=Int(val=r), brackets=brackets):
            offset = int(brackets[0] == '(') # handle if first value is exclusive
            end_offset = int(brackets[1] == ']')
            i = l + iter.i + offset
            if i > r + end_offset - 1:
                cond, val = Bool(False), undefined
            else:
                cond, val = Bool(True), Int(i)
            iter.i += 1
            return Group([cond, val])
        case Range(left=Group(items=[Int(val=r0), Int(val=r1)]), right=Void()|Undefined(), brackets=brackets):
            offset = int(brackets[0] == '(') # handle if first value is exclusive
            step = r1 - r0
            cond, val = Bool(True), Int(r0 + (iter.i + offset) * step)
            iter.i += 1
            return Group([cond, val])
        case Range(left=Group(items=[Int(val=r0), Int(val=r1)]), right=Int(val=r2), brackets=brackets):
            offset = int(brackets[0] == '(') # handle if first value is exclusive
            end_offset = int(brackets[1] == ']')
            step = r1 - r0
            i = r0 + (iter.i + offset) * step
            if i > r2 + end_offset - 1:
                cond, val = Bool(False), undefined
            else:
                cond, val = Bool(True), Int(i)
            iter.i += 1
            return Group([cond, val])
        #TODO: other range cases...
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'iter_next not implemented yet for {iter.item=}')



def evaluate_function_literal(ast: FunctionLiteral, scope: Scope):
    return Closure(fn=ast, scope=scope)

def evaluate_closure(ast: Closure, scope: Scope):
    closure_scope = Scope(ast.scope)
    caller_scope = Scope(scope)
    args, kwargs = scope.meta[ast].call_args or ([], {})
    signature = ast.fn.args

    # attach all the args to scope so body can access them
    dewy_args, dewy_kwargs = resolve_calling_args(signature, args, kwargs, caller_scope, closure_scope)
    for name, value in (dewy_args | dewy_kwargs).items():
        closure_scope.assign(name, value)

    return evaluate(ast.fn.body, closure_scope)


def evaluate_builtin(ast: Builtin, scope: Scope):
    caller_scope = Scope(scope)
    call_args, call_kwargs = scope.meta[ast].call_args or ([], {})
    dewy_args, dewy_kwargs = resolve_calling_args(ast.signature, call_args, call_kwargs, caller_scope)
    py_args, py_kwargs = ast.preprocessor([*dewy_args.values()], dewy_kwargs, caller_scope)
    return ast.action(*py_args, **py_kwargs)


def evaluate_istring(ast: IString, scope: Scope) -> String:
    parts = (py_stringify(i, scope) for i in ast.parts)
    return String(''.join(parts))


def evaluate_express(ast: Express, scope: Scope):
    val = scope.get(ast.id.name).value
    return evaluate(val, scope)

def evaluate_flow(ast: Flow, scope: Scope):
    for branch in ast.branches:
        #TODO: slightly hacky way to get the child scope created by the branch (so we can check if it was entered)
        child_scope = None
        def save_child_scope(scope: Scope):
            nonlocal child_scope
            child_scope = scope

        match branch:
            case Default(): res = evaluate_default(branch, scope, save_child_scope)
            case If(): res = evaluate_if(branch, scope, save_child_scope)
            case Loop(): res = evaluate_loop(branch, scope, save_child_scope)
            case _:
                pdb.set_trace()
                raise NotImplementedError(f'evaluate_flow not implemented for flow type {branch=}')

        # if the branch was entered, return its result
        assert child_scope.meta[branch].was_entered is not None, f'INTERNAL ERROR: {branch=} .was_entered was not set'
        if child_scope.meta[branch].was_entered:
            return res

    # if no branches were entered, return void
    return void

def evaluate_default(ast: Default, scope: Scope, save_child_scope:TypingCallable[[Scope], None]=lambda _: None):
    scope = Scope(scope)
    save_child_scope(scope)
    scope.meta[ast].was_entered = True
    return evaluate(ast.body, scope)

#TODO: this needs improvements!
#Issue URL: https://github.com/david-andrew/dewy-lang/issues/2
def evaluate_if(ast: If, scope: Scope, save_child_scope:TypingCallable[[Scope], None]=lambda _: None):
    scope = Scope(scope)
    save_child_scope(scope)
    scope.meta[ast].was_entered = False
    if cast(Bool, evaluate(ast.condition, scope)).val:
        scope.meta[ast].was_entered = True
        return evaluate(ast.body, scope)

    # is this correct if the If isn't entered?
    return void

#TODO: this needs improvements!
def evaluate_loop(ast: Loop, scope: Scope, save_child_scope:TypingCallable[[Scope], None]=lambda _: None):
    scope = Scope(scope)
    save_child_scope(scope)
    scope.meta[ast].was_entered = False
    while cast(Bool, evaluate(ast.condition, scope)).val:
        scope.meta[ast].was_entered = True
        evaluate(ast.body, scope)

    # for now loops can't return anything
    return void
    # ast.body
    # ast.condition
    # pdb.set_trace()



def int_int_div(l: int, r: int) -> Int | Float | Undefined:
    if r == 0:
        return undefined
    res = l / r
    if res.is_integer():
        return Int(int(res))
    else:
        return Float(res)

def float_float_div(l: int|float, r: int|float) -> Float | Undefined:
    if r == 0:
        return undefined
    return Float(l / r)

class SimpleValue(Protocol, Generic[T]):
    val: T


# @dataclass
# class Dispatch:
#     fn_table: dict[tuple[Any, ...], TypingCallable[..., AST]]
#     type_symmetric: bool = True
#     # etc.
# DispatchTable = dict[type[UnaryPrefixOp|UnaryPostfixOp|BinOp], Dispatch]
# PromotionTable = dict[tuple[type[T], type[U]], TypingCallable[[T|U], T|U]]
# promote_rule(Int, Float64) == Float64
# promote_type(Int, Float64)  # Float64

UnaryDispatchKey =  tuple[type[UnaryPrefixOp]|type[UnaryPostfixOp], type[SimpleValue[T]]]
unary_dispatch_table: dict[UnaryDispatchKey[T], TypingCallable[[T], AST]] = {
    (Not, Int): lambda l: Int(~l),
    (Not, Bool): lambda l: Bool(not l),
    (UnaryPos, Int): lambda l: Int(l),
    (UnaryNeg, Int): lambda l: Int(-l),
    (UnaryMul, Int): lambda l: Int(l),
    (UnaryDiv, Int): lambda l: Int(1/l),
}

BinaryDispatchKey = tuple[type[BinOp], type[SimpleValue[T]], type[SimpleValue[U]]]
# These are all symmetric meaning you can swap the operand types and the same function will be used (but the arguments should not be swapped)
binary_dispatch_table: dict[BinaryDispatchKey[T, U], TypingCallable[[T, U], AST]|TypingCallable[[U, T], AST]] = {
    (And, Int, Int): lambda l, r: Int(l & r),
    (And, Bool, Bool): lambda l, r: Bool(l and r),
    (Or, Int, Int): lambda l, r: Int(l | r),
    (Or, Bool, Bool): lambda l, r: Bool(l or r),
    (Xor, Int, Int): lambda l, r: Int(l ^ r),
    (Xor, Bool, Bool): lambda l, r: Bool(l != r),
    (Nand, Int, Int): lambda l, r: Int(~(l & r)),
    (Nand, Bool, Bool): lambda l, r: Bool(not (l and r)),
    (Nor, Int, Int): lambda l, r: Int(~(l | r)),
    (Nor, Bool, Bool): lambda l, r: Bool(not (l or r)),
    (Add, Int, Int): lambda l, r: Int(l + r),
    (Add, Int, Float): lambda l, r: Float(l + r),
    (Add, Float, Float): lambda l, r: Float(l + r),
    (Sub, Int, Int): lambda l, r: Int(l - r),
    (Sub, Int, Float): lambda l, r: Float(l - r),
    (Sub, Float, Float): lambda l, r: Float(l - r),
    (Mul, Int, Int): lambda l, r: Int(l * r),
    (Mul, Int, Float): lambda l, r: Float(l * r),
    (Mul, Float, Float): lambda l, r: Float(l * r),
    (Div, Int, Int): int_int_div,
    (Div, Int, Float): float_float_div,
    (Div, Float, Float): float_float_div,
    (Mod, Int, Int): lambda l, r: Int(l % r),
    (Mod, Int, Float): lambda l, r: Float(l % r),
    (Mod, Float, Float): lambda l, r: Float(l % r),
    (Pow, Int, Int): lambda l, r: Int(l ** r),
    (Pow, Int, Float): lambda l, r: Float(l ** r),
    (Pow, Float, Float): lambda l, r: Float(l ** r),
    (Less, Int, Int): lambda l, r: Bool(l < r),
    (Less, Int, Float): lambda l, r: Bool(l < r),
    (Less, Float, Float): lambda l, r: Bool(l < r),
    (LessEqual, Int, Int): lambda l, r: Bool(l <= r),
    (LessEqual, Int, Float): lambda l, r: Bool(l <= r),
    (LessEqual, Float, Float): lambda l, r: Bool(l <= r),
    (Greater, Int, Int): lambda l, r: Bool(l > r),
    (Greater, Int, Float): lambda l, r: Bool(l > r),
    (Greater, Float, Float): lambda l, r: Bool(l > r),
    (GreaterEqual, Int, Int): lambda l, r: Bool(l >= r),
    (GreaterEqual, Int, Float): lambda l, r: Bool(l >= r),
    (GreaterEqual, Float, Float): lambda l, r: Bool(l >= r),
    (Equal, Int, Int): lambda l, r: Bool(l == r),
    (Equal, Float, Float): lambda l, r: Bool(l == r),
    (Equal, Bool, Bool): lambda l, r: Bool(l == r),
    (Equal, String, String): lambda l, r: Bool(l == r),
    # (NotEqual, Int, Int): lambda l, r: Bool(l != r),
    (LeftShift, Int, Int): lambda l, r: Int(l << r),
    (RightShift, Int, Int): lambda l, r: Int(l >> r),

}

unsymmetric_binary_dispatch_table: dict[BinaryDispatchKey[T, U], ] = {
    #e.g. (Mul, String, Int): lambda l, r: String(l * r), # if we follow python's behavior
}

# dispatch table for more complicated values that can't be automatically unpacked by the dispatch table
# TODO: actually ideally just have a single table
CustomBinaryDispatchKey = tuple[type[BinOp], type[T], type[U]]
custom_binary_dispatch_table: dict[CustomBinaryDispatchKey[T, U], TypingCallable[[T, U], AST]] = {
    (Add, Array, Array): lambda l, r: Array(l.items + r.items), #TODO: this will be removed in favor of spread. array add will probably be vector add
    # (BroadcastOp, Array, Array): broadcast_array_op,
    # (BroadcastOp, NpArray, NpArray): broadcast_array_op,
    # (BroadcastOp, Int, Array): broadcast_array_op,
    # (BroadcastOp, Float, Array): broadcast_array_op,

}

#TODO: handling short circuiting for logical operators. perhaps have them in a separate dispatch table

def evaluate_binary_dispatch(op: BinOp, scope: Scope):
    # evaluate the operands
    left = evaluate(op.left, scope)
    right = evaluate(op.right, scope)

    # if either operand is undefined, the result is undefined
    if isinstance(left, Undefined) or isinstance(right, Undefined):
        return undefined

    # dispatch to the appropriate function
    key = (type(op), type(left), type(right))
    if key in binary_dispatch_table:
        left, right = cast(SimpleValue[T], left), cast(SimpleValue[U], right)
        return binary_dispatch_table[key](left.val, right.val)
    if key in custom_binary_dispatch_table:
        return custom_binary_dispatch_table[key](left, right)

    # if the key wasn't found, try the reverse key (by swapping the types of the operands)
    reverse_key = (type(op), type(right), type(left))
    if reverse_key in binary_dispatch_table:
        left, right = cast(SimpleValue[U], left), cast(SimpleValue[T], right)
        return binary_dispatch_table[reverse_key](left.val, right.val)
    if reverse_key in custom_binary_dispatch_table:
        return custom_binary_dispatch_table[reverse_key](right, left)

    raise NotImplementedError(f'Binary dispatch not implemented for {key=}')

def evaluate_unary_dispatch(op: UnaryPrefixOp|UnaryPostfixOp, scope: Scope):
    # evaluate the operand
    operand = evaluate(op.operand, scope)

    # if the operand is undefined, the result is undefined
    if isinstance(operand, Undefined):
        return undefined

    # dispatch to the appropriate function
    key = (type(op), type(operand))
    if key in unary_dispatch_table:
        operand = cast(SimpleValue[T], operand)
        return unary_dispatch_table[key](operand.val)

    raise NotImplementedError(f'Unary dispatch not implemented for {key=}')


def evaluate_at_handle(ast: AtHandle, scope: Scope):
    match ast.operand:
        case Identifier(name):
            return scope.get(name).value
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'AtHandle not implemented for {ast.operand=}')

############################ Builtin functions and helpers ############################

# all references to python functions go through this interface to allow for easy swapping
from functools import partial
class BuiltinFuncs:
    printl=print
    print=partial(print, end='')
    readl=input

#TODO: consider adding a flag repr vs str, where initially str is used, but children get repr.
# as is, stringifying should put quotes around strings that are children of other objects
# but top level printed strings should not show their quotes
def py_stringify(ast: AST, scope: Scope, top_level:bool=False) -> str:
    # don't evaluate. already evaluated by resolve_calling_args
    ast = evaluate(ast, scope) if not isinstance(ast, (Builtin, Closure)) else ast
    match ast:
        # types that require special handling (i.e. because they have children that need to be stringified)
        case String(val): return val# if top_level else f'"{val}"'
        case Array(items): return f"[{' '.join(py_stringify(i, scope) for i in items)}]"
        case Dict(items): return f"[{' '.join(py_stringify(kv, scope) for kv in items)}]"
        case PointsTo(left, right): return f'{py_stringify(left, scope)}->{py_stringify(right, scope)}'
        case BidirDict(items): return f"[{' '.join(py_stringify(kv, scope) for kv in items)}]"
        case BidirPointsTo(left, right): return f'{py_stringify(left, scope)}<->{py_stringify(right, scope)}'
        case Range(left, right, brackets): return f'{brackets[0]}{py_stringify_range_operands(left, scope)}..{py_stringify_range_operands(right, scope)}{brackets[1]}'
        case Closure(fn): return f'{fn}'
        case FunctionLiteral() as fn: return f'{fn}'
        case Builtin() as fn: return f'{fn}'
        case Object() as obj: return f'{obj}'
        # case AtHandle() as at: return py_stringify(evaluate(at, scope), scope)

        # can use the built-in __str__ method for these types
        case Int() | Float() | Bool() | Undefined(): return str(ast)

        # TBD what other types need to be handled
        case _:
            pdb.set_trace()
            raise NotImplementedError(f'stringify not implemented for {type(ast)}')
    pdb.set_trace()


    raise NotImplementedError('stringify not implemented yet')

def py_stringify_range_operands(ast: AST, scope: Scope) -> str:
    """helper function to stringify range operands which may be a single value or a tuple (represented as an array)"""
    if isinstance(ast, Array):
        return f"{','.join(py_stringify(i, scope) for i in ast.items)}"
    return py_stringify(ast, scope)

def preprocess_py_print_args(args: list[AST], kwargs: dict[str, AST], scope: Scope) -> tuple[list[Any], dict[str, Any]]:
    py_args = [py_stringify(i, scope, top_level=True) for i in args]
    py_kwargs = {k: py_stringify(v, scope) for k, v in kwargs.items()}
    return py_args, py_kwargs

def py_printl(s:str) -> Void:
    BuiltinFuncs.printl(s)
    return void

def py_print(s:str) -> Void:
    BuiltinFuncs.print(s)
    return void

def py_readl() -> String:
    return String(BuiltinFuncs.readl())
1c:T6a78,from ...tokenizer import tokenize
from ...postok import post_process
from ...typecheck import (
    Scope as TypecheckScope,
    top_level_typecheck_and_resolve,# typecheck_and_resolve,
    typecheck_call, typecheck_index, typecheck_multiply,
    register_typeof, register_typeof_call, short_circuit, typeof, TypeExpr,
    CallableBase, IndexableBase, IndexerBase, MultipliableBase, ObjectBase,
)
from ...parser import top_level_parse, QJux
from ...syntax import (
    AST,
    DeclarationType,
    Type, TypeParam,
    PointsTo, BidirPointsTo,
    ListOfASTs, PrototypeTuple, Block, Array, Group, Range, ObjectLiteral, Dict, BidirDict, UnpackTarget,
    TypedIdentifier,
    Void, void, Undefined, undefined, untyped,
    String, IString,
    Flowable, Flow, If, Loop, Default,
    Identifier, Express, Declare,
    PrototypeBuiltin, Call, Access, Index,
    Assign,
    Int, Bool,
    Range, IterIn,
    BinOp,
    Less, LessEqual, Greater, GreaterEqual, Equal, MemberIn,
    LeftShift, RightShift, LeftRotate, RightRotate, LeftRotateCarry, RightRotateCarry,
    Add, Sub, Mul, Div, IDiv, Mod, Pow,
    And, Or, Xor, Nand, Nor, Xnor,
    UnaryPrefixOp, UnaryPostfixOp,
    Not, UnaryPos, UnaryNeg, UnaryMul, UnaryDiv, AtHandle,
    CycleLeft, CycleRight, Suppress,
    BroadcastOp,
    CollectInto, SpreadOutFrom,
)

from ...postparse import post_parse, FunctionLiteral, Signature, normalize_function_args
from ...utils import BaseOptions, Backend

import platform
from dataclasses import dataclass, field
from pathlib import Path
from typing import Protocol, Literal, TypeVar, Optional
from functools import cache
from itertools import count
from argparse import Namespace, ArgumentParser
import subprocess
import os


import pdb


from functools import cache
from pathlib import Path



class TBD: ...

# command to compile a .qbe file to an executable
# $ qbe <file>.ssa | gcc -x assembler -static -o hello

# bare metal version of qbe (requires some assembly as the entrypoint/syscall interface, e.g. syscalls.s)
# $ qbe -t <target> all.qbe files.qbe toinclude.qbe > program.s
# $ as -o program.o program.s
# $ as -o syscalls.o
# $ ld -o program program.o syscalls.o


# location of this directory (for referencing build files)
here = Path(__file__).parent


OS = Literal['linux', 'apple', 'windows']
Arch = Literal['x86_64', 'arm64', 'riscv64']
QbeSystem = Literal['amd64_sysv', 'amd64_apple', 'arm64', 'arm64_apple', 'rv64']

arch_map: dict[Arch, str] = {
    'x86_64': 'amd64',
    'arm64': 'arm64',
    'riscv64': 'rv64',
}
os_map: dict[OS, str] = {
    'linux': 'sysv',
    'apple': 'apple',
    'windows': 'windows',
}

@dataclass
class Options(BaseOptions):
    target_os: OS
    target_arch: Arch
    target_system: QbeSystem
    run_program: bool
    emit_asm: bool|Path
    emit_qbe: bool|Path

def make_argparser(parent: ArgumentParser) -> None:
    parent.add_argument('-os', type=str, help='Operating system name for cross compilation. If not provided, defaults to current host OS', choices=OS.__args__)
    parent.add_argument('-arch', type=str, help='Architecture name for cross compilation. If not provided, defaults to current host arch', choices=Arch.__args__)
    parent.add_argument('-b', '--build-only', action='store_true', help='Only compile/build the program, do not run it')
    parent.add_argument('--emit-asm', action='flag_or_explicit', const=True, default=False, metavar='PATH', help='Emit final assembly output. If no path is specified, output will be placed in __dewycache__/<program>.s')
    parent.add_argument('--emit-qbe', action='flag_or_explicit', const=True, default=False, metavar='PATH', help='Emit QBE IR output. If no path is specified, output will be placed in __dewycache__/<program>.qbe')

def make_options(args: Namespace) -> Options:
    # get the host system info
    host_os = platform.system().lower()
    host_arch = platform.machine().lower()
    host_system = get_qbe_target(host_arch, host_os)

    target_os: str = args.os if args.os else host_os
    target_arch: str = args.arch if args.arch else host_arch
    target_system = get_qbe_target(target_arch, target_os)

    cross_compiling = target_system != host_system
    run_program = not args.build_only and not cross_compiling

    # collect the bool or path args
    emit_asm = args.emit_asm if isinstance(args.emit_asm, bool) else Path(args.emit_asm)
    emit_qbe = args.emit_qbe if isinstance(args.emit_qbe, bool) else Path(args.emit_qbe)

    return Options(
        tokens=args.tokens,
        verbose=args.verbose,
        # -------------------- #
        target_os=target_os,
        target_arch=target_arch,
        target_system=target_system,
        run_program=run_program,
        emit_asm=emit_asm,
        emit_qbe=emit_qbe,
    )


def qbe_compiler(path: Path, args: list[str], options: Options) -> None:
    # create a __dewycache__ directory if it doesn't exist
    cache_dir = path.parent / '__dewycache__'
    cache_dir.mkdir(exist_ok=True)
    
    # get the source code and tokenize
    src = path.read_text()
    tokens = tokenize(src)
    post_process(tokens)

    # parse tokens into AST
    ast = top_level_parse(tokens)
    ast = post_parse(ast)

    # typecheck and collapse any Quantum ASTs to a concrete selection
    scope = Scope.linux_default()
    ast, scope_map = top_level_typecheck_and_resolve(ast, scope)

    # debug printing
    if options.verbose:
        print(repr(ast))

    # Initialize an *empty* QBE Module
    # qbe = QbeModule()

    # Compile the AST into the QBE module. This will create functions as needed.
    scope = Scope.linux_default()
    qbe = top_level_compile(ast, scope)

    # Check if __main__ was defined by the user's code. If not, add a fallback.
    # pdb.set_trace()
    # main_fn_exists = any(f.name == '$__main__' for f in qbe.functions)
    # if not main_fn_exists:
    #     if options.verbose:
    #         print("Warning: No __main__ function defined in source, adding fallback exit.")
    #     # Add the fallback __main__ that just exits with 0
    #     qbe.functions.append(
    #         QbeFunction(
    #             name='$__main__',
    #             export=True,
    #             args=[QbeArg('%argc', 'l'), QbeArg('%argv', 'l'), QbeArg('%envp', 'l')],
    #             ret='w', # Exit code is typically 'w' (word)
    #             blocks=[QbeBlock('@start', ['ret 0'])]
    #         )
    #     )

    # generate the QBE IR string
    ssa = str(qbe)
    if options.verbose:
        print("--- Generated QBE ---")
        print(ssa)
        print("---------------------")

    # write the qbe to a file
    qbe_file = cache_dir / f'{path.name}.qbe'
    qbe_file.write_text(ssa)

    # get paths to the relevant core files
    syscalls = here / f'{options.target_os}-syscalls-{options.target_arch}.s'
    syscalls = syscalls.relative_to(os.getcwd(), walk_up=True)
    program = cache_dir / path.stem

    # compile the qbe file to assembly
    # assemble the assembly files into object files
    # link the object files into an executable
    with program.with_suffix('.s').open('w') as f:
        subprocess.run(['qbe', '-t', options.target_system, qbe_file], stdout=f, check=True)
    subprocess.run(['as', '-o', program.with_suffix('.o'), program.with_suffix('.s')], check=True)
    subprocess.run(['as', '-o', syscalls.with_suffix('.o'), syscalls], check=True)
    subprocess.run(['ld', '-o', program, program.with_suffix('.o'), syscalls.with_suffix('.o')], check=True)

    # clean up qbe, assembly, and object files
    program.with_suffix('.o').unlink(missing_ok=True)
    syscalls.with_suffix('.o').unlink(missing_ok=True) # Keep syscalls.o optional

    # Handle emit options
    if options.emit_qbe is True:
        print(f'QBE output written to {qbe_file}')
    elif isinstance(options.emit_qbe, Path):
        qbe_file.rename(options.emit_qbe)
        print(f'QBE output written to {options.emit_qbe}')
    else: # False
        qbe_file.unlink(missing_ok=True)

    if options.emit_asm is True:
        print(f'Assembly output written to {program.with_suffix(".s")}')
        print(f'Syscall assembly used: {syscalls}')
    elif isinstance(options.emit_asm, Path):
        program.with_suffix('.s').rename(options.emit_asm)
        print(f'Assembly output written to {options.emit_asm}')
        print(f'Syscall assembly used: {syscalls}')
    else: # False
        program.with_suffix('.s').unlink(missing_ok=True)


    # Run the program
    if options.run_program:
        if options.verbose: print(f'./{program} {" ".join(args)}')
        os.execv(program, [program] + args)


# Main compile entry point (modified slightly)
def top_level_compile(ast: AST, scope: 'Scope') -> 'QbeModule':
    """Top-level compilation function."""
    if not isinstance(ast, Group):
        ast = Group([ast])  # Wrap in a Group if not already
    qbe = QbeModule()
    __main__ = QbeFunction(
        name='$__main__',
        export=True,
        args=[QbeArg('%argc', 'l'), QbeArg('%argv', 'l'), QbeArg('%envp', 'l')],
        ret='w', # Exit code is typically 'w' (word)
        blocks=[start_block:=QbeBlock('@start')]
    )
    qbe.functions.append(__main__)

    compile_group(ast, scope, qbe, start_block)

    # Add a fallback block that will return from the main function
    __main__.blocks.append(QbeBlock('@__fallback_exit__', ['ret 0']))
    # If the start block is empty, add a default return
    # if len(start_block.lines) == 0:
    #     start_block.lines.append('ret 0')

    return qbe
    
    # if isinstance(ast, Void):
    #     return qbe, MetaInfo()

    # Top-level compilation starts without a specific block context.
    # Handlers for top-level definitions (like Assign for functions) must manage this.
    compile(ast, scope, qbe, None) # Start with current_block=None

    return qbe, MetaInfo()



qbe_backend = Backend[Options](
    name='qbe',
    exec=qbe_compiler,
    make_argparser = make_argparser,
    make_options = make_options
)



def get_qbe_target(arch_name: Arch, os_name: OS) -> QbeSystem:
    if arch_name not in arch_map:
        raise ValueError(f"Unsupported architecture: {arch_name}, supported: {list(arch_map.keys())}")
    if os_name not in os_map:
        raise ValueError(f"Unsupported OS: {os_name}, supported: {list(os_map.keys())}")
    
    arch_name = arch_map[arch_name]
    os_name = os_map[os_name]

    qbe_target = arch_name
    if arch_name in ['amd64', 'arm64'] and os_name == 'apple':
        qbe_target += '_apple'
    elif arch_name == 'amd64' and os_name == 'sysv':
        qbe_target += '_sysv'
    
    return qbe_target

# --- Scope and QBE Data Structures ---
@dataclass
class Scope(TypecheckScope):
    # TODO: note that these are only relevant for linux
    # so probably have default versions for other OS environments...
    @staticmethod
    def _make_linux_syscall_builtin(n:int) -> 'Scope._var':
        """Creates a syscall builtin for the given syscall number"""
        return Scope._var(
            DeclarationType.CONST, Type(Builtin),
            Builtin(normalize_function_args(Group([
                TypedIdentifier(Identifier('n'), Type(Int)),
                *[TypedIdentifier(Identifier(f'a{i}'), Type(Int)) for i in range(n)]
            ])), Type(Int))
        )
    @staticmethod
    def linux_default() -> 'Scope':
        """A default scope for when compiling to linux. Contains merely __syscall1__ to __syscall6__"""
        return Scope(vars={f'__syscall{i}__': Scope._make_linux_syscall_builtin(i) for i in range(0, 7)})


    # # Add apple_default(), windows_default() etc. later

# QBE Type definition (can be expanded later for structs etc.)
QbeType = Literal['w', 'l', 's', 'd', 'b', 'h'] | str # Allow custom type names (structs)


# how are specific dewy types represented as QBE values (what is physically passed around)
# most things will probably be `l` and just be a void* pointer under the hood...
dewy_qbe_type_map: dict[Type, QbeType] = {
    Type(Int): 'l',
    # TODO: add more...
}

@dataclass
class QbeBlock:
    label: str
    lines: list[str] = field(default_factory=list)

    def __str__(self) -> str:
        indent = '    '
        # Ensure label starts with @, handle empty lines
        label_str = self.label if self.label.startswith('@') else f'@{self.label}'
        lines_str = '\n'.join(f'{indent}{line}' for line in self.lines if line.strip())
        # Only add newline if there are lines
        return f'{label_str}\n{lines_str}' if lines_str else label_str

@dataclass
class QbeArg:
    name: str
    type: QbeType

    def __str__(self) -> str:
        # Handle potential custom types (structs) prepended with ':'
        type_str = self.type if not self.type.startswith(':') else self.type[1:]
        return f'{type_str} {self.name}'

@dataclass
class QbeFunction:
    name: str
    export: bool
    args: list[QbeArg]
    ret: QbeType | None
    blocks: list[QbeBlock]

    def __str__(self) -> str:
        export_str = 'export ' if self.export else ''
        args_str = ', '.join(map(str, self.args))
        ret_str = f'{self.ret} ' if self.ret else '' # QBE requires space after type if present
        # Filter out empty blocks before joining
        # Ensure there's at least one block string, even if empty, to put inside {}
        block_strs = [str(b) for b in self.blocks if b.label or b.lines]
        blocks_str = '\n'.join(block_strs) if block_strs else ''
        # Ensure there's a newline between header and first block if blocks exist
        sep = '\n' if blocks_str else ''
        return f'{export_str}function {ret_str}{self.name}({args_str}) {{\n{blocks_str}{sep}}}'


@dataclass
class QbeModule:
    functions: list[QbeFunction] = field(default_factory=list)
    global_data: list[str] = field(default_factory=list)
    _counter: count = field(default_factory=lambda: count(0))
    # _symbols: dict[str, TypeExpr] = field(default_factory=dict) # Map temp names to Dewy types
    _symbols: dict[str, str] = field(default_factory=dict) # Map dewy scope names to QBE IR names

    def get_temp(self, prefix: str = "%.") -> str:
        """Gets the next available temporary variable name."""
        return f"{prefix}{next(self._counter)}"

    def __str__(self) -> str:
        # Ensure proper spacing between sections
        data_str = '\n'.join(self.global_data)
        funcs_str = '\n\n'.join(map(str, self.functions))
        sep1 = '\n\n' if data_str and funcs_str else '\n' if data_str or funcs_str else ''
        return f'{data_str}{sep1}{funcs_str}'.strip()


# --- Compilation Logic ---

# return type from compiling. AST so it can go in the scope
class IR(AST):
    qbe_type: QbeType
    qbe_value: str # could be a literal value or a name
    dewy_type: Type
    #TODO: something about the scope it's in... Or more specifically which QBE function it is in and can be accessed from

    def __str__(self) -> str:
        return f'IR(qbe=`{self.qbe_type} {self.qbe_value}`, type=`{self.dewy_type}`)'


T = TypeVar('T', bound=AST)
class CompileFunc(Protocol):
    def __call__(self, ast: T, scope: Scope, qbe: QbeModule, current_block: QbeBlock) -> Optional[IR]:
        """
        Compiles the AST node, potentially adding instructions to current_block
        (if provided and applicable, e.g., inside a function).
        Returns the QBE temporary variable name (%tmpN) or literal ('l 42')
        representing the result of the expression, or None if the node
        represents an action with no return value (like top-level Assign).
        For top-level nodes like function definitions, current_block might be None.
        """
        ...

@dataclass
class MetaInfo:
    """Placeholder for potential future metadata from compilation."""
    pass

# --- Specific Compile Functions ---
@cache
def get_compile_fn_map() -> dict[type[AST], CompileFunc]:
    """Returns the dispatch map for compilation functions."""
    return {
        Declare: compile_declare,
        Assign: compile_assign,
        Express: compile_express,
        Call: compile_call,
        Group: compile_group,
        Int: compile_int,
        String: compile_string,
        And: compile_base_logical_binop,
        Or: compile_base_logical_binop,
        Xor: compile_base_logical_binop,
        Nand: compile_notted_logical_binop,
        Nor: compile_notted_logical_binop,
        Xnor: compile_notted_logical_binop,
        Not: compile_not,
        # Less: compile_less,
        # LessEqual: compile_less_equal,
        # Greater: compile_greater,
        # GreaterEqual: compile_greater_equal,
        # Equal: compile_equal,
        # NotEqual: compile_not_equal,


        # Add other AST types here as they are implemented
    }


def compile(ast: AST, scope: Scope, qbe: QbeModule, current_block: QbeBlock) -> Optional[IR]:
    """Dispatches compilation to the appropriate function based on AST node type."""
    compile_fn_map = get_compile_fn_map()

    ast_type = type(ast)
    if ast_type in compile_fn_map:
        return compile_fn_map[ast_type](ast, scope, qbe, current_block)

    raise NotImplementedError(f'QBE compilation not implemented for AST type: {ast_type}')




def compile_declare(ast: Declare, scope: Scope, qbe: QbeModule, current_block: QbeBlock) -> None:
    """Handles variable declarations."""
    match ast.target:
        case Identifier(name=name):
            pdb.set_trace()
            ...
        case Assign(left=Identifier(name=name)):
            scope.declare(name, void, untyped, ast.decltype)
            compile_assign(ast.target, scope, qbe, current_block)
        case _:
            raise NotImplementedError(f"Declaration target not implemented: {ast.target}")

    return None

def compile_assign(ast: Assign, scope: Scope, qbe: QbeModule, current_block: QbeBlock) -> None:
    """Handles assignments, creating functions when assigning FunctionLiterals."""
    match ast:
        case Assign(left=Identifier(name=name), right=FunctionLiteral(args=signature, body=body)):
            pdb.set_trace()
            ...
            #TODO: save compiling the function literal for later...
            # defer_compile_fn(name, signature, body)
        case Assign(left=Identifier(name=name), right=right):
            rhs = compile(right, scope, qbe, current_block)
            if rhs is None:
                raise ValueError(f'INTERNAL ERROR: attempting to assign some type that doesn\'t produce a value: {name}={right!r}')
            scope.assign(name, rhs)
            qid = qbe.get_temp()
            qbe._symbols[name] = qid
            current_block.lines.append(f'{qid} ={rhs.qbe_type} copy {rhs.qbe_value}')
        case _:
            raise NotImplementedError(f"Assignment target not implemented: left={ast.left}, right={ast.right}")    

    return None

            

def compile_express(ast: Express, scope: Scope, qbe: QbeModule, current_block: QbeBlock) -> IR:
    # get the previous IR for the original value that should be set
    ir_var = scope.get(ast.id.name)
    ir = ir_var.value
    assert isinstance(ir, IR), f'INTERNAL ERROR: expected IR AST in scope for id "{ast.id}", but got {ir!r}'

    #TODO: need a check to verify the IR is contained in the same QBE function as it's being used...

    # value should be in symbol table? there are cases where it wouldn't but that's advanced out of order compilation stuff...
    assert ast.id.name in qbe._symbols, f'TBD if this is an internal error or not. Attempted to express a value which is not in the symbol table from compiling. {ast=!r}'
    express_ir = IR(ir.qbe_type, qbe._symbols[ast.id.name], ir.dewy_type)

    return express_ir


    


def compile_call(ast: Call, scope: Scope, qbe: QbeModule, current_block: QbeBlock) -> Optional[IR]:
    """Compiles a function call."""
    if not isinstance(ast.f, Identifier):
        raise NotImplementedError(f"calling non-identifier functions is not implemented yet. {ast.f} called with args {ast.args}")
    
    # get the QBE name of the function
    f_id = f'${ast.f.name}'

    # get the return type of the function
    f_var = scope.get(ast.f.name)
    match f_var.value:
        case FunctionLiteral(return_type=dewy_return_type) | Builtin(return_type=dewy_return_type) | Closure(FunctionLiteral(return_type=dewy_return_type)):
            # dewy_return_type = return_type
            if not isinstance(dewy_return_type, Type):
                pdb.set_trace()
                dewy_return_type = typeof(dewy_return_type, scope)
            ret_type = dewy_qbe_type_map[dewy_return_type]
        case _:
            raise ValueError(f'Unrecognized AST type to call: {f_var.value!r}')
    

    # convert calling args into a group if not already
    ast_args = ast.args
    match ast_args:
        case Group(): ... # already good
        case Void() | None:  ast_args = Group([])
        case _:       ast_args = Group([ast_args])
    

    qbe_args = [compile(arg, scope, qbe, current_block) for arg in ast_args.items]
    assert not any(arg is None for arg in qbe_args), f"INTERNAL ERROR: function call arguments must produce values: {ast_args}"

    args_str = ', '.join([f'{arg.qbe_type} {arg.qbe_value}' for arg in qbe_args])

    # insert the call with the result being saved to a new temporary id
    ret_id = qbe.get_temp()
    current_block.lines.append(f'{ret_id} ={ret_type} call {f_id}({args_str})')

    return IR(ret_type, ret_id, dewy_return_type)



def compile_group(ast: Group, scope: Scope, qbe: QbeModule, current_block: QbeBlock) -> Optional[IR]:
    """Compiles a group"""

    results = []
    for item in ast.items:
        result = compile(item, scope, qbe, current_block)
        if result is not None:
            results.append(result)
    
    # depending on how many values are present in the result, handle the group differently
    if len(results) == 0:
        return None
    elif len(results) == 1:
        return results[0]

    print('WARNING/TODO: group has multiple values. probably handle at the higher level')


    pdb.set_trace()
    ...
    # raise NotImplementedError(f'groups that express more than 1 value are not implemented yet. {ast} => {list(map(str, results))}')

    # if current_block is None and len(ast.items) > 0:
    #     # This might happen if a Group is the top-level AST node after `compile` starts.
    #     # This case needs refinement. Can a bare group be a valid top-level program?
    #     # For now, assume it needs a block context.
    #     raise ValueError("Cannot compile a Group node outside of a function block context.")
    #     # OR, if valid: Compile last item, but where do instructions go? Needs thought.

    # last_val = None
    # for item in ast.items:
    #     # Ensure we pass the current_block down
    #     last_val = compile(item, scope, qbe, current_block)

    # # The group itself evaluates to its last contained expression's value.
    # return last_val


def compile_int(ast: Int, scope: Scope, qbe: QbeModule, current_block: QbeBlock) -> IR:
    """Returns the QBE representation of an integer literal."""
    # QBE uses direct integers for constants. Prepend type for clarity in instruction.
    # The 'l' type is added by the instruction using this value (e.g., call)
    # TODO: check if the integer overflows, and return a big int
    return IR( 'l', f"{ast.val}", Type(Int))


def compile_string(ast: String, scope: Scope, qbe: QbeModule, current_block: QbeBlock) -> IR:
    """Returns the QBE representation of a string literal."""
    # data $greet = { b "Hello World!\n\0" }
    data_id = qbe.get_temp('$str')
    str_data = f'"{repr(ast.val)[1:-1]}"'
    # qbe.global_data.append(f'data {data_id}.len = {{ l {len(ast.val)} }}')
    # qbe.global_data.append(f'data {data_id}.data = {{ b {str_data}, b 0 }}')
    qbe.global_data.append(f'data {data_id} = {{ b {str_data}, b 0 }}')
    return IR('l', data_id, Type(String))

logical_binop_opcode_map = {
    (And, Int, Int): 'and',
    (Or, Int, Int): 'or',
    (Xor, Int, Int): 'xor',

}
def compile_base_logical_binop(ast: And|Or|Xor, scope: Scope, qbe: QbeModule, current_block: QbeBlock) -> IR:
    """Compiles a base/builtin logical operation."""
    left_ir = compile(ast.left, scope, qbe, current_block)
    assert left_ir is not None, f"INTERNAL ERROR: left side of `{ast.__class__.__name__}` must produce a value: {ast.left!r}"
    right_ir = compile(ast.right, scope, qbe, current_block)
    assert right_ir is not None, f"INTERNAL ERROR: right side of `{ast.__class__.__name__}` must produce a value: {ast.right!r}"
    assert left_ir.qbe_type == right_ir.qbe_type, f"INTERNAL ERROR: `{ast.__class__.__name__}` operands must be the same type: {left_ir.qbe_type} and {right_ir.qbe_type}"
    dewy_res_type = typeof(ast, scope)

    res_id = qbe.get_temp()
    res_type = left_ir.qbe_type

    # get the opcode name associated with this AST
    key = (type(ast), left_ir.dewy_type.t, right_ir.dewy_type.t)
    if key not in logical_binop_opcode_map:
        raise NotImplementedError(f'logical binop not implemented for types {key=}. from {ast!r}')
    opcode = logical_binop_opcode_map[key]


    current_block.lines.append(f'{res_id} ={res_type} {opcode} {left_ir.qbe_value}, {right_ir.qbe_value}')
    return IR(res_type, res_id, dewy_res_type)


def compile_not(ast: Not, scope: Scope, qbe: QbeModule, current_block: QbeBlock) -> IR:
    """use xor x, -1 to handle NOT"""
    operand_ir = compile(ast.operand, scope, qbe, current_block)
    assert operand_ir is not None, f'INTERNAL ERROR: operand of `Not` must produce a value: {ast.operand!r}'
    dewy_res_type = typeof(ast, scope)

    res_id = qbe.get_temp()
    res_type = operand_ir.qbe_type

    current_block.lines.append(f'{res_id} ={res_type} xor {operand_ir.qbe_value}, -1')
    return IR(res_type, res_id, dewy_res_type)


def compile_notted_logical_binop(ast: Nand|Nor|Xnor, scope: Scope, qbe: QbeModule, current_block: QbeBlock) -> IR:

    match ast:
        case Nand(): base_cls = And
        case Nor():  base_cls = Or
        case Xnor(): base_cls = Xor
        case _:
            raise NotImplementedError(f"INTERNAL ERROR: expected Nand, Nor, or Xnor, but got {ast!r}")
    
    composite_ast = Not(base_cls(ast.left, ast.right))
    final_ir = compile_not(composite_ast, scope, qbe, current_block)
    
    return final_ir

# --- Builtin Class Definitions (Keep as is for now) ---
class Builtin(CallableBase):
    signature: Signature
    return_type: AST
    def __str__(self): return f'{self.signature}:> {self.return_type} => ...'

class Closure(CallableBase):
    fn: FunctionLiteral
    scope: Scope
    def __str__(self): return f'{self.fn} with <Scope@{hex(id(self.scope))}>'


def typeof_builtin(builtin: Builtin, scope: Scope, params:bool=False) -> TypeExpr:
    """Returns the Dewy type of a builtin function."""
    pdb.set_trace()
    ...
register_typeof_call(Builtin, typeof_builtin)

def typeof_closure(closure: Closure, scope: Scope, params:bool=False) -> TypeExpr:
    """Returns the Dewy type of a closure."""
    pdb.set_trace()
    ...
register_typeof_call(Closure, typeof_closure)

def typeof_ir(ir: IR, scope: Scope, params:bool=False) -> TypeExpr:
    """Returns the Dewy type of an IR object."""
    return ir.dewy_type

register_typeof(IR, typeof_ir)1d:T8e5,//TODO: uncomment when types are supported
/{
// simple xorshift+ generator
state:uint64 = 123456789
rand = ():uint64 => {
    state xor= state >> 21
    state xor= state << 35
    state xor= state >> 4
    
    return state * 2_685821_657736_338717 //TODO: divide by uint64.MAX (18_446744_073709_551615)
}
half = 9_223372_036854_775807
a = rand <? half
b = rand <? half
c = rand <? half
d = rand <? half
e = rand <? half
f = rand <? half
g = rand <? half
h = rand <? half
i = rand <? half
j = rand <? half
k = rand <? half
l = rand <? half
m = rand <? half
n = rand <? half
o = rand <? half
p = rand <? half
q = rand <? half
r = rand <? half
s = rand <? half
t = rand <? half
u = rand <? half
v = rand <? half
w = rand <? half
x = rand <? half
y = rand <? half
z = rand <? half
}/

//manually specify bools
a = false
b = true
c = true
d = false
e = true
f = false
g = true
h = false
i = true
j = false
k = true
l = false
m = true
n = false
o = true
p = false
q = true
r = false
s = true
t = false
u = true
v = false
w = true
x = false
y = true
z = false



if a
    printl'a'
else if b
    if c
        if d
            if e
                printl'bcde'
            else if f
                if g
                    if h
                        printl'bcdfgh'
                    else if i
                        printl'bcdfgi'
                    else if j
                        printl'bcdfgj'
                    else
                        printl'bcdfg[]'
                else if k
                    printl'bcdfk'
                else if l
                    if m
                        printl'bcdflm'
                    else
                        printl'bcdfl[]'
                else
                    printl'bcdf[]'
            else if n
                printl'bcdn'
            else
                printl'bcd[]'
        else if o
            printl'bco'
        else if p
            if q
                printl'bcpq'
            else
                printl'bcp[]'
        else
            printl'bc[]'
    else
        printl'b[]'
else if r
    printl'r'
else if s
    if t
        printl'st'
    else if u
        printl'su'
if v
    if w
        printl'vw'
    else if x
        printl'x'
if y
    printl'y'
else if z
    printl'z'
else
    printl'[]'
1e:T626,// simplest case
x = 10
plus_5 = () => x + 5
printl(plus_5())

// taking in args
x = 12
my_closure = y => x + y
printl(my_closure(-5))


// returning a closure from a child scope
a = 13
my_closure = {
    b = 10
    fn = c => a + b + c // uses `b` from this scope and `a` from parent scope
    @fn
}
printl(my_closure(5))



// print the handle to the closure itself
printl(@my_closure)
printl(@printl)


// some pathological cases
my_print = {
    x = 5
    s = 'string with internal reference to x={x}'
    @printl(s)
}
my_print


my_print = {
    my_str = {
        x = 5
        s = 'string with internal reference to x={x}'
        s
    }
    fn = @printl(my_str)
    @fn
}
my_print



// Very deeply nested closures combined with partial evaluation
X = 'xpple'
Y = 'yanana'
fn = {
    Z = 'zeach'
    fn = {
        A = () => '@Apricot'
        fn = {
            B = () => '@Blueberry'
            fn = {
                fn = (x y z a b c d) => {
                    printl'x="{x}"\ny="{y}"\nz="{z}"\na="{a}"\nb="{b}"\nc="{c}"\nd="{d}"'
                }
                @fn
            }
            printl'fn={@fn}'
            @fn(b=B) // note just B should evaluate the string on calling fn
        }
        printl'fn={@fn}'
        @fn(a=@A)  // note that @A should evaluate the string inside of the IString
    }
    printl'fn={@fn}'
    @fn(z=Z d='manually assigning D' c='unused C')
}
printl'fn={@fn}'
fn = @fn(y=Y x=X)
printl'fn={@fn}'
fn = @fn(c='C')
printl'fn={@fn}'
printl(@fn)
fn
printl()
printl(@fn(c='a different C'))
fn(c='an even more different C')1f:T57f,//examples of each of the possible function signatures

// all args start out as positional or keyword (regardless of if they have defaults)
// partial application with positional args turns them into keyword only args
// partial application with keyword args turns them into positional args
// TBD unpack types probably need to be positional?
// TBD spread types probably need to be positional? 
//     or can we spread each of the possible types? I feel like spread just collects up everything not already specified in the signature

//////////////// Positional Arguments ////////////////
// no arguments
f0 = () => 0 // can call with `f0` or `f0()`

// 1 positional argument
f1 = x => x + 1     // can call with `f1(5)`
f1b = (x) => x + 1  // can call with `f1b(5)`

// 2 positional arguments
f2 = (x y) => x + y // can call with `f2(5 6)`

// 3 positional arguments
f3 = (x y z) => x + y + z // can call with `f3(5 6 7)`

printl'Positional Arguments'
printl(f0)
printl(f1(5))
printl(f2(5 6))
printl(f3(5 6 7))


//////////////// Optional Arguments ////////////////
// 1 positional and 1 optional keyword-only argument
f2b = (x y=2) => x + y // can call with `f2b(5)` or `f2b(5 y=6)`

// 1 optional keyword-only argument
f1c = (x=2) => x + 1 // can call with 'f1c' or `f1c()` or `f1c(x=5)`

printl'Optional Arguments'
printl(f2b(5))
printl(f2b(5 y=6))
printl(f1c)
printl(f1c(x=3))


//TODO: more examples20:Tbe6,/{ 
    Dewy Docs mdbook preprocessor:
    find code blocks labelled 'dewy' or 'dewy, editable' and convert them to iframes
}/

#main = () => {
    // mdbook calls preprocessor twice, first with args ["supports", <renderer>], 
    // and then if the first call exited with 0, the actual preprocessor run occurs
    // ignore the second argument (meaning support for all renderers)
    if sys.argv.length >? 1 and sys.argv[1] =? "supports"
        sys.exit(0)

    // get and parse the json input from stdin
    // TODO: need to make a json parser
    context, book = parse_json(read())

    loop section in book['sections']
    {
        section['Chapter']['content'] |>= process_markdown

        loop subitem in section['Chapter']['sub_items']
            subitem['Chapter']['content'] |>= process_markdown

        print(dump_json(book))
    }
}


counter = iter[0..]
process_markdown = (input_markdown) => {
    lines = input_markdown.split('\n')

    // lines starting with ```dewy
    starts = [
        loop i in [0..] and line in lines 
            if line[..6] =? '```dewy' 
                i
    ]

    // early return if no dewy code blocks
    if starts.length =? 0
        return input_markdown

    // ```dewy lines that are followed by ', editable' (whitespace invariant)
    editables = [
        loop i in starts
        {
            remainder = lines[i][7..].strip
            remainder[0] = ',' and remainder[1..].strip =? 'editable'
        }
    ]

    // match closing ``` lines
    stops = [
        loop i in starts
            loop line in lines[i..]
                if line =? '```'
                {
                    j
                    break
                }
    ]

    if starts.length not=? stops.length
        printl'Error: mismatched dewy code block starts and ends'
        sys.exit(1)

    return [
        loop 
            start in starts 
            and stop in stops 
            and editable in editables
            and prev_stop in [0 ...(stops.+1)]
        {
            //push the previous non-code block content
            if start >? prev_stop
                lines[prev_stop..start).join('\n')

            i = next(counter) // to give each iframe a unique id
            page = if editable 'demo_only' else 'src_only'
            code = lines(start..stop).join('\n')
            encoded_code = url_quote(code)
            
            // push the iframe replacement
            f'
                <iframe
                    src="https://david-andrew.github.io/iframes/dewy/{page}?src={encoded_code}&id=DewyIframe{i}"
                    style="width: 100%; border-radius: 0.5rem;"
                    id="DewyIframe{i}"
                    frameBorder="0"
                ></iframe>
            '
        }

        // push the last non-code block content
        if stops[-1]+1 <? lines.length
            lines[stops[-1]+1..].join('\n')
 
    ].join('\n')
}


//TODO: implement these functions
let parse_json = () => {}
let dump_json = () => {}
let url_quote = () => {}
let f = () => {}21:Tc05,// examples of syntax used in dewy
// line comments
/{ block/multiline comments }/

// typed declaration
apple: uint64
banana: map<int string>
peach: array<int length=N>  //array of ints with length N...
let pear: set<range>  // let indicates that this is definitely a new declaration, even if the identifier already exists



// unpack assignment examples
A = 1..10
B = [loop a in A -a]
loop [a b] in [A B] {}

// object with nested objects to unpack
my_obj = [
    apple = [1 2 3 4 [
        ultimate_answer = 42
    ]]
    banana = 10
    peach = [
        purple = 23
        blue = 12
        orange = 'orange'
    ]
]

// nested unpack assignment. tbd if the top level is `[unpack, params, etc] = obj`, or `obj as [unpack, params, etc]`
[
    [
        a1
        a2 
        a3 
        a4 
        [answer = ultimate_answer] = a5
    ] = apple
    renamed_banana = banana
    [purple blue orange] = peach
] = my_obj
// unpacked variables are:
//   a1 = 1, a2 = 2, a3 = 3, a4 = 4
//   answer = 42
//   renamed_banana = 10
//   purple = 23, blue = 12, orange = 'orange'

// unpacking dictionaries probably treats them as just the list of key -> value pairs
// unpacking sets, probably just treats the elements like a normal array
// unpacking ranges treats them as a normal array

// `...` can be used in unpack to coalesce extra elements for list-like containers
// there may only be 1 `...` in an unpack (otherwise it would be ambiguous which elements to collect)
// the variable receiving the `...` will be of the same type as the original object being unpacked
my_arr = [1 2 3 4 5 6 7 8 9]
[a1 a2 a3 ...my_arr a8 a9] = my_arr  //a1 = 1, a2 = 2, a3 = 3, my_arr = [4, 5, 6, 7], a8 = 8, a9 = 9

my_dict = ['apple' -> 1 'banana' -> 2 'peach' -> 3 'pie' -> 4]
[d1 ...dict_left] = my_dict //d1 = ('apple' -> 1), dict_left = ['banana' -> 2 'peach' -> 3 'pie' -> 4]

// random range note: for step sizes other than +1, use range_iter constructor e.g. range_iter(start to stop, step=5)
my_range = 1..inf
loop my_range.length >? 0 ( [i ...my_range] = my_range )
// first iteration: i = 1, my_range = 2..inf
// second iteration: i = 2, my_range = 3..inf
// third iteration: i = 3, my_range = 4..inf
// ...
// for forever

//[...my_range i] = my_range //will probably set my_range = 1 to inf, i = inf

// unpacking too many values, or named values that don't exist just sets them to undefined



// assignment expressions (i.e. python's walrus operator from https://www.python.org/dev/peps/pep-0572/)
// Handle a matched regex
if (match = pattern.search(data); match) not =? undefined
{
    // Do something with match
}

// A loop that can't be trivially rewritten using 2-arg iter()
loop (chunk = file.read(8192) chunk.length >? 0)
{
   process(chunk)
}

// Reuse a value that's expensive to compute
[y=f(x) y y**2 y**3]

// Share a subexpression between a comprehension filter clause and its output
filtered_data = [for x in data if (y=f(x) y) not =? undefined y]
//though you could also just write this like so
filtered_data = [for x in data {y=f(x) if y not =? undefined y}]
22:T1510,///////////////////// STRING INTERPOLATION /////////////////////

/{Todo: probably break each section for different syntaxes into different files?}/

//silly example with keyword vs identifier
loop i i

r'this is a raw string \'  expr  'a separate string later'

// simple blocks
{   }
( /{comment inside}/ )
{ 2+2 }
( 2+2 )


//string interpolation
my_string = '2 + 2 = {2+2}'

//complex interpolation
s = "first 10 primes are: {
    primes = [2]
    loop i in [3, 5..) and primes.length <? 10
        if i .% primes |> product not =? 0 
            primes.push(i)
    primes
}"


//alternative prime generator + getting first 10 primes
#ctx
primes = [
    2
    lazy i in [3, 5..)
        if i .% #ctx.primes .=? 0 |> @reduce(, (prev, v) => prev and v)
            i
][..10)

//TBD if there is a parallel way to do this where the i .% primes dispatches each operation, and fails immediately on any returning false
#label
primes = [
    2
    lazy i in [3, 5..)
        if not parallel_or(p => i % p =? 0, #label.primes)
            i
][..10)
//parallel or is like goroutines with cancel once any is true...should have it be more flexible, e.g. able to use any of the boolean keywords that can short circuit
//actually probably don't want to need to specify that it's parallel. Instead if there's an operation over a vector, it gets parallelized if possible.

//nested interpolation
s2 = 'this is an outer string, and {'this is an interior string with "{my_string}" in it'}'





const add = (a:int b:int): int => { /{return sum of a and b}/ }
let div = (a:real b:real): real? => { /{return a / b}/ }

// function type with named default argument
my_func = (s:string kwarg:bool=false): void => {}

//you probably can do the verbose version as well (probably useful for when you're just defining the interface without the implementation)
my_func: (s:string kwarg:bool=false): void



// example annotations for function types
() => ()
() => void
() => bool
int => bool
a: int => bool
(int int) => int
<T>(T T) => T
<T>(a:T b:T) => T

// object type
[a:int? b:string]

//? (optional) is sugar for |void
[a:int|void b:string]

// operators juxtaposed to identifiers
aorb
a or b
a+b


//based number literals
0b1010_0011_0101_0110_1001_1010_1100_1111
0B0101_1111_1010_1110_0011_0101_1001_1100

0t012_221_012_221_012_221_012_221
0t211_001_211_001_211_001_211_001

0q331_231_223_131_331_231_223_131
0Q123_321_123_321_123_321_123_321

0s123_450_123_450_123_450_123_450
0S543_210_543_210_543_210_543_210

0o123_456_701_234_567_012_345_670
0O012_345_670_123_456_701_234_567

0d123_456_789_012_345_678_901_234
0D987_654_321_098_765_432_109_876

0z123_456_789_xe0_123_456_789_xe0
0ZEX9_876_543_210_987_654_321_09E

0x1234_5678_9abc_def0_1234_5678_9abc_def0
0XFEDC_BA98_7654_3210_fedc_ba98_7654_3210

0u0123456789abcdefghijklmnopqrstuv0123456
0UVUTSRQPONMLKJIHGFEDCBA9876543210vutsrq

0r0123456789abcdefghijklmnopqrstuvwxyz012345
0RZYXWVUTSRQPONMLKJIHGFEDCBA9876543210zyxwv

0y0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!$
0Y$!ZYXWVUTSRQPONMLKJIHGFEDCBA9876543210zyxwvutsrqponmlkjihgfedcba

//Units TODO




[a b c] = [1 2 3]                                       //a=1, b=2, c=3
[a [b c]] = [1 [2 3]]                                   //a=1, b=2, c=3
[a [b c] d] = [1 [2 3] 4]                               //a=1, b=2, c=3, d=4
[a ...b] = [1 2 3 4]                                    //a=1, b=[2 3 4]
[a ...b c] = [1 2 3 4 5]                                //a=1, b=[2 3 4], c=5
[a ...b [c [...d e f]]] = [1 2 3 4 [5 [6 7 8 9 10]]]    //a=1, b=[2 3 4], c=5, d=[6 7 8], e=9, f=10



// silly things that are technically valid
x = loop i in [0..10] i //x=10

y = [
    if something 
        x
    else loop i in something_else
        y
    else if z
        z
    else loop i in last_thing
        w
    else
        ()
]


apple & banana
apple&banana
apple | banana
apple|banana





///////////// String prefixes ////////////////
p = s:string => [
    //process s based on / and \ separators
    //store result in this object
    route:array<string> = ...
    filename:string? = ...
    extension:string? = ...
]

p"this/is/a/file/path.ext"

//other prefixes
re"[^i*&2@]"                            // regex literal
t'my_token'                             //token literal. probably my version of enums
r'this is a raw string'                 //raw string. technically handled during tokenizing, there is no r function
(dewy)r'''printl("Hello, World!")'''    //dewy source code literal. uses raw string so that we don't have to worry about {}.

ipa"ษt vษkavit dษus aษพidam tษษพam kษngษพษgatsiษnษskwษ akwaษพum apษlavit maษพia ษt vidit dษus kwษd ษsษt bษnum" //international phonetic alphabet literal
(apl)r"life โ {โ1 โต โจ.โง 3 4 = +/ +โฟ ยฏ1 0 1 โ.โ ยฏ1 0 1 โฝยจ โโต}"  //apl expression literal
apl<|r"life โ {โ1 โต โจ.โง 3 4 = +/ +โฟ ยฏ1 0 1 โ.โ ยฏ1 0 1 โฝยจ โโต}"  //same as above

'''this is a regular string with triple quotes'''
"""this is a regular string with triple quotes"""

///////////// object prefixes ////////////////
//doubly linked list
dll[1 2 3 4 6 5 3 6 3 2]
@linked_list(double=true)[1 2 3 4 6 5 3 6 3 2]

//set literal syntax
set[4 3 6 4 6 4 2 2 4 5]



// silly example for generating a list of ones
ones = n => {l = [...[1..n]] l.=1 l}
ones(10) // [1 1 1 1 1 1 1 1 1 1]
//alternate
ones = n => [loop i in 1..n 1]d:["$","div",null,{"className":"w-screen h-screen absolute top-0 left-0 z-50 bg-black overflow-y-scroll","children":["$","div",null,{"className":"p-2","children":["$","$f",null,{"fallback":null,"children":["$","$L10",null,{"children":["$","$L11",null,{"dewy_interpreter_source":[{"name":"src/__init__.py","code":""},{"name":"src/frontend.py","code":"$12"},{"name":"src/parser.py","code":"$13"},{"name":"src/postok.py","code":"$14"},{"name":"src/postparse.py","code":"$15"},{"name":"src/syntax.py","code":"$16"},{"name":"src/tokenizer.py","code":"$17"},{"name":"src/typecheck.py","code":"$18"},{"name":"src/utils.py","code":"$19"},{"name":"src/backend/__init__.py","code":"$1a"},{"name":"src/backend/arm.py","code":"from ..utils import Backend, BaseOptions\n\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom argparse import ArgumentParser, Namespace\n\n@dataclass\nclass Options(BaseOptions): ...\n\ndef make_argparser(parent: ArgumentParser) -> None:\n    #TODO: any options for the arm backend\n    ...\n\ndef make_options(args: Namespace) -> Options:\n    return Options(\n        tokens=args.tokens,\n        verbose=args.verbose,\n    )\n\ndef arm_compiler(path: Path, args: list[str], options: Options) -> None:\n    raise NotImplementedError('ARM backend is not yet supported')\n\n\narm_backend = Backend[Options](\n    name='arm',\n    exec=arm_compiler,\n    make_argparser=make_argparser,\n    make_options=make_options\n)"},{"name":"src/backend/c.py","code":"from ..utils import Backend, BaseOptions\n\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom argparse import ArgumentParser, Namespace\n\n@dataclass\nclass Options(BaseOptions): ...\n\ndef make_argparser(parent: ArgumentParser) -> None:\n    #TODO: any options for the arm backend\n    ...\n\ndef make_options(args: Namespace) -> Options:\n    return Options(\n        tokens=args.tokens,\n        verbose=args.verbose,\n    )\n\ndef c_compiler(path: Path, args: list[str], options: Options) -> None:\n    raise NotImplementedError('C backend is not yet supported')\n\n\nc_backend = Backend[Options](\n    name='C',\n    exec=c_compiler,\n    make_argparser=make_argparser,\n    make_options=make_options\n)"},{"name":"src/backend/llvm.py","code":"from ..utils import Backend, BaseOptions\n\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom argparse import ArgumentParser, Namespace\n\n@dataclass\nclass Options(BaseOptions): ...\n\ndef make_argparser(parent: ArgumentParser) -> None:\n    #TODO: any options for the arm backend\n    ...\n\ndef make_options(args: Namespace) -> Options:\n    return Options(\n        tokens=args.tokens,\n        verbose=args.verbose,\n    )\n\ndef llvm_compiler(path: Path, args: list[str], options: Options) -> None:\n    raise NotImplementedError('LLVM backend is not yet supported')\n\n\nllvm_backend = Backend[Options](\n    name='llvm',\n    exec=llvm_compiler,\n    make_argparser=make_argparser,\n    make_options=make_options\n)\n"},{"name":"src/backend/python.py","code":"$1b"},{"name":"src/backend/qbe/__init__.py","code":"from .qbe import qbe_backend"},{"name":"src/backend/qbe/qbe.py","code":"$1c"},{"name":"src/backend/riscv.py","code":"from ..utils import Backend, BaseOptions\n\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom argparse import ArgumentParser, Namespace\n\n@dataclass\nclass Options(BaseOptions): ...\n\ndef make_argparser(parent: ArgumentParser) -> None:\n    #TODO: any options for the arm backend\n    ...\n\ndef make_options(args: Namespace) -> Options:\n    return Options(\n        tokens=args.tokens,\n        verbose=args.verbose,\n    )\n\ndef riscv_compiler(path: Path, args: list[str], options: Options) -> None:\n    raise NotImplementedError('RISC-V backend is not yet supported')\n\n\nriscv_backend = Backend[Options](\n    name='riscv',\n    exec=riscv_compiler,\n    make_argparser=make_argparser,\n    make_options=make_options\n)\n"},{"name":"src/backend/shell.py","code":"from ..utils import Backend, BaseOptions\n\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom argparse import ArgumentParser, Namespace\n\n@dataclass\nclass Options(BaseOptions): ...\n\ndef make_argparser(parent: ArgumentParser) -> None:\n    #TODO: any options for the arm backend\n    ...\n\ndef make_options(args: Namespace) -> Options:\n    return Options(\n        tokens=args.tokens,\n        verbose=args.verbose,\n    )\n\ndef shell_compiler(path: Path, args: list[str], options: Options) -> None:\n    \"\"\"this would target sh/powershell/etc. all simultaneously\"\"\"\n    # TODO: find the explanation of how this works\n    # https://en.wikipedia.org/wiki/Polyglot_(computing)\n    raise NotImplementedError('Shell backend is not yet supported')\n\n\nshell_backend = Backend[Options](\n    name='shell',\n    exec=shell_compiler,\n    make_argparser=make_argparser,\n    make_options=make_options\n)\n"},{"name":"src/backend/x86_64.py","code":"from ..utils import Backend, BaseOptions\n\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom argparse import ArgumentParser, Namespace\n\n@dataclass\nclass Options(BaseOptions): ...\n\ndef make_argparser(parent: ArgumentParser) -> None:\n    #TODO: any options for the arm backend\n    ...\n\ndef make_options(args: Namespace) -> Options:\n    return Options(\n        tokens=args.tokens,\n        verbose=args.verbose,\n    )\n\ndef x86_64_compiler(path: Path, args: list[str], options: Options) -> None:\n    raise NotImplementedError('x86_64 backend is not yet supported')\n\n\nx86_64_backend = Backend[Options](\n    name='x86_64',\n    exec=x86_64_compiler,\n    make_argparser=make_argparser,\n    make_options=make_options\n)\n"}],"dewy_examples":{"good_examples":[{"name":"hello.dewy","code":"printl'Hello, World!'"},{"name":"hello_func.dewy","code":"main = () => printl'Hello, World!'\nmain"},{"name":"hello_name.dewy","code":"print\"What's your name? \"\nname = readl\nprintl'Hello {name}!'"},{"name":"hello_loop.dewy","code":"print\"What's your name? \"\nname = readl\ni = 0\nloop i <? 10 {\n    printl'Hello {name}!'\n    i = i + 1\n}"},{"name":"anonymous_func.dewy","code":"(() => printl'Hello from an anonymous function!')()"},{"name":"if_else.dewy","code":"print\"What's your name? \"\nname = readl\nif name =? 'Alice' printl'Hello Alice!'\nelse printl'Hello stranger!'"},{"name":"if_else_if.dewy","code":"print\"What's your name? \"\nname = readl\nif name =? 'Alice' printl'Hello Alice!'\nelse if name =? 'Bob' printl'Hello Bob!'\nelse printl'Hello stranger!'"},{"name":"dangling_else.dewy","code":"// if a then if b then s else s2\n// See: https://en.wikipedia.org/wiki/Dangling_else\n\na = false\nb = true\n\nif a\n    if b\n        printl's'\n    else\n        printl's2'\nelse\n    printl's3'"},{"name":"if_tree.dewy","code":"$1d"},{"name":"loop_in_iter.dewy","code":"loop i in 0,2..20\n    printl(i)"},{"name":"loop_and_iters.dewy","code":"loop i in 0.. and j in (2..20)\n    printl'{i} and {j}'"},{"name":"enumerate_list.dewy","code":"// example enumerating a list\nfruits = ['apple' 'banana' 'peach' 'pear' 'pineapple']\nloop i in 0.. and fruit in fruits\n    printl'{i}: {fruit}'"},{"name":"loop_or_iters.dewy","code":"loop i in (0..20] or j in [0,2..10) \n    printl'{i} or {j}'"},{"name":"nested_loop.dewy","code":"loop i in 0,2..10\n    loop j in 0,2..10\n        printl'{i},{j}'"},{"name":"block_printing.dewy","code":"loop i in 0,2..5 {\n    loop j in 0,2..5 {\n        loop k in 0,2..5 {\n            loop l in 0,2..5 {\n                loop m in 0,2..5 {\n                    printl'{i},{j},{k},{l},{m}'\n                }\n            }\n        }\n    }\n}"},{"name":"row_vs_col.dewy","code":"unit = [1 2 3]\nrow = [1,2,3]\ncol = [[1] [2] [3]] // tbd if better way to make this, but generally not necessary since 1d arrays are treated as column vectors\nmat = [1,2,3 4,5,6 7,8,9]\ntensor = [(1,2,3),(4,5,6),(7,8,9) (10,11,12),(13,14,15),(16,17,18) (19,20,21),(22,23,24),(25,26,27)]\n\n// currently not supported\nmat2 = [\n    1 2 3\n    4 5 6\n    7 8 9\n]\n\n\nprintl(unit)\nprintl(row)\nprintl(col)\nprintl(mat)\nprintl(tensor)\nprintl(mat2)"},{"name":"objects.dewy","code":"obj = [\n    let a = 5\n    let b = 10\n    let fn = () => a + b\n    let fn2 = x => (a + b) * x\n]\n\nprintl(obj)\nprintl(obj.a)\nprintl(obj.b)\nprintl(obj.fn)\n//printl(obj.fn2(5)) // parse issue with . and call() currently causing ambiguity\n\n\nsomething_global = 42\n\nobj2 = [\n    let A = [\n        let x = 5\n        let y = 10\n        let fn = () => x + y\n    ]\n    let B = (X Y) => [\n        let x = X\n        let y = Y\n        let fn = () => x + y\n    ]\n    let C = A.fn + (B(5 10)).fn\n]\n\n\nprintl(obj2)\nprintl(obj2.A)\nprintl(obj2.A.x)\nprintl(obj2.A.y)\nprintl(obj2.A.fn)\nprintl(obj2.@B)\n//TODO: fix parse issue causing needing extra parenthesis/@ for disambiguation here\nprintl((obj2.@B)(5 10))\nprintl(((obj2.@B)(3 4)).x)\nprintl(((obj2.@B)(6 7)).y)\nprintl(((obj2.@B)(8 9)).fn)\nprintl(obj2.C)\n\n// can't access members not in the direct object scope\n//printl(obj.something_global)\n//printl(obj2.something_global)"},{"name":"unpack_array.dewy","code":"s = ['Hello' ['World' '!'] 5 10]\nprintl's={s}'\n\na, b, c, d = s\nprintl'a={a} b={b} c={c} d={d}'\n\na, ...b = s\nprintl'a={a} b={b}'\n\n...a, b = s\nprintl'a={a} b={b}'\n\na, [b c], ...d = s\nprintl'a={a} b={b} c={c} d={d}'\n\na, ...b, c, d, e = s\nprintl'a={a} b={b} c={c} d={d} e={e}'\n\n//error tests\n//a, b, c, d, e = s         //error: not enough values to unpack\n//a, b = s                  //error: too many values to unpack\n//a, ...b, c, d, e, f = s   //error: too many values to unpack\n"},{"name":"unpack_dict.dewy","code":"d1 = ['a' -> 1 'b' -> 2 'c' -> 3]\n\na, b, c = d1\nprintl'a={a} b={b} c={c}'\n\na, ...b = d1\nprintl'a={a} b={b}'\n\n...a, b = d1\nprintl'a={a} b={b}'\n\na, [b c], ...d = d1\nprintl'a={a} b={b} c={c} d={d}'\n\n\n\nd2 = ['a' <-> 1 'b' <-> 2 'c' <-> 3 'd' <-> ['e' -> 4 'f' -> 5]]\n\na, b, c, d = d2\nprintl'a={a} b={b} c={c} d={d}'\n\na, ...b, c, d, e = d2\nprintl'a={a} b={b} c={c} d={d} e={e}'\n\n[ka va], [kb vb], [kc vc], [kd [ke vf]] = d2\nprintl'ka={ka} va={va} kb={kb} vb={vb} kc={kc} vc={vc} kd={kd} ke={ke} vf={vf}'"},{"name":"functions.dewy","code":"let fn = (x y) => x + 5 + y\nprintl(fn(8 1))"},{"name":"partial_functions.dewy","code":"let add = (a b) => a + b\nlet add5 = @add(5)\nlet thirteen = @add5(8)\nlet add7 = @add(a=7)\nlet add10 = @add(b=10)\n\nprintl(add(3 5))\nprintl(add5(2))\nprintl(thirteen)\nprintl(add7(3))\nprintl(add10(3))\n\nlet fortytwo = @add10(32)\nlet fortythree = @add10(0 b=43)\nlet fortyfour = @add10(a=34)\nlet fortyfive = @add5(40)\nlet fortysix = @add5(a=6 40)\n\nprintl(fortytwo)\nprintl(fortythree)\nprintl(fortyfour)\nprintl(fortyfive)\nprintl(fortysix)"},{"name":"closure.dewy","code":"$1e"},{"name":"function_signatures.dewy","code":"$1f"},{"name":"opchains.dewy","code":"// in dewy you can combine arithmetic operators into an opchain\nx = 2\ny = 3\nz = 4\nprintl(+x)          // 0+x\nprintl(/x)          // 1/x\nprintl(/-x)         // 1/-x\nprintl(-/x)         // 0-1/x\nprintl(-x^2)        // 0-x^2\nprintl(/x^2)        // 1/x^2\nprintl(/x-2)        // 1/x-2\nprintl(-x/2)        // 0-x/2\nprintl(25-/2)       // 25-1/2\nprintl(25+-/2)      // 25+0-1/2\nprintl(2^/-+*32)    // 2^(1/(0-(0+(1*32))))\n\nprintl(x*+3)\nprintl(y/-4)\nprintl(z^/2)\n\nprintl(10^/-2)\n\nprintl(100^/2)\nprintl(5+-1)"},{"name":"fizzbuzz-1.dewy","code":"// fizbuzz that works with the current version of dewy\nmultiples = [3 5]\nwords = ['Fizz' 'Buzz']\nloop i in [0..100)\n{\n    printed_words = false\n    loop multiple in multiples and word in words\n    {\n        if i % multiple =? 0 \n        { \n            print(word)\n            printed_words = true\n        }\n    }\n    if not printed_words print(i)\n    printl()\n}"},{"name":"fizzbuzz0.dewy","code":"multiples = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nloop i in [0..100)\n{\n    printed_words = false\n    loop [multiple word] in multiples \n    {\n        if i % multiple =? 0 \n        { \n            print(word)\n            printed_words = true\n        }\n    }\n    if not printed_words print(i)\n    printl\n}"},{"name":"random.dewy","code":"// simple xorshift+ generator\n// could be simplified when types are supported (i.e. `state: uint64` `rand = ():> uint64`, etc.)\n// for now have extra `and` operations to simulate truncation \n\nUINT64_MAX = 0xFFFF_FFFF_FFFF_FFFF\nstate = 123456789\nrand = () => {\n    state xor= state >> 21\n    state xor= state << 35\n    state xor= state >> 4\n    state and= UINT64_MAX\n    \n    state * 2_685821_657736_338717 and UINT64_MAX\n}\n\n\nsum = 0\nloop i in 1..1000 {\n    r = rand / UINT64_MAX\n    r |> printl\n    sum += r\n}\n\nprintl'avg = {sum / 1000}'"},{"name":"primes.dewy","code":"/{simple program for generating prime numbers}/\nprintl(2)\nprimes = [2]\nloop candidate in 3,5..100 {\n    no_factors = true\n    loop p in primes and p*p <? candidate+1 {\n        if candidate % p =? 0 {\n            no_factors = false\n            // break //not supported yet..\n        }\n    }\n    if no_factors {\n        printl(candidate)\n        primes = primes + [candidate]  //TODO: this behavior actually will probably be removed in favor of the below. add between arrays will be like vector addition\n        //primes = [primes... candidate]\n        //primes.push(i) // not supported yet.. ambiguous parse since jux has qint precedence, while . equals the higher precedence\n    }\n}"}],"bad_examples":[{"name":"tensors.dewy","code":"\ntensor1 = [(1,2,3),(4,5,6),(7,8,9) (10,11,12),(13,14,15),(16,17,18) (19,20,21),(22,23,24),(25,26,27)]\n\n\ntensor2 = [\n     1  2  3\n     4  5  6\n     7  8  9\n\n    10 11 12\n    13 14 15\n    16 17 18\n\n    19 20 21\n    22 23 24\n    25 26 27\n]\n\ntensor3 = [\n     1  2  3\n     4  5  6\n     7  8  9\n\n    10 11 12\n    13 14 15\n    16 17 18\n\n    19 20 21\n    22 23 24\n    25 26 27\n\n\n    28 29 30\n    31 32 33\n    34 35 36\n\n    37 38 39\n    40 41 42\n    43 44 45\n\n    46 47 48\n    49 50 51\n    52 53 54\n\n\n    55 56 57\n    58 59 60\n    61 62 63\n\n    64 65 66\n    67 68 69\n    70 71 72\n\n    73 74 75\n    76 77 78\n    79 80 81\n]\n\n// multiline expression means non-whitespace sensitive for outer array, but inner array is whitespace sensitive\ntensor4 = [\n    loop i in 0..3 9i .+\n    [\n        1 2 3\n        4 5 6\n        7 8 9\n    ]\n]\n\n\nprintl(tensor1)\nprintl(tensor2)\nprintl(tensor3)\nprintl(tensor4)"},{"name":"arrays.dewy","code":"arr = [0 1 2 3 4 5 6 7 8 9]\nb = 4 x = 10\narr[2] |> printl\narr[b] |> printl\narr[[2 3 4]] |> printl\narr(2..5) |> printl"},{"name":"unpack_object.dewy","code":"o1 = [a='Hello' b=['World' '!'] c=5 d=10]\n\na, b, c, d = o1\nprintl'a={a} b={b} c={c} d={d}'\n\nb, c = o1\nprintl'b={b} c={c}'\n\na, ...rest = o1\nprintl'a={a} rest={rest}'\n\nd, c, ...rest, b = o1\nprintl'd={d} c={c} rest={rest} b={b}'\n\n[a [b1 b2]=b] = o1\nprintl'a={a} b1={b1} b2={b2}'"},{"name":"declare.dewy","code":"//some examples of declarations\nlet x\nlet y = 10\nlet z: int = 100 + 1000\nlet w: SomeType<a=10 b=20> = 15\nlet v = (a:int b:int):> int => a + b + 10\n\nconst a\nconst b = '{1000/10}'\nconst c: int = { 10000 }\n\nlocal_const ฮฑ\nlocal_const ฮฒ = 100(100)\nlocal_const ฮณ: int = 1_000_000\n\nfixed_type A\nfixed_type B = 0x1000\nfixed_type C: int = 0b1000\n"},{"name":"loop_iter_manual.dewy","code":"it = [0,2..10].iter\n[cond i] = it.next\nloop cond {\n    printl(i)\n    [cond i] = it.next\n}"},{"name":"range_iter_test.dewy","code":"r = 0,2..20\nit = r.iter\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next)\nprintl(it.next) //last iteration. should return [true, 20]\nprintl(it.next) //should return [false, undefined]\nprintl(it.next)\nprintl(it.next)"},{"name":"ops.dewy","code":"// regular opchains\n2^/-3 |> printl\n\n//// broadcast ops\n[1 2 3] .* 4 |> printl\n4 ./ [1 2 3] |> printl\n[1 2 3] .^ [4 5 6] |> printl\n[[1 2 3]] .^ [[4] [5] [6]] |> printl\n\n\n// assignment ops\na = 2\na ^= 3\na |> printl\n\n// broadcast opchains\n[1 2 3] .^/-2 |> printl\n2 .^/-[1 2 3] |> printl\n\n// assignment opchains\na = 2\na ^/-= 3\na |> printl\n\n// broadcast assignment\nb = [1 2 3]\nb .^= 4\nb |> printl\nc = 4\nc ./= [1 2 3]\nc |> printl\n\n// broadcast assignment opchains\nd = [1 2 3]\nd .^/-= 2\nd |> printl\n"},{"name":"shebang.dewy","code":"#!//home/david/dev/dewy-lang/dewy\n\n//shows how shebangs can be syntactically valid as a hashtag `#!` followed by a comment\nprintl'this code was invoked as an executable script with a shebang line!'"},{"name":"fizzbuzz1.dewy","code":"taps = [3 -> 'Fizz' 5 -> 'Buzz' /{7 -> 'Bazz' 11 -> 'Bar'}/]\nrange = [0..100)\n\n//indexing at [new ..] and [.. new] adds singleton dimensions wherever there is new\nword_bools = range[new ..] .% taps.keys[.. new] .=? 0\n\n// ` means transpose, which behaves like python's zip()\nwords_grid = [taps.values word_bools]`.map(\n    [word bools] => bools.map(b => if b word else '')\n)\n\nraw_lines = word_grid`.map(line_words => line_words.join(''))\n\nlines = [raw_lines range]`.map(\n    (raw_line i) => if raw_line.length =? 0 '{i}' else raw_line\n)\n\nlines.join'\\n' |> printl\n"},{"name":"primes2.dewy","code":"// generating primes with more advanced features\n#ctx\nprimes = [\n    2\n    loop i in [3, 5..)\n        if i .% #ctx.primes .=? 0 |> @any |> @not\n            i\n][..10)"},{"name":"mdbook_preprocessor.dewy","code":"$20"},{"name":"random.dewy","code":"// simple xorshift+ generator\n// could be simplified when types are supported (i.e. `state: uint64` `rand = ():> uint64`, etc.)\n// for now have extra `and` operations to simulate truncation \n\nUINT64_MAX = 0xFFFF_FFFF_FFFF_FFFF\nstate = 123456789\nrand = () => {\n    state xor= state >> 21\n    state xor= state << 35\n    state xor= state >> 4\n    state and= UINT64_MAX\n    \n    state * 2_685821_657736_338717 and UINT64_MAX\n}\n\n\nsum = 0\nloop i in 1..1000 {\n    r = rand / UINT64_MAX\n    r |> printl\n    sum += r\n}\n\nprintl'avg = {sum / 1000}'"},{"name":"fast_inverse_sqrt.dewy","code":"\n\n// fast inverse square-root. see: https://en.wikipedia.org/wiki/Fast_inverse_square_root#Overview_of_the_code\nfast_isqrt = (x:f32) => {\n    let y:f32, i:u32\n    \n    i = 0.5x transmute u32      // evil floating point bit level hacking\n    i = 0x5f3759df - (i >> 1)   // what the fuck?\n    y = i transmute f32\n    y *= 1.5 - (0.5x)y^2        // 1st iteration of newton's method\n    //y *= 1.5 - (0.5x)y^2      // 2nd iteration (optional)\n\n    return y\n}\n\n\n//TODO: use autodiff to calculate the derivative automatically?\n//isqrt = (x:number) => 1/x^0.5\n//diff(isqrt)"},{"name":"rule110.dewy","code":"// proof that dewy is turing complete\n// rule 110 would grow the vector from the front, so instead we reverse everything for efficiency\n// for now use parenthesis where precedence filter needed. eventually should be able to remove with precedence filter\n\nprogress = world:vector<bit> => {\n    update:bit = 0\n    loop i in 0..world.length\n    {\n        if i >? 0 world[i-1] = update //TODO: #notfirst handled by compiler unrolling the loop into prelude, interludes, and postlude\n        update = 0b01110110 << (world[i-1..i+1] .?? 0 .<< [2 1 0])\n    }\n    world.push(update)\n}\n\nworld: vector<bit> = [1]\nloop true\n{\n    printl(world)\n    progress(world)\n}"},{"name":"dewy_syntax_examples.dewy","code":"$21"},{"name":"syntax.dewy","code":"$22"},{"name":"tokenizer.dewy","code":"//demo of manual dewy tokenizer written in dewy\n\n// (template) instance of the token class\nTokenBase = [\n    name = 'Token'\n    __repr__ = () => '<{name}>'\n]\nToken = type(TokenBase)\n\n\n// class constructor for Keyword token type\nKeyword = src:string => [\n    ...TokenBase\n    name = 'Keyword'\n    __repr__ = () => '<{name}: {src}>'\n]\n\n\neat_fn_type = src:string :> int?\n\n\n/{\n    Eat a reserved keyword, return the number of characters eaten\n\n    #keyword = {in} | {as} | {loop} | {lazy} | {if} | {and} | {or} | {xor} | {nand} | {nor} | {xnor} | {not}; \n\n    noting that keywords are case insensitive\n}/\neat_keyword = src:string :> int? => {\n    keywords = ['in' 'as' 'loop' 'lazy' 'if' 'and' 'or' 'xor' 'nand' 'nor' 'xnor' 'not']\n    max_len = [loop k in keywords k.length].max\n    lower_src = src[..max_len].lowercase\n    loop k in keywords\n        if lower_src.startswith(k)\n            return k.length\n    return undefined\n}\n"}]}}]}]}]}]}]
